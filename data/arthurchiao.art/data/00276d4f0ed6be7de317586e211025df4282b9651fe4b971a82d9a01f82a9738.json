{
  "Source": "arthurchiao.art",
  "Title": "[译] 以图像识别为例，关于卷积神经网络（CNN）的直观解释（2016）",
  "Link": "https://arthurchiao.art/blog/cnn-intuitive-explanation-zh/",
  "Content": "\u003cdiv class=\"post\"\u003e\n  \n  \u003ch1 class=\"postTitle\"\u003e[译] 以图像识别为例，关于卷积神经网络（CNN）的直观解释（2016）\u003c/h1\u003e\n  \u003cp class=\"meta\"\u003ePublished at 2023-06-11 | Last Update 2023-06-11\u003c/p\u003e\n  \n  \u003ch3 id=\"译者序\"\u003e译者序\u003c/h3\u003e\n\n\u003cp\u003e本文翻译自 2016 年的一篇文章：\n\u003ca href=\"https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/\"\u003eAn Intuitive Explanation of Convolutional Neural Networks\u003c/a\u003e。\u003c/p\u003e\n\n\u003cp\u003e作者以图像识别为例，用图文而非数学公式的方式解释了卷积神经网络的工作原理，\n适合初学者和外行扫盲。\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e译者水平有限，不免存在遗漏或错误之处。如有疑问，敬请查阅原文。\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003e以下是译文。\u003c/p\u003e\n\n\u003chr/\u003e\n\n\u003cul id=\"markdown-toc\"\u003e\n  \u003cli\u003e\u003ca href=\"#译者序\" id=\"markdown-toc-译者序\"\u003e译者序\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#1-卷积神经网络cnn\" id=\"markdown-toc-1-卷积神经网络cnn\"\u003e1 卷积神经网络（CNN）\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#11-应用场景\" id=\"markdown-toc-11-应用场景\"\u003e1.1 应用场景\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#12-起源lenet-1990s\" id=\"markdown-toc-12-起源lenet-1990s\"\u003e1.2 起源：LeNet, 1990s\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#13-现代架构\" id=\"markdown-toc-13-现代架构\"\u003e1.3 现代架构\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#2-cnn直观解释\" id=\"markdown-toc-2-cnn直观解释\"\u003e2 CNN：直观解释\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#21-输入图像像素值组成的矩阵\" id=\"markdown-toc-21-输入图像像素值组成的矩阵\"\u003e2.1 输入：图像（像素值组成的矩阵）\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#22-第一步卷积运算\" id=\"markdown-toc-22-第一步卷积运算\"\u003e2.2 第一步：卷积运算\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#222-动图直观解释\" id=\"markdown-toc-222-动图直观解释\"\u003e2.2.2 动图直观解释\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#222-特征图卷积特征参数\" id=\"markdown-toc-222-特征图卷积特征参数\"\u003e2.2.2 特征图/卷积特征参数\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#23-第二步非线性non-linearity--relu运算\" id=\"markdown-toc-23-第二步非线性non-linearity--relu运算\"\u003e2.3 第二步：非线性（Non Linearity / ReLU）运算\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#24-第三步降采样\" id=\"markdown-toc-24-第三步降采样\"\u003e2.4 第三步：降采样\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#241-原理直观解释\" id=\"markdown-toc-241-原理直观解释\"\u003e2.4.1 原理：直观解释\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#242-小结卷积层--relu--降采样层\" id=\"markdown-toc-242-小结卷积层--relu--降采样层\"\u003e2.4.2 小结：卷积层 + ReLU + 降采样层\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#25-第四步全连接层基于特征分类\" id=\"markdown-toc-25-第四步全连接层基于特征分类\"\u003e2.5 第四步：全连接层：基于特征分类\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#26-第五步反向传播形成闭环\" id=\"markdown-toc-26-第五步反向传播形成闭环\"\u003e2.6 第五步：反向传播：形成闭环\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#3-cnn-完整架构和工作流\" id=\"markdown-toc-3-cnn-完整架构和工作流\"\u003e3 CNN 完整架构和工作流\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#4-案例cnn-学习识别字符-8\" id=\"markdown-toc-4-案例cnn-学习识别字符-8\"\u003e4 案例：CNN 学习识别字符 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e8\u003c/code\u003e\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#41-多层特征\" id=\"markdown-toc-41-多层特征\"\u003e4.1 多层特征\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#42-学习字符-8\" id=\"markdown-toc-42-学习字符-8\"\u003e4.2 学习字符 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e8\u003c/code\u003e\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#5其他-cnn-架构\" id=\"markdown-toc-5其他-cnn-架构\"\u003e5 其他 CNN 架构\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#6-总结\" id=\"markdown-toc-6-总结\"\u003e6 总结\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#参考资料\" id=\"markdown-toc-参考资料\"\u003e参考资料\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003chr/\u003e\n\n\u003ch1 id=\"1-卷积神经网络cnn\"\u003e1 卷积神经网络（CNN）\u003c/h1\u003e\n\n\u003ch2 id=\"11-应用场景\"\u003e1.1 应用场景\u003c/h2\u003e\n\n\u003cp\u003e卷积神经网络（ConvNets 或 CNN）是\u003cstrong\u003e\u003cmark\u003e一类\u003c/mark\u003e\u003c/strong\u003e神经网络（a category of\n\u003ca href=\"\u0026#34;https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/\"\u003eNeural Networks\u003c/a\u003e），\n在图像识别和分类等领域已经证明非常有效。CNN 已经成功用于\n人脸识别、物体和交通标志识别，机器人视觉，自动驾驶等等。下面看两个具体例子。\u003c/p\u003e\n\n\u003cp\u003e图 1 是给一个图片，自动识别其中的内容并生成一句描述，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cnn-intuitive-explanation/1.png\" width=\"80%\" height=\"80%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e图 1：图像识别和自动生成描述。\u003ca href=\"http://cs.stanford.edu/people/karpathy/neuraltalk2/demo.html\"\u003ecs.stanford.edu\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003e图 2 则是 CNN 用于识别日常人和物，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cnn-intuitive-explanation/2.png\" width=\"80%\" height=\"80%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e图 2：图像识别。来自 \u003ca href=\"https://arxiv.org/pdf/1506.01497v3.pdf\"\u003epaper pdf\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003e此外，CNN 在一些自然语言处理任务（如句子分类）中也展现出很不错的效果。\n因此，CNN 在机器学习领域将是一个非常重要的工具。不过，新手学习起来经常比较受挫。\n本文试图拿 \u003cstrong\u003e\u003cmark\u003eCNN for image processing\u003c/mark\u003e\u003c/strong\u003e 为例，向大家直观解释一下 CNN 是如何工作的。\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cul\u003e\n    \u003cli\u003e想了解 neural networks 可戳 \u003ca href=\"https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/\"\u003e this short tutorial on Multi Layer Perceptrons\u003c/a\u003e，不了解也没关系。\u003c/li\u003e\n    \u003cli\u003eMulti Layer Perceptrons（多层感知器/多层感知机）在本文中将称为 “Fully Connected Layers”。\u003c/li\u003e\n  \u003c/ul\u003e\n\u003c/blockquote\u003e\n\n\u003ch2 id=\"12-起源lenet-1990s\"\u003e1.2 起源：LeNet, 1990s\u003c/h2\u003e\n\n\u003cp\u003eLeNet 由 Yann LeCun 在 1988 提出，是最早推动深度学习领域发展的卷积神经网络之一。\n后来经过多次改进，直到 \u003ca href=\"http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf\"\u003eLeNet5\u003c/a\u003e [3]。\n当时 LeNet 架构主要用于\u003cstrong\u003e\u003cmark\u003e字符识别\u003c/mark\u003e\u003c/strong\u003e，\n例如读取邮政编码、数字等。\u003c/p\u003e\n\n\u003ch2 id=\"13-现代架构\"\u003e1.3 现代架构\u003c/h2\u003e\n\n\u003cp\u003e近年来提出了几种新的架构，都是\u003cstrong\u003e\u003cmark\u003e对 LeNet 的改进\u003c/mark\u003e\u003c/strong\u003e，它们都继承了 LeNet 的主要概念。\n因此如果对 LeNet 比较了解，理解现代架构将容易很多。\n下图展示了这类架构是\u003cstrong\u003e\u003cmark\u003e如何学习识别图像\u003c/mark\u003e\u003c/strong\u003e的：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cnn-intuitive-explanation/3.png\" width=\"95%\" height=\"95%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e图 3：基于卷积神经网络识别图像。\u003ca href=\"https://www.clarifai.com/technology\"\u003eimage credit \u003c/a\u003e\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e图中的卷积神经网络在架构上与早期 LeNet 差不多，它对输入图像进行分类（LeNet 主要用于字符识别）；\u003c/li\u003e\n  \u003cli\u003e这个例子中会分成四类：dog, cat, boat, bird，所有概率的总和是 100%（后文会解释）；\u003c/li\u003e\n  \u003cli\u003e从图中可以明显看出，在 boat 图像作为输入时，该网络最终的分类结果中，boat 的概率最高（0.94）。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e根据上图，我们可以看到\u003cstrong\u003e\u003cmark\u003e现代 CNN 主要有四种操作\u003c/mark\u003e\u003c/strong\u003e：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e卷积（Convolution）\u003c/li\u003e\n  \u003cli\u003e非线性（Non Linearity，ReLU)）\u003c/li\u003e\n  \u003cli\u003e池化或降采样（Pooling or Sub Sampling）\u003c/li\u003e\n  \u003cli\u003e分类/全连接层（Classification, Fully Connected Layer）\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e这几个功能是所有卷积神经网络的基本模块。下面我们尝试从直观上来理解每个操作的含义。\u003c/p\u003e\n\n\u003ch1 id=\"2-cnn直观解释\"\u003e2 CNN：直观解释\u003c/h1\u003e\n\n\u003ch2 id=\"21-输入图像像素值组成的矩阵\"\u003e2.1 输入：图像（像素值组成的矩阵）\u003c/h2\u003e\n\n\u003cp\u003e每个图像（image）本质上就是一个由\u003cstrong\u003e\u003cmark\u003e像素值\u003c/mark\u003e\u003c/strong\u003e（pixel values）组成的矩阵：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cnn-intuitive-explanation/8-gif.webp\" width=\"35%\" height=\"35%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e图 4：每个图像都是一个像素值组成的矩阵。[6]\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Channel_(digital_image)\"\u003eChannel\u003c/a\u003e（通道）是一个传统术语，指图像的某一部分数据。\n例如，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e数码相机拍出的图像通常有三个通道 —— red/green/blue —— 可以想象成从下往上依次\u003cstrong\u003e\u003cmark\u003e堆叠的 3 个二维矩阵\u003c/mark\u003e\u003c/strong\u003e（每种颜色一个），每个像素值都在 0 到 255 范围内。\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Grayscale\"\u003egrayscale\u003c/a\u003e（灰度）图像只有一个通道。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e简单起见，本文将只考虑\u003cstrong\u003e\u003cmark\u003e灰度图像\u003c/mark\u003e\u003c/strong\u003e。\n这意味着我们将有一个表示图像的二维矩阵，其中中每个像素的值范围从 0 到 255 —— 0 表示黑色，255 表示白色。\u003c/p\u003e\n\n\u003ch2 id=\"22-第一步卷积运算\"\u003e2.2 第一步：卷积运算\u003c/h2\u003e\n\n\u003cp\u003eCNN（卷积神经网络）的名字来源于\u003ca href=\"http://en.wikipedia.org/wiki/Convolution\"\u003e“卷积”\u003c/a\u003e运算，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e卷积的\u003cstrong\u003e\u003cmark\u003e主要目的\u003c/mark\u003e\u003c/strong\u003e是\u003cstrong\u003e\u003cmark\u003e从输入图像中提取特征\u003c/mark\u003e\u003c/strong\u003e；\u003c/li\u003e\n  \u003cli\u003e通过一个\u003cstrong\u003e\u003cmark\u003e小矩阵\u003c/mark\u003e\u003c/strong\u003e（称为 filter）对输入矩阵进行运算，来学习图像特征（image features）；\u003c/li\u003e\n  \u003cli\u003efilter 保留了像素之间的\u003cstrong\u003e\u003cmark\u003e空间关系\u003c/mark\u003e\u003c/strong\u003e（spatial relationship between pixels）。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e这里不深入探讨卷积的数学细节，只尝试了解它如何处理图像。\u003c/p\u003e\n\n\u003ch3 id=\"222-动图直观解释\"\u003e2.2.2 动图直观解释\u003c/h3\u003e\n\n\u003cp\u003e每个图像都是像素值组成的矩阵。考虑一个 5x5 的图像，其像素值只有 0 和 1（灰度图像的像素值范围为 0 到 255，这里我们进一步简化，像素值只有 0 和 1）：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cnn-intuitive-explanation/binary-image.png\" width=\"25%\" height=\"25%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e然后引入一个 3x3 矩阵作为 filter，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cnn-intuitive-explanation/3x3-matrix.png\" width=\"15%\" height=\"15%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e那么，用 5x5 和 3x3 矩阵来计算卷积的过程就如下面的动图所示，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cnn-intuitive-explanation/convolution_schematic.webp\" width=\"40%\" height=\"40%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e图 5：卷积的计算。计算得到的结果矩阵称为 Convolved Feature or Feature Map（卷积特征，或特征图）。[7]\u003c/p\u003e\n\n\u003cp\u003e计算过程：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e在 5x5 的输入矩阵上，覆盖一个 3x3 矩阵，\u003c/li\u003e\n  \u003cli\u003e对当前 3x3 覆盖的部分，分别与 3x3 矩阵按像素相乘，然后把 9 个值加起来得到一个整数，\u003c/li\u003e\n  \u003cli\u003e按这种方式，以一个像素为单位依次滑动和计算，最后就得到一个输出矩阵（右边）。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e用 CNN 术语来说，\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e3x3 矩阵叫做 \u003cstrong\u003e\u003cmark\u003efilter（滤波器），或 kernel、feature detector 等\u003c/mark\u003e\u003c/strong\u003e；\u003c/li\u003e\n  \u003cli\u003e滑动的粒度（这里是一个像素）叫做 \u003cstrong\u003e\u003cmark\u003estride（步长）\u003c/mark\u003e\u003c/strong\u003e；\u003c/li\u003e\n  \u003cli\u003e最后得到的矩阵叫 Convolved Feature（\u003cstrong\u003e\u003cmark\u003e卷积特征\u003c/mark\u003e\u003c/strong\u003e）或 feature map（\u003cstrong\u003e\u003cmark\u003e特征图\u003c/mark\u003e\u003c/strong\u003e）；\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e显然，对于同一个图像，\u003cstrong\u003e\u003cmark\u003e用的卷积矩阵（filter matrix）不同，得到的特征图（feature maps）也就不同\u003c/mark\u003e\u003c/strong\u003e。\n或者说\u003cstrong\u003e\u003cmark\u003e不同的 filter 可以检测不同的特征\u003c/mark\u003e\u003c/strong\u003e。例如对于下面这张图片，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cnn-intuitive-explanation/animal.png\" width=\"20%\" height=\"20%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e我们可以根据目的（边缘检测、锐化、模糊化等）的不同，选用不同的 filter，得到的效果如下 [8]，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cnn-intuitive-explanation/animal-filter-results.png\" width=\"50%\" height=\"50%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e下面是另一个例子，用动图展示两个不同的 filter 计算卷积得到的 feature map，\u003c/p\u003e\n\n\u003c!-- this gif is too big, i won't like to include it into /assets/image/... as a static file, so reference it from vanilla URL --\u003e\n\u003cp align=\"center\"\u003e\u003cimg src=\"https://ujwlkarn.files.wordpress.com/2016/08/giphy.gif?w=480\u0026amp;zoom=2\" width=\"60%\" height=\"60%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e图 6：卷积运算。Source [\u003ca href=\"http://cs.nyu.edu/~fergus/tutorials/deep_learning_cvpr12/\"\u003e9\u003c/a\u003e]\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e这里需要再提醒一下，image 和 filter 都只是一个数值矩阵（numeric matrix）。\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch3 id=\"222-特征图卷积特征参数\"\u003e2.2.2 特征图/卷积特征参数\u003c/h3\u003e\n\n\u003cp\u003e实际上，CNN 在训练过程中自行\u003cstrong\u003e\u003cmark\u003e学习\u003c/mark\u003e\u003c/strong\u003e（learn）这些 filter 的值\n（尽管我们仍然需要在训练时指定某些参数，例如过 filter 数量、filter size、网络架构等）。\nfilter 数量越多，提取的图像特征就越多，训练出来的网络在识别未见过的图像时效果就越好。\u003c/p\u003e\n\n\u003cp\u003eFeature Map（Convolved Feature）的大小由三个参数 [4] 控制，在训练之前确定：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\n    \u003cp\u003e\u003cstrong\u003e\u003cmark\u003e深度\u003c/mark\u003e\u003c/strong\u003e（depth）：将要用来做卷积运算的 \u003cstrong\u003e\u003cmark\u003efilter 数量\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n    \u003cp\u003e在下图所示的网络中，我们使用三个不同的 filter 对原始图像执行卷积，从而生成三个不同的特征图。\n 可以将这三个特征图视为三个自下向上\u003cstrong\u003e\u003cmark\u003e堆叠（stacking）在一起\u003c/mark\u003e\u003c/strong\u003e的二维矩阵，\n 这也是为什么成它为特征图的\u003cstrong\u003e\u003cmark\u003e“深度”\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n    \u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cnn-intuitive-explanation/convolution-operation.png\" width=\"60%\" height=\"60%\"/\u003e\u003c/p\u003e\n    \u003cp align=\"center\"\u003e图 7：特征图的深度\u003c/p\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003e\u003cstrong\u003e\u003cmark\u003e步长\u003c/mark\u003e\u003c/strong\u003e（stride）：在输入矩阵上滑动滤波器（filter）矩阵的\u003cstrong\u003e\u003cmark\u003e像素数\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n    \u003cul\u003e\n      \u003cli\u003e步长为 1 就是一次将 filter 移动一个像素，步长为 2 就是一次滑动 2 个像素。\u003c/li\u003e\n      \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e步长越大，得到的特征图越小\u003c/mark\u003e\u003c/strong\u003e。\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003e\u003cstrong\u003e\u003cmark\u003e零值填充\u003c/mark\u003e\u003c/strong\u003e（Zero-padding）：有时需要在图像边界用零填充，这样就可以对输入图像的边界像素应用 filter。\u003c/p\u003e\n\n    \u003cul\u003e\n      \u003cli\u003ezero-padding 会使生成的特征图稍大一些；\u003c/li\u003e\n      \u003cli\u003e使用了 zero-padding 称为 \u003cstrong\u003e\u003cmark\u003ewide convolution\u003c/mark\u003e\u003c/strong\u003e，不使用的称为 \u003cstrong\u003e\u003cmark\u003enarrow convolution\u003c/mark\u003e\u003c/strong\u003e [14]。\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch2 id=\"23-第二步非线性non-linearity--relu运算\"\u003e2.3 第二步：非线性（Non Linearity / ReLU）运算\u003c/h2\u003e\n\n\u003cp\u003e每次卷积运算之后，还会跟着一个称为 ReLU（\u003cstrong\u003e\u003cmark\u003eRectified Linear Unit\u003c/mark\u003e\u003c/strong\u003e，修正线性单元）的运算。\n这是一个\u003cstrong\u003e\u003cmark\u003e非线性运算\u003c/mark\u003e\u003c/strong\u003e，输入输出公式为，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cnn-intuitive-explanation/ReLU.png\" width=\"70%\" height=\"70%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e图 8：ReLU 公式与效果（负值置为零）\u003c/p\u003e\n\n\u003cp\u003eReLU 是一个\u003cstrong\u003e\u003cmark\u003e像素级别的操作\u003c/mark\u003e\u003c/strong\u003e，\u003cstrong\u003e\u003cmark\u003e将特征图中所有负值置为零\u003c/mark\u003e\u003c/strong\u003e。\nReLU 的目的是在 CNN 中引入非线性，因为真实世界中的大部分数据都是非线性的，\n我们希望 CNN 能与现实更加匹配。\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e卷积是一种线性运算 —— 逐像素做矩阵乘法和加法，因此我们通过引入像 ReLU 这样的非线性函数来解释（account for）非线性。\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e下图可以清楚地看到 ReLU 操作之后的效果，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cnn-intuitive-explanation/ReLU-result.png\" width=\"85%\" height=\"85%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e图 9：真实特征图执行 ReLU 操作之后的效果。Source [\u003ca href=\"http://mlss.tuebingen.mpg.de/2015/slides/fergus/Fergus_1.pdf\"\u003e10\u003c/a\u003e]\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e左边的特征图是卷积运算之后得到的；\u003c/li\u003e\n  \u003cli\u003e右边是接着做了 ReLU 运算（负数全部置零）之后的效果。也称为\u003cstrong\u003e\u003cmark\u003e“修正之后的”特征图\u003c/mark\u003e\u003c/strong\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e除了 ReLU，还可以使用 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003etanh\u003c/code\u003e 或 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003esigmoid\u003c/code\u003e 等\u003cstrong\u003e\u003cmark\u003e非线性函数\u003c/mark\u003e\u003c/strong\u003e，不过在大部分场景下，ReLU 的性能都更好。\u003c/p\u003e\n\n\u003ch2 id=\"24-第三步降采样\"\u003e2.4 第三步：降采样\u003c/h2\u003e\n\n\u003cp\u003eSpatial Pooling，又称为\u003cstrong\u003e\u003cmark\u003e降采样、下采样\u003c/mark\u003e\u003c/strong\u003e（subsampling、downsampling），\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e目的是在降低 feature map 维度的同时，保留尽量多的重要信息。\u003c/li\u003e\n  \u003cli\u003e有不同的类型：\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eMax/Average/Sum\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 等。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 id=\"241-原理直观解释\"\u003e2.4.1 原理：直观解释\u003c/h3\u003e\n\n\u003cp\u003e图 10 是对一个卷积 + ReLU 之后得到的特征图执行 \u003cstrong\u003e\u003cmark\u003e2x2 降采样\u003c/mark\u003e\u003c/strong\u003e，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cnn-intuitive-explanation/down-sampling.png\" width=\"40%\" height=\"40%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e图 10: Max Pooling. Source [\u003ca href=\"http://cs231n.github.io/convolutional-networks/\"\u003e4\u003c/a\u003e]\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e首先定义一个 spatial neighborhood (2x2 窗口)，\u003c/li\u003e\n  \u003cli\u003e然后执行降采样函数，这里用的是 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eMax\u003c/code\u003e 函数，也就是取其中最大的那个值作为结果，\u003c/li\u003e\n  \u003cli\u003e注意这里滑动窗口的步长是 2，也就是降采样矩阵的尺寸；\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e最终\u003cstrong\u003e\u003cmark\u003e把 4x4 特征图降维到了 2x2\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e下图是对三个卷积结果（rectified feature map）依次执行降采样，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cnn-intuitive-explanation/down-sampling-2.png\" width=\"50%\" height=\"50%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e图 11：对三个维度的特征图执行降采样。\u003c/p\u003e\n\n\u003cp\u003eAverage Pooling 和 sum 类似，只不过取的是平均值和累加和。\n实际中，Max Pooling 效果更好一些。下面是一个效果对比：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cnn-intuitive-explanation/down-sampling-3.png\" width=\"80%\" height=\"80%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e图 12：Max/Sum 降采样函数效果对比。Source [\u003ca href=\"http://mlss.tuebingen.mpg.de/2015/slides/fergus/Fergus_1.pdf\"\u003e10\u003c/a\u003e]\u003c/p\u003e\n\n\u003cp\u003ePooling 的作用是主动降低输入表示（input representation）的空间大小（spatial size） [4]，\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e使输入表示\u003cstrong\u003e\u003cmark\u003e（特征维度）更小\u003c/mark\u003e\u003c/strong\u003e且更易于管理；\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e减少网络参数和计算量\u003c/mark\u003e\u003c/strong\u003e，避免过度拟合（overfitting）[4]；\u003c/li\u003e\n  \u003cli\u003e使网络能\u003cstrong\u003e\u003cmark\u003e容忍输入图像中较小的变换、扭曲和平移\u003c/mark\u003e\u003c/strong\u003e：小扭曲不会改变降采样的输出 —— 因为我们采用相邻区域内的最大值/平均值；\u003c/li\u003e\n  \u003cli\u003e使我们能获得一个几乎\u003cstrong\u003e\u003cmark\u003e跟输入分辨率无关的特征表示\u003c/mark\u003e\u003c/strong\u003e（确切的术语是 “equivariant representation”）：\n  这一点非常重要，因有了这种能力，那无论一个目标是在图像中什么位置，我们都能把它检测出来 [18,19]。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch3 id=\"242-小结卷积层--relu--降采样层\"\u003e2.4.2 小结：卷积层 + ReLU + 降采样层\u003c/h3\u003e\n\n\u003cp\u003e至此，我们已经了解了卷积、ReLU 和降采样的工作原理，它们是构成任何 CNN 的基本构建块。\n如图 13 所示，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cnn-intuitive-explanation/basic-block.png\" width=\"80%\" height=\"80%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e图 13：两个基本处理单元构成的 CNN\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e卷积层 + ReLU + 降采样层\u003c/mark\u003e\u003c/strong\u003e构成一个\u003cstrong\u003e\u003cmark\u003e基本处理单元\u003c/mark\u003e\u003c/strong\u003e；\u003c/li\u003e\n  \u003cli\u003e图中级联了两个这样的处理单元。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e这些层联合在一起，从图像中提取有用特征、引入非线性、减少特征维度，同时在使特征在某种程度上做到\u003cstrong\u003e\u003cmark\u003e缩放和平移不变\u003c/mark\u003e\u003c/strong\u003e [18]。\n降采样之后的输出就可以对图像分类了，具体的分类过程由称为\u003cstrong\u003e\u003cmark\u003e全连接层\u003c/mark\u003e\u003c/strong\u003e的模块来完成。\u003c/p\u003e\n\n\u003ch2 id=\"25-第四步全连接层基于特征分类\"\u003e2.5 第四步：全连接层：基于特征分类\u003c/h2\u003e\n\n\u003cp\u003e全连接层（Fully Connected Layer）是一种传统的\u003cstrong\u003e\u003cmark\u003e多层感知器\u003c/mark\u003e\u003c/strong\u003e（Multi Layer Perceptron），\n在输出层使用 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003esoftmax()\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 函数（也可以使用 SVM 等其他分类器，但本文中使用 softmax）。 \n术语“全连接”表示\u003cstrong\u003e\u003cmark\u003e上一层中的每个神经元都连接到下一层中的每个神经元\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e想了解多层感知器，可参考\u003ca href=\"https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/\"\u003e这篇\u003c/a\u003e文章。\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e卷积层和降采样层的输出表示了输入图像的 high-level 特征。\n\u003cstrong\u003e\u003cmark\u003e全连接层的目的\u003c/mark\u003e\u003c/strong\u003e是基于这些特征，\u003cstrong\u003e\u003cmark\u003e对输入图像进行分类\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e例如，假设我们的图像分类有四种可能，如下图所示（图中省略了全连接层中节点之间的连接），\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cnn-intuitive-explanation/full-connected-layer.png\" width=\"80%\" height=\"80%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e图 14：全连接层\u003c/p\u003e\n\n\u003cp\u003e除了分类的作用，加一个全连接层（通常）还是一种\u003cstrong\u003e\u003cmark\u003e低成本的学习这些特性的非线性组合\u003c/mark\u003e\u003c/strong\u003e的方式\n（learning non-linear combinations of these features）。\n卷积层和降采样层的输出对于分类来说可能已经很好了，但组合二者能达到更好的性能 [11]。\u003c/p\u003e\n\n\u003cp\u003e全连接层 output 的所有可能性加起来是 100%，这是由 softmax activation function 保证的：\nsoftmax 接收任何实值向量输入，将其中的元素归一化到 [0,1] 之间，并且总和等于 1。\u003c/p\u003e\n\n\u003ch2 id=\"26-第五步反向传播形成闭环\"\u003e2.6 第五步：反向传播：形成闭环\u003c/h2\u003e\n\n\u003cp\u003e分类结果（各种可能性的概率）出来之后，再\n使用反向传播（backpropagation）算法计算所有概率的误差梯度，\n并使用\u003cstrong\u003e\u003cmark\u003e梯度下降算法\u003c/mark\u003e\u003c/strong\u003e更新所有滤波器值/权重和参数值，以最小化输出误差。\u003c/p\u003e\n\n\u003cp\u003e然后就又可以从头开始了，有了这种反馈，下一次的迭代会更加准确。\u003c/p\u003e\n\n\u003ch1 id=\"3-cnn-完整架构和工作流\"\u003e3 CNN 完整架构和工作流\u003c/h1\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cmark\u003e卷积层 + 降采样层\u003c/mark\u003e\u003c/strong\u003e充当输入图像的\u003cstrong\u003e\u003cmark\u003e特征提取器\u003c/mark\u003e\u003c/strong\u003e（Feature Extractors），\n而\u003cstrong\u003e\u003cmark\u003e全连接层\u003c/mark\u003e\u003c/strong\u003e充当\u003cstrong\u003e\u003cmark\u003e分类器\u003c/mark\u003e\u003c/strong\u003e（classifier）。\u003c/p\u003e\n\n\u003cp\u003e下图中，由于输入图像是 boat，因此 Boat 类的目标概率为 1，其他三个类的目标概率为 0，即\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eInput Image = Boat\u003c/li\u003e\n  \u003cli\u003eTarget Vector = [0, 0, 1, 0]\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cnn-intuitive-explanation/train-cnn.png\" width=\"80%\" height=\"80%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e图 15: Training the CNN\u003c/p\u003e\n\n\u003cp\u003e卷积网络的训练过程大致如下：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e用\u003cstrong\u003e\u003cmark\u003e随机值\u003c/mark\u003e\u003c/strong\u003e初始化所有 filter 和参数/权重。\u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003e将训练图像作为输入，经过 \u003cstrong\u003e\u003cmark\u003eforward propagation\u003c/mark\u003e\u003c/strong\u003e 步骤（卷积、\nReLU 和降采样以及全连接层中的前向传播），\u003cstrong\u003e\u003cmark\u003e计算出每个类别的输出概率\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n    \u003cul\u003e\n      \u003cli\u003e假设上面的 boat 图像的输出概率是 [0.2, 0.4, 0.1, 0.3]\u003c/li\u003e\n      \u003cli\u003e由于第一个训练示例的权重是随机分配的，因此输出概率也是随机的。\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003e计算输出层的总误差（所有 4 个类的总和）\u003c/p\u003e\n\n    \u003cul\u003e\n      \u003cli\u003e\u003cstrong\u003eTotal Error = ∑  ½ (target probability – output probability) ²\u003c/strong\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003e使用反向传播计算网络中所有权重的误差梯度，并使用\u003cstrong\u003e\u003cmark\u003e梯度下降算法\u003c/mark\u003e\u003c/strong\u003e\n  更新所有滤波器值/权重和参数值，以最小化输出误差。\u003c/p\u003e\n\n    \u003cul\u003e\n      \u003cli\u003e权重根据它们对总误差的贡献按比例调整。\u003c/li\u003e\n      \u003cli\u003e当再次输入相同的图像时，输出概率可能就是 [0.1, 0.1, 0.7, 0.1]，更接近目标向量 [0, 0, 1, 0]，\n这意味着网络已经学会通过调整其权重/过滤器来正确分类该特定图像，从而减少输出误差。\u003c/li\u003e\n      \u003cli\u003e过滤器数量、过滤器大小、网络架构等参数在步骤 1 之前都已固定，并且在训练过程中不会更改——只有过滤器矩阵和连接权重的值会更新。\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e对训练集中的所有图像重复步骤 2-4。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e以上就是训练 CNN 的步骤，结束之后 CNN 的所有权重和参数都已经过优化，能够正确地对训练集中的图像进行分类。\u003c/p\u003e\n\n\u003cp\u003e当一个新的（没见过的）图像被输入到 CNN 中时，网络将通过 forward propagation\n计算输出每个类别的概率。 如果训练集足够大，网络将大概率将新图像分类到正确的类别中。\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e以上步骤已经过简化，以避免过多数学细节，让读者对训练过程有个直觉上的理解。\n想了解进一步有关细节，可参考 [4,12]。\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e上面的例子中使用了两组卷积层和降采样层。实际上可以在单个 CNN 中重复任意层。\n另外，每个卷积层之后不是必须要跟着一个降采样层。如图 16 所示，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cnn-intuitive-explanation/car.png\" width=\"80%\" height=\"80%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e图 16：多层 CNN 及每层的效果。Source [\u003ca href=\"http://cs231n.github.io/convolutional-networks/\"\u003e4\u003c/a\u003e]\u003c/p\u003e\n\n\u003ch1 id=\"4-案例cnn-学习识别字符-8\"\u003e4 案例：CNN 学习识别字符 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e8\u003c/code\u003e\u003c/h1\u003e\n\n\u003ch2 id=\"41-多层特征\"\u003e4.1 多层特征\u003c/h2\u003e\n\n\u003cp\u003e一般来说，卷积步骤越多，网络能学到的特征就可以越复杂。 例如，在图像分类中，CNN\n可能会\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e在第一层学习从原始像素\u003cstrong\u003e\u003cmark\u003e检测物体边缘\u003c/mark\u003e\u003c/strong\u003e，然后\u003c/li\u003e\n  \u003cli\u003e基于边缘检测判断第二层中的\u003cstrong\u003e\u003cmark\u003e简单形状\u003c/mark\u003e\u003c/strong\u003e，然后\u003c/li\u003e\n  \u003cli\u003e使用这些形状来\u003cstrong\u003e\u003cmark\u003e检测更高级别的特征\u003c/mark\u003e\u003c/strong\u003e，例如\u003cstrong\u003e\u003cmark\u003e面部形状\u003c/mark\u003e\u003c/strong\u003e [14]。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e下图是用\n\u003ca href=\"http://web.eecs.umich.edu/~honglak/icml09-ConvolutionalDeepBeliefNetworks.pdf\"\u003eConvolutional Deep Belief Network\u003c/a\u003e\n学习到的特征，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cnn-intuitive-explanation/deep-belief.png\" width=\"50%\" height=\"50%\"/\u003e\u003c/p\u003e\n\u003cp class=\"graf--p graf-after--p\" style=\"text-align:center;\"\u003e图 17：Convolutional Deep Belief Network 学习特征：\n第一层检测边缘；第二次检测形状；第三张检测人脸。\nSource [\u003ca href=\"http://web.eecs.umich.edu/~honglak/icml09-ConvolutionalDeepBeliefNetworks.pdf\"\u003e21\u003c/a\u003e]\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e这里只是一个简单的例子，现实中的卷积滤波器可能会检测到对人类无意义的物体。\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch2 id=\"42-学习字符-8\"\u003e4.2 学习字符 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e8\u003c/code\u003e\u003c/h2\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eAdam Harley 在 MNIST 手写数字数据库 [13] 上训练卷积神经网络，创造了惊人的可视化效果。\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e下面看一个 CNN 网络如何处理输入字符 “8”。输入图像包含 1024 像素（32x32），\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cnn-intuitive-explanation/conv-all.png\" width=\"80%\" height=\"80%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e图 18: Visualizing a CNN trained on handwritten digits. \n注意 ReLU 没画出来。\nSource [\u003ca href=\"http://scs.ryerson.ca/~aharley/vis/conv/flat.html\"\u003e13\u003c/a\u003e]\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e第一层卷积：由 6 个独立的 5×5（步长 1）滤波器与输入图像卷积，产生\u003cstrong\u003e\u003cmark\u003e深度为 6 \u003c/mark\u003e\u003c/strong\u003e的特征图。\u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003e第一层降采样：分别对 1 中的六个特征图进行 \u003cstrong\u003e\u003cmark\u003e2×2 Max 降采样\u003c/mark\u003e\u003c/strong\u003e（步长为 2）。\n  注意由于用的是 Max，因此 2x2 网格中的最大值（\u003cstrong\u003e\u003cmark\u003e最亮的像素\u003c/mark\u003e\u003c/strong\u003e）的像素进入了降采样结果层。\u003c/p\u003e\n\n    \u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cnn-intuitive-explanation/8-downsample.png\" width=\"60%\" height=\"60%\"/\u003e\u003c/p\u003e\n    \u003cp align=\"center\"\u003e图 19：卷积 + 降采样。Source [\u003ca href=\"http://scs.ryerson.ca/~aharley/vis/conv/flat.html\"\u003e13\u003c/a\u003e]\u003c/p\u003e\n  \u003c/li\u003e\n  \u003cli\u003e第二层卷积：对 2 的结果进行 \u003cstrong\u003e\u003cmark\u003e5x5 卷积\u003c/mark\u003e\u003c/strong\u003e（步长 1）；\u003c/li\u003e\n  \u003cli\u003e第二层降采样：对 3 的结果进行\u003cstrong\u003e\u003cmark\u003e降采样\u003c/mark\u003e\u003c/strong\u003e；\u003c/li\u003e\n  \u003cli\u003e第一层全连接：120 neurons；\u003c/li\u003e\n  \u003cli\u003e第二层全连接：100 neurons；\u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003e第三层全连接：10 neurons，分别代表 \u003cstrong\u003e\u003cmark\u003e0-9 这 10 个字符\u003c/mark\u003e\u003c/strong\u003e作为输出 —— 也称为 Output layer\u003c/p\u003e\n\n    \u003cp\u003e注意到 output 层中 node \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e8\u003c/code\u003e \u003cstrong\u003e\u003cmark\u003e最亮\u003c/mark\u003e\u003c/strong\u003e，这表示分类结果中 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e8\u003c/code\u003e 的概率最大。\u003c/p\u003e\n\n    \u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cnn-intuitive-explanation/8-output.png\" width=\"100%\" height=\"100%\"/\u003e\u003c/p\u003e\n    \u003cp align=\"center\"\u003e图 20: Visualizing the Filly Connected Layers. Source [\u003ca href=\"http://scs.ryerson.ca/~aharley/vis/conv/flat.html\"\u003e13\u003c/a\u003e]\u003c/p\u003e\n  \u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch1 id=\"5其他-cnn-架构\"\u003e5 其他 CNN 架构\u003c/h1\u003e\n\n\u003cp\u003e上世纪 90 年代初期就出现卷积神经网络了，本文介绍的 LeNet 只是其中一种，\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003eLeNet (1990s)\u003c/mark\u003e\u003c/strong\u003e: 本文介绍的就是这个；\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e1990s to 2012:\u003c/strong\u003e In the years from late 1990s to early 2010s\nconvolutional neural network were in incubation. As more and more data and\ncomputing power became available, tasks that convolutional neural networks\ncould tackle became more and more interesting.\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eAlexNet (2012) – \u003c/strong\u003eIn 2012, Alex Krizhevsky (and\nothers) released \u003ca href=\"https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf\"\u003eAlexNet\u003c/a\u003e\nwhich was a deeper and much wider version of the LeNet and won by a large\nmargin the difficult ImageNet Large Scale Visual Recognition Challenge\n(ILSVRC) in 2012. It was a significant breakthrough with respect to the\nprevious approaches and the current widespread application of CNNs can be\nattributed to this work.\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eZF Net (2013) –\u003c/strong\u003e The ILSVRC 2013 winner was a\nConvolutional Network from Matthew Zeiler and Rob Fergus. It became known as\nthe \u003ca href=\"http://arxiv.org/abs/1311.2901\"\u003eZFNet\u003c/a\u003e (short for Zeiler \u0026amp;\nFergus Net). It was an improvement on AlexNet by tweaking the architecture\nhyperparameters.\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eGoogLeNet (2014) – \u003c/strong\u003eThe ILSVRC 2014 winner was a\nConvolutional Network from \u003ca href=\"http://arxiv.org/abs/1409.4842\"\u003eSzegedy\net al.\u003c/a\u003e from Google. Its main contribution was the development of an\n\u003cem\u003eInception Module\u003c/em\u003e that dramatically reduced the number of parameters in the\nnetwork (4M, compared to AlexNet with 60M).\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eVGGNet (2014) –\u003c/strong\u003e The runner-up in ILSVRC 2014 was the\nnetwork that became known as the \u003ca href=\"http://www.robots.ox.ac.uk/~vgg/research/very_deep/\"\u003eVGGNet\u003c/a\u003e. Its\nmain contribution was in showing that the depth of the network (number of\nlayers) is a critical component for good performance.\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003eResNets (2015)\u003c/mark\u003e\u003c/strong\u003e \u003ca href=\"http://arxiv.org/abs/1512.03385\"\u003eResidual Network\u003c/a\u003e developed by\nKaiming He (and others), winner of ILSVRC 2015. 目前最好的 CNN 模型（as of May 2016）。\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eDenseNet (August 2016) – \u003c/strong\u003eRecently published by Gao\nHuang (and others), the \u003ca href=\"http://arxiv.org/abs/1608.06993\"\u003eDensely Connected Convolutional Network\u003c/a\u003e\n has each layer directly connected to every other layer in a\nfeed-forward fashion. The DenseNet has been shown to obtain significant\nimprovements over previous state-of-the-art architectures on five highly\ncompetitive object recognition benchmark tasks. Check out the Torch\nimplementation \u003ca href=\"https://github.com/liuzhuang13/DenseNet\"\u003ehere\u003c/a\u003e.\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch1 id=\"6-总结\"\u003e6 总结\u003c/h1\u003e\n\n\u003cp\u003e本文试图用尽量简单的术语来解释卷积神经网络（CNN）背后的主要概念，\n希望能让读者对其工作原理有一些直观理解。\u003c/p\u003e\n\n\u003ch1 id=\"参考资料\"\u003e参考资料\u003c/h1\u003e\n\n\u003col\u003e\n  \u003cli\u003e\u003ca href=\"https://github.com/karpathy/neuraltalk2\"\u003ekarpathy/neuraltalk2\u003c/a\u003e: Efficient Image Captioning code in Torch, \u003ca href=\"http://cs.stanford.edu/people/karpathy/neuraltalk2/demo.html\"\u003eExamples\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003eShaoqing Ren, \u003cem\u003eet al, \u003c/em\u003e“Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks”, 2015, \u003ca href=\"http://arxiv.org/pdf/1506.01497v3.pdf\"\u003earXiv:1506.01497 \u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://medium.com/towards-data-science/neural-network-architectures-156e5bad51ba\"\u003eNeural Network Architectures\u003c/a\u003e, Eugenio Culurciello’s blog\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"http://cs231n.github.io/convolutional-networks/\"\u003eCS231n Convolutional Neural Networks for Visual Recognition, Stanford\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://www.clarifai.com/technology\"\u003eClarifai / Technology\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://medium.com/@ageitgey/machine-learning-is-fun-part-3-deep-learning-and-convolutional-neural-networks-f40359318721#.2gfx5zcw3\"\u003eMachine Learning is Fun! Part 3: Deep Learning and Convolutional Neural Networks\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"http://deeplearning.stanford.edu/wiki/index.php/Feature_extraction_using_convolution\"\u003eFeature extraction using convolution, Stanford\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Kernel_(image_processing)\"\u003eWikipedia article on Kernel (image processing) \u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"http://cs.nyu.edu/~fergus/tutorials/deep_learning_cvpr12\"\u003eDeep Learning Methods for Vision, CVPR 2012 Tutorial \u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"http://mlss.tuebingen.mpg.de/2015/slides/fergus/Fergus_1.pdf\"\u003eNeural Networks by Rob Fergus, Machine Learning Summer School 2015\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"http://stats.stackexchange.com/a/182122/53914\"\u003eWhat do the fully connected layers do in CNNs? \u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"http://andrew.gibiansky.com/blog/machine-learning/convolutional-neural-networks/\"\u003eConvolutional Neural Networks, Andrew Gibiansky \u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003eA. W. Harley, “An Interactive Node-Link Visualization of Convolutional Neural Networks,” in ISVC, pages 867-877, 2015 (\u003ca href=\"http://scs.ryerson.ca/~aharley/vis/harley_vis_isvc15.pdf\"\u003elink\u003c/a\u003e). \u003ca href=\"http://scs.ryerson.ca/~aharley/vis/conv/flat.html\"\u003eDemo\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/\"\u003eUnderstanding Convolutional Neural Networks for NLP\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"http://andrew.gibiansky.com/blog/machine-learning/convolutional-neural-networks/\"\u003eBackpropagation in Convolutional Neural Networks\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner\u0026#39;s-Guide-To-Understanding-Convolutional-Neural-Networks-Part-2/\"\u003eA Beginner’s Guide To Understanding Convolutional Neural Networks\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003eVincent Dumoulin, \u003cem\u003eet al\u003c/em\u003e, “A guide to convolution arithmetic for deep learning”, 2015, \u003ca href=\"http://arxiv.org/pdf/1603.07285v1.pdf\"\u003earXiv:1603.07285\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://github.com/rasbt/python-machine-learning-book/blob/master/faq/difference-deep-and-normal-learning.md\"\u003eWhat is the difference between deep learning and usual machine learning?\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://www.quora.com/How-is-a-convolutional-neural-network-able-to-learn-invariant-features\"\u003eHow is a convolutional neural network able to learn invariant features?\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"http://journal.frontiersin.org/article/10.3389/frobt.2015.00036/full\"\u003eA Taxonomy of Deep Convolutional Neural Nets for Computer Vision\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003eHonglak Lee, \u003cem\u003eet al\u003c/em\u003e, “Convolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations” (\u003ca href=\"http://web.eecs.umich.edu/~honglak/icml09-ConvolutionalDeepBeliefNetworks.pdf\"\u003elink\u003c/a\u003e)\u003c/li\u003e\n\u003c/ol\u003e\n\n\n  \u003c!-- POST NAVIGATION --\u003e\n  \u003cdiv class=\"postNav clearfix\"\u003e\n     \n      \u003ca class=\"prev\" href=\"/blog/transformers-from-scratch-zh/\"\u003e\u003cspan\u003e« [译] Transformer 是如何工作的：600 行 Python 代码实现两个（文本分类+文本生成）Transformer（2019）\u003c/span\u003e\n      \n    \u003c/a\u003e\n      \n      \n      \u003ca class=\"next\" href=\"/blog/gpu-prices/\"\u003e\u003cspan\u003eGPU Prices Quick Reference (2023) »\u003c/span\u003e\n       \n      \u003c/a\u003e\n     \n  \u003c/div\u003e\n\u003c/div\u003e",
  "Date": "2023-06-11T00:00:00Z",
  "Author": "Arthur Chiao"
}