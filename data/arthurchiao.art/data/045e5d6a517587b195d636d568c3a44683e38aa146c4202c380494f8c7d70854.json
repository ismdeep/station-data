{
  "Source": "arthurchiao.art",
  "Title": "Monitoring Ceph Object Storage",
  "Link": "https://arthurchiao.art/blog/monitoring-ceph-obj-storage/",
  "Content": "\u003cdiv class=\"post\"\u003e\n  \n  \u003ch1 class=\"postTitle\"\u003eMonitoring Ceph Object Storage\u003c/h1\u003e\n  \u003cp class=\"meta\"\u003ePublished at 2018-10-08 | Last Update \u003c/p\u003e\n  \n  \u003cp\u003eCeph is a widely-used distributed file system which supports \u003cstrong\u003eobject storage, block storage, and distributed file system (Ceph FS)\u003c/strong\u003e.\nWe (\u003ca href=\"https://github.com/CtripCloud\"\u003e\u003cstrong\u003eCtrip Cloud\u003c/strong\u003e\u003c/a\u003e) use ceph to provide object storage service in our private cloud, with \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e10+\u003c/code\u003e clusters (for historical reasons, each cluster is not very large), providing a total \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e10+ PB\u003c/code\u003e effective capacity.\u003c/p\u003e\n\n\u003cp\u003eCeh is powerful and complicated, but it seems that there haven’t a\ncorresponding monitoring solutions which is mature and widely used.\nIn this article, we show our one for the object storage part.\u003c/p\u003e\n\n\u003ch2 id=\"1-introduction\"\u003e1. Introduction\u003c/h2\u003e\n\n\u003cp\u003eWe provide object storage service through Swift and S3 APIs.\u003c/p\u003e\n\n\u003cp\u003eTo better exhibit the cluster status, we design to collect the following metrics:\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\u003cstrong\u003eCeph internal metrics\u003c/strong\u003e: cluster health status, IOPS, commit/apply latency, usage, OSD status, etc\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eCeph REST API status\u003c/strong\u003e: request/response time, success rate, upload/download latency and bandwidth, slow requests, per-node stats, per-bucket stats, etc\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch2 id=\"2-design-and-implementation\"\u003e2. Design and Implementation\u003c/h2\u003e\n\n\u003ch3 id=\"21-collect-internal-metrics\"\u003e2.1 Collect Internal Metrics\u003c/h3\u003e\n\n\u003cp\u003eInternal metrics could be retrieved through ceph commands, for example, \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eceph -s\u003c/code\u003e\nor \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eceph status\u003c/code\u003e describes ceph cluster healthy status. We developed\na command line tool called \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eceph-collector\u003c/code\u003e to get those info by executing ceph\ncommands (specifying json output).\u003c/p\u003e\n\n\u003cp\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eceh-collector\u003c/code\u003e is similar to \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eceph-exporter\u003c/code\u003e[1] in concept, but we write\ndirectly to influxdb rather than promethues. Part of the initial code was based\non telegraf \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eceph input\u003c/code\u003e collector [2].\u003c/p\u003e\n\n\u003ch3 id=\"22-collect-requests-information\"\u003e2.2 Collect Requests Information\u003c/h3\u003e\n\n\u003cp\u003eCeph provides REST API for uploading, downloading and deleting files. RGW (Rados\nGateway) handles all the requests, which internally uses \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ecivetweb\u003c/code\u003e - an\nembedded web server.\u003c/p\u003e\n\n\u003cp\u003eMost oftenly, requests info is recorded in a web servers’ access log, for\n\u003ccode class=\"language-plaintext highlighter-rouge\"\u003ecivetweb\u003c/code\u003e, however, the log has little information. So, we added \u003ccode class=\"language-plaintext highlighter-rouge\"\u003enginx\u003c/code\u003e as a\nreserver proxy sitting before RGW, and\ncustomized the nginx configuration to export info we needed.\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/monitoring-ceph/nginx_proxy.png\" width=\"50%\" height=\"50%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eNon-extensive tests show that adding nginx introduced little performance drop.\u003c/p\u003e\n\n\u003ch3 id=\"23-overall-monitoring-solution\"\u003e2.3 Overall Monitoring Solution\u003c/h3\u003e\n\n\u003cp\u003eThe overall solution consists of following components:\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eTelegraf - collect requests details from nginx access log\u003c/li\u003e\n  \u003cli\u003eceph-collector - collect ceph internal metrics by ceph CLI interfaces\u003c/li\u003e\n  \u003cli\u003eInfluxdb - time series database, storing collected data points\u003c/li\u003e\n  \u003cli\u003eGrafana - dashboard\u003c/li\u003e\n  \u003cli\u003eRunDeck - alerting\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch2 id=\"3-implementation\"\u003e3 Implementation\u003c/h2\u003e\n\n\u003ch3 id=\"31-ceph-collector\"\u003e3.1 ceph-collector\u003c/h3\u003e\n\n\u003cp\u003eImplemented in go and built to a binary.\nRun every 5 minutes and write data directly to influxdb.\u003c/p\u003e\n\n\u003cp\u003eData collecting code initially based on telegraf ceph input, then added some new\nmetrics and removed some old ones. Support configuration and logging, each\ndefault to \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e/etc/ceph-collector/conf.json\u003c/code\u003e and\n\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e/etc/ceph-collector/ceph-collector.log\u003c/code\u003e.\u003c/p\u003e\n\n\u003ch3 id=\"32-reverse-proxy\"\u003e3.2 Reverse Proxy\u003c/h3\u003e\n\n\u003ch4 id=\"ceph-conf\"\u003eCeph Conf\u003c/h4\u003e\n\n\u003cp\u003eChange litsening port to 8080, and \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eswift storage URL\u003c/code\u003e to nginx listening URL:\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003ergw_frontends = civetwebport = 8080\nswift_storage_url=\u0026#34;http://\u0026lt;url\u0026gt;\u0026#34;\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003ch4 id=\"nginx-conf\"\u003eNginx Conf\u003c/h4\u003e\n\n\u003cp\u003eExcept listening on 80 and forwarding all requests to 8080, nginx conf should\nalso be carefully tuned:\u003c/p\u003e\n\n\u003ch5 id=\"1-access-log-format\"\u003e1. access log format\u003c/h5\u003e\n\n\u003cp\u003eShould include additional fields, e.g response time, upload size, download size\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003elog_format  main  \u0026#39;$remote_addr - $remote_user [$time_local] \u0026#34;$request\u0026#34; \u0026#39;\n                  \u0026#39;$status $request_length $body_bytes_sent \u0026#34;$http_referer\u0026#34; \u0026#39;\n                  \u0026#39;\u0026#34;$http_user_agent\u0026#34; \u0026#34;$http_x_forwarded_for\u0026#34; \u0026#34;$request_time\u0026#34;\u0026#39;;\n\naccess_log  /var/log/nginx/access.log  main;\nerror_log  /var/log/nginx/error.log warn;\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003ch5 id=\"2-disable-request-buffering\"\u003e2. disable request buffering\u003c/h5\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e# this is necessary for disabling request buffering in all cases\nproxy_http_version 1.1;\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003ch5 id=\"3-body-size-set-to-unlimited-as-there-are-large-file-uploadsdownloads\"\u003e3. body size set to unlimited as there are large file uploads/downloads\u003c/h5\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e# disable any limits to avoid HTTP 413 for large image uploads\nclient_max_body_size 0;\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003ch5 id=\"4-disallow-nginx-to-modify-uri-eg-nginx-default-will-merge-double-slashs34\"\u003e4. disallow nginx to modify URI (e.g. nginx default will merge double slashs)[3][4]\u003c/h5\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003eproxy_pass http://radosgw;\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003ch3 id=\"33-collect-requests-info\"\u003e3.3 Collect Requests Info\u003c/h3\u003e\n\n\u003cp\u003eWe use telegraf input parser to match and split request’s fields from nginx access log.\nOutput to influxdb directly.\u003c/p\u003e\n\n\u003ch2 id=\"4-monitoring-dashboard\"\u003e4 Monitoring Dashboard\u003c/h2\u003e\n\n\u003cp\u003eWe group the monitoring metrics into 4 parts:\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eNginx (REST API Status)\u003c/li\u003e\n  \u003cli\u003eError Responses (4xx, 5xx)\u003c/li\u003e\n  \u003cli\u003eSlow Uploads/Downloads\u003c/li\u003e\n  \u003cli\u003eCeph internal Metrics\u003c/li\u003e\n  \u003cli\u003eNginx Requests Details\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch3 id=\"41-nginx\"\u003e4.1 Nginx\u003c/h3\u003e\n\n\u003ch4 id=\"411-uploaddownload-latency\"\u003e4.1.1 upload/download latency\u003c/h4\u003e\n\n\u003cul\u003e\n  \u003cli\u003estatistics: max, min, average, P99, P90, P50.\u003c/li\u003e\n  \u003cli\u003eresponse time graph: upload, download, delete, list\u003c/li\u003e\n  \u003cli\u003elist count, list latency: list operation is time-consuming in distributed object storage systems\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThe latency metrics filtered out requests with body size larger than 1MB, since we observiced that 98% objects in our cluster are below the size.\u003c/p\u003e\n\n\u003ch4 id=\"412-api-status\"\u003e4.1.2 API status\u003c/h4\u003e\n\n\u003cul\u003e\n  \u003cli\u003eAPI success rate\u003c/li\u003e\n  \u003cli\u003e4xx and 5xx count\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch4 id=\"413-uploaddownload-requests\"\u003e4.1.3 upload/download requests\u003c/h4\u003e\n\n\u003cul\u003e\n  \u003cli\u003eupload/download count, and slow upload/download count\u003c/li\u003e\n  \u003cli\u003erequests and slow requests distribution over storage nodes\u003c/li\u003e\n  \u003cli\u003eupload/download size distribution\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch4 id=\"414-uploaddownload-bandwidth\"\u003e4.1.4 upload/download bandwidth\u003c/h4\u003e\n\n\u003cp\u003eper-bucket and total bandwidth.\u003c/p\u003e\n\n\u003ch4 id=\"415-sum-up\"\u003e4.1.5 Sum Up\u003c/h4\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/monitoring-ceph/dashboard-1-nginx.jpg\" width=\"100%\" height=\"100%\"/\u003e\u003c/p\u003e\n\n\u003ch3 id=\"42-error-responses-details\"\u003e4.2 Error Responses Details\u003c/h3\u003e\n\n\u003cp\u003eThis part shows request/response details of:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e4xx requests/responses: client side request error\u003c/li\u003e\n  \u003cli\u003e5xx requests/responses: internal server error\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/monitoring-ceph/dashboard-2-error.jpg\" width=\"100%\" height=\"100%\"/\u003e\u003c/p\u003e\n\n\u003ch3 id=\"43-slow-uploadsdownloads\"\u003e4.3 Slow Uploads/Downloads\u003c/h3\u003e\n\n\u003cul\u003e\n  \u003cli\u003eslow uploads requests/responses: slow upload requests\u003c/li\u003e\n  \u003cli\u003eslow download requests/responses\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/monitoring-ceph/dashboard-3-slow.jpg\" width=\"100%\" height=\"100%\"/\u003e\u003c/p\u003e\n\n\u003ch3 id=\"44-ceph-internal-metrics\"\u003e4.4 Ceph Internal Metrics\u003c/h3\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/monitoring-ceph/dashboard-4-internal.jpg\" width=\"95%\" height=\"95%\"/\u003e\u003c/p\u003e\n\n\u003ch4 id=\"441-ceph-healthy-status\"\u003e4.4.1 Ceph healthy status\u003c/h4\u003e\n\n\u003ch4 id=\"442-iops-read-write\"\u003e4.4.2 IOPS: read, write\u003c/h4\u003e\n\n\u003cp\u003eExcept the IOPS stats from ceph, we also predicted the upper bound of read and write IOPS.\nThe write uppper bound is calculated with following formula (we got the initial model from ebay, and improved it):\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003eupper bound = \u0026lt;total_osd\u0026gt; * \u0026lt;max_write_iops_per_osd\u0026gt; / \u0026lt;replica\u0026gt; / \u0026lt;write_times\u0026gt;\n\nmax_write_iops_per_osd = 100  # 100 IOPS is a moderate value for SATA\nreplica = 3                   # 3 replication\nwrite_times = 2               # filestore will first write journal, then write to disk\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eMax read IOPS is similar, but it doesn’t need to the \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ewrite_times\u003c/code\u003e factor.\u003c/p\u003e\n\n\u003cp\u003eThe predicted uppper bounds fit nicely to our observed data:\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/monitoring-ceph/iops.jpg\" width=\"50%\" height=\"50%\"/\u003e\u003c/p\u003e\n\n\u003ch4 id=\"443-usage\"\u003e4.4.3 Usage\u003c/h4\u003e\n\n\u003cp\u003eTotal size, used size, total objects, and their growth over time.\u003c/p\u003e\n\n\u003ch4 id=\"444-commitapply-latency\"\u003e4.4.4 Commit/Apply latency\u003c/h4\u003e\n\n\u003ch4 id=\"445-osd-status\"\u003e4.4.5 OSD status\u003c/h4\u003e\n\n\u003cp\u003eMonitoring total OSDs, \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ein\u003c/code\u003e OSDs, and \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eup\u003c/code\u003e OSDs.\u003c/p\u003e\n\n\u003ch4 id=\"446-recovery-status\"\u003e4.4.6 Recovery status\u003c/h4\u003e\n\n\u003cp\u003eDegraded PGs, misplaced PGs, etc\u003c/p\u003e\n\n\u003ch3 id=\"45-nginx-details\"\u003e4.5 Nginx Details\u003c/h3\u003e\n\n\u003cp\u003eDetails requests/responses:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eupload requests\u003c/li\u003e\n  \u003cli\u003edownload requests\u003c/li\u003e\n  \u003cli\u003edelete requests\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/monitoring-ceph/dashboard-5-logs.jpg\" width=\"95%\" height=\"95%\"/\u003e\u003c/p\u003e\n\n\u003ch2 id=\"5-alerting\"\u003e5 Alerting\u003c/h2\u003e\n\n\u003cp\u003eTo notify our developers in the first time when the ceph cluster misbehaves, we configure alerting rules based on our monitoring metrics.\u003c/p\u003e\n\n\u003ch3 id=\"51-alerting-on-ceph-un-healthy\"\u003e5.1 Alerting on Ceph un-healthy\u003c/h3\u003e\n\n\u003ch3 id=\"52-alerting-on-osd-down\"\u003e5.2 Alerting on OSD down\u003c/h3\u003e\n\n\u003ch3 id=\"53-alerting-on-5xx-responses\"\u003e5.3 Alerting on 5xx Responses\u003c/h3\u003e\n\n\u003ch3 id=\"54-nomarl-physical-server-alertings\"\u003e5.4 Nomarl physical server alertings\u003c/h3\u003e\n\n\u003cp\u003eWe also have many alerting rules for the ceph nodes: swap space, memory, bandwidth, etc. But this is done by another monitoring system in our corporation, so we do not detail them here.\u003c/p\u003e\n\n\u003ch2 id=\"6-summary-and-future-work\"\u003e6 Summary and Future Work\u003c/h2\u003e\n\n\u003cp\u003eIn this article we proposed an monitoring solution for ceph object storage service. We implemented the solution with the help of some popular open source components. We also developed a command line tool to collect ceph internal metrics.\nThe final monitoring system is fairly helpful to our R\u0026amp;D and DevOps team, relying on which we have found many bottlenecks of the cluster, and done corresponding optimizations, the cluster performance has increased ~40% since then (maybe another dedicated blog for performance tuning).\u003c/p\u003e\n\n\u003cp\u003eApart from monitoring, our system also supports alerting. When the cluster encounters problems, such as internal server errors or OSD down, we are notified in the first time through email, which is very helpful for us to minimize service down time.\u003c/p\u003e\n\n\u003ch2 id=\"references\"\u003eReferences\u003c/h2\u003e\n\n\u003col\u003e\n  \u003cli\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eceph-exporter\u003c/code\u003e, https://github.com/digitalocean/ceph_exporter\u003c/li\u003e\n  \u003cli\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003etelegraf ceph\u003c/code\u003e, https://github.com/influxdata/telegraf/blob/master/plugins/inputs/ceph/ceph.go\u003c/li\u003e\n  \u003cli\u003ehttps://stackoverflow.com/questions/14832780/nginx-merge-slashes-redirect\u003c/li\u003e\n  \u003cli\u003ehttps://stackoverflow.com/questions/20496963/avoid-nginx-decoding-query-parameters-on-proxy-pass-equivalent-to-allowencodeds\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch2 id=\"appendix\"\u003eAppendix\u003c/h2\u003e\n\n\u003col\u003e\n  \u003cli\u003eNginx Conf: \u003ca href=\"/assets/img/monitoring-ceph/nginx.conf\"\u003enginx.conf\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003eCeph Conf: \u003ca href=\"/assets/img/monitoring-ceph/ceph.conf\"\u003eceph.conf\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003eTelegraf Conf: \u003ca href=\"/assets/img/monitoring-ceph/ceph_nginx.conf\"\u003eceph_nginx.conf\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003eGrafana Conf: \u003ca href=\"/assets/img/monitoring-ceph/grafana.json\"\u003egrafana.json\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\n\n  \u003c!-- POST NAVIGATION --\u003e\n  \u003cdiv class=\"postNav clearfix\"\u003e\n     \n      \u003ca class=\"prev\" href=\"/blog/vim-pickups-001-zh/\"\u003e\u003cspan\u003e« Vim Pickups 001\u003c/span\u003e\n      \n    \u003c/a\u003e\n      \n      \n      \u003ca class=\"next\" href=\"/blog/play-with-container-network-if/\"\u003e\u003cspan\u003ePlay With Container Network Interface »\u003c/span\u003e\n       \n      \u003c/a\u003e\n     \n  \u003c/div\u003e\n\u003c/div\u003e",
  "Date": "2018-10-08T00:00:00Z",
  "Author": "Arthur Chiao"
}