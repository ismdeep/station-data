{
  "Source": "arthurchiao.art",
  "Title": "GPU Performance (Data Sheets) Quick Reference (2023)",
  "Link": "https://arthurchiao.art/blog/gpu-data-sheets/",
  "Content": "\u003cdiv class=\"post\"\u003e\n  \n  \u003ch1 class=\"postTitle\"\u003eGPU Performance (Data Sheets) Quick Reference (2023)\u003c/h1\u003e\n  \u003cp class=\"meta\"\u003ePublished at 2023-10-25 | Last Update 2024-02-15\u003c/p\u003e\n  \n  \u003cp\u003eThis post provides a concise reference for the performance of popular GPU\nmodels from NVIDIA and Huawei/HiSilicon, primarily intended for personal use.\u003c/p\u003e\n\n\u003chr/\u003e\n\n\u003cul id=\"markdown-toc\"\u003e\n  \u003cli\u003e\u003ca href=\"#1-introduction\" id=\"markdown-toc-1-introduction\"\u003e1 Introduction\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#naming-convention-of-nvidia-gpus\" id=\"markdown-toc-naming-convention-of-nvidia-gpus\"\u003eNaming convention of NVIDIA GPUs\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#2-comparison-of-l2l4t4a10v100\" id=\"markdown-toc-2-comparison-of-l2l4t4a10v100\"\u003e2 Comparison of \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eL2/L4/T4/A10/V100\u003c/code\u003e\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#3-comparison-of-a100a800h100h800ascend-910b\" id=\"markdown-toc-3-comparison-of-a100a800h100h800ascend-910b\"\u003e3 Comparison of \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eA100/A800/H100/H800/Ascend 910B\u003c/code\u003e\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#31-note-on-inter-gpu-bandwidth-hccs-vs-nvlink\" id=\"markdown-toc-31-note-on-inter-gpu-bandwidth-hccs-vs-nvlink\"\u003e3.1 Note on inter-GPU bandwidth: \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eHCCS vs. NVLINK\u003c/code\u003e\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#4-comparison-of-h20l20ascend-910b\" id=\"markdown-toc-4-comparison-of-h20l20ascend-910b\"\u003e4 Comparison of \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eH20\u003c/code\u003e/\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eL20\u003c/code\u003e/\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eAscend 910B\u003c/code\u003e\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#5-notes-on-us-chip-export-controls-targeting-china\" id=\"markdown-toc-5-notes-on-us-chip-export-controls-targeting-china\"\u003e5 Notes on US “Chip Export Controls” targeting China\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#51-export-controls-202210\" id=\"markdown-toc-51-export-controls-202210\"\u003e5.1 Export Controls \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e2022.10\u003c/code\u003e\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#52-export-controls-202310\" id=\"markdown-toc-52-export-controls-202310\"\u003e5.2 Export Controls \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e2023.10\u003c/code\u003e\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003chr/\u003e\n\n\u003ch1 id=\"1-introduction\"\u003e1 Introduction\u003c/h1\u003e\n\n\u003ch2 id=\"naming-convention-of-nvidia-gpus\"\u003eNaming convention of NVIDIA GPUs\u003c/h2\u003e\n\n\u003cp\u003eThe first letter in GPU model names denote their GPU architectures, with:\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eT\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e for Turing;\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eA\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e for Ampere;\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eV\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e for Volta;\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eH\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e for Hopper; 2022\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eL\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e for Ada Lovelace;\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch1 id=\"2-comparison-of-l2l4t4a10v100\"\u003e2 Comparison of \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eL2/L4/T4/A10/V100\u003c/code\u003e\u003c/h1\u003e\n\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth style=\"text-align: left\"\u003e \u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eL2\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eL4\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eT4\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eA10\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eA30\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eV100 PCIe/SMX2\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eDesigned for\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eData center\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eData center\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eData center\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e(Desktop) \u003cstrong\u003e\u003cmark\u003eGraphics-intensive\u003c/mark\u003e\u003c/strong\u003e workloads\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eDesktop\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eData center\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eYear\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e2023\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e2023\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e2018\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e2020\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e2017\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eManufacturing\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e12nm\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e12nm\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eArchitecture\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eAda Lovelace\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eAda Lovelace\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eTuring\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eAmpere\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eAmpere\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eVolta\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eMax Power\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e72W\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e70 watts\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e150 watts\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e165 watts\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e250/300watts\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eGPU Mem\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e24GB GDDR6\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e24GB\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e16GB GDDR6\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e24GB GDDR6\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e24GB HBM2\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e16/32GB \u003cmark\u003eHBM2\u003c/mark\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eGPU Mem BW\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e300 GB/s\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e300 GB/s\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e400 GB/s\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e600 GB/s\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e933GB/s\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e900 GB/s\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eInterconnect\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003ePCIe Gen4 64GB/s\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003ePCIe Gen4 64GB/s\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003ePCIe Gen3 32GB/s\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003ePCIe Gen4 66 GB/s\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003ePCIe Gen4 64GB/s, NVLINK 200GB/s\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003ePCIe Gen3 32GB/s, NVLINK \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e300GB/s\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eFP32                \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eTFLOPS\u003c/code\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e24.1\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e30.3\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e8.1\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e31.2\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e10.3\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e14/15.7\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eTF32                \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eTFLOPS\u003c/code\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e48.3\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e120*\u003c/code\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eBFLOAT16 TensorCore \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eTFLOPS\u003c/code\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e95.6\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e242*\u003c/code\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e125\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e165\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eFP16 TensorCore     \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eTFLOPS\u003c/code\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e242*\u003c/code\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e125\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e165\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eINT8 TensorCore     \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eTFLOPS\u003c/code\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e193/193\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e485*\u003c/code\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e250\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e330\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eINT4 TensorCore     \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eTFLOPS\u003c/code\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eNO\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e661\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp\u003eNotes:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e*\u003c/code\u003e: \u003cstrong\u003e\u003cmark\u003ewith sparsity\u003c/mark\u003e\u003c/strong\u003e; Specifications 1/2 lower without sparsity.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eDatasheets:\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\u003ca href=\"https://nvdam.widen.net/s/rvq98gbwsw/l4-datasheet-2595652\"\u003eL4\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://www.nvidia.com/en-us/data-center/tesla-t4/\"\u003eT4\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://www.nvidia.com/en-us/data-center/products/a10-gpu/\"\u003eA10\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://www.nvidia.com/en-us/data-center/products/a30-gpu/\"\u003eA30\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://www.nvidia.com/en-us/data-center/v100/\"\u003eV100-PCIe/V100-SXM2/V100S-PCIe\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch1 id=\"3-comparison-of-a100a800h100h800ascend-910b\"\u003e3 Comparison of \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eA100/A800/H100/H800/Ascend 910B\u003c/code\u003e\u003c/h1\u003e\n\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth style=\"text-align: left\"\u003e \u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eA800 (PCIe/SXM)\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eA100 (PCIe/SXM)\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003e\u003cmark\u003eHuawei Ascend 910B\u003c/mark\u003e\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eH800  (PCIe/SXM)\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eH100 (PCIe/SXM)\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eYear\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e2022\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e2020\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e2023\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e2022\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e2022\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eManufacturing\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e7nm\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e7nm\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e7+nm\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e4nm\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e4nm\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eArchitecture\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eAmpere\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eAmpere\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eHUAWEI Da Vinci\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eHopper\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eHopper\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eMax Power\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e300/400 watt\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e300/400 watt\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e400 watt\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e350/700 watt\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eGPU Mem\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e80G HBM2e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e80G HBM2e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e64G HBM2e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e80G HBM3\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e80G HBM3\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eGPU Mem BW\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1935/2039 GB/s\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e2/3.35 TB/s\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eGPU Interconnect (\u003cstrong\u003e\u003cmark\u003eone-to-one max bandwidth\u003c/mark\u003e\u003c/strong\u003e)\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eNVLINK 400GB/s\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003ePCIe Gen4 64GB/s, NVLINK 600GB/s\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eHCCS \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e56GB/s\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eNVLINK 400GB/s\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003ePCIe Gen5 128GB/s, NVLINK \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e900GB/s\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eGPU Interconnect (\u003cstrong\u003e\u003cmark\u003eone-to-many total bw\u003c/mark\u003e\u003c/strong\u003e)\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eNVLINK 400GB/s\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003ePCIe Gen4 64GB/s, NVLINK 600GB/s\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eHCCS \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e392GB/s\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eNVLINK 400GB/s\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003ePCIe Gen5 128GB/s, NVLINK \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e900GB/s\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eFP32                \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eTFLOPS\u003c/code\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e19.5\u003c/code\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e51 | 67*\u003c/code\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eTF32 (TensorFloat)  \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eTFLOPS\u003c/code\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e156 | 312*\u003c/code\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e756 | 989*\u003c/code\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eBFLOAT16 TensorCore \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eTFLOPS\u003c/code\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e156 | 312*\u003c/code\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eFP16 TensorCore     \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eTFLOPS\u003c/code\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e312 | 624*\u003c/code\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e320\u003c/code\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e1513 | 1979*\u003c/code\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eFP8 TensorCore      \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eTFLOPS\u003c/code\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eNOT support\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eNOT support\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e3026 | 3958*\u003c/code\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eINT8 TensorCore     \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eTFLOPS\u003c/code\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e624 | 1248*\u003c/code\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e640\u003c/code\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e3026/3958*\u003c/code\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp\u003eNotes:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e*\u003c/code\u003e: \u003cstrong\u003e\u003cmark\u003ewith sparsity\u003c/mark\u003e\u003c/strong\u003e; Specifications 1/2 lower without sparsity.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eH100 vs. A100 in one word: \u003cstrong\u003e\u003cmark\u003e 3x performance, 2x price\u003c/mark\u003e\u003c/strong\u003e.\u003c/p\u003e\n\n\u003cp\u003eDatasheets:\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\u003ca href=\"https://www.nvidia.com/en-us/data-center/a100/\"\u003eA100\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://www.nvidia.com/en-us/data-center/h100/\"\u003eH100\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://www.hisilicon.com/en/products/Ascend/Ascend-910\"\u003e\u003cdel\u003eHuawei Ascend-910B\u003c/del\u003e\u003c/a\u003e (404)\u003c/li\u003e\n  \u003cli\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e910\u003c/code\u003e paper: \u003ca href=\"https://ieeexplore.ieee.org/abstract/document/9407221\"\u003eAscend: a Scalable and Unified Architecture for Ubiquitous Deep Neural Network Computing\u003c/a\u003e, HPCA, 2021\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch2 id=\"31-note-on-inter-gpu-bandwidth-hccs-vs-nvlink\"\u003e3.1 Note on inter-GPU bandwidth: \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eHCCS vs. NVLINK\u003c/code\u003e\u003c/h2\u003e\n\n\u003cp\u003eFor 8-card A800 and 910B modules: 910B HCCS has a total bandwidth of \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e392GB/s\u003c/code\u003e,\nwhich appears to be comparable to A800 NVLink (\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e400GB/s\u003c/code\u003e). However, there are\nsome differences. To clarify them,\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\n    \u003cp\u003eNVIDIA NVLink: \u003cstrong\u003e\u003cmark\u003efull-mesh topology\u003c/mark\u003e\u003c/strong\u003e as below, so (bi-directional)\n\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eGPU-to-GPU max bandwidth\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e is \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e400GB/s\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\n(note that below is \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e8*A100\u003c/code\u003e module, 600GB/s, \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e8*A800\u003c/code\u003e shares a similar full-mesh topology);\u003c/p\u003e\n\n    \u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/gpu-notes/8x-a100-node-hw-topo.png\" width=\"100%\" height=\"100%\"/\u003e\u003c/p\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eHuawei HCCS: \u003cstrong\u003e\u003cmark\u003epeer-to-peer topology\u003c/mark\u003e\u003c/strong\u003e (no stuffs like NVSwitch chip), so (bi-directional)\n\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eGPU-to-GPU max bandwidth\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e is \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e56GB/s\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e;\u003c/p\u003e\n\n    \u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/gpu-notes/ascend-910b-x8-topo.png\" width=\"50%\" height=\"50%\"/\u003e\u003c/p\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch1 id=\"4-comparison-of-h20l20ascend-910b\"\u003e4 Comparison of \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eH20\u003c/code\u003e/\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eL20\u003c/code\u003e/\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eAscend 910B\u003c/code\u003e\u003c/h1\u003e\n\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth style=\"text-align: left\"\u003e \u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003e\u003cmark\u003eHuawei Ascend 910B\u003c/mark\u003e\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eL20  (PCIe)\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eH20  (PCIe/SXM)\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eH100 (PCIe/SXM)\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eYear\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e2023\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e2023\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e2023\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e2022\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eManufacturing\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e7+nm\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e4nm\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e4nm\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e4nm\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eArchitecture\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eHUAWEI Da Vinci\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eAda Lovelace\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eHopper\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eHopper\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eMax Power\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e400 watt\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e275W\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e400W\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e350/700 watt\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eGPU Mem\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e64G HBM2e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e48G GDDR6\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e80G HBM3\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e80G HBM3\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eGPU Mem BW\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e864GB/s\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e\u003cmark\u003e4.0TB/s\u003c/mark\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e2/3.35 TB/s\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eL2 Cache\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e96MB\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e60MB\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eGPU Interconnect (\u003cstrong\u003e\u003cmark\u003eone-to-one max bandwidth\u003c/mark\u003e\u003c/strong\u003e)\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eHCCS 56GB/s\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003ePCIe Gen4 64GB/s\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003ePCIe Gen5 128GB/s, \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eNVLINK 900GB/s\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003ePCIe Gen5 128GB/s, NVLINK 900GB/s\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eGPU Interconnect (\u003cstrong\u003e\u003cmark\u003eone-to-many total bw\u003c/mark\u003e\u003c/strong\u003e)\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eHCCS 392GB/s\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003ePCIe Gen4 64GB/s\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003ePCIe Gen5 128GB/s, \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eNVLINK 900GB/s\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003ePCIe Gen5 128GB/s, NVLINK 900GB/s\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eFP32\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e59.8 TFLOPS\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e44 TFLOPS\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e51/67 TFLOPS\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eTF32 (TensorFloat)\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e59.8 TFLOPS\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e74 TFLOPS\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e756/989 TFLOPS\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eBFLOAT16 TensorCore\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e119/119 TFLOPS\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e148/148 TFLOPS\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eFP16 TensorCore\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e320 TFLOPS\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1513/1979 TFLOPS\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eFP8 TensorCore\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e3026/3958 TFLOPS\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eINT8 TensorCore\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e640 TFLOPS\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e239/239 TFLOPS\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e296/296 TFLOPS\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e3026/3958 TFLOPS\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003ch1 id=\"5-notes-on-us-chip-export-controls-targeting-china\"\u003e5 Notes on US “Chip Export Controls” targeting China\u003c/h1\u003e\n\n\u003ch2 id=\"51-export-controls-202210\"\u003e5.1 Export Controls \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e2022.10\u003c/code\u003e\u003c/h2\u003e\n\n\u003cp\u003eAccording to\n\u003ca href=\"https://www.federalregister.gov/documents/2022/10/13/2022-21658/implementation-of-additional-export-controls-certain-advanced-computing-and-semiconductor\"\u003eImplementation of Additional Export Controls: Certain Advanced Computing and Semiconductor Manufacturing Items; Supercomputer and Semiconductor End Use; Entity List Modification\u003c/a\u003e,\nfor chips that can be shipped to the Chinese market, the following conditions must be met:\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eaggregate bidirectional transfer rate must \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e\u0026lt; 600 Gbyte/s\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e; \u003cstrong\u003e\u003cmark\u003eAND\u003c/mark\u003e\u003c/strong\u003e,\u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eaggregated processing performance must \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e\u0026lt; 4800 bit TOPS\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e (TFLOPS), which is\nequivalent to:\u003c/p\u003e\n\n    \u003cul\u003e\n      \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e\u0026lt; 300 TFLOPS FP16\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/li\u003e\n      \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e\u0026lt; 150 TFLOPS FP32\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eA100 and H100 are subjected to these restrictions, that’s why there are tailored versions: A800 and H800.\u003c/p\u003e\n\n\u003ch2 id=\"52-export-controls-202310\"\u003e5.2 Export Controls \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e2023.10\u003c/code\u003e\u003c/h2\u003e\n\n\u003cp\u003eAccording to\n\u003ca href=\"https://www.federalregister.gov/documents/2023/10/25/2023-23055/implementation-of-additional-export-controls-certain-advanced-computing-items-supercomputer-and\"\u003eImplementation of Additional Export Controls: Certain Advanced Computing Items; Supercomputer and Semiconductor End Use; Updates and Corrections\u003c/a\u003e,\n\u003cstrong\u003e\u003cmark\u003ein addition to\u003c/mark\u003e\u003c/strong\u003e the above \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e2022.10 Export Controls\u003c/code\u003e, chips that\nmeet one of the following conditions are also \u003cstrong\u003e\u003cmark\u003eprohibited\u003c/mark\u003e\u003c/strong\u003e from being sold in the Chinese market:\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\n    \u003cp\u003etotal processing performance in \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e2400~4800 bit TOPS\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\n  AND performance density in \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e1.6~5.92\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e;\u003c/p\u003e\n\n    \u003cp\u003e2400 bit TOPS is equivalent to:\u003c/p\u003e\n\n    \u003cul\u003e\n      \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e150 TFLOPS FP16\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/li\u003e\n      \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e75 TFLOPS FP32\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003etotal processing performance \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e\u0026gt;= 1600 bit TOPS\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\n  AND performance density in \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e3.2~5.92\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e;\u003c/p\u003e\n  \u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eThese restrictions cover most high-performance GPUs, including the old model A800.\nHowever, it should be noted that there is also room for low-computing-power\nbut high-transfer-rate models, such as the rumored “148TFLOPS + 96GB HBM + 900GB/s\nNVLink” H20 GPU.\u003c/p\u003e\n\n\u003chr/\u003e\n\n\u003cp\u003e\u003ca href=\"https://notbyai.fyi\"\u003e\u003cimg src=\"/assets/img/Written-By-Human-Not-By-AI-Badge-white.svg\" alt=\"Written by Human, Not by AI\"/\u003e\u003c/a\u003e\n\u003ca href=\"https://notbyai.fyi\"\u003e\u003cimg src=\"/assets/img/Written-By-Human-Not-By-AI-Badge-black.svg\" alt=\"Written by Human, Not by AI\"/\u003e\u003c/a\u003e\u003c/p\u003e\n\n\n  \u003c!-- POST NAVIGATION --\u003e\n  \u003cdiv class=\"postNav clearfix\"\u003e\n     \n      \u003ca class=\"prev\" href=\"/blog/gpu-advanced-notes-2-zh/\"\u003e\u003cspan\u003e« GPU 进阶笔记（二）：华为昇腾 910B GPU 相关（2023）\u003c/span\u003e\n      \n    \u003c/a\u003e\n      \n      \n      \u003ca class=\"next\" href=\"/blog/kvm-host-in-a-few-lines-of-code-zh/\"\u003e\u003cspan\u003e[译] 100 行 C 代码创建一个 KVM 虚拟机（2019） »\u003c/span\u003e\n       \n      \u003c/a\u003e\n     \n  \u003c/div\u003e\n\u003c/div\u003e",
  "Date": "2023-10-25T00:00:00Z",
  "Author": "Arthur Chiao"
}