{
  "Source": "arthurchiao.art",
  "Title": "[译][论文] LLaMA：开放和高效的基础语言模型集（Meta/Facebook，2022）",
  "Link": "https://arthurchiao.art/blog/llama-paper-zh/",
  "Content": "\u003cdiv class=\"post\"\u003e\n  \n  \u003ch1 class=\"postTitle\"\u003e[译][论文] LLaMA：开放和高效的基础语言模型集（Meta/Facebook，2022）\u003c/h1\u003e\n  \u003cp class=\"meta\"\u003ePublished at 2023-07-10 | Last Update 2023-08-12\u003c/p\u003e\n  \n  \u003ch3 id=\"译者序\"\u003e译者序\u003c/h3\u003e\n\n\u003cp\u003e本文翻译自 2022 年 Meta（facebook）的大模型论文：\n\u003ca href=\"https://arxiv.org/abs/2302.13971\"\u003eLLaMA: Open and Efficient Foundation Language Models\u003c/a\u003e。\u003c/p\u003e\n\n\u003cp\u003e作者阵容：Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet,\nMarie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric\nHambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave,\nGuillaume Lample。\u003c/p\u003e\n\n\u003cp\u003e一些工程信息：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\n    \u003cp\u003eLLaMA 只使用公开可用数据集进行训练，模型已开源；\u003c/p\u003e\n\n    \u003cul\u003e\n      \u003cli\u003e基于 \u003cstrong\u003e\u003cmark\u003etransformer\u003c/mark\u003e\u003c/strong\u003e 架构；\u003c/li\u003e\n      \u003cli\u003e训练数据集大小：\u003cstrong\u003e\u003cmark\u003e1.4T 个 tokens\u003c/mark\u003e\u003c/strong\u003e；\u003c/li\u003e\n      \u003cli\u003e参数范围 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e7B~65B\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e；\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003e使用更多 token 进行训练，而不是狂堆参数，一样能取得不错的性能。\u003c/p\u003e\n\n    \u003cul\u003e\n      \u003cli\u003e\u003cstrong\u003e\u003cmark\u003eLLaMA-13B\u003c/mark\u003e\u003c/strong\u003e 在大多数基准测试中\u003cstrong\u003e\u003cmark\u003e优于 GPT-3（175B）\u003c/mark\u003e\u003c/strong\u003e；\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003e用户更想要的可能是一个\u003cstrong\u003e\u003cmark\u003e推理速度最快\u003c/mark\u003e\u003c/strong\u003e而不是\u003cstrong\u003e\u003cmark\u003e训练速度最快\u003c/mark\u003e\u003c/strong\u003e的模型；此时模型大小就非常重要，\u003c/p\u003e\n\n    \u003cul\u003e\n      \u003cli\u003eLLaMA 可以在单个 GPU 上运行；\u003c/li\u003e\n      \u003cli\u003e\u003cstrong\u003e\u003cmark\u003eLLaMA-13B 可以在单个 V100 上运行\u003c/mark\u003e\u003c/strong\u003e；\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003e训练成本\u003c/p\u003e\n\n    \u003cul\u003e\n      \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e2048 个 A100\u003c/mark\u003e\u003c/strong\u003e 80GB GPU 上，开发和训练约 5 个月；\u003c/li\u003e\n      \u003cli\u003e训练 65B 模型时，在 \u003cstrong\u003e\u003cmark\u003e2048 个 A100\u003c/mark\u003e\u003c/strong\u003e 80GB GPU 上能处理约\n\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e380 tokens/second/GPU\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e，因此 1.4T token\n的数据集训练一次大约需要 \u003cstrong\u003e\u003cmark\u003e21 天\u003c/mark\u003e\u003c/strong\u003e；\u003c/li\u003e\n      \u003cli\u003e耗能约 2638 MWh，折算排放 1015 吨  CO\u003csub\u003e2\u003c/sub\u003e。\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e\u003cstrong\u003e译者水平有限，不免存在遗漏或错误之处。如有疑问，敬请查阅原文。\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003e以下是译文。\u003c/p\u003e\n\n\u003chr/\u003e\n\n\u003cul id=\"markdown-toc\"\u003e\n  \u003cli\u003e\u003ca href=\"#译者序\" id=\"markdown-toc-译者序\"\u003e译者序\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#摘要\" id=\"markdown-toc-摘要\"\u003e摘要\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#1-引言\" id=\"markdown-toc-1-引言\"\u003e1 引言\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#11-大模型训练更多参数-vs-更大的数据集\" id=\"markdown-toc-11-大模型训练更多参数-vs-更大的数据集\"\u003e1.1 大模型训练：更多参数 vs 更大的数据集\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#12-llama减少参数增大数据集\" id=\"markdown-toc-12-llama减少参数增大数据集\"\u003e1.2 LLaMA：减少参数，增大数据集\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#13-内容组织\" id=\"markdown-toc-13-内容组织\"\u003e1.3 内容组织\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#2-方法approach\" id=\"markdown-toc-2-方法approach\"\u003e2 方法（Approach）\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#21-预训练数据pre-training-data\" id=\"markdown-toc-21-预训练数据pre-training-data\"\u003e2.1 预训练数据（Pre-training Data）\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#211-数据集\" id=\"markdown-toc-211-数据集\"\u003e2.1.1 数据集\u003c/a\u003e            \u003cul\u003e\n              \u003cli\u003e\u003ca href=\"#english-commoncrawl-67\" id=\"markdown-toc-english-commoncrawl-67\"\u003eEnglish CommonCrawl [67%]\u003c/a\u003e\u003c/li\u003e\n              \u003cli\u003e\u003ca href=\"#c4-15\" id=\"markdown-toc-c4-15\"\u003eC4 [15%]\u003c/a\u003e\u003c/li\u003e\n              \u003cli\u003e\u003ca href=\"#github-45\" id=\"markdown-toc-github-45\"\u003eGithub [4.5%]\u003c/a\u003e\u003c/li\u003e\n              \u003cli\u003e\u003ca href=\"#wikipedia-45\" id=\"markdown-toc-wikipedia-45\"\u003eWikipedia [4.5%]\u003c/a\u003e\u003c/li\u003e\n              \u003cli\u003e\u003ca href=\"#gutenberg-and-books3-45\" id=\"markdown-toc-gutenberg-and-books3-45\"\u003eGutenberg and Books3 [4.5%]\u003c/a\u003e\u003c/li\u003e\n              \u003cli\u003e\u003ca href=\"#arxiv-25\" id=\"markdown-toc-arxiv-25\"\u003eArXiv [2.5%]\u003c/a\u003e\u003c/li\u003e\n              \u003cli\u003e\u003ca href=\"#stack-exchange-2\" id=\"markdown-toc-stack-exchange-2\"\u003eStack Exchange [2%]\u003c/a\u003e\u003c/li\u003e\n            \u003c/ul\u003e\n          \u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#212-tokenizer分词器\" id=\"markdown-toc-212-tokenizer分词器\"\u003e2.1.2 Tokenizer（分词器）\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#22-架构architecture\" id=\"markdown-toc-22-架构architecture\"\u003e2.2 架构（Architecture）\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#221-改进\" id=\"markdown-toc-221-改进\"\u003e2.2.1 改进\u003c/a\u003e            \u003cul\u003e\n              \u003cli\u003e\u003ca href=\"#预归一化pre-normalization受-gpt3-启发\" id=\"markdown-toc-预归一化pre-normalization受-gpt3-启发\"\u003e预归一化（Pre-normalization）：受 GPT3 启发\u003c/a\u003e\u003c/li\u003e\n              \u003cli\u003e\u003ca href=\"#swiglu-激活函数受-palm-启发\" id=\"markdown-toc-swiglu-激活函数受-palm-启发\"\u003eSwiGLU 激活函数：受 PaLM 启发\u003c/a\u003e\u003c/li\u003e\n              \u003cli\u003e\u003ca href=\"#旋转嵌入rotary-embeddings受-gptneo-启发\" id=\"markdown-toc-旋转嵌入rotary-embeddings受-gptneo-启发\"\u003e旋转嵌入（Rotary Embeddings）：受 GPTNeo 启发\u003c/a\u003e\u003c/li\u003e\n            \u003c/ul\u003e\n          \u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#222-不同-llama-模型的超参数\" id=\"markdown-toc-222-不同-llama-模型的超参数\"\u003e2.2.2 不同 LLaMA 模型的超参数\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#23-优化器optimizer\" id=\"markdown-toc-23-优化器optimizer\"\u003e2.3 优化器（Optimizer）\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#24-高效实现efficient-implementation提高训练速度\" id=\"markdown-toc-24-高效实现efficient-implementation提高训练速度\"\u003e2.4 高效实现（Efficient implementation）：提高训练速度\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#3-主要结果main-results\" id=\"markdown-toc-3-主要结果main-results\"\u003e3 主要结果（Main results）\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#31-常识推理common-sense-reasoning\" id=\"markdown-toc-31-常识推理common-sense-reasoning\"\u003e3.1 常识推理（Common Sense Reasoning）\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#32-闭卷问答closed-book-question-answering\" id=\"markdown-toc-32-闭卷问答closed-book-question-answering\"\u003e3.2 闭卷问答（Closed-book Question Answering）\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#33-阅读理解reading-comprehension\" id=\"markdown-toc-33-阅读理解reading-comprehension\"\u003e3.3 阅读理解（Reading Comprehension）\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#34-数学推理mathematical-reasoning\" id=\"markdown-toc-34-数学推理mathematical-reasoning\"\u003e3.4 数学推理（Mathematical reasoning）\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#35-代码生成code-generation\" id=\"markdown-toc-35-代码生成code-generation\"\u003e3.5 代码生成（Code generation）\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#36-大规模多任务语言理解massive-multitask-language-understanding\" id=\"markdown-toc-36-大规模多任务语言理解massive-multitask-language-understanding\"\u003e3.6 大规模多任务语言理解（Massive Multitask Language Understanding）\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#37-训练过程中性能的变化\" id=\"markdown-toc-37-训练过程中性能的变化\"\u003e3.7 训练过程中性能的变化\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#4-指令微调instruction-finetuning\" id=\"markdown-toc-4-指令微调instruction-finetuning\"\u003e4 指令微调（Instruction Finetuning）\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#5-bias-toxicity-and-misinformation\" id=\"markdown-toc-5-bias-toxicity-and-misinformation\"\u003e5 Bias, Toxicity and Misinformation\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#51-realtoxicityprompts\" id=\"markdown-toc-51-realtoxicityprompts\"\u003e5.1 RealToxicityPrompts\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#52-crows-pairs\" id=\"markdown-toc-52-crows-pairs\"\u003e5.2 CrowS-Pairs\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#53-winogender\" id=\"markdown-toc-53-winogender\"\u003e5.3 WinoGender\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#54-truthfulqa\" id=\"markdown-toc-54-truthfulqa\"\u003e5.4 TruthfulQA\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#6-碳足迹carbon-footprint\" id=\"markdown-toc-6-碳足迹carbon-footprint\"\u003e6 碳足迹（Carbon footprint）\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#7-相关工作related-work\" id=\"markdown-toc-7-相关工作related-work\"\u003e7 相关工作（Related work）\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#71-architecture\" id=\"markdown-toc-71-architecture\"\u003e7.1 Architecture\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#72-scaling\" id=\"markdown-toc-72-scaling\"\u003e7.2 Scaling\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#8-总结\" id=\"markdown-toc-8-总结\"\u003e8 总结\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#致谢\" id=\"markdown-toc-致谢\"\u003e致谢\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#参考文献\" id=\"markdown-toc-参考文献\"\u003e参考文献\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#附录略\" id=\"markdown-toc-附录略\"\u003e附录（略）\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003chr/\u003e\n\n\u003cscript type=\"text/x-mathjax-config\"\u003e\n  \tMathJax.Hub.Config({\n    \textensions: [\"tex2jax.js\"],\n    \tjax: [\"input/TeX\", \"output/HTML-CSS\"],\n    \ttex2jax: {\n      \t\tinlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n      \t\tdisplayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ],\n    \t\tprocessEscapes: true\n\t    },\n\t\t\"HTML-CSS\": {\n\t\t\tavailableFonts: [], preferredFont: null,\n\t\t\twebFont: \"Neo-Euler\",\n\t\t\tmtextFontInherit: true\n\t\t},\n\t\tTeX: {\n\t\t\textensions: [\"color.js\"],\n\t\t\tMacros: {\n\t\t\t\tlgc: [\"{\\\\color{my-light-green} #1}\", 1],\n\t\t\t\tgc: [\"{\\\\color{my-green} #1}\", 1],\n\t\t\t\tlrc: [\"{\\\\color{my-light-red} #1}\", 1],\n\t\t\t\trc: [\"{\\\\color{my-red} #1}\", 1],\n\t\t\t\tlbc: [\"{\\\\color{my-light-blue} #1}\", 1],\n\t\t\t\tbc: [\"{\\\\color{my-blue} #1}\", 1],\n\t\t\t\tkc: [\"{\\\\color{my-gray} #1}\", 1],\n\t\t\t\tloc: [\"{\\\\color{my-light-orange} #1}\", 1],\n\t\t\t\toc: [\"{\\\\color{my-orange} #1}\", 1],\n\n\t\t\t\ta: [\"\\\\mathbf a\"],\n\t\t\t\tA: [\"\\\\mathbf A\"],\n\t\t\t\tb: [\"\\\\mathbf b\"],\n\t\t\t\tB: [\"\\\\mathbf B\"],\n\t\t\t\tc: [\"\\\\mathbf c\"],\n\t\t\t\tC: [\"\\\\mathbf C\"],\n\t\t\t\td: [\"\\\\mathbf d\"],\n\t\t\t\tD: [\"\\\\mathbf D\"],\n\t\t\t\tE: [\"\\\\mathbf E\"],\n\t\t\t\tI: [\"\\\\mathbf I\"],\n\t\t\t\tL: [\"\\\\mathbf L\"],\n\t\t\t\tm: [\"\\\\mathbf m\"],\n\t\t\t\tM: [\"\\\\mathbf M\"],\n\t\t\t\tr: [\"\\\\mathbf r\"],\n\t\t\t\ts: [\"\\\\mathbf s\"],\n\t\t\t\tt: [\"\\\\mathbf t\"],\n\t\t\t\tS: [\"\\\\mathbf S\"],\n\t\t\t\tx: [\"\\\\mathbf x\"],\n\t\t\t\tz: [\"\\\\mathbf z\"],\n\t\t\t\tv: [\"\\\\mathbf v\"],\n\t\t\t\ty: [\"\\\\mathbf y\"],\n\t\t\t\tk: [\"\\\\mathbf k\"],\n\t\t\t\tbp: [\"\\\\mathbf p\"],\n\t\t\t\tP: [\"\\\\mathbf P\"],\n\t\t\t\tq: [\"\\\\mathbf q\"],\n\t\t\t\tQ: [\"\\\\mathbf Q\"],\n\t\t\t\tr: [\"\\\\mathbf r\"],\n\t\t\t\tR: [\"\\\\mathbf R\"],\n\t\t\t\tSig: [\"\\\\mathbf \\\\Sigma\"],\n\t\t\t\tt: [\"\\\\mathbf t\"],\n\t\t\t\tT: [\"\\\\mathbf T\"],\n\t\t\t\te: [\"\\\\mathbf e\"],\n\t\t\t\tX: [\"\\\\mathbf X\"],\n\t\t\t\tu: [\"\\\\mathbf u\"],\n\t\t\t\tU: [\"\\\\mathbf U\"],\n\t\t\t\tv: [\"\\\\mathbf v\"],\n\t\t\t\tV: [\"\\\\mathbf V\"],\n\t\t\t\tw: [\"\\\\mathbf w\"],\n\t\t\t\tW: [\"\\\\mathbf W\"],\n\t\t\t\tY: [\"\\\\mathbf Y\"],\n\t\t\t\tz: [\"\\\\mathbf z\"],\n\t\t\t\tZ: [\"\\\\mathbf Z\"],\n\t\t\t\tp: [\"\\\\,\\\\text{.}\"],\n\t\t\t\ttab: [\"\\\\hspace{0.7cm}\"],\n\n\t\t\t\tsp: [\"^{\\\\small\\\\prime}\"],\n\n\n\t\t\t\tmR: [\"{\\\\mathbb R}\"],\n\t\t\t\tmC: [\"{\\\\mathbb C}\"],\n\t\t\t\tmN: [\"{\\\\mathbb N}\"],\n\t\t\t\tmZ: [\"{\\\\mathbb Z}\"],\n\n\t\t\t\tdeg: [\"{^\\\\circ}\"],\n\n\n\t\t\t\targmin: [\"\\\\underset{#1}{\\\\text{argmin}}\", 1],\n\t\t\t\targmax: [\"\\\\underset{#1}{\\\\text{argmax}}\", 1],\n\n\t\t\t\tco: [\"\\\\;\\\\text{cos}\"],\n\t\t\t\tsi: [\"\\\\;\\\\text{sin}\"]\n\t\t\t}\n\t\t}\n  \t});\n\n  \tMathJax.Hub.Register.StartupHook(\"TeX color Ready\", function() {\n     \tMathJax.Extension[\"TeX/color\"].colors[\"my-green\"] = '#677d00';\n     \tMathJax.Extension[\"TeX/color\"].colors[\"my-light-green\"] = '#acd373';\n     \tMathJax.Extension[\"TeX/color\"].colors[\"my-red\"] = '#b13e26';\n     \tMathJax.Extension[\"TeX/color\"].colors[\"my-light-red\"] = '#d38473';\n     \tMathJax.Extension[\"TeX/color\"].colors[\"my-blue\"] = '#306693';\n       \tMathJax.Extension[\"TeX/color\"].colors[\"my-light-blue\"] = '#73a7d3';\n       \tMathJax.Extension[\"TeX/color\"].colors[\"my-gray\"] = '#999';\n       \tMathJax.Extension[\"TeX/color\"].colors[\"my-orange\"] = '#E69500';\n       \tMathJax.Extension[\"TeX/color\"].colors[\"my-light-orange\"] = '#FFC353';\n\n\n\t});\n\u003c/script\u003e\n\n\u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js\"\u003e\n\u003c/script\u003e\n\n\u003ch1 id=\"摘要\"\u003e摘要\u003c/h1\u003e\n\n\u003cp\u003e本文介绍 LLaMA，一个包含 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e7B~65B\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e（70~650 亿）\n参数的\u003cstrong\u003e\u003cmark\u003e基础语言模型集\u003c/mark\u003e\u003c/strong\u003e（a collection of foundation language models）。\n我们用\u003cstrong\u003e\u003cmark\u003e数万亿个（trillions of） token\u003c/mark\u003e\u003c/strong\u003e \n训练这些模型，证明了使用\u003cstrong\u003e\u003cmark\u003e公开数据集\u003c/mark\u003e\u003c/strong\u003e就能训练出最先进的模型，\n而并非必须使用专有和私有数据集。特别是，\u003cstrong\u003e\u003cmark\u003eLLaMA-13B 在大多数基准测试中优于 GPT-3（175B）\u003c/mark\u003e\u003c/strong\u003e\n，而 LLaMA-65B 则与最佳模型 Chinchilla-70B 和 PaLM-540B 相当。\n我们已经将所有模型\u003ca href=\"https://github.com/facebookresearch/llama\"\u003e开源\u003c/a\u003e，供社区研究。\u003c/p\u003e\n\n\u003ch1 id=\"1-引言\"\u003e1 引言\u003c/h1\u003e\n\n\u003cp\u003e在大规模文本语料库（massive corpora of texts）上训练的\u003cstrong\u003e\u003cmark\u003e大型语言模型\u003c/mark\u003e\u003c/strong\u003e\n（Large Languages Models, LLM），已经有能力\u003cstrong\u003e\u003cmark\u003e根据给定的文本指令\u003c/mark\u003e\u003c/strong\u003e（textual instructions）\n或示例（a few examples）\u003cstrong\u003e\u003cmark\u003e执行新任务\u003c/mark\u003e\u003c/strong\u003e（Brown 等，2020）。\u003c/p\u003e\n\n\u003cp\u003e这些 \u003cstrong\u003e\u003cmark\u003efew-shot\u003c/mark\u003e\u003c/strong\u003e 属性首先出现在\u003cstrong\u003e\u003cmark\u003e将模型扩展到足够大的规模时\u003c/mark\u003e\u003c/strong\u003e（Kaplan 等，2020），\n在此之后，出现了很多进一步扩展这些模型的工作（Chowdhery 等，2022；Rae 等，2021），\n它们都遵循了这样一个假设：\u003cstrong\u003e\u003cmark\u003e更多的参数将产生更好的性能\u003c/mark\u003e\u003c/strong\u003e。\n然而，Hoffmann 等（2022）的最新工作表明，对于给定的计算预算（compute budget），\n最佳性能并非来自那些最大的模型，而是来自那些\u003cstrong\u003e\u003cmark\u003e在更多数据上训练出来的较小模型\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e“few-shot” 指一个模型有能力根据给定的\u003cstrong\u003e少量\u003c/strong\u003e示例去执行其他的类似任务的能力。译注。\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch2 id=\"11-大模型训练更多参数-vs-更大的数据集\"\u003e1.1 大模型训练：更多参数 vs 更大的数据集\u003c/h2\u003e\n\n\u003cp\u003eHoffmann 等（2022）提出 scaling laws，目标是针对给定的\u003cstrong\u003e\u003cmark\u003e训练\u003c/mark\u003e\u003c/strong\u003e（training）\n计算预算（compute budget），如何最佳地扩展（scale）\u003cstrong\u003e\u003cmark\u003e数据集和模型大小\u003c/mark\u003e\u003c/strong\u003e。\n但是，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e这个模型没有考虑\u003cstrong\u003e\u003cmark\u003e推理\u003c/mark\u003e\u003c/strong\u003e（inference）预算，在提供大规模推理时，这一点尤其重要：\n在这种情况下，给定一个性能目标，我们更想要的是一个\u003cstrong\u003e\u003cmark\u003e推理速度最快\u003c/mark\u003e\u003c/strong\u003e而非训练速度最快的模型。\u003c/li\u003e\n  \u003cli\u003e对于一个给定的性能要求，训练一个\u003cstrong\u003e\u003cmark\u003e大模型\u003c/mark\u003e\u003c/strong\u003e（a large model）可能是一种更便宜的方式；\n但对于最终的\u003cstrong\u003e\u003cmark\u003e推理\u003c/mark\u003e\u003c/strong\u003e来说，\u003cstrong\u003e\u003cmark\u003e较小的模型+更长的训练时间\u003c/mark\u003e\u003c/strong\u003e（a smaller one trained longer）反而更实惠。\n例如，Hoffmann 等（2022）建议用 200B tokens 来训练 10B 模型，但我们发现即使在\n1T 个 token 之后，7B 模型的性能仍在随着 token 的增多而提高。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 id=\"12-llama减少参数增大数据集\"\u003e1.2 LLaMA：减少参数，增大数据集\u003c/h2\u003e\n\n\u003cp\u003e本文的重点是：对于给定的不同推理预算（inference budgets），\n通过\u003cstrong\u003e\u003cmark\u003e使用更多 token 进行训练\u003c/mark\u003e\u003c/strong\u003e的方式（超过业内常用的 token 规模）\n来获得最佳的性能（the best possible performance）。\n由此得到的模型我们称为 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eLLaMA\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e。\nLLaMA 的参数范围在 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e7B ~ 65B\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e，性能则与目前业界最佳的一些大语言模型相当。\n例如，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003eLLaMA-13B\u003c/mark\u003e\u003c/strong\u003e 在大多数基准测试中\u003cstrong\u003e\u003cmark\u003e优于 GPT-3\u003c/mark\u003e\u003c/strong\u003e，\n尽管参数连后者的 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e10%\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 都不到；\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003eLLaMA 可以在单个 GPU 上运行\u003c/mark\u003e\u003c/strong\u003e，\n因此使大模型的获取和研究更容易，而不再只是少数几个大厂的专利；\u003c/li\u003e\n  \u003cli\u003e在高端系列上，LLaMA-65B 也与最佳的大语言模型（如 Chinchilla 或 PaLM-540B）性能相当。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e与 Chinchilla、PaLM、GPT-3 不同，我们\u003cstrong\u003e\u003cmark\u003e只使用公开数据\u003c/mark\u003e\u003c/strong\u003e（publicly available data），\n因此我们的工作是开源兼容的；\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e相比之下，大多数现有模型依赖于不公开或没有文档的数据（not publicly available or undocumented），例如\n“Books–2TB” 和 “Social media conversations”；\u003c/li\u003e\n  \u003cli\u003e也存在一些例外，例如 OPT（Zhang 等，2022）、GPT-NeoX（Black 等，2022）、BLOOM（Scao 等，2022）和 GLM（Zeng 等，2022），\n但它们的性能都无法与 PaLM-62B 或 Chinchilla 相比。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 id=\"13-内容组织\"\u003e1.3 内容组织\u003c/h2\u003e\n\n\u003cp\u003e本文接下来的内容组织如下：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e描述我们对 Transformer 架构（Vaswani 等，2017）所做的改动，以及我们的训练方法:\u003c/li\u003e\n  \u003cli\u003e给出 LLaMA 的性能，基于标准基准测试与其他 LLM 进行比较；\u003c/li\u003e\n  \u003cli\u003e使用 responsible AI 社区的最新基准测试，揭示 LLaMA 模型中存在的一些偏见和毒性（biases and toxicity）。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch1 id=\"2-方法approach\"\u003e2 方法（Approach）\u003c/h1\u003e\n\n\u003cp\u003e我们的\u003cstrong\u003e\u003cmark\u003e训练方法与前人的一些工作\u003c/mark\u003e\u003c/strong\u003e（Brown 等，2020；Chowdhery 等，2022）\u003cstrong\u003e\u003cmark\u003e类似\u003c/mark\u003e\u003c/strong\u003e，\n并受到 Chinchilla scaling laws（Hoffmann 等，2022）的启发。\n我们使用一个标准的 optimizer 在大量文本数据上训练\u003cstrong\u003e\u003cmark\u003e大型 Transformers\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003ch2 id=\"21-预训练数据pre-training-data\"\u003e2.1 预训练数据（Pre-training Data）\u003c/h2\u003e\n\n\u003ch3 id=\"211-数据集\"\u003e2.1.1 数据集\u003c/h3\u003e\n\n\u003cp\u003e训练数据集有几种不同来源，涵盖了多个领域，如表 1 所示。\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e数据集\u003c/td\u003e\n      \u003ctd\u003e占比\u003c/td\u003e\n      \u003ctd\u003e迭代次数（Epochs）\u003c/td\u003e\n      \u003ctd\u003e数据集大小（Disk size）\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003eCommonCrawl\u003c/td\u003e\n      \u003ctd\u003e67.0%\u003c/td\u003e\n      \u003ctd\u003e1.10\u003c/td\u003e\n      \u003ctd\u003e3.3 TB\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003eC4\u003c/td\u003e\n      \u003ctd\u003e15.0%\u003c/td\u003e\n      \u003ctd\u003e1.06\u003c/td\u003e\n      \u003ctd\u003e783 GB\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003eGithub\u003c/td\u003e\n      \u003ctd\u003e4.5%\u003c/td\u003e\n      \u003ctd\u003e0.64\u003c/td\u003e\n      \u003ctd\u003e328 GB\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003eWikipedia\u003c/td\u003e\n      \u003ctd\u003e4.5%\u003c/td\u003e\n      \u003ctd\u003e2.45\u003c/td\u003e\n      \u003ctd\u003e83 GB\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003eBooks\u003c/td\u003e\n      \u003ctd\u003e4.5%\u003c/td\u003e\n      \u003ctd\u003e2.23\u003c/td\u003e\n      \u003ctd\u003e85 GB\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003eArXiv\u003c/td\u003e\n      \u003ctd\u003e2.5%\u003c/td\u003e\n      \u003ctd\u003e1.06\u003c/td\u003e\n      \u003ctd\u003e92 GB\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003eStackExchange\u003c/td\u003e\n      \u003ctd\u003e2.0%\u003c/td\u003e\n      \u003ctd\u003e1.03\u003c/td\u003e\n      \u003ctd\u003e78 GB\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp align=\"center\"\u003e表 1：\u003cmark\u003e预训练数据\u003c/mark\u003e。\u003cbr/\u003e\n其中 epochs 是用 1.4T tokens 预训练时的迭代次数。用 1T tokens 预训练时也是用的这个数据集比例。\n\u003c/p\u003e\n\n\u003cp\u003e这里的数据集大部分都是\u003cstrong\u003e\u003cmark\u003e其他 LLM 训练用过的\u003c/mark\u003e\u003c/strong\u003e，\n但我们只用其中公开可得（publicly available）的部分，并且要保持开源兼容（compatible with open sourcing）。\n因此最后得到的就是一个混合数据集。\u003c/p\u003e\n\n\u003ch4 id=\"english-commoncrawl-67\"\u003eEnglish CommonCrawl [67%]\u003c/h4\u003e\n\n\u003cp\u003e我们使用 CCNet pipeline（Wenzek 等，2020）对 2017~2020 的五个 CommonCrawl dumps 进行预处理。\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e在行级别（line level）上对数据去重，\u003c/li\u003e\n  \u003cli\u003e使用 fastText 线性分类器进行语言识别，去掉非英文网页，\u003c/li\u003e\n  \u003cli\u003e使用 ngram 语言模型过滤掉一些低质量内容。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e此外，我们还训练了一个线性模型，将页面分为两类：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e被 Wikipedia 引用过的网页；\u003c/li\u003e\n  \u003cli\u003e没有被 Wikipedia 引用过的（随机采样网页）；\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e并将第二类丢弃。\u003c/p\u003e\n\n\u003ch4 id=\"c4-15\"\u003eC4 [15%]\u003c/h4\u003e\n\n\u003cp\u003e在前期探索性实验中，我们观察到使用\u003cstrong\u003e\u003cmark\u003e多样化的预处理 CommonCrawl 数据集\u003c/mark\u003e\u003c/strong\u003e可以提高性能。\n因此，我们将公开可用的 C4 数据集（Raffel 等，2020）也包含到了训练数据中。\u003c/p\u003e\n\n\u003cp\u003e对 C4 的预处理也是\u003cstrong\u003e\u003cmark\u003e去重和语言识别\u003c/mark\u003e\u003c/strong\u003e：与 CCNet 的主要区别在于质量过滤（quality filtering），\n主要依赖于启发式方法（heuristics），例如是否存在标点符号或网页中单词和句子的数量。\u003c/p\u003e\n\n\u003ch4 id=\"github-45\"\u003eGithub [4.5%]\u003c/h4\u003e\n\n\u003cp\u003e使用了 Google BigQuery 上公开可用的 GitHub 数据集，但仅保留其中用 Apache、BSD 和 MIT license 的项目。\n此外，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e基于行长度（line length），字母或数字字符（alphanumeric characters）比例等，用启发式方法过滤掉低质量文件；\u003c/li\u003e\n  \u003cli\u003e使用正则表达式删除一些模板段落（boilerplate），例如 headers；\u003c/li\u003e\n  \u003cli\u003e在文件级别上使用精确匹配对得到的数据集进行去重。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch4 id=\"wikipedia-45\"\u003eWikipedia [4.5%]\u003c/h4\u003e\n\n\u003cp\u003e使用了 2022 年 6 月至 8 月的一部分 Wikipedia dumps，\n覆盖 20 种语言（use either the Latin or Cyrillic\nscripts）：bg、ca、cs、da、de、en、es、fr、hr、hu、it、nl、pl、pt、ro、ru、sl、sr、sv、uk。\u003c/p\u003e\n\n\u003cp\u003e删掉了其中的超链接、注释和其他 formatting boilerplate。\u003c/p\u003e\n\n\u003ch4 id=\"gutenberg-and-books3-45\"\u003eGutenberg and Books3 [4.5%]\u003c/h4\u003e\n\n\u003cp\u003e训练数据集中包含两个书籍语料库：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eGutenberg Project：\u003cstrong\u003e\u003cmark\u003e公版书\u003c/mark\u003e\u003c/strong\u003e（public domain books）；\u003c/li\u003e\n  \u003cli\u003eBooks3 section of ThePile（Gao 等，2020）：一个用于训练大语言模型的\u003cstrong\u003e\u003cmark\u003e公开可用\u003c/mark\u003e\u003c/strong\u003e数据集。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e在书级别（book level）去重，内容超过 90% 重复的书会被剔除出去。\u003c/p\u003e\n\n\u003ch4 id=\"arxiv-25\"\u003eArXiv [2.5%]\u003c/h4\u003e\n\n\u003cp\u003e为了让训练数据集包含一定的科学数据（scientific data），我们对一些 arXiv Latex 文件做处理之后加到训练数据集。\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e按照 Lewkowycz 等（2022）的方法，删除了 the first section 之前的所有内容以及参考文献，\u003c/li\u003e\n  \u003cli\u003e从 .tex 文件中删除了注释，\u003c/li\u003e\n  \u003cli\u003e对作者编写的定义和宏（definitions and macros written by users）做了内联展开（inline-expand），使得论文更加一致（increase consistency across papers）。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch4 id=\"stack-exchange-2\"\u003eStack Exchange [2%]\u003c/h4\u003e\n\n\u003cp\u003eStack Exchange 是一个高质量的问答网站，涵盖了从计算机科学到化学等各种领域。\n我们的训练数据集包括了一个 Stack Exchange dump，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e保留其中最大的 28 个网站的数据，\u003c/li\u003e\n  \u003cli\u003e从文本中删除了 HTML tags ，\u003c/li\u003e\n  \u003cli\u003e按分数（从高到低）对答案进行了排序。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 id=\"212-tokenizer分词器\"\u003e2.1.2 Tokenizer（分词器）\u003c/h3\u003e\n\n\u003cp\u003e我们使用 bytepair encoding（BPE）算法（Sennrich 等，2015）对数据进行\ntokenization，算法实现采用的是 Sentence-Piece（Kudo 和 Richardson，2018）。需要\n说明的是，为了 decompose unknown UTF-8 characters，我们将所有 numbers 拆分为单个\ndigits，再 fallback 到 bytes。\u003c/p\u003e\n\n\u003cp\u003e最终，我们的\u003cstrong\u003e\u003cmark\u003e整个训练数据集\u003c/mark\u003e\u003c/strong\u003e在 tokenization 后包含大约 \u003cstrong\u003e\u003cmark\u003e1.4T 个 token\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e对于大多数训练数据，每个 token 在训练期间仅使用一次；\u003c/li\u003e\n  \u003cli\u003e维基百科和书籍是个例外，会被使用两次（two epochs）。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 id=\"22-架构architecture\"\u003e2.2 架构（Architecture）\u003c/h2\u003e\n\n\u003cp\u003e与最近大语言模型的研究趋势一致，我们的网络也\u003cstrong\u003e\u003cmark\u003e基于 Transformer 架构\u003c/mark\u003e\u003c/strong\u003e（Vaswani 等，2017）。\n但做了很多改进，也借鉴了其他模型（例如 PaLM）中的一些技巧。\u003c/p\u003e\n\n\u003ch3 id=\"221-改进\"\u003e2.2.1 改进\u003c/h3\u003e\n\n\u003cp\u003e以下是与原始架构的主要差异，\u003c/p\u003e\n\n\u003ch4 id=\"预归一化pre-normalization受-gpt3-启发\"\u003e预归一化（Pre-normalization）：受 GPT3 启发\u003c/h4\u003e\n\n\u003cp\u003e为了提高\u003cstrong\u003e\u003cmark\u003e训练稳定性\u003c/mark\u003e\u003c/strong\u003e，我们对每个 Transformer sub-layer 的\u003cstrong\u003e\u003cmark\u003e输入\u003c/mark\u003e\u003c/strong\u003e进行归一化，而不是对\u003cstrong\u003e\u003cmark\u003e输出\u003c/mark\u003e\u003c/strong\u003e进行归一化。\n这里使用由 Zhang 和 Sennrich（2019）提出的 RMSNorm 归一化函数。\u003c/p\u003e\n\n\u003ch4 id=\"swiglu-激活函数受-palm-启发\"\u003eSwiGLU 激活函数：受 PaLM 启发\u003c/h4\u003e\n\n\u003cp\u003e用 SwiGLU 激活函数替换 ReLU 非线性，该函数由 Shazeer（2020）提出，目的是\u003cstrong\u003e\u003cmark\u003e提升性能\u003c/mark\u003e\u003c/strong\u003e。\n但我们使用的维度是 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e2/3 * 4d\u003c/code\u003e，而不是 PaLM 中的 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e4d\u003c/code\u003e。\u003c/p\u003e\n\n\u003ch4 id=\"旋转嵌入rotary-embeddings受-gptneo-启发\"\u003e旋转嵌入（Rotary Embeddings）：受 GPTNeo 启发\u003c/h4\u003e\n\n\u003cp\u003e去掉了绝对位置嵌入（absolute positional embeddings），并在每个网络层中添加旋转位置嵌入（rotary positional embeddings，RoPE）。\nRoPE 由 Su 等（2021）提出。\u003c/p\u003e\n\n\u003ch3 id=\"222-不同-llama-模型的超参数\"\u003e2.2.2 不同 LLaMA 模型的超参数\u003c/h3\u003e\n\n\u003cp\u003e不同模型的超参数详细信息见表 2。\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth style=\"text-align: left\"\u003eparams\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003edimension\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003en heads\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003en layers\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003elearning rate\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003ebatch size\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003en tokens\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003e6.7B\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e4096\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e32\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e32\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e3.0e\u003csup\u003e-4\u003c/sup\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e4M\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1.0T\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003e13.0B\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e5120\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e40\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e40\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e3.0e\u003csup\u003e-4\u003c/sup\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e4M\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1.0T\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003e32.5B\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e6656\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e52\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e60\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1.5e\u003csup\u003e-4\u003c/sup\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e4M\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1.4T\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003e65.2B\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e8192\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e64\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e80\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1.5e\u003csup\u003e-4\u003c/sup\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e4M\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1.4T\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp align=\"center\"\u003e表 2：\nModel sizes, architectures, and optimization hyper-parameters.\n\u003c/p\u003e\n\n\u003ch2 id=\"23-优化器optimizer\"\u003e2.3 优化器（Optimizer）\u003c/h2\u003e\n\n\u003cul\u003e\n  \u003cli\u003e使用 AdamW 优化器（Loshchilov 和 Hutter，2017）对模型进行训练，具体超参数：$\\beta_1 = 0.9, \\beta_2 = 0.95$；\u003c/li\u003e\n  \u003cli\u003e使用一个 cosine learning rate schedule，最终的学习率达到了最大学习率的 10％；\u003c/li\u003e\n  \u003cli\u003e使用 0.1 的权重衰减（weight decay）和 1.0 的梯度裁剪（gradient clipping）；\u003c/li\u003e\n  \u003cli\u003e使用 2,000 个 warmup steps，并根据模型大小来调整 learning rate 和 batch size。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 id=\"24-高效实现efficient-implementation提高训练速度\"\u003e2.4 高效实现（Efficient implementation）：提高训练速度\u003c/h2\u003e\n\n\u003cp\u003e我们进行了几项优化来提高模型的训练速度。\u003c/p\u003e\n\n\u003cp\u003e首先，我们使用 \u003cstrong\u003e\u003cmark\u003ecausal multi-head attention\u003c/mark\u003e\u003c/strong\u003e 的一个高效实现来\u003cstrong\u003e\u003cmark\u003e减少内存占用和运行时\u003c/mark\u003e\u003c/strong\u003e。\n这种实现是受 Rabe 和 Staats（2021）的启发，并使用 Dao 等（2022）的反向传播，现在 \u003ca href=\"https://github.com/facebookresearch/xformers\"\u003exformers 库\u003c/a\u003e 中已经提供了。\n优化原理：由于语言建模任务存在因果特性，因此可以不存储注意力权重（attention weights），不计算那些已经被掩码（masked）的 key/query scores。\u003c/p\u003e\n\n\u003cp\u003e为进一步提高训练效率，我们通过 \u003cstrong\u003e\u003cmark\u003echeckpoint\u003c/mark\u003e\u003c/strong\u003e 技术，\n减少了在反向传播期间需要重新计算的激活数量。更具体地说，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e我们保存了计算成本高昂的激活，例如线性层的输出。实现方式是\u003cstrong\u003e\u003cmark\u003e手动实现 Transformer 层的反向函数\u003c/mark\u003e\u003c/strong\u003e，而不用 PyTorch autograd。\u003c/li\u003e\n  \u003cli\u003e如 Korthikanti 等（2022）中提到的，\n为了充分受益于这种优化，我们需要通过模型和序列并行（model and sequence parallelism）来\u003cstrong\u003e\u003cmark\u003e减少模型的内存使用\u003c/mark\u003e\u003c/strong\u003e。\u003c/li\u003e\n  \u003cli\u003e此外，我们还尽可能地 overlap 激活计算和 GPU 之间的网络通信（由于 all_reduce 操作）。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e训练 65B 参数的模型时，我们的代码在 \u003cstrong\u003e\u003cmark\u003e2048 个 A100 80GB GPU\u003c/mark\u003e\u003c/strong\u003e 上能处理约\n\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e380 tokens/second/GPU\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e。这意味着 1.4T token 的数据集上训练大约需要 \u003cstrong\u003e\u003cmark\u003e21 天\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003ch1 id=\"3-主要结果main-results\"\u003e3 主要结果（Main results）\u003c/h1\u003e\n\n\u003cp\u003e参考前人工作（Brown 等，2020），我们测试了\u003cstrong\u003e\u003cmark\u003e零样本（zero-shot）和少样本（few-shot）\u003c/mark\u003e\u003c/strong\u003e两种任务，\n进行总共 20 个基准测试：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e零样本：提供任务的文本描述和一个测试示例。模型可以使用开放式生成（open-ended generation）提供答案，或对提议的答案进行排名（ranks the proposed answers）。\u003c/li\u003e\n  \u003cli\u003e少样本：提供一些（1~64 个）任务示例和一个测试示例。模型将此文本作为输入并生成答案，或对不同选项进行排名（ranks different options）。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e我们将 LLaMA 与其他基础模型进行比较，包括\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e未开源模型（non-publicly available）：GPT-3（Brown 等，2020）、Gopher（Rae 等，2021）、Chinchilla（Hoffmann 等，2022）和 PaLM（Chowdhery 等，2022），\u003c/li\u003e\n  \u003cli\u003e开源模型：OPT 模型（Zhang 等，2022）、GPT-J（Wang 和 Komatsuzaki，2021）和 GPTNeo（Black 等，2022）。\u003c/li\u003e\n  \u003cli\u003e在第 4 节中，我们还将简要比较 LLaMA 与 instruction-tuned 模型，如 OPT-IML（Iyer 等，2022）和 Flan-PaLM（Chung 等，2022）。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e我们在自由形式生成任务（free-form generation）和多项选择（multiple choice）任务上评估 LLaMA。\n多项选择任务的目标是在提供的上下文基础上，从一组给定选项中选择最合适的。我们使用的最合适标准就是可性能最高（highest likelihood）。\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e对于大部分数据集，我们遵循 Gao 等（2021）的方法，使用由完成字符数归一化的可能性（likelihood normalized by the number of characters），\u003c/li\u003e\n  \u003cli\u003e对于少量数据集（OpenBookQA，BoolQ），我们遵循 Brown 等（2020）的方法，根据在“Answer:”上下文中给定的完成可能性（likelihood of the completion given “Answer:” as context），用公式表示就是 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eP(completion|context) / P(completion|\u0026#34;Answer:\u0026#34;)\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 id=\"31-常识推理common-sense-reasoning\"\u003e3.1 常识推理（Common Sense Reasoning）\u003c/h2\u003e\n\n\u003cp\u003e使用下面八个标准的常识推理基准测试：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eBoolQ（Clark 等，2019）\u003c/li\u003e\n  \u003cli\u003ePIQA（Bisk 等，2020）\u003c/li\u003e\n  \u003cli\u003eSIQA（Sap 等，2019）\u003c/li\u003e\n  \u003cli\u003eHellaSwag（Zellers 等，2019）\u003c/li\u003e\n  \u003cli\u003eWinoGrande（Sakaguchi 等，2021）\u003c/li\u003e\n  \u003cli\u003eOpenBookQA（Mihaylov 等，2018）\u003c/li\u003e\n  \u003cli\u003e\u0026amp; 8. ARC easy 和 challenge（Clark 等，2018）\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e这些数据集包括 Cloze 和 Winograd 风格的任务，以及多项选择题。\n与语言建模社区类似，我们使用零样本设置进行评估。\n在表 3 中，我们与各种规模的现有模型进行比较。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/llama-paper/table-3.png\" width=\"75%\" height=\"75%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e表 3：Zero-shot performance on Common Sense Reasoning tasks\u003c/p\u003e\n\n\u003cp\u003e几点说明：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e除了 BoolQ，LLaMA-65B 在其他所有基准测试都优于 Chinchilla-70B。\u003c/li\u003e\n  \u003cli\u003e同样，该模型在除了 BoolQ 和 WinoGrande 之外的所有地方都超过了 PaLM-540B。\u003c/li\u003e\n  \u003cli\u003eLLaMA-13B 模型尽管比 GPT-3 小 90％ 多，但在大多数基准测试中表现比 GPT-3 还好。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 id=\"32-闭卷问答closed-book-question-answering\"\u003e3.2 闭卷问答（Closed-book Question Answering）\u003c/h2\u003e\n\n\u003cp\u003e我们将 LLaMA 与现有的大语言模型进行比较，在两个闭卷问答基准测试：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e自然问题（Kwiatkowski 等，2019）\u003c/li\u003e\n  \u003cli\u003eTriviaQA（Joshi 等，2017）。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e对于这两个基准测试，在相同设置（例如，模型不能访问那些有助于回答问题的文档）下，\n取得了完全相同的性能（exact match performance）。\n表 4 和表 5 分别是在这两个 benchmark 上的结果，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/llama-paper/table-4.png\" width=\"50%\" height=\"50%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e表 4：NaturalQuestions. Exact match performance.\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/llama-paper/table-5.png\" width=\"50%\" height=\"50%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e表 5：TriviaQA. Zero-shot and few-shot exact match performance on the filtered dev set.\u003c/p\u003e\n\n\u003cp\u003e在这两个基准测试中，LLaMA-65B 在零样本和少样本设置中都实现了 state-of-the-arts 的性能。\n更重要的是，LLaMA-13B 在这些基准测试中与 GPT-3 和 Chinchilla 相比也具有竞争力，尽管参数只有后者的 10%~20％（5-10 smaller）。\n在推理场景，\u003cstrong\u003e\u003cmark\u003eLLaMA-13B 能在单个 V100 GPU\u003c/mark\u003e\u003c/strong\u003e 上运行。\u003c/p\u003e\n\n\u003ch2 id=\"33-阅读理解reading-comprehension\"\u003e3.3 阅读理解（Reading Comprehension）\u003c/h2\u003e\n\n\u003cp\u003e阅读理解能力测试基于 “RACE 阅读理解基准测试”（Lai 等，2017）。\n这个数据集是从\u003cstrong\u003e\u003cmark\u003e为中国初中和高中生设计的英文阅读理解考试\u003c/mark\u003e\u003c/strong\u003e中收集的。\n一些设置遵循 Brown 等（2020），测试结果见表 6，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/llama-paper/table-6.png\" width=\"40%\" height=\"40%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e表 6：阅读理解能力测试。Zero-shot accuracy.\u003c/p\u003e\n\n\u003cp\u003e在这些基准测试中，LLaMA-65B 与 PaLM-540B 相当，而 LLaMA-13B 比 GPT-3 好几个百分点。\u003c/p\u003e\n\n\u003ch2 id=\"34-数学推理mathematical-reasoning\"\u003e3.4 数学推理（Mathematical reasoning）\u003c/h2\u003e\n\n\u003cp\u003e在两个数学推理基准测试上评估模型：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eMATH（Hendrycks 等，2021）：一个包含 12K 个\u003cstrong\u003e\u003cmark\u003e初中和高中\u003c/mark\u003e\u003c/strong\u003e数学问题的数据集，LaTeX 格式；\u003c/li\u003e\n  \u003cli\u003eGSM8k（Cobbe 等，2021）：一个\u003cstrong\u003e\u003cmark\u003e初中\u003c/mark\u003e\u003c/strong\u003e数学问题集。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e表 7 比较了 PaLM 和 Minerva（Lewkowycz 等，2022）进行比较。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/llama-paper/table-7.png\" width=\"45%\" height=\"45%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e表 7：量化推理数据集（quantitative reasoning datasets）上的模型性能。\u003cbr/\u003e\nFor majority voting, we use the same setup as Minerva, with k = 256 samples for MATH\nand k = 100 for GSM8k (Minerva 540B uses k = 64 for MATH and and k = 40 for GSM8k). LLaMA-65B\noutperforms Minerva 62B on GSM8k, although it has not been fine-tuned on mathematical data.\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eMinerva 是一系列在 ArXiv 和 Math Web Pages 中提取的 38.5B token 上 finetune 而成的 PaLM 模型，\u003c/li\u003e\n  \u003cli\u003ePaLM 和 LLaMA 都没有在数学数据上进行 finetune 。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003ePaLM 和 Minerva 的性能数字取自 Lewkowycz 等（2022），我们分别用和不用 maj1@k 进行了比较。\nmaj1@k 表示我们为每个问题生成 k 个样本，并进行多数投票（Wang 等，2022）。\u003c/p\u003e\n\n\u003cp\u003e在 GSM8k 上，可以看到 LLaMA-65B 优于 Minerva-62B，尽管它没有在数学数据上进行微调。\u003c/p\u003e\n\n\u003ch2 id=\"35-代码生成code-generation\"\u003e3.5 代码生成（Code generation）\u003c/h2\u003e\n\n\u003cp\u003e评估模型从给出的自然语言描述来生成代码的能力，使用了两个基准测试：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eHumanEval（Chen 等，2021）\u003c/li\u003e\n  \u003cli\u003eMBPP（Austin 等，2021）\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e这两个测试，都是给模型几句关于程序的描述，以及一些输入输出示例。\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eIn HumanEval, it also receives a function signature, and the prompt is formatted as natural\ncode with the textual description and tests in a program that fits the description and satisfies the\ntest cases.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e在表 8 中，我们将 LLaMA 的 pass@1 得分与未在代码上进行微调的现有语言模型进行了比较，即 PaLM 和 LaMDA（Thoppilan 等，2022）。\nPaLM 和 LLaMA 是在包含相似数量的代码 token 的数据集上训练的。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/llama-paper/table-8.png\" width=\"50%\" height=\"50%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e表 8：Model performance for code generation.\nWe report the pass@ score on HumanEval and MBPP.\nHumanEval generations are done in zero-shot and\nMBBP with 3-shot prompts similar to Austin et al.\n(2021). The values marked with  are read from figures\nin Chowdhery et al. (2022).\u003c/p\u003e\n\n\u003cp\u003e如表 8 所示，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e对于类似数量的参数，LLaMA 优于其他一般模型，如 LaMDA 和 PaLM，它们没有专门针对代码进行训练或微调。\u003c/li\u003e\n  \u003cli\u003eLLaMA 具有 13B 参数及以上，在 HumanEval 和 MBPP 上均优于 LaMDA 137B。\u003c/li\u003e\n  \u003cli\u003eLLaMA 65B 也优于 PaLM 62B，即使它的训练时间更长。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e本表中 pass@1 结果是通过 temperature=0.1 采样得到的。\npass@100 和 pass@80 指标是通过 temperature=0.8 获得的。\n我们使用与 Chen 等（2021）相同的方法来获得 pass@k 的无偏估计。\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e通过在代码特定 token 上进行微调，可以提高生成代码的性能。例如，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003ePaLM-Coder（Chowdhery 等，2022）将 PaLM 在 HumanEval 上的 pass@1 分数从 PaLM 的 26.2％提高到 36％。\u003c/li\u003e\n  \u003cli\u003e其他\u003cstrong\u003e\u003cmark\u003e专门针对代码进行训练的模型\u003c/mark\u003e\u003c/strong\u003e在这些任务上也表现比\u003cstrong\u003e\u003cmark\u003e通用模型\u003c/mark\u003e\u003c/strong\u003e更好（Chen 等，2021; Nijkamp 等，2022; Fried 等，2022）。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e在代码 token 上进行微调超出了本文的范围。\u003c/p\u003e\n\n\u003ch2 id=\"36-大规模多任务语言理解massive-multitask-language-understanding\"\u003e3.6 大规模多任务语言理解（Massive Multitask Language Understanding）\u003c/h2\u003e\n\n\u003cp\u003e大规模多任务语言理解基准测试（\u003cstrong\u003e\u003cmark\u003eMMLU\u003c/mark\u003e\u003c/strong\u003e）由 Hendrycks 等（2020）提出，\n包括涵盖人文、STEM 和社会科学等各种知识领域的多项选择题。\n我们在 5-shot 设置下使用基准测试提供的示例来评估我们的模型，结果如表 9 所示，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/llama-paper/table-9.png\" width=\"75%\" height=\"75%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e表 9：Massive Multitask Language Understanding (MMLU). Five-shot accuracy\u003c/p\u003e\n\n\u003cp\u003e可以看到，LLaMA-65B 落后于 Chinchilla-70B 和 PaLM-540B 几个百分点，并且在大部分领域都是如此。\n一个可能的解释是我们在预训练数据中使用了有限数量的书籍和学术论文，即 ArXiv、Gutenberg 和 Books3，总共只有 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e177GB\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e，\n而后两个模型是在多达 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e2TB\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 的书籍上进行训练的。\nGopher、Chinchilla 和 PaLM 使用的大量书籍可能也解释了为什么 Gopher 在这个基准测试中表现优于 GPT-3，而在其他基准测试中表现只是差不多。\u003c/p\u003e\n\n\u003ch2 id=\"37-训练过程中性能的变化\"\u003e3.7 训练过程中性能的变化\u003c/h2\u003e\n\n\u003cp\u003e在训练过程中，我们跟踪了 LLaMA 在一些问题回答和常识基准测试上的性能，如图 2，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/llama-paper/figure-2.png\" width=\"90%\" height=\"90%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e图 2：Evolution of performance on question answering and common sense reasoning during training\u003c/p\u003e\n\n\u003cp\u003e在大多数基准测试中，性能随着 token 数量稳步提高，并与模型的 training perplexity 相关（见图 1）。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/llama-paper/figure-1.png\" width=\"50%\" height=\"50%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e图 1：Training loss over train tokens for the 7B,\n13B, 33B, and 65 models. LLaMA-33B and LLaMA-\n65B were trained on 1.4T tokens. The smaller models\nwere trained on 1.0T tokens. All models are trained\nwith a batch size of 4M tokens.\u003c/p\u003e\n\n\u003cp\u003eSIQA 和 WinoGrande 是例外。\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e特别是在 SIQA 上，我们观察到性能变化很大，这可能表明这个基准测试不可靠；\u003c/li\u003e\n  \u003cli\u003e在 WinoGrande 上，性能与training perplexity的相关性不太好：LLaMA-33B 和 LLaMA-65B 在训练期间的性能相似。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch1 id=\"4-指令微调instruction-finetuning\"\u003e4 指令微调（Instruction Finetuning）\u003c/h1\u003e\n\n\u003cp\u003e在本节中，我们将说明简单地在指令数据上进行微调，就会迅速提高在 MMLU 上的性能。\u003c/p\u003e\n\n\u003cp\u003e尽管 LLaMA-65B 的未微调版本已经能够 follow 基本指令，但我们观察到进行一点微调可以提高在 MMLU 上的性能，\n并能进一步提高模型 follow 指令的能力。\n由于这不是本文的重点，我们只进行了一次实验，遵循 Chung 等（2022）的相同协议来训练一个指令模型 LLaMA-I。\nLLaMA-I 在 MMLU 上的结果见表 10，与当前中等规模的指令微调模型 OPT-IML（Iyer 等，2022）和\nFlan-PaLM 系列（Chung 等，2022）进行了比较：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/llama-paper/table-10.png\" width=\"35%\" height=\"35%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e表 10：Instruction finetuning – MMLU (5-shot).\nComparison of models of moderate size with and without\ninstruction finetuning on MMLU.\u003c/p\u003e\n\n\u003cp\u003e尽管这里使用的指令微调方法很简单，但我们在 MMLU 上达到了 68.9％。\nLLaMA-I（65B）在 MMLU 上优于现有的中等规模指令微调模型，但仍远远落后于最先进的 GPT code-davinci-002 在 MMLU 上的 77.4（数字来自 Iyer 等（2022））。有关 57 个任务的 MMLU 性能详细信息，请参见附录的表 16。\u003c/p\u003e\n\n\u003ch1 id=\"5-bias-toxicity-and-misinformation\"\u003e5 Bias, Toxicity and Misinformation\u003c/h1\u003e\n\n\u003cp\u003eLarge language models have been showed to reproduce\nand amplify biases that are existing in\nthe training data (Sheng et al., 2019; Kurita et al.,\n2019), and to generate toxic or offensive content\n(Gehman et al., 2020). As our training dataset\ncontains a large proportion of data from the Web,\nwe believe that it is crucial to determine the potential\nfor our models to generate such content.\nTo understand the potential harm of LLaMA-65B,\nwe evaluate on different benchmarks that measure\ntoxic content production and stereotypes detection.\nWhile we have selected some of the standard benchmarks\nthat are used by the language model community\nto indicate some of the issues with these\nmodels, these evaluations are not sufficient to fully\nunderstand the risks associated with these models.\u003c/p\u003e\n\n\u003ch2 id=\"51-realtoxicityprompts\"\u003e5.1 RealToxicityPrompts\u003c/h2\u003e\n\n\u003cp\u003eLanguage models can generate toxic language, e.g.,\ninsults, hate speech or threats. There is a very large\nrange of toxic content that a model can generate,\nmaking a thorough evaluation challenging. Several\nrecent work (Zhang et al., 2022; Hoffmann et al.,\n2022) have considered the RealToxicityPrompts\nbenchmark (Gehman et al., 2020) as an indicator\nof how toxic is their model. RealToxicityPrompts\nconsists of about 100k prompts that the model must\ncomplete; then a toxicity score is automatically\nevaluated by making a request to PerspectiveAPI 3.\nWe do not have control over the pipeline used by\nthe third-party PerspectiveAPI, making comparison\nwith previous models difficult.\nFor each of the 100k prompts, we greedily generate\nwith our models, and measure their toxicity\nscore. The score per prompt ranges from 0\n(non-toxic) to 1 (toxic). In Table 11, we report our\naveraged score on basic and respectful prompt categories\nof RealToxicityPrompts. These scores are\n“comparable” with what we observe in the literature\n(e.g., 0.087 for Chinchilla) but the methodologies\ndiffer between these work and ours (in\nterms of sampling strategy, number of prompts and\ntime of API). We observe that toxicity increases\nwith the size of the model, especially for Respectful\nprompts. This was also observed in previous\nwork (Zhang et al., 2022), with the notable exception\nof Hoffmann et al. (2022) where they do not\nsee a difference between Chinchilla and Gopher,\ndespite different sizes. This could be explained by\nthe fact that the larger model, Gopher, has worse\nperformance than Chinchilla, suggesting that the\nrelation between toxicity and model size may only\napply within a model family.\u003c/p\u003e\n\n\u003ch2 id=\"52-crows-pairs\"\u003e5.2 CrowS-Pairs\u003c/h2\u003e\n\n\u003cp\u003eWe evaluate the biases in our model on the CrowSPairs\n(Nangia et al., 2020). This dataset allows to\nmeasure biases in 9 categories: gender, religion,\nrace/color, sexual orientation, age, nationality, disability,\nphysical appearance and socioeconomic status.\nEach example is composed of a stereotype and\nan anti-stereotype, we measure the model preference\nfor the stereotypical sentence using the perplexity\nof both sentences in a zero-shot setting.\nHigher scores thus indicate higher bias. We compare\nwith GPT-3 and OPT-175B in Table 12.\nLLaMA compares slightly favorably to both\nmodels on average. Our model is particularly biased\nin the religion category (+10% compared to\nOPT-175B), followed by age and gender. We expect\nthese biases to come from CommonCrawl despite\nmultiple filtering steps.\u003c/p\u003e\n\n\u003ch2 id=\"53-winogender\"\u003e5.3 WinoGender\u003c/h2\u003e\n\n\u003cp\u003eTo further investigate the biases of our model on\nthe gender category, we look at the WinoGender\nbenchmark (Rudinger et al., 2018), a co-reference\nresolution dataset. WinoGender is made of Winograd\nschema, and biases are evaluated by determining\nif a model co-reference resolution performance\nis impacted by the gender of the pronoun.\nMore precisely, each sentence has three mentions:\nan “occupation”, a “participant”, and a\n“pronoun” where the pronoun is co-referencing\neither the occupation or participant. We prompt\nthe model to determine the co-reference relation\nand measure if it does so correctly according to\nthe context of the sentence. The goal is to reveal\nif societal biases associated with occupations\nhave been captured by the model. For example,\na sentence in the WinoGender dataset is “The\nnurse notified the patient that his shift would be\nending in an hour.”, which is followed by ‘His’\nrefers to. We then compare the perplexity of the\ncontinuations the nurse and the patient to perform\nco-reference resolution with the model. We\nevaluate the performance when using 3 pronouns:\n“her/her/she”, “his/him/he” and “their/them/someone”\n(the different choices corresponding to the\ngrammatical function of the pronoun.\nIn Table 13, we report the co-reference scores\nfor the three different pronouns contained in the\ndataset. We observe that our model is significantly\nbetter at performing co-reference resolution for\nthe “their/them/someone” pronouns than for the\n“her/her/she” and “his/him/he” pronouns. A similar\nobservation was made in previous work (Rae\net al., 2021; Hoffmann et al., 2022), and is likely\nindicative of gender bias. Indeed, in the case of the\n“her/her/she” and “his/him/he” pronouns, the model\nis probably using the majority gender of the occupation\nto perform co-reference resolution, instead\nof using the evidence of the sentence.\nTo further investigate this hypothesis, we look\nat the set of “gotcha” cases for the “her/her/she”\nand “his/him/he” pronouns in the WinoGender\ndataset. Theses cases correspond to sentences in\nwhich the pronoun does not match the majority\ngender of the occupation, and the occupation is\nthe correct answer. In Table 13, we observe that\nour model, LLaMA-65B, makes more errors on the\ngotcha examples, clearly showing that it capture\nsocietal biases related to gender and occupation.\nThe drop of performance exists for “her/her/she”\nand “his/him/he” pronouns, which is indicative of\nbiases regardless of gender.\u003c/p\u003e\n\n\u003cp\u003eIn Table 14, we report the performance of our\nmodels on both questions to measure truthful models\nand the intersection of truthful and informative.\nCompared to GPT-3, our model scores higher in\nboth categories, but the rate of correct answers is\nstill low, showing that our model is likely to hallucinate\nincorrect answers.\u003c/p\u003e\n\n\u003ch2 id=\"54-truthfulqa\"\u003e5.4 TruthfulQA\u003c/h2\u003e\n\n\u003cp\u003eTruthfulQA (Lin et al., 2021) aims to measure the\ntruthfulness of a model, i.e., its ability to identify\nwhen a claim is true. Lin et al. (2021) consider\nthe definition of “true” in the sense of “literal truth\nabout the real world”, and not claims that are only\ntrue in the context of a belief system or tradition.\nThis benchmark can evaluate the risks of a model\nto generate misinformation or false claims. The\nquestions are written in diverse style, cover 38 categories\nand are designed to be adversarial.\u003c/p\u003e\n\n\u003ch1 id=\"6-碳足迹carbon-footprint\"\u003e6 碳足迹（Carbon footprint）\u003c/h1\u003e\n\n\u003cp\u003e训练 LLaMA 消耗了大量能源，排放了很多二氧化碳。我们遵循最近的文献，将总能耗和产生的碳足迹分解在表 15 中，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/llama-paper/table-15.png\" width=\"60%\" height=\"60%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e表 15：\nCarbon footprint of training different models in the same data center. We follow Wu et al. (2022)\nto compute carbon emission of training OPT, BLOOM and our models in the same data center. For the power\nconsumption of a A100-80GB, we take the thermal design power for NVLink systems, that is 400W. We take a\nPUE of 1.1 and a carbon intensity factor set at the national US average of 0.385 kg CO2e per KWh.\n\u003c/p\u003e\n\n\u003cp\u003e我们采用 Wu 等(2022)的公式来估算训练模型所需的\u003cstrong\u003e\u003cmark\u003e瓦时数\u003c/mark\u003e\u003c/strong\u003e（Watt-hour, Wh）和\u003cstrong\u003e\u003cmark\u003e碳排放量\u003c/mark\u003e\u003c/strong\u003e（carbon emissions）。\n对于瓦时数，我们使用以下公式：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cmark\u003e\u003ccode\u003eWh = GPU-h * (GPU power consumption) * PUE\u003c/code\u003e\u003c/mark\u003e\u003c/p\u003e\n\n\u003cp\u003e其中，我们的功率使用效率（PUE）为 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e1.1\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e。\n产生的碳排放量取决于用于训练所在的数据中心的位置。例如，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eBLOOM 使用排放 0.057kg CO\u003csub\u003e2\u003c/sub\u003eeq/KWh 的电网，产生 27 tCO\u003csub\u003e2\u003c/sub\u003eeq 的排放量，\u003c/li\u003e\n  \u003cli\u003eOPT 使用排放 0.231kg CO\u003csub\u003e2\u003c/sub\u003eeq/KWh 的电网，导致 82 tCO\u003csub\u003e2\u003c/sub\u003eeq 的排放量。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e在本研究中，我们感兴趣的是在同一个数据中心的情况下，不同模型训练的碳排放成本。\n因此，我们不考虑数据中心的位置，并使用美国国家平均碳强度系数（carbon intensity factor） 0.385kg CO2eq/KWh。\n那么此时就有，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cmark\u003e\u003ccode\u003etCO\u003csub\u003e2\u003c/sub\u003eeq = MWh * 0:385\u003c/code\u003e\u003c/mark\u003e\u003c/p\u003e\n\n\u003cp\u003e我们对 OPT 和 BLOOM 采用相同的公式进行公平比较。\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e对于 OPT，我们假设训练需要在 992 个 A100-80GB 上进行 34 天（参见他们的日志 4）。\u003c/li\u003e\n  \u003cli\u003e我们在 \u003cstrong\u003e\u003cmark\u003e2048 个 A100 80GB 上，用了约 5 个月\u003c/mark\u003e\u003c/strong\u003e时间来开发 LLaMA。\n根据前面的公式，计算得到 LLaMA 的训练成本约为 2638 MWh，总排放量为 1015 tCO\u003csub\u003e2\u003c/sub\u003eeq。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e我们希望 LLaMA 的发布有助于减少未来的碳排放，因为它训练已经完成（很多情况下大家直接用或者进行微调就行了）:\n而且其中一些小参数模型可以在单个 GPU 上运行。\u003c/p\u003e\n\n\u003ch1 id=\"7-相关工作related-work\"\u003e7 相关工作（Related work）\u003c/h1\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cmark\u003e语言模型\u003c/mark\u003e\u003c/strong\u003e是单词、 token 或字符组成的\u003cstrong\u003e\u003cmark\u003e序列的概率分布\u003c/mark\u003e\u003c/strong\u003e\n（probability distributions over sequences of words, tokens or characters）(Shannon, 1948, 1951)。\u003c/p\u003e\n\n\u003cp\u003e这个任务通常被描述为\u003cstrong\u003e\u003cmark\u003e对下一个 token 的预测\u003c/mark\u003e\u003c/strong\u003e，在自然语言处理（Bahl 等，1983；Brown 等，1990）中很早就是一个核心问题了。\nTuring（1950）提出通过\u003cstrong\u003e\u003cmark\u003e“模仿游戏”\u003c/mark\u003e\u003c/strong\u003e（imitation game），使用语言来衡量机器智能，\n因此\u003cstrong\u003e\u003cmark\u003e语言建模\u003c/mark\u003e\u003c/strong\u003e（language modeling）成为了\u003cstrong\u003e\u003cmark\u003e衡量人工智能进展的基准\u003c/mark\u003e\u003c/strong\u003e（Mahoney，1999）。\u003c/p\u003e\n\n\u003ch2 id=\"71-architecture\"\u003e7.1 Architecture\u003c/h2\u003e\n\n\u003cp\u003eTraditionally, language models\nwere based on n-gram count statistics (Bahl\net al., 1983), and various smoothing techniques\nwere proposed to improve the estimation of rare\nevents (Katz, 1987; Kneser and Ney, 1995). In the\npast two decades, neural networks have been successfully\napplied to the language modelling task,\nstarting from feed forward models (Bengio et al.,\n2000), recurrent neural networks (Elman, 1990;\nMikolov et al., 2010) and LSTMs (Hochreiter and\nSchmidhuber, 1997; Graves, 2013). More recently,\ntransformer networks, based on self-attention, have\nled to important improvements, especially for capturing\nlong range dependencies (Vaswani et al.,\n2017; Radford et al., 2018; Dai et al., 2019).\u003c/p\u003e\n\n\u003ch2 id=\"72-scaling\"\u003e7.2 Scaling\u003c/h2\u003e\n\n\u003cp\u003eThere is a long history of scaling for\nlanguage models, for both the model and dataset\nsizes. Brants et al. (2007) showed the benefits of\nusing language models trained on 2 trillion tokens,\nresulting in 300 billion n-grams, on the quality of\nmachine translation. While this work relied on a\nsimple smoothing technique, called Stupid Backoff,\nHeafield et al. (2013) later showed how to scale\nKneser-Ney smoothing to Web-scale data. This\nallowed to train a 5-gram model on 975 billions tokens\nfrom CommonCrawl, resulting in a model\nwith 500 billions n-grams (Buck et al., 2014).\nChelba et al. (2013) introduced the One Billion\nWord benchmark, a large scale training dataset to\nmeasure the progress of language models.\u003c/p\u003e\n\n\u003cp\u003eIn the context of neural language models, Jozefowicz\net al. (2016) obtained state-of-the-art results\non the Billion Word benchmark by scaling\nLSTMs to 1 billion parameters. Later, scaling\ntransformers lead to improvement on many NLP\ntasks. Notable models include BERT (Devlin et al.,\n2018), GPT-2 (Radford et al., 2019), Megatron-\nLM (Shoeybi et al., 2019), and T5 (Raffel et al.,\n2020). A significant breakthrough was obtained\nwith GPT-3 (Brown et al., 2020), a model with\n175 billion parameters. This lead to a series of\nLarge Language Models, such as Jurassic-1 (Lieber\net al., 2021), Megatron-Turing NLG (Smith et al.,\n2022), Gopher (Rae et al., 2021), Chinchilla (Hoffmann\net al., 2022), PaLM (Chowdhery et al., 2022),\nOPT (Zhang et al., 2022), and GLM (Zeng et al.,\n2022). Hestness et al. (2017) and Rosenfeld et al.\n(2019) studied the impact of scaling on the performance\nof deep learning models, showing the existence\nof power laws between the model and dataset\nsizes and the performance of the system. Kaplan\net al. (2020) derived power laws specifically for\ntransformer based language models, which were\nlater refined by Hoffmann et al. (2022), by adapting\nthe learning rate schedule when scaling datasets.\nFinally, Wei et al. (2022) studied the effect of scaling\non the abilities of large language models.\u003c/p\u003e\n\n\u003ch1 id=\"8-总结\"\u003e8 总结\u003c/h1\u003e\n\n\u003cp\u003eIn this paper, we presented a series of language\nmodels that are released openly, and competitive\nwith state-of-the-art foundation models. Most\nnotably, LLaMA-13B outperforms GPT-3 while\nbeing more than 10 smaller, and LLaMA-65B is\ncompetitive with Chinchilla-70B and PaLM-540B.\nUnlike previous studies, we show that it is possible\nto achieve state-of-the-art performance by training\nexclusively on publicly available data, without\nresorting to proprietary datasets. We hope that\nreleasing these models to the research community\nwill accelerate the development of large language\nmodels, and help efforts to improve their robustness\nand mitigate known issues such as toxicity and\nbias. Additionally, we observed like Chung et al.\n(2022) that finetuning these models on instructions\nlead to promising results, and we plan to further\ninvestigate this in future work. Finally, we plan to\nrelease larger models trained on larger pretraining\ncorpora in the future, since we have seen a constant\nimprovement in performance as we were scaling.\u003c/p\u003e\n\n\u003ch1 id=\"致谢\"\u003e致谢\u003c/h1\u003e\n\n\u003cp\u003eWe thank Daniel Haziza, Francisco Massa, Jeremy Reizenstein, Artem Korenev, and\nPatrick Labatut from the xformers team. We thank Susan Zhang and Stephen Roller\nfor their support on data deduplication. We thank Luca Wehrstedt, Vegard Mella,\nand Pierre-Emmanuel Mazaré for their support on training stability. We thank\nShubho Sengupta, Kalyan Saladi, and all the AI infra team for their support. We\nthank Jane Yu for her input on evaluation. We thank Yongyi Hu for his help on\ndata collection.\u003c/p\u003e\n\n\u003ch1 id=\"参考文献\"\u003e参考文献\u003c/h1\u003e\n\n\u003col\u003e\n  \u003cli\u003eJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. 2021. Program synthesis with large language models.\u003c/li\u003e\n  \u003cli\u003eLalit R Bahl, Frederick Jelinek, and Robert L Mercer.  1983. A maximum likelihood approach to continuous speech recognition. IEEE transactions on pattern analysis and machine intelligence, pages 179– 190.\u003c/li\u003e\n  \u003cli\u003eYoshua Bengio, Réjean Ducharme, and Pascal Vincent.  2000. A neural probabilistic language model. Advances in neural information processing systems, 13.\u003c/li\u003e\n  \u003cli\u003eYonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. 2020. Piqa: Reasoning about physical commonsense in natural language. In Proceedings of the AAAI conference on artificial intelligence, pages 7432–7439.\u003c/li\u003e\n  \u003cli\u003eSid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. 2022.  Gpt-neox-20b: An open-source autoregressive language model. arXiv preprint arXiv:2204.06745.\u003c/li\u003e\n  \u003cli\u003eThorsten Brants, Ashok C. Popat, Peng Xu, Franz J.  Och, and Jeffrey Dean. 2007. Large language models in machine translation. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 858–867, Prague, Czech Republic. Association for Computational Linguistics.\u003c/li\u003e\n  \u003cli\u003ePeter F Brown, John Cocke, Stephen A Della Pietra, Vincent J Della Pietra, Frederick Jelinek, John Lafferty, Robert L Mercer, and Paul S Roossin. 1990. A statistical approach to machine translation. Computational linguistics, 16(2):79–85.\u003c/li\u003e\n  \u003cli\u003eTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam Mc- Candlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners.\u003c/li\u003e\n  \u003cli\u003eChristian Buck, Kenneth Heafield, and Bas Van Ooyen.  2014. N-gram counts and language models from the common crawl. In LREC, volume 2, page 4.\u003c/li\u003e\n  \u003cli\u003eCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and Tony Robinson.  2013. One billion word benchmark for measuring progress in statistical language modeling. arXiv preprint arXiv:1312.3005.\u003c/li\u003e\n  \u003cli\u003eMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N.  Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code.\u003c/li\u003e\n  \u003cli\u003eAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022.  Palm: Scaling language modeling with pathways.\u003c/li\u003e\n  \u003cli\u003eHyung Won Chung, Le Hou, S. Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Wei Yu, Vincent Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed Huai hsin Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc Le, and Jason Wei. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416.\u003c/li\u003e\n  \u003cli\u003eChristopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. 2019. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044.\u003c/li\u003e\n  \u003cli\u003ePeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering?  try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457.\u003c/li\u003e\n  \u003cli\u003eKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.\u003c/li\u003e\n  \u003cli\u003eZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov.  2019. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860.\u003c/li\u003e\n  \u003cli\u003eTri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, and Christopher Ré. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness.  arXiv preprint arXiv:2205.14135.  Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding.  arXiv preprint arXiv:1810.04805.\u003c/li\u003e\n  \u003cli\u003eJeffrey L Elman. 1990. Finding structure in time. Cognitive science, 14(2):179–211.\u003c/li\u003e\n  \u003cli\u003eDaniel Fried, Armen Aghajanyan, Jessy Lin, Sida Wang, Eric Wallace, Freda Shi, Ruiqi Zhong, Wentau Yih, Luke Zettlemoyer, and Mike Lewis. 2022.  Incoder: A generative model for code infilling and synthesis. arXiv preprint arXiv:2204.05999.\u003c/li\u003e\n  \u003cli\u003eLeo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020. The Pile: An 800gb dataset of diverse text for language modeling.  arXiv preprint arXiv:2101.00027.\u003c/li\u003e\n  \u003cli\u003eLeo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, Jason Phang, Laria Reynolds, Eric Tang, Anish Thite, BenWang, KevinWang, and Andy Zou. 2021.  A framework for few-shot language model evaluation.\u003c/li\u003e\n  \u003cli\u003eSamuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A Smith. 2020. Realtoxicityprompts: Evaluating neural toxic degeneration in language models. arXiv preprint arXiv:2009.11462.\u003c/li\u003e\n  \u003cli\u003eAlex Graves. 2013. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850.\u003c/li\u003e\n  \u003cli\u003eKenneth Heafield, Ivan Pouzyrevsky, Jonathan H Clark, and Philipp Koehn. 2013. Scalable modified kneserney language model estimation. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 690–696.\u003c/li\u003e\n  \u003cli\u003eDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.  2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300.\u003c/li\u003e\n  \u003cli\u003eDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874.\u003c/li\u003e\n  \u003cli\u003eJoel Hestness, Sharan Narang, Newsha Ardalani, Gregory Diamos, Heewoo Jun, Hassan Kianinejad, Md Patwary, Mostofa Ali, Yang Yang, and Yanqi Zhou. 2017. Deep learning scaling is predictable, empirically. arXiv preprint arXiv:1712.00409.\u003c/li\u003e\n  \u003cli\u003eSepp Hochreiter and Jürgen Schmidhuber. 1997.  Long short-term memory. Neural computation, 9(8):1735–1780.\u003c/li\u003e\n  \u003cli\u003eJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and Laurent Sifre. 2022. Training compute-optimal large language models.\u003c/li\u003e\n  \u003cli\u003eSrinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Dániel Simig, Ping Yu, Kurt Shuster, TianluWang, Qing Liu, Punit Singh Koura, et al.  2022. Opt-iml: Scaling language model instruction meta learning through the lens of generalization.  arXiv preprint arXiv:2212.12017.\u003c/li\u003e\n  \u003cli\u003eMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension.  arXiv preprint arXiv:1705.03551.\u003c/li\u003e\n  \u003cli\u003eRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. 2016. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410.\u003c/li\u003e\n  \u003cli\u003eJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei.  2020. Scaling laws for neural language models.  arXiv preprint arXiv:2001.08361.\u003c/li\u003e\n  \u003cli\u003eSlava Katz. 1987. Estimation of probabilities from sparse data for the language model component of a speech recognizer. IEEE transactions on acoustics, speech, and signal processing, 35(3):400–401.\u003c/li\u003e\n  \u003cli\u003eReinhard Kneser and Hermann Ney. 1995. Improved backing-off for m-gram language modeling. In 1995 international conference on acoustics, speech, and signal processing, volume 1, pages 181–184. IEEE.\u003c/li\u003e\n  \u003cli\u003eVijay Korthikanti, Jared Casper, Sangkug Lym, Lawrence McAfee, Michael Andersch, Mohammad Shoeybi, and Bryan Catanzaro. 2022. Reducing activation recomputation in large transformer models.  arXiv preprint arXiv:2205.05198.\u003c/li\u003e\n  \u003cli\u003eTaku Kudo and John Richardson. 2018. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing.  arXiv preprint arXiv:1808.06226.\u003c/li\u003e\n  \u003cli\u003eKeita Kurita, Nidhi Vyas, Ayush Pareek, AlanWBlack, and Yulia Tsvetkov. 2019. Quantifying social biases in contextual word representations. In 1st ACL Workshop on Gender Bias for Natural Language Processing.\u003c/li\u003e\n  \u003cli\u003eTom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. 2019. Natural questions: a benchmark for question answering research. Transactions of the Association for Computational Linguistics, 7:453–466.\u003c/li\u003e\n  \u003cli\u003eGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. 2017. Race: Large-scale reading comprehension dataset from examinations. arXiv preprint arXiv:1704.04683.\u003c/li\u003e\n  \u003cli\u003eAitor Lewkowycz, Anders Johan Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. 2022. Solving quantitative reasoning problems with language models. In Advances in Neural Information Processing Systems.\u003c/li\u003e\n  \u003cli\u003eOpher Lieber, Or Sharir, Barak Lenz, and Yoav Shoham. 2021. Jurassic-1: Technical details and evaluation. White Paper. AI21 Labs, 1.  Stephanie Lin, Jacob Hilton, and Owain Evans. 2021.  Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958.\u003c/li\u003e\n  \u003cli\u003eIlya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101.  Matthew V Mahoney. 1999. Text compression as a test for artificial intelligence. AAAI/IAAI, 970.\u003c/li\u003e\n  \u003cli\u003eTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a suit of armor conduct electricity?  a new dataset for open book question answering.  arXiv preprint arXiv:1809.02789.\u003c/li\u003e\n  \u003cli\u003eTomas Mikolov, Martin Karafiát, Lukas Burget, Jan Cernocky, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Interspeech, pages 1045–1048. Makuhari.\u003c/li\u003e\n  \u003cli\u003eNikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R. Bowman. 2020. CrowS-pairs: A challenge dataset for measuring social biases in masked language models. In EMNLP 2020.\u003c/li\u003e\n  \u003cli\u003eErik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2022. Codegen: An open large language model for code with multi-turn program synthesis.  arXiv preprint arXiv:2203.13474.\u003c/li\u003e\n  \u003cli\u003eLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, CarrollWainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022.  Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems.\u003c/li\u003e\n  \u003cli\u003eMarkus N Rabe and Charles Staats. 2021. Selfattention does not need o(n2) memory. arXiv preprint arXiv:2112.05682.\u003c/li\u003e\n  \u003cli\u003eAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training.\u003c/li\u003e\n  \u003cli\u003eAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners.  OpenAI blog, 1(8):9.\u003c/li\u003e\n  \u003cli\u003eJack W. Rae, Sebastian Borgeaud, Trevor Cai, Katie Millican, Jordan Hoffmann, Francis Song, John Aslanides, Sarah Henderson, Roman Ring, Susannah Young, Eliza Rutherford, Tom Hennigan, Jacob Menick, Albin Cassirer, Richard Powell, George van den Driessche, Lisa Anne Hendricks, Maribeth Rauh, Po-Sen Huang, Amelia Glaese, Johannes Welbl, Sumanth Dathathri, Saffron Huang, Jonathan Uesato, John Mellor, Irina Higgins, Antonia Creswell, Nat McAleese, Amy Wu, Erich Elsen, Siddhant Jayakumar, Elena Buchatskaya, David Budden, Esme Sutherland, Karen Simonyan, Michela Paganini, Laurent Sifre, Lena Martens, Xiang Lorraine Li, Adhiguna Kuncoro, Aida Nematzadeh,\nElena Gribovskaya, Domenic Donato, Angeliki Lazaridou, Arthur Mensch, Jean-Baptiste Lespiau, Maria Tsimpoukelli, Nikolai Grigorev, Doug Fritz, Thibault Sottiaux, Mantas Pajarskas, Toby Pohlen, Zhitao Gong, Daniel Toyama, Cyprien de Masson d’Autume, Yujia Li, Tayfun Terzi, Vladimir Mikulik, Igor Babuschkin, Aidan Clark, Diego de Las Casas, Aurelia Guy, Chris Jones, James Bradbury, Matthew Johnson, Blake Hechtman, LauraWeidinger, Iason Gabriel,William Isaac, Ed Lockhart, Simon Osindero, Laura Rimell, Chris Dyer, Oriol Vinyals, Kareem Ayoub, Jeff Stanway, Lorrayne Bennett, Demis Hassabis, Koray Kavukcuoglu, and Geoffrey Irving. 2021. Scaling language models: Methods, analysis \u0026amp; insights from training gopher.\u003c/li\u003e\n  \u003cli\u003eColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer.  The Journal of Machine Learning Research, 21(1):5485–5551.\u003c/li\u003e\n  \u003cli\u003eJonathan S Rosenfeld, Amir Rosenfeld, Yonatan Belinkov, and Nir Shavit. 2019. A constructive prediction of the generalization error across scales. arXiv preprint arXiv:1909.12673.\u003c/li\u003e\n  \u003cli\u003eRachel Rudinger, Jason Naradowsky, Brian Leonard, and Benjamin Van Durme. 2018. Gender bias in coreference resolution. In NAACL-HLT 2018.\u003c/li\u003e\n  \u003cli\u003eKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99–106.\u003c/li\u003e\n  \u003cli\u003eMaarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. 2019. Socialiqa: Commonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728.\u003c/li\u003e\n  \u003cli\u003eTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili´c, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, Matthias Gallé, et al. 2022. Bloom: A 176bparameter open-access multilingual language model.  arXiv preprint arXiv:2211.05100.\u003c/li\u003e\n  \u003cli\u003eRico Sennrich, Barry Haddow, and Alexandra Birch.  2015. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909.\u003c/li\u003e\n  \u003cli\u003eClaude E Shannon. 1948. A mathematical theory of communication. The Bell system technical journal, 27(3):379–423.\u003c/li\u003e\n  \u003cli\u003eClaude E Shannon. 1951. Prediction and entropy of printed english. Bell system technical journal, 30(1):50–64.\u003c/li\u003e\n  \u003cli\u003eNoam Shazeer. 2020. Glu variants improve transformer.  arXiv preprint arXiv:2002.05202.\u003c/li\u003e\n  \u003cli\u003eEmily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. 2019. The woman worked as a babysitter: On biases in language generation. arXiv preprint arXiv:1909.01326.\u003c/li\u003e\n  \u003cli\u003eMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro.  2019. Megatron-lm: Training multi-billion parameter language models using model parallelism.  arXiv preprint arXiv:1909.08053.\u003c/li\u003e\n  \u003cli\u003eShaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared Casper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, Elton Zhang, Rewon Child, Reza Yazdani Aminabadi, Julie Bernauer, Xia Song, Mohammad Shoeybi, Yuxiong He, Michael Houston, Saurabh Tiwary, and Bryan Catanzaro.  2022. Using deepspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model.\u003c/li\u003e\n  \u003cli\u003eJianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. 2021. Roformer: Enhanced transformer with rotary position embedding.  arXiv preprint arXiv:2104.09864.\u003c/li\u003e\n  \u003cli\u003eRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, YaGuang Li, Hongrae Lee, Huaixiu Steven Zheng, Amin Ghafouri, Marcelo Menegali, Yanping Huang, Maxim Krikun, Dmitry Lepikhin, James Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts, Maarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-Ching Chang, Igor Krivokon, Will Rusch, Marc Pickett, Pranesh Srinivasan, Laichee Man, Kathleen Meier-Hellstern, Meredith Ringel Morris, Tulsee Doshi, Renelito Delos Santos, Toju Duke, Johnny Soraker, Ben Zevenbergen, Vinodkumar Prabhakaran, Mark Diaz, Ben Hutchinson, Kristen Olson, Alejandra Molina, Erin Hoffman-John, Josh Lee, Lora Aroyo, Ravi Rajakumar, Alena Butryna, Matthew Lamm, Viktoriya Kuzmina, Joe Fenton, Aaron Cohen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-Arcas, Claire Cui, Marian Croak, Ed Chi, and Quoc Le. 2022. Lamda: Language models for dialog applications.\u003c/li\u003e\n  \u003cli\u003eA. M. Turing. 1950. Computing Machinery and Intelligence.  [Oxford University Press, Mind Association].\u003c/li\u003e\n  \u003cli\u003eAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. \u003cstrong\u003e\u003cmark\u003eAttention is all you need\u003c/mark\u003e\u003c/strong\u003e. In Advances in Neural Information Processing Systems 30, pages 5998–6008.\u003c/li\u003e\n  \u003cli\u003eBen Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax.\u003c/li\u003e\n  \u003cli\u003eXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2022. Self-consistency improves chain of thought reasoning in language models.\u003c/li\u003e\n  \u003cli\u003eJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al.  2022. Emergent abilities of large language models.  arXiv preprint arXiv:2206.07682.\u003c/li\u003e\n  \u003cli\u003eGuillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Edouard Grave. 2020. CCNet: Extracting high quality monolingual datasets from web crawl data. In Language Resources and Evaluation Conference.\u003c/li\u003e\n  \u003cli\u003eCarole-Jean Wu, Ramya Raghavendra, Udit Gupta, Bilge Acun, Newsha Ardalani, Kiwan Maeng, Gloria Chang, Fiona Aga, Jinshi Huang, Charles Bai, et al. 2022. Sustainable ai: Environmental implications, challenges and opportunities. Proceedings of Machine Learning and Systems, 4:795–813.\u003c/li\u003e\n  \u003cli\u003eRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830.\u003c/li\u003e\n  \u003cli\u003eAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Peng Zhang, Yuxiao Dong, and Jie Tang. 2022. Glm-130b: An open bilingual pre-trained model.\u003c/li\u003e\n  \u003cli\u003eBiao Zhang and Rico Sennrich. 2019. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32.\u003c/li\u003e\n  \u003cli\u003eSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al.  2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068.\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch1 id=\"附录略\"\u003e附录（略）\u003c/h1\u003e\n\n\u003cp\u003e见原文。\u003c/p\u003e\n\n\n  \u003c!-- POST NAVIGATION --\u003e\n  \u003cdiv class=\"postNav clearfix\"\u003e\n     \n      \u003ca class=\"prev\" href=\"/blog/writing-is-magic-zh/\"\u003e\u003cspan\u003e« [译] 长文写作的魔力（2022）\u003c/span\u003e\n      \n    \u003c/a\u003e\n      \n      \n      \u003ca class=\"next\" href=\"/blog/llm-practical-guide-zh/\"\u003e\u003cspan\u003e[译][论文] 大语言模型（LLM）综述与实用指南（Amazon，2023） »\u003c/span\u003e\n       \n      \u003c/a\u003e\n     \n  \u003c/div\u003e\n\u003c/div\u003e",
  "Date": "2023-07-10T00:00:00Z",
  "Author": "Arthur Chiao"
}