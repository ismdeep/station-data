{
  "Source": "arthurchiao.art",
  "Title": "Ctrip Network Architecture Evolution in the Cloud Computing Era",
  "Link": "https://arthurchiao.art/blog/ctrip-network-arch-evolution/",
  "Content": "\u003cdiv class=\"post\"\u003e\n  \n  \u003ch1 class=\"postTitle\"\u003eCtrip Network Architecture Evolution in the Cloud Computing Era\u003c/h1\u003e\n  \u003cp class=\"meta\"\u003ePublished at 2019-04-17 | Last Update 2019-05-05\u003c/p\u003e\n  \n  \u003ch2 id=\"preface\"\u003ePreface\u003c/h2\u003e\n\n\u003cp\u003eThis article comes from my talk \u003cstrong\u003e\u003cem\u003eCtrip Network Architecture Evolution in the\nCloud Computing Era\u003c/em\u003e\u003c/strong\u003e in \u003ca href=\"https://www.bagevent.com/event/GOPS2019-shenzhen\"\u003eGOPS 2019 Shenzhen\u003c/a\u003e\n(a tech conference in Chinese).\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e中文版：\u003ca href=\"/blog/ctrip-network-arch-evolution/\"\u003e云计算时代携程的网络架构变迁\u003c/a\u003e。\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003chr/\u003e\n\n\u003cul id=\"markdown-toc\"\u003e\n  \u003cli\u003e\u003ca href=\"#preface\" id=\"markdown-toc-preface\"\u003ePreface\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#about-me\" id=\"markdown-toc-about-me\"\u003eAbout Me\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#0-about-ctrip-cloud\" id=\"markdown-toc-0-about-ctrip-cloud\"\u003e0 About Ctrip Cloud\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#network-evolution-timeline\" id=\"markdown-toc-network-evolution-timeline\"\u003eNetwork Evolution Timeline\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#1-vlan-based-l2-network\" id=\"markdown-toc-1-vlan-based-l2-network\"\u003e1 VLAN-based L2 Network\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#11-requirements\" id=\"markdown-toc-11-requirements\"\u003e1.1 Requirements\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#12-solution-openstack-provider-network-model\" id=\"markdown-toc-12-solution-openstack-provider-network-model\"\u003e1.2 Solution: OpenStack Provider Network Model\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#13-hw-network-topology\" id=\"markdown-toc-13-hw-network-topology\"\u003e1.3 HW Network Topology\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#14-host-network-topology\" id=\"markdown-toc-14-host-network-topology\"\u003e1.4 Host Network Topology\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#15-summary\" id=\"markdown-toc-15-summary\"\u003e1.5 Summary\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#advantages\" id=\"markdown-toc-advantages\"\u003eAdvantages\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#disadvantags\" id=\"markdown-toc-disadvantags\"\u003eDisadvantags\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#2-sdn-based-large-l2-network\" id=\"markdown-toc-2-sdn-based-large-l2-network\"\u003e2 SDN-based Large L2 Network\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#21-new-challenges\" id=\"markdown-toc-21-new-challenges\"\u003e2.1 New Challenges\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#22-solution-openstack--sdn\" id=\"markdown-toc-22-solution-openstack--sdn\"\u003e2.2 Solution: OpenStack + SDN\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#hw-topology\" id=\"markdown-toc-hw-topology\"\u003eHW Topology\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#sdn-control-and-data-plane\" id=\"markdown-toc-sdn-control-and-data-plane\"\u003eSDN: Control and Data Plane\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#sdn-components-and-implementation\" id=\"markdown-toc-sdn-components-and-implementation\"\u003eSDN: Components And Implementation\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#23-hw--sw-topology\" id=\"markdown-toc-23-hw--sw-topology\"\u003e2.3 HW + SW Topology\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#24-spawn-an-instance\" id=\"markdown-toc-24-spawn-an-instance\"\u003e2.4 Spawn An Instance\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#25-summary\" id=\"markdown-toc-25-summary\"\u003e2.5 Summary\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#hw\" id=\"markdown-toc-hw\"\u003eHW\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#sw\" id=\"markdown-toc-sw\"\u003eSW\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#multi-tenancy--vpc-support\" id=\"markdown-toc-multi-tenancy--vpc-support\"\u003eMulti-tenancy \u0026amp; VPC support\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#3-k8s--hybrid-cloud-network\" id=\"markdown-toc-3-k8s--hybrid-cloud-network\"\u003e3 K8S \u0026amp; Hybrid Cloud Network\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#31-k8s-network-in-private-cloud\" id=\"markdown-toc-31-k8s-network-in-private-cloud\"\u003e3.1 K8S Network In Private Cloud\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#311-network-requirements\" id=\"markdown-toc-311-network-requirements\"\u003e3.1.1 Network Requirements\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#312-solution-extend-sdn-to-support-mesosk8s\" id=\"markdown-toc-312-solution-extend-sdn-to-support-mesosk8s\"\u003e3.1.2 Solution: Extend SDN to Support Mesos/K8S\u003c/a\u003e            \u003cul\u003e\n              \u003cli\u003e\u003ca href=\"#neutron-changes\" id=\"markdown-toc-neutron-changes\"\u003eNeutron Changes\u003c/a\u003e\u003c/li\u003e\n              \u003cli\u003e\u003ca href=\"#new-k8s-cni-plugin-for-neutron\" id=\"markdown-toc-new-k8s-cni-plugin-for-neutron\"\u003eNew K8S CNI plugin for neutron\u003c/a\u003e\u003c/li\u003e\n              \u003cli\u003e\u003ca href=\"#existing-network-servicescomponents-upgrade\" id=\"markdown-toc-existing-network-servicescomponents-upgrade\"\u003eExisting network services/components upgrade\u003c/a\u003e\u003c/li\u003e\n            \u003c/ul\u003e\n          \u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#313-pod-drifting\" id=\"markdown-toc-313-pod-drifting\"\u003e3.1.3 Pod Drifting\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#314-summary\" id=\"markdown-toc-314-summary\"\u003e3.1.4 Summary\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#315-future-architecture\" id=\"markdown-toc-315-future-architecture\"\u003e3.1.5 Future Architecture\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#32-k8s-on-public-cloud\" id=\"markdown-toc-32-k8s-on-public-cloud\"\u003e3.2 K8S on Public Cloud\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#321-requirements\" id=\"markdown-toc-321-requirements\"\u003e3.2.1 Requirements\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#322-k8s-network-solution-on-aws\" id=\"markdown-toc-322-k8s-network-solution-on-aws\"\u003e3.2.2 K8S Network Solution on AWS\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#323-vpcs-over-the-globe\" id=\"markdown-toc-323-vpcs-over-the-globe\"\u003e3.2.3 VPCs over the globe\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#4-cloud-native-solutions\" id=\"markdown-toc-4-cloud-native-solutions\"\u003e4 Cloud Native Solutions\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#41-cilium-overview\" id=\"markdown-toc-41-cilium-overview\"\u003e4.1 Cilium Overview\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#42-host-networking\" id=\"markdown-toc-42-host-networking\"\u003e4.2 Host Networking\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#43-multi-host-networking\" id=\"markdown-toc-43-multi-host-networking\"\u003e4.3 Multi-host networking\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#44-pros--cons\" id=\"markdown-toc-44-pros--cons\"\u003e4.4 Pros \u0026amp; Cons\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#pros\" id=\"markdown-toc-pros\"\u003ePros\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#cons\" id=\"markdown-toc-cons\"\u003eCons\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#references\" id=\"markdown-toc-references\"\u003eReferences\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003chr/\u003e\n\n\u003ch2 id=\"about-me\"\u003eAbout Me\u003c/h2\u003e\n\n\u003cp\u003eI’m a senior achitect at Ctrip cloud, currently lead the network \u0026amp; storage\ndevelopment team, focusing on \u003cstrong\u003enetwork virtualization\u003c/strong\u003e and \u003cstrong\u003edistributed\nstorage\u003c/strong\u003e.\u003c/p\u003e\n\n\u003ch1 id=\"0-about-ctrip-cloud\"\u003e0 About Ctrip Cloud\u003c/h1\u003e\n\n\u003cp\u003eCtrip’s cloud computing team started at ~2013.\u003c/p\u003e\n\n\u003cp\u003eWe started our business by providing compute resources to our internal customers\nbased on OpenStack. Since then, we have developed our own baremetal platform,\nand further, deployed container platforms like Mesos and K8S.\u003c/p\u003e\n\n\u003cp\u003eIn recent years, we have packed all our cloud services into a unified platform,\nwe named it CDOS - \u003cstrong\u003e\u003cem\u003eCtrip Datacenter Operating System\u003c/em\u003e\u003c/strong\u003e.\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/ctrip-net-evolution/1.jpg\" width=\"40%\" height=\"40%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 1. Ctrip Datacenter Operation System (CDOS)\u003c/p\u003e\n\n\u003cp\u003eCDOS manages all our compute, network and storage resources on both private and\npublic cloud (from vendors). In the private cloud, we provision VM, BM and container\ninstances. In the public cloud, we have integrated public cloud vendors like\nAWS, Tecent cloud, UCloud, etc to provide VM and container instances to our internal\ncustomers.\u003c/p\u003e\n\n\u003ch2 id=\"network-evolution-timeline\"\u003eNetwork Evolution Timeline\u003c/h2\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/ctrip-net-evolution/2.jpg\" width=\"70%\" height=\"70%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 2. Timeline of the Network Architecture Evolution\u003c/p\u003e\n\n\u003cp\u003eFig 2 is a rough timeline of our network evolution.\u003c/p\u003e\n\n\u003cp\u003eAt 2013, we started building our private cloud based on OpenStack, and we chose\na simple VLAN-based \u003cstrong\u003eL2 network\u003c/strong\u003e model. The underlying HW network topolopy was\ntraditional hierarchical network model (3-layer network model).\u003c/p\u003e\n\n\u003cp\u003eAt 2016, we evolved to a SDN-based \u003cstrong\u003elarge L2 network\u003c/strong\u003e, and the underlying HW\nnetwork evolved to Spine-Leaf model.\u003c/p\u003e\n\n\u003cp\u003eStarting from 2017, we began to deploy container platforms (mesos, k8s) on both\nprivate and public cloud. In the private cloud, we extended our SDN\nsolution to integration container networks. On the public cloud, we also\ndesigned our container network solution, and connected the public and private\ncloud.\u003c/p\u003e\n\n\u003cp\u003eNow (2019), we are doing some investigations on \u003cstrong\u003ecloud native solutions\u003c/strong\u003e to\naddress the new challenges we are facing.\u003c/p\u003e\n\n\u003cp\u003eLet’s dig into those solutions in more detail.\u003c/p\u003e\n\n\u003ch1 id=\"1-vlan-based-l2-network\"\u003e1 VLAN-based L2 Network\u003c/h1\u003e\n\n\u003cp\u003eAt 2013, we started building our private cloud based on OpenStack, provisioning\nVM and BM instances to our internal customers.\u003c/p\u003e\n\n\u003ch2 id=\"11-requirements\"\u003e1.1 Requirements\u003c/h2\u003e\n\n\u003cp\u003eThe requirements for network were as follows:\u003c/p\u003e\n\n\u003cp\u003eFirst, performance of the virtualized network should not be too bad compared\nwith baremetal networks, measuring with metrics such as instance-to-instance\nlatency, throughput, etc.\u003c/p\u003e\n\n\u003cp\u003eSecondly, it should have some L2 isolations to prevent common L2 problems e.g. flooding.\u003c/p\u003e\n\n\u003cp\u003eThirdly, and this is really important - the \u003cstrong\u003einstance IP should be routable\u003c/strong\u003e.\nThat is to say, we could not utilize any tunnling techniques within the host.\u003c/p\u003e\n\n\u003cp\u003eAt last, security concerns were less critical at that time. If sacrificing a\nlittle security could give us a significant performance increase, that would be\nacceptable. As in a private cloud environment, we have other means to ensure\nsecurity.\u003c/p\u003e\n\n\u003ch2 id=\"12-solution-openstack-provider-network-model\"\u003e1.2 Solution: OpenStack Provider Network Model\u003c/h2\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/ctrip-net-evolution/3.jpg\" width=\"25%\" height=\"25%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 3. OpenStack Provider Network Model\u003c/p\u003e\n\n\u003cp\u003eAfter some investigation, we chose the OpenStack \u003cstrong\u003e\u003cem\u003eprovider network model\u003c/em\u003e\u003c/strong\u003e\n[1], as depicted in Fig 3.\u003c/p\u003e\n\n\u003cp\u003eThe provider network model has following characteristics:\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\u003cstrong\u003eL2 forwarding\u003c/strong\u003e within host, \u003cstrong\u003eL2 forwarding + L3 routing\u003c/strong\u003e outside host\u003c/li\u003e\n  \u003cli\u003eTenant gateways configured on HW devices. So this is a \u003cstrong\u003eSW + HW solution\u003c/strong\u003e,\nrather than a pure SW solution\u003c/li\u003e\n  \u003cli\u003eInstance \u003cstrong\u003eIP routable\u003c/strong\u003e\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eHigh performance\u003c/strong\u003e. This comes mainly from two aspects:\n    \u003cul\u003e\n      \u003cli\u003eNo overlay encapsulation/decapsulation\u003c/li\u003e\n      \u003cli\u003eGateways configured on HW device, which has far more better performance\ncompared with SW implemented virtual routers (L3 agent) in OpenStack\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eOther aspects in our design:\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eL2 segmentation: VLAN\u003c/li\u003e\n  \u003cli\u003eML2: OVS\u003c/li\u003e\n  \u003cli\u003eL2 Agent：Neutron OVS Agent\u003c/li\u003e\n  \u003cli\u003eL3 Agent: NO\u003c/li\u003e\n  \u003cli\u003eDHCP: NO\u003c/li\u003e\n  \u003cli\u003eFloating IP: NO\u003c/li\u003e\n  \u003cli\u003eSecurity Group：NO\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch2 id=\"13-hw-network-topology\"\u003e1.3 HW Network Topology\u003c/h2\u003e\n\n\u003cp\u003eThe HW network topology in our data center is Fig 4.\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/ctrip-net-evolution/4.png\" width=\"60%\" height=\"60%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 4. Physical Network Topology in the Datacenter\u003c/p\u003e\n\n\u003cp\u003eThe bottom part are rack rows.  Each blade in the rack had two physical\nNIC, connected to two adjacent ToR for physical HA.\u003c/p\u003e\n\n\u003cp\u003eThe above part is a typical \u003cstrong\u003e\u003cem\u003eaccess - aggregate - core\u003c/em\u003e\u003c/strong\u003e hierarchical network\nmodel. Aggregate layer communicates with access layer via L2 forwarding, and\nwith core layer via L3 routing.\u003c/p\u003e\n\n\u003cp\u003eAll OpenStack network gateways are configured on core routers. Besides, there\nare HW firewalls connected to core routers to perform some security\nenforcements.\u003c/p\u003e\n\n\u003ch2 id=\"14-host-network-topology\"\u003e1.4 Host Network Topology\u003c/h2\u003e\n\n\u003cp\u003eThe virtual network topology within a compute node is shown in Fig 5.\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/ctrip-net-evolution/5.png\" width=\"75%\" height=\"75%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 5. Designed Virtual Network Topology within A Compute Node\u003c/p\u003e\n\n\u003cp\u003eSome highlights:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eTwo OVS bridges \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebr-int\u003c/code\u003e and \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebr-bond\u003c/code\u003e, connected directly\u003c/li\u003e\n  \u003cli\u003eTwo physical NICs bonded into one virtual device, attached to \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebr-bond\u003c/code\u003e\u003c/li\u003e\n  \u003cli\u003eHost IP address also configured on \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebr-bond\u003c/code\u003e, serving as management IP\u003c/li\u003e\n  \u003cli\u003eAll instance devices (virtual NICs) attached to \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebr-int\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eIn the picture, \u003ccode class=\"language-plaintext highlighter-rouge\"\u003einst1\u003c/code\u003e and \u003ccode class=\"language-plaintext highlighter-rouge\"\u003einst2\u003c/code\u003e are two instances from different networks.\nThe numbered devices started from \u003ccode class=\"language-plaintext highlighter-rouge\"\u003einst1\u003c/code\u003e and ended at \u003ccode class=\"language-plaintext highlighter-rouge\"\u003einst2\u003c/code\u003e is just the\npacket traversing path between the two (cross network) instances. As can be\nseen, there are 18 hops in total.\u003c/p\u003e\n\n\u003cp\u003eIn contrast, Fig 6 shows the topology in legacy OpenStack provider network\nmodel.\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/ctrip-net-evolution/6.png\" width=\"75%\" height=\"75%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 6. Virtual Network Topology within A Compute Node in Legacy OpenStack\u003c/p\u003e\n\n\u003cp\u003eThe biggest difference here is: \u003cstrong\u003ea Linux bridge sits between each instance\nand \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebr-int\u003c/code\u003e\u003c/strong\u003e. This is because OpenStack supports a feature called “security\ngroup”, which uses \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eiptables\u003c/code\u003e in the behind. Unfortunately, OVS ports do not\nsupport \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eiptables\u003c/code\u003e rules; but Linux bridge ports do support, so in OpenStack\na Linux bridge is inserted between each instance and \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebr-int\u003c/code\u003e.\u003c/p\u003e\n\n\u003cp\u003eExcept this, other parts are similar, so in this circumstance, the total hops is\n24.\u003c/p\u003e\n\n\u003ch2 id=\"15-summary\"\u003e1.5 Summary\u003c/h2\u003e\n\n\u003ch3 id=\"advantages\"\u003eAdvantages\u003c/h3\u003e\n\n\u003cp\u003eFirst of all, we simplified OpenStack deployment architecture, removed some\ncomponents that we did not need, e.g. L3 agent, DHCP agent, Neutron metadata\nagent, etc, and we no longer needed a standalone network node. For a team which\njust started private cloud with not so much experience, the developing and\noperating costs became relatively low.\u003c/p\u003e\n\n\u003cp\u003eSecondly, we simplified the host network topology by removing security groups.\nThe total hops between two instances from different networks was decreased from\n24 to 18, thus has lower latency.\u003c/p\u003e\n\n\u003cp\u003eThirdly, we had our gateways configured on HW devices, which had far more\nbetter performance than OpenStack’s pure SW solutions.\u003c/p\u003e\n\n\u003cp\u003eAnd at last, the instance IP was routable, which benifited a lot to upper layer\nsystems such as tracking and monitoring systems.\u003c/p\u003e\n\n\u003ch3 id=\"disadvantags\"\u003eDisadvantags\u003c/h3\u003e\n\n\u003cp\u003eFirst, as has been said, we removed security groups. So the security is\nsacrified at some extent. We compensated this partly by enforcing some security\nrules on HW firewalls.\u003c/p\u003e\n\n\u003cp\u003eSecondly, the network provision process was less automatic. For example,\nwe had to configure the core routers whenever we add/delete networks to/from\nOpenStack. Although these operations have a very low frequency, the impact of\ncore router misconfiguration is dramatic - it could affect the entire network.\u003c/p\u003e\n\n\u003ch1 id=\"2-sdn-based-large-l2-network\"\u003e2 SDN-based Large L2 Network\u003c/h1\u003e\n\n\u003ch2 id=\"21-new-challenges\"\u003e2.1 New Challenges\u003c/h2\u003e\n\n\u003cp\u003eTime arrived 2016, due to the scale expansion of our cluster and network, the\nVLAN-based L2 network reached some limitations.\u003c/p\u003e\n\n\u003cp\u003eFirst of all, if you are familiar with data center networks you may know that\n\u003cstrong\u003ehierachical network model is hard to scale\u003c/strong\u003e.\u003c/p\u003e\n\n\u003cp\u003eSecondly, all the OpenStack gateways were configured on the core\nrouters, which made them the potential bottleneck and \u003cstrong\u003esingle point of failure\u003c/strong\u003e.\nAnd more, core router failures will disrupt the entire network, so the \u003cstrong\u003efailure\nradius is very large\u003c/strong\u003e.\u003c/p\u003e\n\n\u003cp\u003eAnother limitation is that our blade was equipped with 2 x 1 Gbps NICs, which was\ntoo old for modern data center and morden applications.\u003c/p\u003e\n\n\u003cp\u003eBesides, the VLAN has its own limitations: flooding in large VLAN segmentations\nis still a problem, and number of avialble VLAN IDs is less than 4096.\u003c/p\u003e\n\n\u003cp\u003eOn the other hand, we had also some new needs, as such:\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eOur corp acquired some companies during the past years, and the networks of\nthose companies needed to connect/integrate to ours. At network\nlevel, we’d like to treat those subsidiary companies as tenants, so we had\nmultitenancy and VPC needs.\u003c/li\u003e\n  \u003cli\u003eWe’d like the network provision more automatic, with little human\nintervention.\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch2 id=\"22-solution-openstack--sdn\"\u003e2.2 Solution: OpenStack + SDN\u003c/h2\u003e\n\n\u003cp\u003eRegarding to these requirements, we designed a \u003cstrong\u003eHW+SW, OpenStack+SDN\u003c/strong\u003e \nsolution jointly with the \u003cstrong\u003e\u003cem\u003edata center network team\u003c/em\u003e\u003c/strong\u003e in our corporation,\nshifted the network \u003cstrong\u003efrom L2 to Large L2\u003c/strong\u003e.\u003c/p\u003e\n\n\u003ch3 id=\"hw-topology\"\u003eHW Topology\u003c/h3\u003e\n\n\u003cp\u003eFor the HW network topology, we evolved from the traditional 3-layer hierachical\nmodel to \u003cstrong\u003eSpine-Leaf model\u003c/strong\u003e, which gets more and more popular in modern data\ncenters.\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/ctrip-net-evolution/7.png\" width=\"70%\" height=\"70%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 7. Spine-Leaf Topology in the New Datacenter\u003c/p\u003e\n\n\u003cp\u003eSpine-Leaf model is full-mesh connected, which means every device in Spine\nlayer connects to every device in Leaf layer, and there is no connectitiy among\nnodes in the same layer. This connectivity pattern brings many benifits:\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\u003cstrong\u003eShorter traversing path and estimable latency\u003c/strong\u003e: a server could reach any\nother server in exactly 3 hops (server-to-server latency)\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eEase of expansing\u003c/strong\u003e: to increase the bandwidth, just add a node in one\nlayer and connect it to all other nodes in the other layer\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eMore resilent to HW failures\u003c/strong\u003e: all nodes are active, node failure radius\nis far more smaller than in hierachical model\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eFor blades, we upgraded the NICs to 10 Gbps, and further to 25 Gbps.\u003c/p\u003e\n\n\u003ch3 id=\"sdn-control-and-data-plane\"\u003eSDN: Control and Data Plane\u003c/h3\u003e\n\n\u003cp\u003eWe have separate control and data planes [2]:\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eData plane: VxLAN\u003c/li\u003e\n  \u003cli\u003eControl plane: MP-BGP EVPN\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eThese are standard RFC protocols, refer to [2] for more protocol details and use\ncases.\u003c/p\u003e\n\n\u003cp\u003eOne additional benefit of this model is that it supports \u003cstrong\u003edistributed\ngateway\u003c/strong\u003e, which means all leaf nodes are acted as (active) gateways, which\neliminates the performance bottleneck of traditional gateways on core routers.\u003c/p\u003e\n\n\u003cp\u003eThis solution physically support multitenancy (via VRF).\u003c/p\u003e\n\n\u003ch3 id=\"sdn-components-and-implementation\"\u003eSDN: Components And Implementation\u003c/h3\u003e\n\n\u003cp\u003eWe developed our own SDN controller \u003cstrong\u003eCtrip Network Controller\u003c/strong\u003e (CNC).\u003c/p\u003e\n\n\u003cp\u003eCNC is a central SDN controller, and manges all Spine and Leaf nodes. It\nintegrates with Neutron server via Neutron plugins, and is able to dynamically\nadd configurations to Spine/Leaf nodes.\u003c/p\u003e\n\n\u003cp\u003eNeutron changes:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eAdd CNC ML2 \u0026amp; L3 plugins\u003c/li\u003e\n  \u003cli\u003eNew finite state machine (FSM) for port status\u003c/li\u003e\n  \u003cli\u003eNew APIs interact with CNC\u003c/li\u003e\n  \u003cli\u003eDB schema changes\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eBelow is the monitoring panel for the neutron port states in a real data center.\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/ctrip-net-evolution/8.png\" width=\"40%\" height=\"40%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 8. Monitoring Panel for Neutron Port States\u003c/p\u003e\n\n\u003ch2 id=\"23-hw--sw-topology\"\u003e2.3 HW + SW Topology\u003c/h2\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/ctrip-net-evolution/9.png\" width=\"90%\" height=\"90%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 9. HW + SW Topology of the Designed SDN Solution\u003c/p\u003e\n\n\u003cp\u003eFig 9 is the overall HW + SW topology.\u003c/p\u003e\n\n\u003cp\u003eVxLAN encap/decap is done on the leaf\nnodes. If we draw a horizeontal line to cross all Leaf nodes, this line splits\nthe entire network into underlay and overlay. The bottom part (below leaf)\nbelongs to underlay and is isolated by VLAN; The above part (above leaf) is\noverlay and isolated by VxLAN.\u003c/p\u003e\n\n\u003cp\u003eUnderlay is controlled by Neutron server, OVS and neutron-ovs-agent, overlay is\ncontrolled by CNC. CNC integrates with Neutron via Neutron plugins.\u003c/p\u003e\n\n\u003cp\u003eAs has been said, this is a joint work by cloud network team \u0026amp; data center\nnetwork team. We cloud network team focuses mainly on the underlay part.\u003c/p\u003e\n\n\u003ch2 id=\"24-spawn-an-instance\"\u003e2.4 Spawn An Instance\u003c/h2\u003e\n\n\u003cp\u003eIn this solution, when spawning an instance, how the instance’s network gets reachable?\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/ctrip-net-evolution/10.png\" width=\"90%\" height=\"90%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 10. Flow of Spawn An Instance\u003c/p\u003e\n\n\u003cp\u003eMajor steps depicted in Fig 10:\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eNova API (controller node): Create instance -\u0026gt; schedule to one compute node\u003c/li\u003e\n  \u003cli\u003eNova compute: spawn instance on this node\u003c/li\u003e\n  \u003cli\u003eNova compute -\u0026gt; Neutron server: create neutron port\u003c/li\u003e\n  \u003cli\u003eNeutron server: create port (IP, MAC, GW, etc)\u003c/li\u003e\n  \u003cli\u003eNeutron server -\u0026gt; CNC plugin -\u0026gt; CNC: send port info\u003c/li\u003e\n  \u003cli\u003eCNC: save port info to its own DB\u003c/li\u003e\n  \u003cli\u003eNeutron server -\u0026gt; Nova compute: return the created port’s info\u003c/li\u003e\n  \u003cli\u003eNova compute: create network device (virtual NIC) for instance, configure device (IP, MAC, GW, etc), then attach it to OVS\u003c/li\u003e\n  \u003cli\u003eOVS agent: detect new device attached -\u0026gt; configure OVS (add flow) -\u0026gt; \u003cstrong\u003eUnderlay network OK\u003c/strong\u003e\u003c/li\u003e\n  \u003cli\u003eNova compute -\u0026gt; Neutron server: update port \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ehost_id\u003c/code\u003e. The message is something like this: port \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e1234\u003c/code\u003e is on host \u003ccode class=\"language-plaintext highlighter-rouge\"\u003enode-1\u003c/code\u003e\u003c/li\u003e\n  \u003cli\u003eNeutron server -\u0026gt; CNC: update port \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ehost_id\u003c/code\u003e, something like this: port \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e1234\u003c/code\u003e is on host \u003ccode class=\"language-plaintext highlighter-rouge\"\u003enode-1\u003c/code\u003e\u003c/li\u003e\n  \u003cli\u003eCNC: retrieve database, get the leaf interfaces that \u003ccode class=\"language-plaintext highlighter-rouge\"\u003enode-1\u003c/code\u003e connected to, dynamically add configurations to these interfaces -\u0026gt; \u003cstrong\u003eOverlay network OK\u003c/strong\u003e\u003c/li\u003e\n  \u003cli\u003eBoth underlay and overlay networks OK -\u0026gt; instance reachable\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eIn Fig 10, black lines are legacy OpenStack flows, and blue lines are newly\nadded by us.\u003c/p\u003e\n\n\u003ch2 id=\"25-summary\"\u003e2.5 Summary\u003c/h2\u003e\n\n\u003cp\u003eA summary of the SDN-based large L2 network solution.\u003c/p\u003e\n\n\u003ch3 id=\"hw\"\u003eHW\u003c/h3\u003e\n\n\u003cp\u003eFirst, HW network model evolved from hierarchical (3-layer) network to\nSpine-Leaf (2-tier). With the Spine-Leaf full-mesh connectivity,\nserver-to-server latency gets more lower.  Spine-Leaf also supports distributed\ngateway, which means all leaf nodes act as gateway for the same network, not\nonly decreased the traversing path, but also alleviated the bottleneck of\ncentral gateways.\u003c/p\u003e\n\n\u003cp\u003eAnother benefit of full-mesh connectivity is that the HW network are now more\nresilient to failures. All devices are active rather than active-backup\n(traditional 3-layer model), thus when one device fails, it has far more\nsmaller failure radius.\u003c/p\u003e\n\n\u003ch3 id=\"sw\"\u003eSW\u003c/h3\u003e\n\n\u003cp\u003eFor the SW part, we developed our own SDN controller, and integrated it with\nOpenStack neutron via plugins.  The SDN controller cloud dynamically send\nconfigurations to HW devices.\u003c/p\u003e\n\n\u003cp\u003eAlthough we have only mentioned VM here, this solution actually\nsupports both VM and BM provision.\u003c/p\u003e\n\n\u003ch3 id=\"multi-tenancy--vpc-support\"\u003eMulti-tenancy \u0026amp; VPC support\u003c/h3\u003e\n\n\u003cp\u003eAt last, this solution supports multi-tenancy and VPC.\u003c/p\u003e\n\n\u003ch1 id=\"3-k8s--hybrid-cloud-network\"\u003e3 K8S \u0026amp; Hybrid Cloud Network\u003c/h1\u003e\n\n\u003cp\u003eAt 2017, we started to deploy container platforms, migrating some\napplications from VM/BM to containers.\u003c/p\u003e\n\n\u003cp\u003eContainer orchestrators (e.g. Mesos, K8S) has different characteristics compared\nwith VM orchestrator (e.g OpenStack), such as:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eLarge scale instances, 10K ~ 100K containers per cluster is commonly seen\u003c/li\u003e\n  \u003cli\u003eHigher deploy/destroy frequencies\u003c/li\u003e\n  \u003cli\u003eShorter spawn/destroy time: ~10s (VM: ~100s)\u003c/li\u003e\n  \u003cli\u003eContainer failure/drifting is the norm rather than exception\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 id=\"31-k8s-network-in-private-cloud\"\u003e3.1 K8S Network In Private Cloud\u003c/h2\u003e\n\n\u003cp\u003eCharacteristics of container platform raised new requirements to the network.\u003c/p\u003e\n\n\u003ch3 id=\"311-network-requirements\"\u003e3.1.1 Network Requirements\u003c/h3\u003e\n\n\u003cp\u003eFirst, The network API must be high performance, and supporting concurrency.\u003c/p\u003e\n\n\u003cp\u003eSecondly, whether using an agent or a binary to configure network (create vNICs\nand configure them), it should be fast enough.\u003c/p\u003e\n\n\u003cp\u003eTo sucessfully sell container platforms to our customers, we must keep\na considerable amount of compatibility with existing systems.\u003c/p\u003e\n\n\u003cp\u003eOne of these is: we must \u003cstrong\u003ekeep the IP address unchanged when container drifts\nfrom one node to another\u003c/strong\u003e. This is an anti-pattern to container platform’s\nphilosophy, as those orchestrators are desgined to weaken the IP address: users\nshould only see the service that a container exposed, but not the IP address of\na single container.  The reason why we have to comprimise here is that in\nOpenStack age, VM migration keeps the IP unchanged. So lots of outer systems\nassumed that IP address is an immutable attribute of an instance during its\nlifecycle, and they designed their systems based on this assumption. If we\nsuddenly break this assumption, lots of systems (SOA, SLB, etc) need to be\nrefactored, and this is out of our control.\u003c/p\u003e\n\n\u003ch3 id=\"312-solution-extend-sdn-to-support-mesosk8s\"\u003e3.1.2 Solution: Extend SDN to Support Mesos/K8S\u003c/h3\u003e\n\n\u003cp\u003eIn private cloud, we decided to extend our SDN solution to integrate container\nnetworks. We reused existing infrastructures, including Neutron, CNC, OVS,\nNeutron-OVS-Agent. And then developed a CNI plugin for neutron.\u003c/p\u003e\n\n\u003cp\u003eSome changes or newly added components listed below.\u003c/p\u003e\n\n\u003ch4 id=\"neutron-changes\"\u003eNeutron Changes\u003c/h4\u003e\n\n\u003cp\u003eFirst, we added some new APIs, e.g. legacy Neutron supports only allocating port\nby network ID, we added label attributes to Neutron \u003ccode class=\"language-plaintext highlighter-rouge\"\u003enetworks\u003c/code\u003e model, supporting\nallocating port by network labels. For example, CNI plugin will say, \u003cstrong\u003e\u003cem\u003e“I want\na port allocated from any network with ‘prod-env’ label”\u003c/em\u003e\u003c/strong\u003e. This decouples K8S\nfrom OpenStack details and is more scalable, because a label could mapping to\nany number of networks.\u003c/p\u003e\n\n\u003cp\u003eNext, we did some performance optimizations:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eAdd Bulk port API\u003c/li\u003e\n  \u003cli\u003eDatabase access optimizations\u003c/li\u003e\n  \u003cli\u003eAsync API for high concurrency\u003c/li\u003e\n  \u003cli\u003eCritical path refactor\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eWe also backported some new features from upstream, e.g. graceful OVS agent\nrestart, a big benefit for network operators.\u003c/p\u003e\n\n\u003ch4 id=\"new-k8s-cni-plugin-for-neutron\"\u003eNew K8S CNI plugin for neutron\u003c/h4\u003e\n\n\u003cp\u003eK8S CNI plugin creates and deletes networks for each Pod. The jobs it does are\nmuch the same with other CNI plugins (e.g Calico, Flannel): creates veth pair,\nattaches to OVS and container netns, configures MAC, IP, GW, etc.\u003c/p\u003e\n\n\u003cp\u003eTwo big differences seperating it from other plugins:\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eCommunicate with Neutron (central IPAM) to allocate/free port (IP address)\u003c/li\u003e\n  \u003cli\u003eUpdate port information to neutron server after finishing\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch4 id=\"existing-network-servicescomponents-upgrade\"\u003eExisting network services/components upgrade\u003c/h4\u003e\n\n\u003cp\u003eWe also upgraded some network infra. E.g. we’ve hit some OVS bugs\nduring past few years:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eovs-vswitchd 100% CPU bug [3]\u003c/li\u003e\n  \u003cli\u003eOVS port mirror bug [4]\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eSo we upgraded OVS to the latest LTS \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e2.5.6\u003c/code\u003e, which has solved those bugs.\u003c/p\u003e\n\n\u003ch3 id=\"313-pod-drifting\"\u003e3.1.3 Pod Drifting\u003c/h3\u003e\n\n\u003cp\u003eNetwork steps in starting a container are much the same as in\nspawning a VM in Fig. 10, so we do not detail it here.\u003c/p\u003e\n\n\u003cp\u003eFig 11 shows how the IP address stayed unchanged during container drifting. The\nkey point is: CNI plugin knows how to join some Pod labels into a port \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ename\u003c/code\u003e.\nThis \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ename\u003c/code\u003e is unique index, so the second node (node B) could get the IP\naddress information from neutron with this name.\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/ctrip-net-evolution/11.png\" width=\"80%\" height=\"80%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 11. Pod drifting with the same IP within a K8S cluster \u003c/p\u003e\n\n\u003ch3 id=\"314-summary\"\u003e3.1.4 Summary\u003c/h3\u003e\n\n\u003cp\u003eA quick summary:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eWe integrated container platform into existing infra in a short time\u003c/li\u003e\n  \u003cli\u003eSingle global IPAM manages all VM/BM/container networks\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eSum up, \u003cstrong\u003ethis is the latest network solution int private cloud\u003c/strong\u003e.\u003c/p\u003e\n\n\u003cp\u003eCurrent deployment scale of this new solution:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e4 availability zones (AZ)\u003c/li\u003e\n  \u003cli\u003eUp to 500+ physical nodes (VM/BM/Container hosts) per AZ\u003c/li\u003e\n  \u003cli\u003eUp to 500+ instances per host\u003c/li\u003e\n  \u003cli\u003eUp to 20K+ instances per AZ\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 id=\"315-future-architecture\"\u003e3.1.5 Future Architecture\u003c/h3\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/ctrip-net-evolution/12.png\" width=\"85%\" height=\"85%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 12. Layered view of the future network architecture\u003c/p\u003e\n\n\u003cp\u003eFig 12 is an architucture we’d like to achieve in the future.\u003c/p\u003e\n\n\u003cp\u003eFirst, the network will be split into underlay and overlay planes. IaaS and other\nInfra services deploy in underlay network, e.g OpenStack, CNC. Then creating VPC\nin overlay networks, and deploying VM and BM instances in VPCs. These have been\nachieved.\u003c/p\u003e\n\n\u003cp\u003eOne K8S cluster will be kept within one VPC, and each cluster manages its own\nnetworks. All access via IP address should be kept within that cluster, and all\naccess from outside of the cluster should go through Ingress - the K8S native\nway. We haven’t achieved this, because it needs lots of SW and HW system\nrefactors.\u003c/p\u003e\n\n\u003ch2 id=\"32-k8s-on-public-cloud\"\u003e3.2 K8S on Public Cloud\u003c/h2\u003e\n\n\u003ch3 id=\"321-requirements\"\u003e3.2.1 Requirements\u003c/h3\u003e\n\n\u003cp\u003eCtrip started its internationalization in recent years, in the techical layer,\nwe should be able to support global deployment, which means provisioning\nresources outside mainland China.\u003c/p\u003e\n\n\u003cp\u003eBuilding overseas private cloud is not practical, as the designing and building\nprocess will take too much time. So we chose to purchase public cloud resources,\ndeploy and manage our own K8S clusters (rather than using vendor-managed clusters,\ne.g. AWS EKS [10]) and integrate them to our private cloud infra, turning CDOS\ninto a hybrid cloud platform. CDOS API will abstract out all vendor-specific\ndetails, and provide a unified API to our internal customers/systems.\u003c/p\u003e\n\n\u003cp\u003eThis work involves networking solutions on public cloud platforms.\u003c/p\u003e\n\n\u003ch3 id=\"322-k8s-network-solution-on-aws\"\u003e3.2.2 K8S Network Solution on AWS\u003c/h3\u003e\n\n\u003cp\u003eTaking AWS as example, let’s see our K8S network solution.\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/ctrip-net-evolution/13.png\" width=\"70%\" height=\"70%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 13. K8S network solution on public cloud vendor (AWS)\u003c/p\u003e\n\n\u003cp\u003eFirst, spawning EC2 instances as K8S nodes on AWS. Then we developed a CNI\nplugin to dynamically plug/unplug ENI to EC2 [5, 6].  The ENIs were given to\nPods as its vNIC.\u003c/p\u003e\n\n\u003cp\u003eWe developed a global IPAM service (just like Neutron in OpenStack) and deployed\nin VPC, it manages all network resources, and calls AWS APIs for real\nallocation/deallocation.\u003c/p\u003e\n\n\u003cp\u003eThe CNI plugin also supports attach/detach floating IP to Pods.  And again, the\nIP address stays the same when Pod drifts from one node to another.  This is\nachieved by ENI drifting.\u003c/p\u003e\n\n\u003ch3 id=\"323-vpcs-over-the-globe\"\u003e3.2.3 VPCs over the globe\u003c/h3\u003e\n\n\u003cp\u003eFig 14 is the global picture of our VPCs in both private and public cloud.\u003c/p\u003e\n\n\u003cp\u003eWe have some VPCs in our private cloud distributed in Shanghai and Nantong.\nOutside mainland China, we have VPCs on public cloud regions, including\nSeoul, Moscow, Frankfurt, California, Hong Kong, Melborne, and many more.\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/ctrip-net-evolution/14.png\" width=\"70%\" height=\"70%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 14. VPCs distributed over the globe \u003c/p\u003e\n\n\u003cp\u003eNetwork segments of VPCs on both prviate and public cloud are arranged to be\nnon-overlapped, so we connect them with direct connect techniques, and the IP\nis routable (if needed).\u003c/p\u003e\n\n\u003cp\u003eOK, right here, I have introduced all of the major aspects of our network\nevolution. In the next, let’s see some new challenges in the cloud native age.\u003c/p\u003e\n\n\u003ch1 id=\"4-cloud-native-solutions\"\u003e4 Cloud Native Solutions\u003c/h1\u003e\n\n\u003cp\u003eThe current network solution faced some new challenges in cloud native era:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eCentral IPAM may be the new bottleneck, and Neutron is not designed for performance\u003c/li\u003e\n  \u003cli\u003eCloud native prefers local IPAM (IPAM per host)\u003c/li\u003e\n  \u003cli\u003eLarge failure radius: IP drifting among entire AZ\u003c/li\u003e\n  \u003cli\u003eDense deployment of containers will hit HW limit of leaf nodes\u003c/li\u003e\n  \u003cli\u003eIncreasingly strong host firewall (L4-L7) needs\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eSo we are doing some investigations on new solutions, e.g. Calico, Cilium.\nCalico has been widely used nowadays, so I’ll skip it and give some\nintroduction to a relatively less well-known solution: Cilium.\u003c/p\u003e\n\n\u003ch2 id=\"41-cilium-overview\"\u003e4.1 Cilium Overview\u003c/h2\u003e\n\n\u003cp\u003eCilium is a brand-new solution [7], and it needs Kernel 4.8+.\u003c/p\u003e\n\n\u003cp\u003eCilium’s core relies on eBPF/BPF, which is a bytecode sandbox in Linux kernel.\nIf you never heard of this, think BPF as iptables, it could hook and\nmodify packets in the kernel stack, we will tell the difference later.\u003c/p\u003e\n\n\u003cp\u003eCilium relies on BPF to achieve connectivity \u0026amp; security.\nIt has following components:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eCLI\u003c/li\u003e\n  \u003cli\u003ePlugin for orchestrator (Mesos, K8S, etc) integration\u003c/li\u003e\n  \u003cli\u003ePolicy repository (etcd or consul)\u003c/li\u003e\n  \u003cli\u003eHost agent (also acts as local IPAM)\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/ctrip-net-evolution/15.png\" width=\"60%\" height=\"60%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 15. Cilium\u003c/p\u003e\n\n\u003ch2 id=\"42-host-networking\"\u003e4.2 Host Networking\u003c/h2\u003e\n\n\u003cp\u003eAny networking solution could be split into two major parts:\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eHost-networking: instance-to-instance communication, and instance-to-host communication\u003c/li\u003e\n  \u003cli\u003eMulti-host-networking: cross-host and/or cross-subnet instance-to-instance communication\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eLet’s see the host-networking of Cilium.\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/ctrip-net-evolution/16.png\" width=\"45%\" height=\"45%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 16. Cilium host-networking\u003c/p\u003e\n\n\u003cp\u003eFirst, each host runs a Cilium agent, the agent acts as local IPAM, and manages its CIDR.\nUpon starting, it creates a veth pair named \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ecilium_host \u0026lt;--\u0026gt; cilium_net\u003c/code\u003e, and\nsets the first IP address of the CIDR to \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ecilium_host\u003c/code\u003e, which then acts as the\ngateway of the CIDR.\u003c/p\u003e\n\n\u003cp\u003eWhen starting a Pod in this host, the CNI plugin will:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eallocate an IP address from this CIDR\u003c/li\u003e\n  \u003cli\u003ecreate a veth pair for the Pod\u003c/li\u003e\n  \u003cli\u003econfigure the IP, gateway info to Pod\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThen the topology will look like Fig 16. Note that there is no OVS or\nLinux bridges among the Pods’ veth pairs and \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ecilium_host \u0026lt;--\u0026gt; cilium_net\u003c/code\u003e.\nActually there are also no special ARP entries or route entries to connect the\nveth pairs. Then how does the packet been forwarded when it reaches the veth\npair end? The answer is \u003cstrong\u003eBPF code\u003c/strong\u003e. CNI plugin will \u003cstrong\u003egenerate BPF rules,\ncompile them and inject them into kernel\u003c/strong\u003e to bridge the gaps between veth pairs.\u003c/p\u003e\n\n\u003cp\u003eSummary of cilium host networking:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eInst-to-inst: BPF + Kernel Stack L2 forward\u003c/li\u003e\n  \u003cli\u003eInst-to-host: BPF + L3 Routing\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 id=\"43-multi-host-networking\"\u003e4.3 Multi-host networking\u003c/h2\u003e\n\n\u003cp\u003eFor multi-host networking, Cilium provides two commonly used ways:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eVxLAN overlay\u003c/li\u003e\n  \u003cli\u003eBGP direct routing\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eIf using VxLAN, Cilium will create a \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ecilium_vxlan\u003c/code\u003e device in each host, and do\nVxLAN encap/decap by software. The performance will be a big concern, although\nVxLAN HW offload will partly alleviate the burden.\u003c/p\u003e\n\n\u003cp\u003eBGP is another choice. In this case, you need to run a BGP agent in each host,\nthe BGP agent will do peering with outside network. This needs data center\nnetwork support. \u003cdel\u003eOn public cloud, you could also try the BGP API\u003c/del\u003e.\nBGP solution has better performance compared with VxLAN overlay, and more\nimportantly, it makes the container IP routable.\u003c/p\u003e\n\n\u003ch2 id=\"44-pros--cons\"\u003e4.4 Pros \u0026amp; Cons\u003c/h2\u003e\n\n\u003cp\u003eHere is a brief comparison according to my understanding and experiment.\u003c/p\u003e\n\n\u003ch3 id=\"pros\"\u003ePros\u003c/h3\u003e\n\n\u003cul\u003e\n  \u003cli\u003eK8S-native L4-L7 security policy support\u003c/li\u003e\n  \u003cli\u003eHigh performance network policy enforcement\u003c/li\u003e\n  \u003cli\u003eTheoretical complexity: BPF O(1) vs iptables O(n) [11]\u003c/li\u003e\n  \u003cli\u003eHigh performance forwarding plane (veth pair, IPVLAN)\u003c/li\u003e\n  \u003cli\u003eDual stack support (IPv4/IPv6)\u003c/li\u003e\n  \u003cli\u003eSupport run over flannel (Cilium only handles network policy)\u003c/li\u003e\n  \u003cli\u003eActive community\n    \u003cul\u003e\n      \u003cli\u003eDevelopment driven by a company\u003c/li\u003e\n      \u003cli\u003eCore developers from kernel community\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 id=\"cons\"\u003eCons\u003c/h3\u003e\n\n\u003cp\u003e\u003cstrong\u003eLatest kernel (4.8+ at least, 4.14+ better) needed\u003c/strong\u003e. lots of companies’\nPROD environments run kernels older than this.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eNot enough user stories \u0026amp; best practices yet\u003c/strong\u003e. Everyone says Cilium is\nbrilliant, but no one has claimed they have deployed Cilium at large scale in\ntheir \u003cstrong\u003ePROD\u003c/strong\u003e environments.\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eHigh dev \u0026amp; ops costs\u003c/strong\u003e. Compared with iptables-based solutions, e.g. Calico,\nbig companies usually have customization needs because all of reasons, e.g.\ncompatibility with old systems to not break the business. The development\nwould need a gentle understanding with kernel stack: you should be familir\nwith kernel data structures, know the packet traversing path, have a\nconsiderable experience with C programming - BPF code is written in C.\u003c/p\u003e\n\n\u003cp\u003eTrouble shooting and debugging. You should equipped yourself with Cilium trouble\nshooting skills, which are different from iptables-based solutions. While in\nmany cases, their maybe a shortage of proper trouble shooting tools.\u003c/p\u003e\n\n\u003cp\u003eBut at last, \u003cstrong\u003eCilium/eBPF is still one of the most exciting techs rised in\nrecent years\u003c/strong\u003e, and it’s still under fast developing. So, have a try and find\nthe fun!\u003c/p\u003e\n\n\u003ch1 id=\"references\"\u003eReferences\u003c/h1\u003e\n\n\u003col\u003e\n  \u003cli\u003e\u003ca href=\"https://docs.openstack.org/neutron/rocky/admin/intro-os-networking.html\"\u003eOpenStack Doc: Networking Concepts\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://www.cisco.com/c/en/us/products/collateral/switches/nexus-7000-series-switches/white-paper-c11-737022.pdf\"\u003eCisco Data Center Spine-and-Leaf Architecture: Design Overview\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://mail.openvswitch.org/pipermail/ovs-dev/2014-October/290600.html\"\u003eovs-vswitchd: Fix high cpu utilization when acquire idle lock fails\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://bugs.launchpad.net/cloud-archive/+bug/1639273\"\u003eopenvswitch port mirroring only mirrors egress traffic\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://github.com/lyft/cni-ipvlan-vpc-k8s\"\u003eLyft CNI plugin\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://www.slideshare.net/aspyker/container-world-2018\"\u003eNetflix: run container at scale\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://cilium.io/\"\u003eCilium Project\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://arthurchiao.github.io/blog/cilium-cheat-sheet/\"\u003eCilium Cheat Sheet\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://arthurchiao.github.io/blog/cilium-code-walk-through-create-network/\"\u003eCilium Code Walk Through: CNI Create Network\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://aws.amazon.com/eks/\"\u003eAmazon EKS - Managed Kubernetes Service\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://www.slideshare.net/ThomasGraf5/cilium-bringing-the-bpf-revolution-to-kubernetes-networking-and-security\"\u003eCilium: API Aware Networking \u0026amp; Network Security for Microservices using BPF \u0026amp; XDP\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\n\n  \u003c!-- POST NAVIGATION --\u003e\n  \u003cdiv class=\"postNav clearfix\"\u003e\n     \n      \u003ca class=\"prev\" href=\"/blog/how-to-make-linux-microservice-aware-with-cilium-zh/\"\u003e\u003cspan\u003e« [译] 如何基于 Cilium 和 eBPF 打造可感知微服务的 Linux（InfoQ, 2019）\u003c/span\u003e\n      \n    \u003c/a\u003e\n      \n      \n      \u003ca class=\"next\" href=\"/blog/gobgp-cheat-sheet/\"\u003e\u003cspan\u003eGoBGP Cheat Sheet »\u003c/span\u003e\n       \n      \u003c/a\u003e\n     \n  \u003c/div\u003e\n\u003c/div\u003e",
  "Date": "2019-04-17T00:00:00Z",
  "Author": "Arthur Chiao"
}