{
  "Source": "arthurchiao.art",
  "Title": "[译] 文生图（text-to-image）简史：扩散模型（diffusion models）的崛起与发展（2022）",
  "Link": "https://arthurchiao.art/blog/rise-of-diffusion-based-models-zh/",
  "Content": "\u003cdiv class=\"post\"\u003e\n  \n  \u003ch1 class=\"postTitle\"\u003e[译] 文生图（text-to-image）简史：扩散模型（diffusion models）的崛起与发展（2022）\u003c/h1\u003e\n  \u003cp class=\"meta\"\u003ePublished at 2024-01-21 | Last Update 2024-01-21\u003c/p\u003e\n  \n  \u003ch3 id=\"译者序\"\u003e译者序\u003c/h3\u003e\n\n\u003cp\u003e本文翻译自 2022 年的一篇英文博客：\n\u003ca href=\"https://maciejdomagala.github.io/generative_models/2022/06/06/The-recent-rise-of-diffusion-based-models.html\"\u003eThe recent rise of diffusion-based models\u003c/a\u003e，\n另外也参考其他资料补充了一点内容，主要方便自己粗浅理解。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/assets/img/rise-of-diffusion-based-models/timeline.png\" width=\"70%\"/\u003e\n\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e Fig. 文生图（text-to-image）近几年演进 \u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e由于译者水平有限，本文不免存在错误之处。如有疑问，请查阅原文。\u003c/strong\u003e\u003c/p\u003e\n\n\u003chr/\u003e\n\n\u003cul id=\"markdown-toc\"\u003e\n  \u003cli\u003e\u003ca href=\"#译者序\" id=\"markdown-toc-译者序\"\u003e译者序\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#1-openai-dalle起于文本潜入图像202101\" id=\"markdown-toc-1-openai-dalle起于文本潜入图像202101\"\u003e1 OpenAI \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eDALL·E\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e：起于文本，潜入图像，\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e2021.01\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#11-gpt-3-2020基于-transformer-架构的多模态大语言模型\" id=\"markdown-toc-11-gpt-3-2020基于-transformer-架构的多模态大语言模型\"\u003e1.1 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eGPT-3\u003c/code\u003e (2020)：基于 transformer 架构的多模态大语言模型\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#12-dalle-202101transformer-架构扩展到计算机视觉领域\" id=\"markdown-toc-12-dalle-202101transformer-架构扩展到计算机视觉领域\"\u003e1.2 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eDALL·E\u003c/code\u003e (2021.01)：transformer 架构扩展到计算机视觉领域\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#13-量化文本-图像匹配程度clip-模型\" id=\"markdown-toc-13-量化文本-图像匹配程度clip-模型\"\u003e1.3 量化“文本-图像”匹配程度：\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eCLIP\u003c/code\u003e 模型\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#14-小结\" id=\"markdown-toc-14-小结\"\u003e1.4 小结\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#2-diffusion高斯去噪扩散称王202112\" id=\"markdown-toc-2-diffusion高斯去噪扩散称王202112\"\u003e2 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eDiffusion\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e：高斯去噪，扩散称王，\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e2021.12\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#21-几种图像生成模型ganvaeflow-baseddiffusion\" id=\"markdown-toc-21-几种图像生成模型ganvaeflow-baseddiffusion\"\u003e2.1 几种图像生成模型：GAN/VAE/Flow-based/Diffusion\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#22-正向图像扩散forward-image-diffusion\" id=\"markdown-toc-22-正向图像扩散forward-image-diffusion\"\u003e2.2 正向图像扩散（forward image diffusion）\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#221-基本原理\" id=\"markdown-toc-221-基本原理\"\u003e2.2.1 基本原理\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#222-数学描述\" id=\"markdown-toc-222-数学描述\"\u003e2.2.2 数学描述\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#23-反向图像扩散reverse-image-diffusion\" id=\"markdown-toc-23-反向图像扩散reverse-image-diffusion\"\u003e2.3 反向图像扩散（reverse image diffusion）\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#231-基本原理\" id=\"markdown-toc-231-基本原理\"\u003e2.3.1 基本原理\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#232-数学表示\" id=\"markdown-toc-232-数学表示\"\u003e2.3.2 数学表示\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#24-引导扩散guiding-the-diffusion\" id=\"markdown-toc-24-引导扩散guiding-the-diffusion\"\u003e2.4 引导扩散（guiding the diffusion）\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#3-glide文本引导定向扩散202204\" id=\"markdown-toc-3-glide文本引导定向扩散202204\"\u003e3 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eGLIDE\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e：文本引导，定向扩散，\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e2022.04\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#31-架构\" id=\"markdown-toc-31-架构\"\u003e3.1 架构\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#32-工作原理\" id=\"markdown-toc-32-工作原理\"\u003e3.2 工作原理\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#33-小结\" id=\"markdown-toc-33-小结\"\u003e3.3 小结\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#4-dalle-2取长补短先验称奇202204\" id=\"markdown-toc-4-dalle-2取长补短先验称奇202204\"\u003e4 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eDALL·E 2\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e：取长补短，先验称奇，\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e2022.04\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#41-架构unclip--prior--decoder\" id=\"markdown-toc-41-架构unclip--prior--decoder\"\u003e4.1 架构：\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eunCLIP = prior + decoder\u003c/code\u003e\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#42-the-prior\" id=\"markdown-toc-42-the-prior\"\u003e4.2 The prior\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#421-为什么需要-prior-层为什么单单-clip-不够\" id=\"markdown-toc-421-为什么需要-prior-层为什么单单-clip-不够\"\u003e4.2.1 为什么需要 prior 层（为什么单单 CLIP 不够）\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#422-prior-选型decoder-only-transformer\" id=\"markdown-toc-422-prior-选型decoder-only-transformer\"\u003e4.2.2 prior 选型：decoder-only transformer\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#423-损失函数\" id=\"markdown-toc-423-损失函数\"\u003e4.2.3 损失函数\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#424-小结\" id=\"markdown-toc-424-小结\"\u003e4.2.4 小结\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#42-the-decoder基于-glide-的改进\" id=\"markdown-toc-42-the-decoder基于-glide-的改进\"\u003e4.2 The decoder：基于 GLIDE 的改进\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#43-引导扩散和上采样\" id=\"markdown-toc-43-引导扩散和上采样\"\u003e4.3 引导扩散和上采样\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#5-google-imagen删繁就简扩散三连202205\" id=\"markdown-toc-5-google-imagen删繁就简扩散三连202205\"\u003e5 Google \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eImagen\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e：删繁就简，扩散三连，\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e2022.05\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#51-架构t5-xxl--diffusion--diffusion--diffusion\" id=\"markdown-toc-51-架构t5-xxl--diffusion--diffusion--diffusion\"\u003e5.1 架构：\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eT5-XXL + Diffusion + Diffusion + Diffusion\u003c/code\u003e\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#52-与-glidedalle-2-等架构的不同\" id=\"markdown-toc-52-与-glidedalle-2-等架构的不同\"\u003e5.2 与 GLIDE、DALL·E 2 等架构的不同\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#521-使用预训练的-transformer-t5-xxl-而不是从头开始训练\" id=\"markdown-toc-521-使用预训练的-transformer-t5-xxl-而不是从头开始训练\"\u003e5.2.1 使用预训练的 transformer (\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eT5-XXL\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e) 而不是从头开始训练\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#522-使用更高效的底层神经网络efficient-u-net\" id=\"markdown-toc-522-使用更高效的底层神经网络efficient-u-net\"\u003e5.2.2 使用更高效的底层神经网络（efficient U-net）\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#523-使用-conditioning-augmentation-来增强图像保真度image-fidelity\" id=\"markdown-toc-523-使用-conditioning-augmentation-来增强图像保真度image-fidelity\"\u003e5.2.3 使用 conditioning augmentation 来增强图像保真度（image fidelity）\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#53-小结\" id=\"markdown-toc-53-小结\"\u003e5.3 小结\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#6-总结\" id=\"markdown-toc-6-总结\"\u003e6 总结\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#61-如何评估模型好坏\" id=\"markdown-toc-61-如何评估模型好坏\"\u003e6.1 如何评估模型好坏\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#62-好玩儿的才刚开始\" id=\"markdown-toc-62-好玩儿的才刚开始\"\u003e6.2 好玩儿的才刚开始\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#参考资料\" id=\"markdown-toc-参考资料\"\u003e参考资料\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003chr/\u003e\n\n\u003c!-- mathjax support --\u003e\n\u003cscript type=\"text/javascript\" async=\"\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/latest.js?config=TeX-MML-AM_CHTML\"\u003e\n\u003c/script\u003e\n\n\u003cstyle\u003e\n  .MathJax_Display, .MJXc-display, .MathJax_SVG_Display {\n      overflow-x: auto;\n      overflow-y: hidden;\n   }\n\u003c/style\u003e\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/assets/img/rise-of-diffusion-based-models/golden.jpeg\" width=\"50%\"/\u003e\n\u003c/p\u003e\n\n\u003cp align=\"center\"\u003eSources: \n\u003ca href=\"https://openai.com/dall-e-2/\"\u003eOpenAI DALL·E 2\u003c/a\u003e\n\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cmark\u003e生成式建模\u003c/mark\u003e\u003c/strong\u003e（generative modeling）近几年发展神速，\n网上也涌现出了大批令人惊叹的纯 AI 生成图片。\n本文试图总结\u003cstrong\u003e\u003cmark\u003e文生图\u003c/mark\u003e\u003c/strong\u003e（text-to-image）领域近几年的发展，\n尤其是各种\u003cstrong\u003e\u003cmark\u003e扩散模型\u003c/mark\u003e\u003c/strong\u003e（diffusion models）—— 它们已经是业界的标杆架构。\u003c/p\u003e\n\n\u003ch1 id=\"1-openai-dalle起于文本潜入图像202101\"\u003e1 OpenAI \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eDALL·E\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e：起于文本，潜入图像，\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e2021.01\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/h1\u003e\n\n\u003ch2 id=\"11-gpt-3-2020基于-transformer-架构的多模态大语言模型\"\u003e1.1 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eGPT-3\u003c/code\u003e (2020)：基于 transformer 架构的多模态大语言模型\u003c/h2\u003e\n\n\u003cp\u003e2020 年，OpenAI 发布了 GPT-3 模型 [1]，这是一个基于 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eTransformer\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e \n架构的多模态大语言模型，能够完成机器翻译、文本生成、语义分析等任务，\n也迅速被视为最先进的语言建模方案（language modeling solutions）。\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cul\u003e\n    \u003cli\u003e\u003ca href=\"/blog/transformers-from-scratch-zh/\"\u003eTransformer 是如何工作的：600 行 Python 代码实现两个（文本分类+文本生成）Transformer（2019）\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"/blog/gpt-as-a-finite-state-markov-chain-zh/\"\u003eGPT 是如何工作的：200 行 Python 代码实现一个极简 GPT（2023）\u003c/a\u003e\u003c/li\u003e\n  \u003c/ul\u003e\n\u003c/blockquote\u003e\n\n\u003ch2 id=\"12-dalle-202101transformer-架构扩展到计算机视觉领域\"\u003e1.2 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eDALL·E\u003c/code\u003e (2021.01)：transformer 架构扩展到计算机视觉领域\u003c/h2\u003e\n\n\u003cp\u003eDALL·E [7] 可以看作是将 Transformer（\u003cstrong\u003e\u003cmark\u003e语言领域\u003c/mark\u003e\u003c/strong\u003e）的能力自然扩展到\u003cstrong\u003e\u003cmark\u003e计算机视觉领域\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e如何根据提示文本生成图片？DALL·E 提出了一种两阶段算法：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\n    \u003cp\u003e训练一个离散 VAE (Variational AutoEncoder) 模型，将图像（images）压缩成 \u003cstrong\u003e\u003cmark\u003eimage tokens\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n    \u003cp\u003eVAE 是一种神经网络架构，属于 probabilistic graphical models and variational Bayesian methods 家族。\u003c/p\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003e将编码之后的\u003cstrong\u003e\u003cmark\u003e文本片段\u003c/mark\u003e\u003c/strong\u003e（encoded text snippet）与 \u003cstrong\u003e\u003cmark\u003eimage tokens\u003c/mark\u003e\u003c/strong\u003e\n  拼在一起（\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003econcatenate\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e），\n  训练一个自回归 Transformer，学习\u003cstrong\u003e\u003cmark\u003e文本和图像之间的联合分布\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n  \u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e最终是在从网上获取的 250 million 个文本-图像对（text-image pairs）上进行训练的。\u003c/p\u003e\n\n\u003ch2 id=\"13-量化文本-图像匹配程度clip-模型\"\u003e1.3 量化“文本-图像”匹配程度：\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eCLIP\u003c/code\u003e 模型\u003c/h2\u003e\n\n\u003cp\u003e训练得到模型之后，就能通过\u003cstrong\u003e\u003cmark\u003e推理\u003c/mark\u003e\u003c/strong\u003e生成图像。但\u003cstrong\u003e\u003cmark\u003e如何评估生成图像的好坏\u003c/mark\u003e\u003c/strong\u003e呢？\u003c/p\u003e\n\n\u003cp\u003eOpenAI 提出了一种名为 CLIP 的 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eimage and text linking\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 方案 [9]，\n它能量化\u003cstrong\u003e\u003cmark\u003e文本片段（text snippet）与其图像表示（image representation）的匹配程度\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e抛开所有技术细节，训练这类模型的思路很简单：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e将文本片段进行编码，得到 \\(\\mathbf{T}_{i}\\)；\u003c/li\u003e\n  \u003cli\u003e将图像进行编码，得到 \\(\\mathbf{I}_{i}\\)；\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e对 400 million 个 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e(image, text)\u003c/code\u003e 进行这样的操作，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/assets/img/rise-of-diffusion-based-models/clip.png\" width=\"65%\"/\u003e\n\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\nF.g CLIP contrastive pre-training 工作原理 [9].\n（文本大意：\u003cmark\u003e澳大利亚小狗\u003c/mark\u003e）。\n\u003c/p\u003e\n\n\u003cp\u003e基于这种\u003cem\u003e映射\u003c/em\u003e方式，就能够评估生成的图像符合文本输入的程度。\u003c/p\u003e\n\n\u003ch2 id=\"14-小结\"\u003e1.4 小结\u003c/h2\u003e\n\n\u003cp\u003eDALL·E 在 AI 和其他领域都引发了广泛的关注和讨论。\n不过热度还没持续太久，风头就被另一个方向抢走了。\u003c/p\u003e\n\n\u003ch1 id=\"2-diffusion高斯去噪扩散称王202112\"\u003e2 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eDiffusion\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e：高斯去噪，扩散称王，\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e2021.12\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/h1\u003e\n\n\u003cp\u003eSohl-Dickstein 等提出了一种图像生成的新思想 —— 扩散模型（diffusion models） [2]。\n套用 AI 领域的熟悉句式，就是\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eAll you need is \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003ediffusion\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch2 id=\"21-几种图像生成模型ganvaeflow-baseddiffusion\"\u003e2.1 几种图像生成模型：GAN/VAE/Flow-based/Diffusion\u003c/h2\u003e\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/assets/img/rise-of-diffusion-based-models/generative-models.png\" width=\"70%\"/\u003e\n\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\nFig. 几种生成式模型（generative models）[13]\n\u003c/p\u003e\n\n\u003cp\u003eDiffusion 模型受到了非平衡热力学（non-equilibrium thermodynamics）的启发，但其背后是一些\u003cstrong\u003e\u003cmark\u003e有趣的数学概念\u003c/mark\u003e\u003c/strong\u003e。\n它仍然有大家已经熟悉的 encoder-decoder 结构，但底层思想与传统的 VAE（variational autoencoders）已经不同。\u003c/p\u003e\n\n\u003cp\u003e要理解这个模型，需要从原理和数学上描述正向和反向扩散过程。\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e公式看不懂可忽略，仅靠本文这点篇幅也是不可能推导清楚的。感兴趣可移步 [13-15]。\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch2 id=\"22-正向图像扩散forward-image-diffusion\"\u003e2.2 正向图像扩散（forward image diffusion）\u003c/h2\u003e\n\n\u003ch3 id=\"221-基本原理\"\u003e2.2.1 基本原理\u003c/h3\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cmark\u003e向图像逐渐添加高斯噪声\u003c/mark\u003e\u003c/strong\u003e，直到图像完全无法识别。\u003c/p\u003e\n\n\u003cp\u003e这个过程可以被形式化为\u003cstrong\u003e\u003cmark\u003e顺序扩散马尔可夫链\u003c/mark\u003e\u003c/strong\u003e（Markov chain of sequential diffusion steps）。\u003c/p\u003e\n\n\u003ch3 id=\"222-数学描述\"\u003e2.2.2 数学描述\u003c/h3\u003e\n\n\u003cul\u003e\n  \u003cli\u003e假设\u003cstrong\u003e\u003cmark\u003e图像\u003c/mark\u003e\u003c/strong\u003e服从某种\u003cstrong\u003e\u003cmark\u003e初始分布\u003c/mark\u003e\u003c/strong\u003e \\(q(\\mathbf{x}_{0})\\)，\u003c/li\u003e\n  \u003cli\u003e那么，我们可以对这个分布\u003cstrong\u003e\u003cmark\u003e采样\u003c/mark\u003e\u003c/strong\u003e，\u003cstrong\u003e\u003cmark\u003e得到一个图像\u003c/mark\u003e\u003c/strong\u003e \\(\\mathbf{x}_{0}\\)，\u003c/li\u003e\n  \u003cli\u003e接下来，我们希望执行一系列的扩散步骤，\\(\\mathbf{x}_{0} \\to \\mathbf{x}_{1} \\to ... \\to \\mathbf{x}_{T}\\)，\u003cstrong\u003e\u003cmark\u003e每次扩散都使图像越来越模糊\u003c/mark\u003e\u003c/strong\u003e。\u003c/li\u003e\n  \u003cli\u003e如何添加噪声？由一个 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003enoising schedule\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e（加噪计划/调度） \\(\\{\\beta_{t}\\}^{T}_{t=1}\\) 定义，\n对于每个 \\(t = 1,...,T\\)，有 \\(\\beta_{t} \\in (0,1)\\)。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e基于以上定义，我们就可以将正向扩散过程描述为\u003c/p\u003e\n\n\\[q\\left(\\mathbf{x}_{t} \\mid \\mathbf{x}_{t-1}\\right)=\\mathcal{N}\\left(\\sqrt{1-\\beta_{t}} \\mathbf{x}_{t-1}, \\beta_{t} \\mathbf{I}\\right).\\]\n\n\u003cblockquote\u003e\n  \u003cp\u003e注，\u003c/p\u003e\n\n  \u003col\u003e\n    \u003cli\u003e\\(\\mathcal{N}\\) 可能来自 next 首字母，表示下一个状态的概率分布（马尔科夫状态转移概率）；\u003c/li\u003e\n  \u003c/ol\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e几点解释：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e随着加噪次数的增多 \\((T \\to \\infty)\\)，\u003cstrong\u003e\u003cmark\u003e最终分布\u003c/mark\u003e\u003c/strong\u003e \\(q(\\mathbf{x}_{T})\\)\n将趋近于常见的\u003cstrong\u003e\u003cmark\u003e各向同性高斯分布\u003c/mark\u003e\u003c/strong\u003e（isotropic Gaussian distribution），这使得未来的采样非常简单和高效。\u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003e使用高斯核加噪还有一个好处 —— 可以\u003cstrong\u003e\u003cmark\u003e绕过中间加噪步骤，直接得到任意中间状态\u003c/mark\u003e\u003c/strong\u003e（intermediate latent state），\n  这要归功于 reparametrization。直接采样，\u003c/p\u003e\n\n\\[q\\left(\\mathbf{x}_{t} \\mid \\mathbf{x}_{0}\\right)=\\mathcal{N}\\left(\\sqrt{\\bar{\\alpha}_{t}} \\mathbf{x}_{0},\\left(1-\\bar{\\alpha}_{t}\\right) \\mathbf{I}\\right) = \\sqrt{\\bar{\\alpha}_{t}} \\mathbf{x}_{0}+\\sqrt{1-\\bar{\\alpha}_{t}} \\cdot \\epsilon,\\]\n\n    \u003cp\u003e其中 \\(\\alpha_{t} := 1-\\beta_{t}\\)，\\(\\bar{\\alpha}_{t} := \\prod_{k=0}^{t}\\alpha_{k}\\)，\\(\\epsilon \\sim \\mathcal{N}(0, \\mathbf{I})\\)。\n这里的 \\(\\epsilon\\) 表示高斯噪声 —— \u003cstrong\u003e\u003cmark\u003e这个公式对于模型训练至关重要\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n  \u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch2 id=\"23-反向图像扩散reverse-image-diffusion\"\u003e2.3 反向图像扩散（reverse image diffusion）\u003c/h2\u003e\n\n\u003ch3 id=\"231-基本原理\"\u003e2.3.1 基本原理\u003c/h3\u003e\n\n\u003cp\u003e正向过程定义好了，是否能定义一个反向过程 \\(q\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}\\right)\\)，\n从噪声回溯到图像呢？\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/assets/img/rise-of-diffusion-based-models/DDPM.png\" width=\"80%\"/\u003e\n\u003c/p\u003e\n\u003cp align=\"center\"\u003e Fig. The Markov chain of forward (reverse) diffusion process of generating a sample by slowly adding (removing) noise [13] \u003c/p\u003e\n\n\u003cp\u003e首先，从概念上来说，是不行的；\u003c/p\u003e\n\n\u003cp\u003e其次，这需要 marginalization over the entire data distribution。\n要从加噪样本返回到起始分布 \\(q(\\bf{x}_{0})\\)，必须对所有可能从噪声中得到 \\(\\mathbf{x}_{0}\\)\n的方式进行 marginalization，包括所有中间状态。\n这意味着计算积分 \\(\\int q(\\mathbf{x}_{0:T})d\\mathbf{x}_{1:T}\\)，这是不可行的。\u003c/p\u003e\n\n\u003cp\u003e不过，虽然无法精确计算，但可以近似！\n核心思想是以可学习网络（\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003elearnable network\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e）的形式，\n\u003cstrong\u003e\u003cmark\u003e近似反向扩散过程\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003ch3 id=\"232-数学表示\"\u003e2.3.2 数学表示\u003c/h3\u003e\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/assets/img/rise-of-diffusion-based-models/diffusion-example.png\" width=\"70%\"/\u003e\n\u003c/p\u003e\n\u003cp align=\"center\"\u003e Fig. An example of training a diffusion model for modeling a 2D swiss roll data. [2]\u003c/p\u003e\n\n\u003cp\u003e实现这一目标的第一步是\u003cstrong\u003e\u003cmark\u003e估计去噪步骤的均值和协方差\u003c/mark\u003e\u003c/strong\u003e（mean and covariance）：\u003c/p\u003e\n\n\\[p_{\\theta}\\left(\\mathbf{x}_{t-1} \\mid \\mathbf{x}_{t}\\right)=\\mathcal{N}(\\mu_{\\theta}(\\mathbf{x}_{t}, t), \\Sigma_{\\theta}(\\mathbf{x}_{t}, t) ).\\]\n\n\u003cp\u003e在实践中，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e可以通过神经网络估计 \\(\\mu_{\\theta}(\\mathbf{x}_{t}, t)\\)，\u003c/li\u003e\n  \u003cli\u003e\\(\\Sigma_{\\theta}(\\mathbf{x}_{t}, t)\\) 可以固定为与 noising schedule 相关的常数，例如 \\(\\beta_{t}\\mathbf{I}\\)。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e用这种方式估计 \\(\\mu_{\\theta}(\\mathbf{x}_{t}, t)\\) 是可行的，但 Ho 等 [3] 提出了另一种训练方法：\n训练一个神经网络 \\(\\epsilon_{\\theta}(\\mathbf{x}_{t}, t)\\)\n来预测前面公式 \\(q\\left(\\mathbf{x}_{t} \\mid \\mathbf{x}_{0}\\right)\\) 中的噪声 \\(\\epsilon\\)。\u003c/p\u003e\n\n\u003cp\u003e与 Ho 等的方法类似[3]，训练过程包括以下步骤：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e采样图像 \\(\\mathbf{x}_{0}\\sim q(\\bf{x}_{0})\\)，\u003c/li\u003e\n  \u003cli\u003e在扩散过程中选择特定的步骤 \\(t \\sim U(\\{1,2,...,T\\})\\)，\u003c/li\u003e\n  \u003cli\u003e添加噪声 \\(\\epsilon \\sim \\mathcal{N}(0,\\mathbf{I})\\)，\u003c/li\u003e\n  \u003cli\u003e估计噪声 \\(\\epsilon_{\\theta}(\\mathbf{x}_{t}, t)= \\epsilon_{\\theta}(\\sqrt{\\bar{\\alpha}_{t}} \\mathbf{x}_{0}+\\sqrt{1-\\bar{\\alpha}_{t}} \\cdot \\epsilon, t)\\)，\u003c/li\u003e\n  \u003cli\u003e通过梯度下降学习网络上的损失 \\(\\nabla_{\\theta}  \\|\\epsilon - \\epsilon_{\\theta}(\\mathbf{x}_{t}, t)\\|^{2}\\)。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e一般来说，损失可以表示为\u003c/p\u003e\n\n\\[L_{\\text{diffusion}}=\\mathbb{E}_{t, \\mathbf{x}_{0}, \\epsilon}\\left[\\left\\|\\epsilon-\\epsilon_{\\theta}\\left(\\mathbf{x}_{t}, t\\right)\\right\\|^{2}\\right],\\]\n\n\u003cp\u003e这里的公式、参数化和推导都没有详细展开，\u003cstrong\u003e\u003cmark\u003e想深入了解推导过程，强烈推荐 [13-15]\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e以上已经解释了为什么扩散模型也是一种生成模型。\n一旦模型 \\(\\epsilon_{\\theta}(\\mathbf{x}_{t}, t)\\)\n训练好，就可以用它从加躁的 \\(\\mathbf{x}_{t}\\) 回溯到原始图像 \\(\\mathbf{x}_{0}\\)。\n由于从各向同性高斯分布中采样噪声非常简单，我们可以获得无限的图像变化。\u003c/p\u003e\n\n\u003ch2 id=\"24-引导扩散guiding-the-diffusion\"\u003e2.4 引导扩散（guiding the diffusion）\u003c/h2\u003e\n\n\u003cp\u003e如果在训练过程中\u003cstrong\u003e\u003cmark\u003e向神经网络提供额外的信息\u003c/mark\u003e\u003c/strong\u003e，就可以\u003cstrong\u003e\u003cmark\u003e引导图像的生成\u003c/mark\u003e\u003c/strong\u003e。\n假设图像已经打标（labeled），关于图像的类别信息 \\(y\\) 就可以送到 class-conditional diffusion model \\(\\epsilon_{\\theta}(\\mathbf{x}_{t}, t \\mid y)\\) 中。\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\n    \u003cp\u003e引入指导的一种方式是\u003cstrong\u003e\u003cmark\u003e训练一个单独的模型\u003c/mark\u003e\u003c/strong\u003e，该模型作为\u003cstrong\u003e\u003cmark\u003e噪声图像的分类器\u003c/mark\u003e\u003c/strong\u003e（classifier of noisy images）。\u003c/p\u003e\n\n    \u003cp\u003e在每个去噪步骤中，分类器检查图像是否以正确的方向去噪，并将自己的损失函数梯度计入扩散模型的整体损失中。\u003c/p\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eHo \u0026amp; Salimans 提出了一种\u003cstrong\u003e\u003cmark\u003e无需训练额外分类器\u003c/mark\u003e\u003c/strong\u003e，就能将类别信息输入模型的方法 [5]。\u003c/p\u003e\n\n    \u003cp\u003e训练过程中，模型 \\(\\epsilon_{\\theta}(\\mathbf{x}_{t}, t \\mid y)\\)\n 有时（以固定概率）类别标签被替换为空标签 \\(\\emptyset\\)，也就是不显示实际的类别 \\(y\\)。\n 因此，它\u003cstrong\u003e\u003cmark\u003e学会了在有和没有引导的情况下进行扩散\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n    \u003cp\u003e对于推理，模型进行两次预测，一次给定类别标签 \\(\\epsilon_{\\theta}(\\mathbf{x}_{t}, t \\mid y)\\)，一次不给定 \\(\\epsilon_{\\theta}(\\mathbf{x}_{t}, t \\mid \\emptyset)\\)。\n 模型的最终预测变成乘以引导比例（guidance scale） \\(s \\geqslant 1\\)，\u003c/p\u003e\n\n\\[\\hat{\\epsilon}_{\\theta}\\left(\\mathbf{x}_{t}, t \\mid y\\right)=\\epsilon_{\\theta}\\left(\\mathbf{x}_{t}, t \\mid \\emptyset\\right)+s \\cdot\\left(\\epsilon_{\\theta}\\left(\\mathbf{x}_{t}, t \\mid y\\right)-\\epsilon_{\\theta}\\left(\\mathbf{x}_{t}, t \\mid \\emptyset\\right)\\right)\\]\n\n    \u003cp\u003e这种无分类器引导（classifier-free guidance）复用主模型的理解力，不需要额外的分类器，Nichol 等的研究显示这种方式效果更好 [6]。\u003c/p\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch1 id=\"3-glide文本引导定向扩散202204\"\u003e3 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eGLIDE\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e：文本引导，定向扩散，\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e2022.04\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/h1\u003e\n\n\u003cp\u003e以上介绍了扩散模型的工作原理，现在要回答的两个问题是：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e如何使用文本信息（textual information）来引导扩散模型？\u003c/li\u003e\n  \u003cli\u003e如何确保模型的质量足够好？\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eGLIDE 论文见提出了非常新颖和有趣的见解 [6]。\u003c/p\u003e\n\n\u003ch2 id=\"31-架构\"\u003e3.1 架构\u003c/h2\u003e\n\n\u003cp\u003e三个主要组件：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e一个基于 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eUNet\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 的模型：负责扩散的\u003cstrong\u003e\u003cmark\u003e视觉部分\u003c/mark\u003e\u003c/strong\u003e（visual part of the diffusion learning），\u003c/li\u003e\n  \u003cli\u003e一个基于 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eTransformer\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 的模型：负责将文本片段转换成\u003cstrong\u003e\u003cmark\u003e文本嵌入\u003c/mark\u003e\u003c/strong\u003e（creating a text embedding from a snippet of text），\u003c/li\u003e\n  \u003cli\u003e一个 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eupsampling\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 扩散模型：\u003cstrong\u003e\u003cmark\u003e增大输出图像的分辨率\u003c/mark\u003e\u003c/strong\u003e。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e前两个组件生成一个文本引导的图像，最后一个组件用于扩大图像并保持质量。\u003c/p\u003e\n\n\u003ch2 id=\"32-工作原理\"\u003e3.2 工作原理\u003c/h2\u003e\n\n\u003cp\u003eGLIDE 模型的核心是著名的 UNet 架构 [8]，用于扩散。\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e串联几个下采样和上采样卷积的残差层。\u003c/li\u003e\n  \u003cli\u003e还包括 attention 层，这对于同时进行文本处理至关重要。\u003c/li\u003e\n  \u003cli\u003e模型有约 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e2.3b\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 个参数，并在与 DALL·E 相同的数据集上进行了训练。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e用于引导的文本（text used for guidance）编码为 token，并送入 transformer 模型。\nGLIDE 中使用的 transformer 有约 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e1.2b\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 个参数，\n由 24 个 2048-width 的残差块构建。transformer 的输出有两个目的：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e最终 embedding token 用作 \\(\\epsilon_{\\theta}(\\mathbf{x}_{t}, t \\mid y)\\) 中的 class embedding \\(y\\)，\u003c/li\u003e\n  \u003cli\u003efinal layer of token embeddings 添加到模型的\u003cstrong\u003e每个\u003c/strong\u003e attention layer 中。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e很明显，为了生成图像的准确性，大量精力放在了确保模型获得足够的与文本相关的上下文（text-related context）。\n根据 text snippet embedding，模型将编码的文本与 attention 上下文拼接（concatenate），并在训练期间使用无分类器引导。\u003c/p\u003e\n\n\u003cp\u003e最后，使用扩散模型，通过一个 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eImageNet upsampler\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\n将图像从低分辨率转成高分辨率。\u003c/p\u003e\n\n\u003ch2 id=\"33-小结\"\u003e3.3 小结\u003c/h2\u003e\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/assets/img/rise-of-diffusion-based-models/glide-gen-images.png\"/\u003e\n\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\nFig. GLIDE 效果。提示词 \u0026#34;a corgi in a field\u0026#34;（田野里一只柯基） [6]\n\u003c/p\u003e\n\n\u003cp\u003eGLIDE 融合了近年的几项技术精华，为文本引导图像生成带来了新的启示。\n考虑到 DALL·E 模型是基于不同结构（非扩散）构建的，因此，可以说 GLIDE\n开启了\u003cstrong\u003e\u003cmark\u003e扩散式文生图时代\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003ch1 id=\"4-dalle-2取长补短先验称奇202204\"\u003e4 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eDALL·E 2\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e：取长补短，先验称奇，\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e2022.04\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/h1\u003e\n\n\u003cp\u003eOpenAI 团队马不停蹄，在 2022 年 4 月份以 DALL·E 2 [7] 再次震撼了整个互联网。\n它\u003cstrong\u003e\u003cmark\u003e组合\u003c/mark\u003e\u003c/strong\u003e了前面介绍的 CLIP 模型和 GLIDE 架构的精华。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/assets/img/rise-of-diffusion-based-models/unCLIP.png\" width=\"80%\"/\u003e\n\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e Fig. Visualization of DALL·E 2 two-stage mechanism. [13] \u003c/p\u003e\n\n\u003ch2 id=\"41-架构unclip--prior--decoder\"\u003e4.1 架构：\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eunCLIP = prior + decoder\u003c/code\u003e\u003c/h2\u003e\n\n\u003cp\u003e两个主要基础组件（也是两个阶段），\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eprior\u003c/li\u003e\n  \u003cli\u003edecoder\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e二者组合产生图像输出。整个机制名为 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eunCLIP\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e，\n如果还记得前面介绍的 CLIP 机制，就能从 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eunCLIP\u003c/code\u003e 这个名字猜到底层可能是如何工作的。\u003c/p\u003e\n\n\u003ch2 id=\"42-the-prior\"\u003e4.2 The prior\u003c/h2\u003e\n\n\u003cp\u003e第一阶段称为 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eprior\u003c/code\u003e，作用是将标题 —— 例如 “a corgi playing a flame throwing trumpet” ——\n从文本转换成\u003cstrong\u003e\u003cmark\u003e文本嵌入\u003c/mark\u003e\u003c/strong\u003e（text embedding）。\u003c/p\u003e\n\n\u003cp\u003e这个通过一个\u003cstrong\u003e\u003cmark\u003e冻结参数的 CLIP 模型\u003c/mark\u003e\u003c/strong\u003e实现的。\u003c/p\u003e\n\n\u003ch3 id=\"421-为什么需要-prior-层为什么单单-clip-不够\"\u003e4.2.1 为什么需要 prior 层（为什么单单 CLIP 不够）\u003c/h3\u003e\n\n\u003cp\u003e前面介绍过，CLIP 模型记录的 text embedding 和 image embedding 的联合分布。所以，\n直觉上来说，有了一个经过良好训练的 CLIP 模型，只要经过下面简单三步就能完成文生图的任务：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e将文本（提示词）转换成对应的 text embedding；\u003c/li\u003e\n  \u003cli\u003e将 text embedding 输入 CLIP 模型，获取最佳的 image embedding；\u003c/li\u003e\n  \u003cli\u003e用 image embedding 通过扩散生成图像。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e这里有问题吗？有，在\u003cstrong\u003e\u003cmark\u003e第 2 步\u003c/mark\u003e\u003c/strong\u003e。DALL·E 2 的作者给了一个很好的解释：\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e“（在 CLIP 空间里）可能有无数的图像与给定的标题一致，因此两个编码器的输出不会完全一致。\n因此，\u003cstrong\u003e\u003cmark\u003e需要一个单独的 prior 模型来将 text embedding “翻译”为对应的 image embedding\u003c/mark\u003e\u003c/strong\u003e\n”。\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e下面是对比：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/assets/img/rise-of-diffusion-based-models/unclip-vs-others.png\"/\u003e\n\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\nFig. 分别通过\u003cmark\u003e三种方式生成的图片\u003c/mark\u003e：仅标题（caption）、标题+CLIP 和 prior-based。[7]\n\u003c/p\u003e\n\n\u003ch3 id=\"422-prior-选型decoder-only-transformer\"\u003e4.2.2 prior 选型：decoder-only transformer\u003c/h3\u003e\n\n\u003cp\u003e作者对 prior 模型测试了两类模型：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e自回归模型（autoregressive model）\u003c/li\u003e\n  \u003cli\u003e扩散模型（diffusion model）\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e本文只讨论第二种：扩散 prior 模型。因为从计算角度来看，它的\u003cstrong\u003e\u003cmark\u003e性能优于自回归模型\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e为了训练 prior 模型，选择一个 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003edecoder-only Transformer\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e，\n通过以下几个输入进行训练：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e已编码的文本（encoded text）\u003c/li\u003e\n  \u003cli\u003eCLIP text embedding\u003c/li\u003e\n  \u003cli\u003eembedding for the diffusion timestep\u003c/li\u003e\n  \u003cli\u003e加噪的 image embedding\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e目标是\u003cstrong\u003e\u003cmark\u003e输出一个无噪的 image embedding \u003c/mark\u003e\u003c/strong\u003e（unnoised image embedding）\\(z_{i}\\)。\u003c/p\u003e\n\n\u003ch3 id=\"423-损失函数\"\u003e4.2.3 损失函数\u003c/h3\u003e\n\n\u003cp\u003e直接预测未加噪声的 image embedding 而不是预测噪声更合适，这与之前讨论的 Ho 等提出的训练方式不同。\n因此，回顾前面引导模型中扩散损失的公式\u003c/p\u003e\n\n\\[L_{\\text{diffusion}}=\\mathbb{E}_{t, \\mathbf{x}_{0}, \\epsilon}\\left[\\left\\|\\epsilon-\\epsilon_{\\theta}\\left(\\mathbf{x}_{t}, t\\mid y\\right)\\right\\|^{2}\\right],\\]\n\n\u003cp\u003e我们可以将 prior 扩散损失（the prior diffusion loss）表示为\u003c/p\u003e\n\n\\[L_{\\text{prior:diffusion}}=\\mathbb{E}_{t}\\left[\\left\\|z_{i}-f_{\\theta}\\left({z}_{i}^{t}, t \\mid y\\right)\\right\\|^{2}\\right],\\]\n\n\u003cp\u003e其中\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\\(f_{\\theta}\\)：prior 模型\u003c/li\u003e\n  \u003cli\u003e\\({z}_{i}^{t}\\)：带噪图像的嵌入\u003c/li\u003e\n  \u003cli\u003e\\(t\\)：时间戳\u003c/li\u003e\n  \u003cli\u003e\\(y\\)：用于引导的标题。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 id=\"424-小结\"\u003e4.2.4 小结\u003c/h3\u003e\n\n\u003cp\u003e以上就是 unCLIP 的前半部分，旨在生成一个能够将文本中的所有重要信息封装到 CLIP 样式的 image embedding 中的模型。\n有了这个模型之后，就能根据用户输入的文本得到一个 image embedding。\n而有了 image embedding，就能基于扩散模型\u003cstrong\u003e\u003cmark\u003e反向\u003c/mark\u003e\u003c/strong\u003e（un-）生成最终的视觉输出 ——\n这就是 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eunCLIP\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 名称的由来 —— 从 image embedding 回溯到图像，与训练 CLIP image encoder 的过程相反。\u003c/p\u003e\n\n\u003cp\u003e接下来看 DALL·E 2 中是如何实现这个反向过程的。\u003c/p\u003e\n\n\u003ch2 id=\"42-the-decoder基于-glide-的改进\"\u003e4.2 The decoder：基于 GLIDE 的改进\u003c/h2\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e\u003cstrong\u003e\u003cmark\u003e一个扩散模型的尽头是另一个扩散模型！\u003c/mark\u003e\u003c/strong\u003e（After one diffusion model it is time for another diffusion model!）。\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e在 DALL·E 2 中，“另一个扩散模型”就是前面已经介绍过的 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eGLIDE\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e对 GLIDE 做了点修改，将 prior 输出的 CLIP image embedding 添加到 vanilla GLIDE text encoder 中。\n其实这正是 \u003cstrong\u003e\u003cmark\u003eprior 的训练目标 - 为 decoder 提供信息\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003ch2 id=\"43-引导扩散和上采样\"\u003e4.3 引导扩散和上采样\u003c/h2\u003e\n\n\u003cp\u003e引导方式与普通的 GLIDE 一样。为改进效果，10% 概率将 CLIP embedding 设置为 \\(\\emptyset\\)，50% 概率设置文本标题 \\(y\\)。\u003c/p\u003e\n\n\u003cp\u003e跟 GLIDE 一样，在图像生成之后，利用另一个扩散模型进行上采样。\n这次用了两个上采样模型（而不是原始 GLIDE 中的一个），一个将图像从 64x64 增加到 256x256，另一个进一步提高分辨率到 1024x1024。\u003c/p\u003e\n\n\u003ch1 id=\"5-google-imagen删繁就简扩散三连202205\"\u003e5 Google \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eImagen\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e：删繁就简，扩散三连，\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e2022.05\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/h1\u003e\n\n\u003cp\u003eDALL·E 2 发布不到两个月，\nGoogle Brain 团队也展示了自己的最新成果 - Imagen（Saharia 等 [7]）。\u003c/p\u003e\n\n\u003ch2 id=\"51-架构t5-xxl--diffusion--diffusion--diffusion\"\u003e5.1 架构：\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eT5-XXL + Diffusion + Diffusion + Diffusion\u003c/code\u003e\u003c/h2\u003e\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/assets/img/rise-of-diffusion-based-models/imagen-arch.png\" width=\"60%\"/\u003e\n\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e Fig. Overview of Imagen architecture. [7] \u003c/p\u003e\n\n\u003cp\u003eImagen 架构在结构上非常简单：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e预训练的\u003cstrong\u003e\u003cmark\u003e文本模型\u003c/mark\u003e\u003c/strong\u003e用于创建 embedding，然后用这些 embedding 扩散成图像；\u003c/li\u003e\n  \u003cli\u003e通过超分辨率扩散模型增加分辨率。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e但架构中还是有一些新颖之处，比如模型本身和训练过程，总体来说还是先进一些。\n这里只介绍下它与前面几个模型不同之处。\u003c/p\u003e\n\n\u003ch2 id=\"52-与-glidedalle-2-等架构的不同\"\u003e5.2 与 GLIDE、DALL·E 2 等架构的不同\u003c/h2\u003e\n\n\u003ch3 id=\"521-使用预训练的-transformer-t5-xxl-而不是从头开始训练\"\u003e5.2.1 使用预训练的 transformer (\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eT5-XXL\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e) 而不是从头开始训练\u003c/h3\u003e\n\n\u003cp\u003e与 OpenAI 的工作相比，这是\u003cstrong\u003e\u003cmark\u003e核心区别\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e对于 text embedding，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eGLIDE 使用了一个\u003cstrong\u003e\u003cmark\u003e新的、经过专门训练的\u003c/mark\u003e\u003c/strong\u003e transformer 模型；\u003c/li\u003e\n  \u003cli\u003eImagen 使用了一个\u003cstrong\u003e\u003cmark\u003e预训练的、冻结的 T5-XXL\u003c/mark\u003e\u003c/strong\u003e 模型 [4]。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e这里的想法是，T5-XXL 模型在语言处理方面比仅在图像标题上训练的模型\u003cstrong\u003e\u003cmark\u003e有更多的上下文\u003c/mark\u003e\u003c/strong\u003e，\n因此能够\u003cstrong\u003e\u003cmark\u003e在不需要额外微调的情况下产生更有价值的 embedding\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003ch3 id=\"522-使用更高效的底层神经网络efficient-u-net\"\u003e5.2.2 使用更高效的底层神经网络（efficient U-net）\u003c/h3\u003e\n\n\u003cp\u003e使用了称为 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eEfficient U-net\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 的升级版神经网络，\n作为\u003cstrong\u003e\u003cmark\u003e超分辨率\u003c/mark\u003e\u003c/strong\u003e扩散模型的核心。\u003c/p\u003e\n\n\u003cp\u003e比之前的版本更节省内存，更简单，并且收敛速度更快。\n主要来自残差块和网络内部值的额外缩放。细节详见 [7]。\u003c/p\u003e\n\n\u003ch3 id=\"523-使用-conditioning-augmentation-来增强图像保真度image-fidelity\"\u003e5.2.3 使用 conditioning augmentation 来增强图像保真度（image fidelity）\u003c/h3\u003e\n\n\u003cp\u003eImagen 可以视为是\u003cstrong\u003e\u003cmark\u003e一系列扩散模型\u003c/mark\u003e\u003c/strong\u003e，因此在\u003cstrong\u003e\u003cmark\u003e模型连接处\u003c/mark\u003e\u003c/strong\u003e\n（areas where the models are linked）可以进行增强。\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eHo 等提出了一种称为条件增强（conditioning augmentation）的解决方案[10]。\n简单来说就是在将低分辨率图像\u003cstrong\u003e\u003cmark\u003e输入超分辨率模型之前\u003c/mark\u003e\u003c/strong\u003e对其 apply\n\u003cstrong\u003e\u003cmark\u003e多个 data augmentation 技术\u003c/mark\u003e\u003c/strong\u003e，如高斯模糊。\u003c/li\u003e\n  \u003cli\u003e还有一些对于低 FID score 和高图像保真度至关重要的资源（例如 dynamic thresholding），\n论文 [7] 中有详细解释。但这些方法的核心已经在前几节都涵盖了。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 id=\"53-小结\"\u003e5.3 小结\u003c/h2\u003e\n\n\u003cp\u003e截至 2022.04，Google’s Imagen 是最好的 text-to-image generation 模型。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/assets/img/rise-of-diffusion-based-models/imagen-images.png\"/\u003e\n\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\nFig. Imagen 根据提示词生成的一些图片。[7]\n\u003c/p\u003e\n\n\u003ch1 id=\"6-总结\"\u003e6 总结\u003c/h1\u003e\n\n\u003ch2 id=\"61-如何评估模型好坏\"\u003e6.1 如何评估模型好坏\u003c/h2\u003e\n\n\u003cp\u003eImagen 的作者提供了两种评估方式，详见论文 [7]。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/assets/img/rise-of-diffusion-based-models/eval-models-1.png\" width=\"40%\"/\u003e\n\u003c/p\u003e\n\u003cp align=\"center\"\u003e Fig. Comparison of several models. [7] \u003c/p\u003e\n\n\u003ch2 id=\"62-好玩儿的才刚开始\"\u003e6.2 好玩儿的才刚开始\u003c/h2\u003e\n\n\u003cp\u003e除了图像生成能力，文生图模型还有许多有趣的特征，比如\u003cstrong\u003e\u003cmark\u003e图像修复、风格转换和图像编辑\u003c/mark\u003e\u003c/strong\u003e等等。\u003c/p\u003e\n\n\u003cp\u003e另一方面，扩散模型也还存在一些缺点，例如与以前的模型相比，采样速度较慢[16]。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\n  \u003cimg src=\"/assets/img/rise-of-diffusion-based-models/eval-models-2.png\" width=\"50%\"/\u003e\n\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e不同类型的文生图模型的考虑因素/优缺点 [16] \u003c/p\u003e\n\n\u003cp\u003e最后，对于喜欢深入实现细节的人，强烈推荐 [19]，这是一些 github 项目，\n众人手撸实现那些没有公开代码的模型。\u003c/p\u003e\n\n\u003ch1 id=\"参考资料\"\u003e参考资料\u003c/h1\u003e\n\n\u003col\u003e\n  \u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2005.14165\"\u003eLanguage Models are Few-Shot Learners\u003c/a\u003e Tom B. Brown et al. 2020\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://arxiv.org/abs/1503.03585v8\"\u003e\u003cmark\u003eDeep Unsupervised Learning using Nonequilibrium Thermodynamics\u003c/mark\u003e\u003c/a\u003e 扩散数学原理，2015\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2006.11239v2\"\u003e\u003cmark\u003eDenoising Diffusion Probabilistic Models\u003c/mark\u003e\u003c/a\u003e 去噪扩散模型论文，基于 2，同一作者，2020\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2002.08910?fbclid=IwAR1nm66Of1JzM5cnJvHtjuy0w-5JLYOSoTpteXXZSr0JcSkyJuApIPJlsHQ\"\u003eHow Much Knowledge Can You Pack Into the Parameters of a Language Model?\u003c/a\u003e Adam Roberts, Colin Raffel, Noam Shazeer. 2020\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://openreview.net/pdf?id=qw8AKxfYbI\"\u003eClassifier-Free Diffusion Guidance\u003c/a\u003e Jonathan Ho, Tim Salimans. 2021\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2112.10741?s=09\"\u003e\u003cmark\u003eGLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models\u003c/mark\u003e\u003c/a\u003e Alex Nichol et al. 2021\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2102.12092\"\u003eZero-Shot Text-to-Image Generation\u003c/a\u003e Aditya Ramesh et al. 2021\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2105.05233?curius=520\"\u003eDiffusion Models Beat GANs on Image Synthesis\u003c/a\u003e Prafulla Dhariwal, Alex Nichol. 2021\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2103.00020\"\u003eLearning Transferable Visual Models From Natural Language Supervision\u003c/a\u003e Alec Radford et al. 2021\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2106.15282\"\u003eCascaded Diffusion Models for High Fidelity Image Generation\u003c/a\u003e Jonathan Ho et al. 2021\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2204.06125\"\u003eHierarchical Text-Conditional Image Generation with CLIP Latents\u003c/a\u003e Aditya Ramesh et al. 2022\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://arxiv.org/abs/2205.11487\"\u003ePhotorealistic Text-to-Image Diffusion Models with Deep Language Understanding\u003c/a\u003e Chitwan Saharia et al. 2022\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://lilianweng.github.io/posts/2021-07-11-diffusion-models/\"\u003eWhat are Diffusion Models?\u003c/a\u003e 数学推导，Lilian Weng. 2021\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://angusturner.github.io/generative_models/2021/06/29/diffusion-probabilistic-models-I.html\"\u003eDiffusion Models as a kind of VAE\u003c/a\u003e 数学推导，Angus Turner. 2021\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://ayandas.me/blog-tut/2021/12/04/diffusion-prob-models.html\"\u003eAn introduction to Diffusion Probabilistic Models\u003c/a\u003e 数学推导，Ayan Das. 2021\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://developer.nvidia.com/blog/improving-diffusion-models-as-an-alternative-to-gans-part-1/\"\u003eImproving Diffusion Models as an Alternative To GANs, Part 1\u003c/a\u003e Arash Vahdat, Karsten Kreis. 2022\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://docs.google.com/spreadsheets/d/1y7nAbmR4FREi6npB1u-Bo3GFdwdOPYJc617rBOxIRHY/htmlview?pru=AAABgRqAJJQ*agF3cOZ-eQVuWLxxWEwiWQ#gid=0\"\u003eDrawBench prompts\u003c/a\u003e Google Brain team. 2022\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://www.reddit.com/r/dalle2/\"\u003eDALL·E 2 subreddit\u003c/a\u003e Reddit. 2022\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://github.com/lucidrains?tab=repositories\"\u003ePhil Wang’s repositories\u003c/a\u003e Phil Wang. 2022\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003chr/\u003e\n\n\u003cp\u003e\u003ca href=\"https://notbyai.fyi\"\u003e\u003cimg src=\"/assets/img/Written-By-Human-Not-By-AI-Badge-white.svg\" alt=\"Written by Human, Not by AI\"/\u003e\u003c/a\u003e\n\u003ca href=\"https://notbyai.fyi\"\u003e\u003cimg src=\"/assets/img/Written-By-Human-Not-By-AI-Badge-black.svg\" alt=\"Written by Human, Not by AI\"/\u003e\u003c/a\u003e\u003c/p\u003e\n\n\n  \u003c!-- POST NAVIGATION --\u003e\n  \u003cdiv class=\"postNav clearfix\"\u003e\n     \n      \u003ca class=\"prev\" href=\"/blog/gpu-advanced-notes-3-zh/\"\u003e\u003cspan\u003e« GPU 进阶笔记（三）：华为 NPU/GPU 演进（2024）\u003c/span\u003e\n      \n    \u003c/a\u003e\n      \n      \n      \u003ca class=\"next\" href=\"/blog/linux-cpu-1-zh/\"\u003e\u003cspan\u003eLinux 服务器功耗与性能管理（一）：CPU 硬件基础（2024） »\u003c/span\u003e\n       \n      \u003c/a\u003e\n     \n  \u003c/div\u003e\n\u003c/div\u003e",
  "Date": "2024-01-21T00:00:00Z",
  "Author": "Arthur Chiao"
}