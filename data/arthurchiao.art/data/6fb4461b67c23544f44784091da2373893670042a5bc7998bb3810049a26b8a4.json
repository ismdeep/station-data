{
  "Source": "arthurchiao.art",
  "Title": "Trip.com: First Step towards Cloud Native Networking",
  "Link": "https://arthurchiao.art/blog/trip-first-step-towards-cloud-native-networking/",
  "Content": "\u003cdiv class=\"post\"\u003e\n  \n  \u003ch1 class=\"postTitle\"\u003eTrip.com: First Step towards Cloud Native Networking\u003c/h1\u003e\n  \u003cp class=\"meta\"\u003ePublished at 2020-01-19 | Last Update 2020-04-25\u003c/p\u003e\n  \n  \u003cp\u003e\u003cstrong\u003eUpdate [2020-04-25]\u003c/strong\u003e: This post was summarized by Cilium official blog:\n\u003ca href=\"https://cilium.io/blog/2020/02/05/how-trip-com-uses-cilium\"\u003eUser Story - How Trip.com uses Cilium\u003c/a\u003e.\u003c/p\u003e\n\n\u003chr/\u003e\n\n\u003cul id=\"markdown-toc\"\u003e\n  \u003cli\u003e\u003ca href=\"#1-problems-and-requirements\" id=\"markdown-toc-1-problems-and-requirements\"\u003e1. Problems and requirements\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#11-limitations-of-current-networking-scheme\" id=\"markdown-toc-11-limitations-of-current-networking-scheme\"\u003e1.1 Limitations of current networking scheme\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#12-re-examine-current-solution\" id=\"markdown-toc-12-re-examine-current-solution\"\u003e1.2 Re-examine current solution\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#2-cloud-native-l3-network\" id=\"markdown-toc-2-cloud-native-l3-network\"\u003e2. Cloud-native L3 network\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#21-mainstream-networking-solutions-in-the-industry\" id=\"markdown-toc-21-mainstream-networking-solutions-in-the-industry\"\u003e2.1 Mainstream networking solutions in the industry\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#neutronovs-large-l2-network\" id=\"markdown-toc-neutronovs-large-l2-network\"\u003eNeutron+OVS large L2 network\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#calicoflannel\" id=\"markdown-toc-calicoflannel\"\u003eCalico/Flannel\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#cilium\" id=\"markdown-toc-cilium\"\u003eCilium\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#22-cloud-native-solution\" id=\"markdown-toc-22-cloud-native-solution\"\u003e2.2 Cloud-native solution\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#3-customizations\" id=\"markdown-toc-3-customizations\"\u003e3. Customizations\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#31-cross-host-networking\" id=\"markdown-toc-31-cross-host-networking\"\u003e3.1. Cross-host networking\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#311-on-premises-direct-routing-with-ciliumbird\" id=\"markdown-toc-311-on-premises-direct-routing-with-ciliumbird\"\u003e3.1.1 On-premises: direct routing with Cilium+Bird\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#312-aws-eni-mode-with-custom-configurations\" id=\"markdown-toc-312-aws-eni-mode-with-custom-configurations\"\u003e3.1.2 AWS: ENI mode with custom configurations\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#32-custom-feature-fixed-ip-statefulset\" id=\"markdown-toc-32-custom-feature-fixed-ip-statefulset\"\u003e3.2. Custom feature: fixed IP statefulset\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#33-monitoring--alerting\" id=\"markdown-toc-33-monitoring--alerting\"\u003e3.3. Monitoring \u0026amp; Alerting\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#34-cilium-etcd\" id=\"markdown-toc-34-cilium-etcd\"\u003e3.4. Cilium etcd\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#35-deployment\" id=\"markdown-toc-35-deployment\"\u003e3.5. Deployment\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#351-out-of-band-deployment-docker-compose--salt\" id=\"markdown-toc-351-out-of-band-deployment-docker-compose--salt\"\u003e3.5.1 Out-of-band deployment: \u003ccode class=\"language-plaintext highlighter-rouge\"\u003edocker-compose + salt\u003c/code\u003e\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#352-simultaneously-run-two-networking-solutions-in-one-k8s-cluster\" id=\"markdown-toc-352-simultaneously-run-two-networking-solutions-in-one-k8s-cluster\"\u003e3.5.2 Simultaneously run two networking solutions in one K8S cluster\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#36-more-custom-configurations\" id=\"markdown-toc-36-more-custom-configurations\"\u003e3.6. More custom configurations\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#4-current-deployment-status\" id=\"markdown-toc-4-current-deployment-status\"\u003e4. Current deployment status\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#references\" id=\"markdown-toc-references\"\u003eReferences\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003chr/\u003e\n\n\u003cp\u003eIn a \u003ca href=\"https://arthurchiao.github.io/blog/ctrip-network-arch-evolution/\"\u003eprevious sharing\u003c/a\u003e, we\nshowed how our network virtualization schemes have evolved in the past 7 years.\nAt the end of that speech, we mentioned that we were evaluating some cloud-native\nsolutions. This post serves as a successor of that sharing. We will update some\nrecent progresses in the below.\u003c/p\u003e\n\n\u003cp\u003eTo have a better understanding, let’s start from re-describing the problems and\nrequirements we were facing.\u003c/p\u003e\n\n\u003ch1 id=\"1-problems-and-requirements\"\u003e1. Problems and requirements\u003c/h1\u003e\n\n\u003ch2 id=\"11-limitations-of-current-networking-scheme\"\u003e1.1 Limitations of current networking scheme\u003c/h2\u003e\n\n\u003cp\u003eAs has been explained in the \u003ca href=\"https://arthurchiao.github.io/blog/ctrip-network-arch-evolution/\"\u003eprevious post\u003c/a\u003e, for\nsome practical reasons, we extended our Neutron-based networking stack into\nsupporting container platforms (Mesos, Kubernetes), thus we have one networking\ncomponent simultaneously serving three platforms:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eOpenStack\u003c/li\u003e\n  \u003cli\u003eBaremetal (internally developed system)\u003c/li\u003e\n  \u003cli\u003eKubernetes (and previously - Mesos)\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/trip-first-step-towards-cloud-native-networking/legacy-solution.png\" width=\"40%\" height=\"40%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 1-1. Single network service (Neutron) powers 3 platforms: OpenStack, Baremetal and Kubernetes\u003c/p\u003e\n\n\u003cp\u003eWhile this benefits a lot in the process of transforming from virtual machines\nto containers, it becomes a performance bottleneck when the cluster grows large,\ne.g.  total instances (VM+Container+BM) exceed \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e40K\u003c/code\u003e. At such scale, allocating\nan IP address may take \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e10s\u003c/code\u003e or even more.\u003c/p\u003e\n\n\u003cp\u003eAlthough we’ve devoted much efforts into optimization, which have reduced the API\nresponse time by two orders of magnitude, the overall performance is still\nun-satisfactory, especially considering the expected cluster scale in the not\nfar future. Essentially this is because, Neutron is a networking solution for\nvirtual machine platform, which has a much smaller scale (in terms of instances)\nand is comfortable with \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eseconds\u003c/code\u003e-level API responses.\u003c/p\u003e\n\n\u003cp\u003eTo ultimately solve these problems, we have to re-examine the current\nproblems and requirements.\u003c/p\u003e\n\n\u003ch2 id=\"12-re-examine-current-solution\"\u003e1.2 Re-examine current solution\u003c/h2\u003e\n\n\u003cp\u003eAfter many times iterations of current problems reviewing as well as future\nneeds clarification, we listed five critical problems/requirements of our networking:\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/trip-first-step-towards-cloud-native-networking/problems.png\" width=\"100%\" height=\"100%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 1-2. Problems and requirements\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eAs a \u003cstrong\u003ecentral IPAM\u003c/strong\u003e, Neutron \u003cstrong\u003eprohibits Kubernetes clusters from growing\neven more larger\u003c/strong\u003e.\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eLarge L2 network\u003c/strong\u003e has \u003cstrong\u003einherent HW bottlenecks\u003c/strong\u003e, e.g. in the 3-tier\nhierarchical topology, the core router/switch has to \u003cstrong\u003emaintain forwarding\nentries for each instance\u003c/strong\u003e, and this has a hardware limit (e.g. \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e64K*70% = 48K\u003c/code\u003e\neffective entries [5]). This means that the total instance in this physical network can not\nexceed \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e48K\u003c/code\u003e - for Kubernetes, this is really a bad news.\u003c/li\u003e\n  \u003cli\u003eCurrent networking solution is \u003cstrong\u003enon-K8S-native\u003c/strong\u003e, so it doesn’t support\nfeatures such as Kubernetes Service (ClusterIP), thus \u003cstrong\u003emany applications\n(e.g. Spinnaker, Istio) can not be deployed or migrated to container platform\u003c/strong\u003e.\u003c/li\u003e\n  \u003cli\u003eLacking of \u003cstrong\u003ehost level firewalls\u003c/strong\u003e - or network policies in K8S. \u003cstrong\u003eAll rules\nare applied on HW FW\u003c/strong\u003e, which is a big burden and becomes unmaintainable.\u003c/li\u003e\n  \u003cli\u003eSeperate solutions for on-premises and AWS, result to high\ndevelopment and maintainance costs.\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch1 id=\"2-cloud-native-l3-network\"\u003e2. Cloud-native L3 network\u003c/h1\u003e\n\n\u003cp\u003eBased on the above analysis, we started to survey and evaluate next generation\nnetworking solutions. Correspoinding to the questions above, our new solution\nshould provide:\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eDe-centralized IPAM: local IPAM on each node\u003c/li\u003e\n  \u003cli\u003eNo HW bottleneck: use L3 networking between hosts\u003c/li\u003e\n  \u003cli\u003eK8S-native: support all K8S functionalities natively\u003c/li\u003e\n  \u003cli\u003eNetwork policy: host or application level network security rules on each node\u003c/li\u003e\n  \u003cli\u003eSingle solution covers both on-premises and AWS\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch2 id=\"21-mainstream-networking-solutions-in-the-industry\"\u003e2.1 Mainstream networking solutions in the industry\u003c/h2\u003e\n\n\u003cp\u003eLooking at some of the mainstream networking solutions in the past 10 years.\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/trip-first-step-towards-cloud-native-networking/network-evolution-1.png\" width=\"60%\" height=\"60%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 2-1. Network evolutions\u003c/p\u003e\n\n\u003ch3 id=\"neutronovs-large-l2-network\"\u003eNeutron+OVS large L2 network\u003c/h3\u003e\n\n\u003cp\u003eIn 2010s, along with the concept of “Cloud Computing”, OpenStack quickly\nbecame the dominant virtualization platform in open source community. Along with\nthis trend is its networking stack: Neutron+OVS based large layer 2 network.\u003c/p\u003e\n\n\u003cp\u003eThere are two choices for cross-host networking in this large layer 2 network:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003etunnling (software VxLAN): this doesn’t involve physical network’s awareness,\nbut suffers from performance issues\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003edirect forwarding\u003c/strong\u003e (provider network model): needs physical network’s\nawareness, all gateways configured on HW router in the underlying physical network\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eIf you are using “provider network model” like us, you may encounter the above\nmentioned HW bottlenecks when the cluster is really large (e.g. \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e50K+\u003c/code\u003e instances).\u003c/p\u003e\n\n\u003ch3 id=\"calicoflannel\"\u003eCalico/Flannel\u003c/h3\u003e\n\n\u003cp\u003eContainer platforms get more and more popular since ~2005.\nK8s-native solutions such as Flannel and Calico evolves with this trend.\nCompared with the central IPAM model in OpenStack, container platforms favor\nlocal IPAM - one IPAM on each host.\u003c/p\u003e\n\n\u003cp\u003eBut, this solutions suffers from severe performance issues when the cluster goes\nreally large [1]. Essentially this is because those solutions are based on\n\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eiptables\u003c/code\u003e, which is a chain design, thus has \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eO(n)\u003c/code\u003e complexity, and it’s also\nhard to troubleshooting when there are tens thousands of iptables rules on each\nnode.\u003c/p\u003e\n\n\u003cp\u003eSo there comes some optimizated solutions to alleviate this problem.\nAmong them is \u003ca href=\"https://github.com/cilium/cilium\"\u003eCilium\u003c/a\u003e.\u003c/p\u003e\n\n\u003ch3 id=\"cilium\"\u003eCilium\u003c/h3\u003e\n\n\u003cp\u003eCilium is also a K8s-native solution, but solves the performance problem by\nutilizing a new kernel technology: eBPF. eBPF rules bases on hashing, so it has\n\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eO(1)\u003c/code\u003e complexity.\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/trip-first-step-towards-cloud-native-networking/ebpf-kills-iptables.png\" width=\"40%\" height=\"40%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 2-2. eBPF kills iptables, image from [6]\u003c/p\u003e\n\n\u003cp\u003eYou can find more detailed performance comparison in \u003ca href=\"https://cilium.io/blog/\"\u003eCilium’s blog website\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eAfter several POC verifications, we decicded to adopt Cilium as our next generation\nnetworking solution. This is a 10-year leap for us in terms of networking stack.\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/trip-first-step-towards-cloud-native-networking/network-evolution-2.png\" width=\"60%\" height=\"60%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 2-3. Network evolutions\u003c/p\u003e\n\n\u003ch2 id=\"22-cloud-native-solution\"\u003e2.2 Cloud-native solution\u003c/h2\u003e\n\n\u003cp\u003eBased on massive research and real environment testings, we decided to adopt\nCilium as our next generation networking plan.\u003c/p\u003e\n\n\u003cp\u003eThe high level topology looks like this:\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/trip-first-step-towards-cloud-native-networking/new-solution-topo.png\" width=\"70%\" height=\"70%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 2-4. High level topology of the new solution\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003efor intra-host networking: all instances connects to Cilium\u003c/li\u003e\n  \u003cli\u003efor inter-host networking\n    \u003cul\u003e\n      \u003cli\u003eon-premises: using BGP for direct routing\u003c/li\u003e\n      \u003cli\u003eAWS: using ENI (Cilium natively supports)\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eBenefits of this solution:\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eDe-centralized IPAM, no performance bottleneck of central IPAM\u003c/li\u003e\n  \u003cli\u003eNo HW bottleneck - replace large L2 networking with L3 network\u003c/li\u003e\n  \u003cli\u003eK8s-native: new cloud-native applications can be deployed\u003c/li\u003e\n  \u003cli\u003eSecurity: network policies\u003c/li\u003e\n  \u003cli\u003eOne solution for both on-premises and AWS\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch1 id=\"3-customizations\"\u003e3. Customizations\u003c/h1\u003e\n\n\u003cp\u003eHardly could a solution be rolled out into production environment without any\nchanges/customizations, especially when you already have large clusters with\ncritical businesses running on them for years. Thus we spent much time on\nidentifying the couplings between our business and networking, try our best to\nmake business users unaware of the underlying changes.\u003c/p\u003e\n\n\u003cp\u003eBesides, we also explored our own way for efficient deployment, upgrade and\nmaintanance.\u003c/p\u003e\n\n\u003cp\u003eBelow are some of our configurations/customizations that may differ from vanilla\nCilium deployments.\u003c/p\u003e\n\n\u003ch2 id=\"31-cross-host-networking\"\u003e3.1. Cross-host networking\u003c/h2\u003e\n\n\u003ch3 id=\"311-on-premises-direct-routing-with-ciliumbird\"\u003e3.1.1 On-premises: direct routing with Cilium+Bird\u003c/h3\u003e\n\n\u003cp\u003eWe use BGP for inter-host communication, and choose\n\u003ca href=\"https://bird.network.cz/\"\u003eBird\u003c/a\u003e [2] (\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e2.x\u003c/code\u003e version) as BGP agent.\u003c/p\u003e\n\n\u003cp\u003eCilium official documentation recommends \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ekube-router\u003c/code\u003e as BGP agent, while it is\na nice agent for automatic bootstrap, it has limited functionalities compared\nwith bird, e.g. BFD, ECMP, which are very important when considering advanced\nfeatures and performance issues.\u003c/p\u003e\n\n\u003cp\u003eFor metric collecting, we use\n\u003ca href=\"https://github.com/czerwonk/bird_exporter\"\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebird_exporter\u003c/code\u003e\u003c/a\u003e [3], but we made\nsome changes and built our own docker images, and use \u003ccode class=\"language-plaintext highlighter-rouge\"\u003edaemonset\u003c/code\u003e to deploy it.\nBesides, we also created our own moniotring dashboards and alertings.\u003c/p\u003e\n\n\u003ch3 id=\"312-aws-eni-mode-with-custom-configurations\"\u003e3.1.2 AWS: ENI mode with custom configurations\u003c/h3\u003e\n\n\u003cp\u003eOn AWS, we deploy Cilium with ENI mode, but also with some customizations.\u003c/p\u003e\n\n\u003cp\u003eFor example, Cilium agent will create a \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eCiliumNode\u003c/code\u003e CRD on agent start, but the\ndefault configuration leaves the ENI spec (\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eENI\u003c/code\u003e field in \u003ccode class=\"language-plaintext highlighter-rouge\"\u003estruct NetConf\u003c/code\u003e)\nempty, which results \u003cstrong\u003ethe agent arbitrarily choose a subnetTag for creating ENI\nand pre-allocate IPs\u003c/strong\u003e. This causes problems for us as some subnets are not\nmeant to be used by Pod, e.g. the outbound subnet.\u003c/p\u003e\n\n\u003cp\u003eLuckily, Cilium provides a way to workaround this (but doesn’t provide detailed\ndocumentation \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e:(\u003c/code\u003e, we walked through the source code and made things eventually\nwork). Here is our workaround:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eadd \u003cstrong\u003ecostom ENI configurations\u003c/strong\u003e to CNI conf file (\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e/etc/cni/net.d/xx-cilium-cni-eni.conf\u003c/code\u003e)\u003c/li\u003e\n  \u003cli\u003especify \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e--read-eni-configuration \u0026lt;file\u0026gt;\u003c/code\u003e to \u003cstrong\u003eexplicitly load this conf file\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003ewith these two steps, the agent will choose the correct subnets for ENI/IP allocating.\u003c/p\u003e\n\n\u003cp\u003eThis fashion also allows us to \u003cstrong\u003epermanently specify\nmin-allocate/pre-allocate/etc parameters\u003c/strong\u003e. In comparison, \u003cstrong\u003ein the default\nsettings, runtime values will be overwrite by default values when agent\nrestart\u003c/strong\u003e.\u003c/p\u003e\n\n\u003ch2 id=\"32-custom-feature-fixed-ip-statefulset\"\u003e3.2. Custom feature: fixed IP statefulset\u003c/h2\u003e\n\n\u003cp\u003eWe added fixed IP funtionality (only for statefulset) for our special case - an\nintermediate step torwards true cloud-native for some applications.\u003c/p\u003e\n\n\u003cp\u003eThis code is a little bit dirty, but it is loosely coupled with upstream code,\nso we could rebase to the newest upstream code with just \u003ccode class=\"language-plaintext highlighter-rouge\"\u003egit rebase \u0026lt;tag\u0026gt;\u003c/code\u003e.\u003c/p\u003e\n\n\u003cp\u003eBut this feature currently relies on \u003cstrong\u003esticky scheduler\u003c/strong\u003e, which is a simple k8s\nscheduler implemented by us internally (you could also find similar schedulers\non github), so this feature is not ready to be widely used by other users.\u003c/p\u003e\n\n\u003ch2 id=\"33-monitoring--alerting\"\u003e3.3. Monitoring \u0026amp; Alerting\u003c/h2\u003e\n\n\u003cp\u003eCilium officially recommends \u003cstrong\u003eGrafana + Prometheus\u003c/strong\u003e for monitoring and\nalerting, and provides yaml files to create those infrastructures. It’s easy to\nget started.\u003c/p\u003e\n\n\u003cp\u003eBut we re-used our eixsting monitoring \u0026amp; alerting infrastructures, which is\nquick similar but internally optimized:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003einternally optimized grafana\u003c/li\u003e\n  \u003cli\u003einternally developed agent for metric collecting\u003c/li\u003e\n  \u003cli\u003einternally optimized \u003ca href=\"https://github.com/VictoriaMetrics/VictoriaMetrics\"\u003eVictoriaMetrics\u003c/a\u003e (compatible with prometheus)\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eBesides, we created our own dashboards based on those metrics. Below lists some\nof them (resize your page to see them more clearly):\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/trip-first-step-towards-cloud-native-networking/dashboard-all-clusters.png\" width=\"100%\" height=\"100%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 3-1. Metrics of all Cilium-enabled clusters (legacy nodes + cilium nodes)\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/trip-first-step-towards-cloud-native-networking/dashboard-agent-status.png\" width=\"100%\" height=\"100%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 3-2. Metrics of single cluster\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/trip-first-step-towards-cloud-native-networking/dashboard-top-n.png\" width=\"100%\" height=\"100%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 3-3. Top N nodes by various metrics in single cluster\u003c/p\u003e\n\n\u003ch2 id=\"34-cilium-etcd\"\u003e3.4. Cilium etcd\u003c/h2\u003e\n\n\u003cp\u003eWe setup independent etcd clusters for Cilium, which doesn’t rely on K8S.\u003c/p\u003e\n\n\u003ch2 id=\"35-deployment\"\u003e3.5. Deployment\u003c/h2\u003e\n\n\u003cp\u003eWell, this differs a lot from the community.\u003c/p\u003e\n\n\u003ch3 id=\"351-out-of-band-deployment-docker-compose--salt\"\u003e3.5.1 Out-of-band deployment: \u003ccode class=\"language-plaintext highlighter-rouge\"\u003edocker-compose + salt\u003c/code\u003e\u003c/h3\u003e\n\n\u003cp\u003eWe don’t want Cilium relies on K8s - at least currently. Cilium is an underlying\nservice for K8s, not the opposite. (What’s more, we may even consider supporting\nOpenStack with Cilium - but don’t be suprised, we have’t decided yet).\u003c/p\u003e\n\n\u003cp\u003eSo, we use \u003ccode class=\"language-plaintext highlighter-rouge\"\u003edocker-compose\u003c/code\u003e + \u003ccode class=\"language-plaintext highlighter-rouge\"\u003esalt\u003c/code\u003e for deployment, which doesn’t rely on K8S.\nThis enables us to:\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eeffectively \u003cstrong\u003e“tolerate all taints”\u003c/strong\u003e (which K8S daemonset doesn’t support)\u003c/li\u003e\n  \u003cli\u003eenable us to \u003cstrong\u003ecompletely control rolling-update process\u003c/strong\u003e (which may span\nover weeks for really large PROD clusters) according to our needs\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eAnother benefit of docker-compose over daemonset is that we can have \u003cstrong\u003edistinct\nconfigurations at node-level\u003c/strong\u003e for cilium-agent, while the latter uses configmap,\nwhich is limited to cluster-level.\u003c/p\u003e\n\n\u003cp\u003eWhat’s more, we intentionally made the docker-compose files compatible with\nupstream images - which means, for example, if a node doesn’t need fixed\nIP feature, we could just \u003ccode class=\"language-plaintext highlighter-rouge\"\u003edocker-compose up\u003c/code\u003e the cilium agent with\ncommunity cilium images.\u003c/p\u003e\n\n\u003ch3 id=\"352-simultaneously-run-two-networking-solutions-in-one-k8s-cluster\"\u003e3.5.2 Simultaneously run two networking solutions in one K8S cluster\u003c/h3\u003e\n\n\u003cp\u003eWe added new cilium-powered nodes to our existing cluster, so there are actually\ntwo types of nodes working together:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003enodes with legacy networking solution: based on \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eNeutron + OVS + Neutron-OVS-Agent + CustomCNI\u003c/code\u003e\u003c/li\u003e\n  \u003cli\u003enodes with cilium networking solution: \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eCilium+Bird\u003c/code\u003e (on-premises)\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eWe managed to let the Pods on these two types of nodes reachable to each other.\nOne of our next plans is to migrate the Pods from legacy node to new Cilium\nnode.\u003c/p\u003e\n\n\u003ch2 id=\"36-more-custom-configurations\"\u003e3.6. More custom configurations\u003c/h2\u003e\n\n\u003cul\u003e\n  \u003cli\u003eOn-premises \u0026amp; AWS: turnoff \u003ccode class=\"language-plaintext highlighter-rouge\"\u003emasqurade\u003c/code\u003e, make PodIP routable outside of k8s cluster\u003c/li\u003e\n  \u003cli\u003eOn-premises: turnoff auto allocate node CIDR in controller manager\n(\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e--allocate-node-cidrs=false\u003c/code\u003e), we explicitly allocate node CIDR (PodCIDR) to\neach node\u003c/li\u003e\n  \u003cli\u003eOn-premises: custom/simple L4LB based on Cilium+BGP, implementing K8S\nexternalIP feature\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch1 id=\"4-current-deployment-status\"\u003e4. Current deployment status\u003c/h1\u003e\n\n\u003cp\u003eWe have been stably running Cilium in our production environments for several months.\u003c/p\u003e\n\n\u003cp\u003eNow we have \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e6K+\u003c/code\u003e pods on Cilium, which span over PROD and UAT/FAT, as well as\non-premises and AWS.\u003c/p\u003e\n\n\u003cp\u003eBesides, we also have \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e10K+\u003c/code\u003e GitlabCI jobs on Cilium each\nday, with each job launches a new Pod (and terminates it after job finishes).\u003c/p\u003e\n\n\u003cp\u003eThis only accounts for a very small part of our total Pods, and\nin year 2020, we will start to migrate those legacy Pods to Cilium.\u003c/p\u003e\n\n\u003ch2 id=\"references\"\u003eReferences\u003c/h2\u003e\n\n\u003col\u003e\n  \u003cli\u003e\u003ca href=\"https://docs.google.com/presentation/d/1BaIAywY2qqeHtyGZtlyAp89JIZs59MZLKcFLxKE6LyM/edit#slide=id.p3\"\u003eScaling Kubernetes to Support 50,000 Services\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://bird.network.cz/\"\u003eThe BIRD Internet Routing Daemon Project\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://github.com/czerwonk/bird_exporter\"\u003eGithub: bird_exporter\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://github.com/VictoriaMetrics/VictoriaMetrics\"\u003eGithub: VictoriaMetrics\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://www.cisco.com/c/en/us/products/collateral/switches/nexus-7000-series-switches/data_sheet_c78-728410.html\"\u003eCisco Nexus 7700 F3-Series 24-Port 40 Gigabit Ethernet Module Data Sheet\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://cilium.io/blog/2018/04/24/cilium-10/\"\u003eCilium 1.0: Bringing the BPF Revolution to Kubernetes Networking and Security\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\n\n  \u003c!-- POST NAVIGATION --\u003e\n  \u003cdiv class=\"postNav clearfix\"\u003e\n     \n      \u003ca class=\"prev\" href=\"/blog/short-history-of-okr-zh/\"\u003e\u003cspan\u003e« [译] OKR 极简史（OReilly, 2016）\u003c/span\u003e\n      \n    \u003c/a\u003e\n      \n      \n      \u003ca class=\"next\" href=\"/blog/everything-is-distributed-zh/\"\u003e\u003cspan\u003e[译] 一切系统都是分布式的（OReilly, 2015） »\u003c/span\u003e\n       \n      \u003c/a\u003e\n     \n  \u003c/div\u003e\n\u003c/div\u003e",
  "Date": "2020-01-19T00:00:00Z",
  "Author": "Arthur Chiao"
}