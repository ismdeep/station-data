{
  "Source": "arthurchiao.art",
  "Title": "[译][论文] BERT：预训练深度双向 Transformers 做语言理解（Google，2019）",
  "Link": "https://arthurchiao.art/blog/bert-paper-zh/",
  "Content": "\u003cdiv class=\"post\"\u003e\n  \n  \u003ch1 class=\"postTitle\"\u003e[译][论文] BERT：预训练深度双向 Transformers 做语言理解（Google，2019）\u003c/h1\u003e\n  \u003cp class=\"meta\"\u003ePublished at 2024-03-10 | Last Update 2024-03-24\u003c/p\u003e\n  \n  \u003ch3 id=\"译者序\"\u003e译者序\u003c/h3\u003e\n\n\u003cp\u003e本文翻译自 2019 年 Google 的论文：\n\u003ca href=\"https://arxiv.org/abs/1810.04805\"\u003eBETT: Pre-training of Deep Bidirectional Transformers for Language Understanding\u003c/a\u003e。\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e@article{devlin2018bert,\n  title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding},\n  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},\n  journal={arXiv preprint arXiv:1810.04805},\n  year={2018}\n}\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003e与 GPT 一样，BERT \u003cstrong\u003e\u003cmark\u003e也基于 transformer 架构\u003c/mark\u003e\u003c/strong\u003e，\n从诞生时间来说，它位于 GPT-1 和 GPT-2 之间，是有代表性的现代 transformer 之一，\n现在仍然在很多场景中使用，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/llm-practical-guide/fig-1.png\" width=\"90%\" height=\"90%\"/\u003e\u003c/p\u003e\n\u003cp\u003e大模型进化树，可以看到 BERT 所处的年代和位置。来自 \u003ca href=\"/blog/llm-practical-guide-zh/\"\u003e大语言模型（LLM）综述与实用指南（Amazon，2023）\u003c/a\u003e。\u003c/p\u003e\n\n\u003cp\u003e根据 \u003ca href=\"/blog/transformers-from-scratch-zh/\"\u003eTransformer 是如何工作的：600 行 Python 代码实现 self-attention 和两类 Transformer（2019）\u003c/a\u003e，\nBERT 是首批 \u003cstrong\u003e\u003cmark\u003e在各种自然语言任务上达到人类水平\u003c/mark\u003e\u003c/strong\u003e的 transformer 模型之一。\n预训练和 fine-tuning \u003cstrong\u003e\u003cmark\u003e代码\u003c/mark\u003e\u003c/strong\u003e：\u003ca href=\"https://github.com/google-research/bert\"\u003egithub.com/google-research/bert\u003c/a\u003e。\u003c/p\u003e\n\n\u003cp\u003eBERT 模型只有 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e0.1b ~ 0.3b\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 大小，因此在 CPU 上也能较流畅地跑起来。\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e译者水平有限，不免存在遗漏或错误之处。如有疑问，敬请查阅原文。\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003e以下是译文。\u003c/p\u003e\n\n\u003chr/\u003e\n\n\u003cul id=\"markdown-toc\"\u003e\n  \u003cli\u003e\u003ca href=\"#译者序\" id=\"markdown-toc-译者序\"\u003e译者序\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#摘要\" id=\"markdown-toc-摘要\"\u003e摘要\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#1-引言\" id=\"markdown-toc-1-引言\"\u003e1 引言\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#11-pre-trained-model-适配具体下游任务的两种方式\" id=\"markdown-toc-11-pre-trained-model-适配具体下游任务的两种方式\"\u003e1.1 Pre-trained model 适配具体下游任务的两种方式\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#12-以-openai-gpt-为代表的单向架构存在的问题\" id=\"markdown-toc-12-以-openai-gpt-为代表的单向架构存在的问题\"\u003e1.2 以 OpenAI GPT 为代表的单向架构存在的问题\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#13-bert-创新之处\" id=\"markdown-toc-13-bert-创新之处\"\u003e1.3 BERT 创新之处\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#14-本文贡献\" id=\"markdown-toc-14-本文贡献\"\u003e1.4 本文贡献\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#2-相关工作\" id=\"markdown-toc-2-相关工作\"\u003e2 相关工作\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#21-无监督基于特征unsupervised-feature-based的方法\" id=\"markdown-toc-21-无监督基于特征unsupervised-feature-based的方法\"\u003e2.1 无监督基于特征（Unsupervised Feature-based）的方法\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#22-无监督基于微调unsupervised-fine-tuning的方法\" id=\"markdown-toc-22-无监督基于微调unsupervised-fine-tuning的方法\"\u003e2.2 无监督基于微调（Unsupervised Fine-tuning）的方法\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#23-基于监督数据的转移学习transfer-learning-from-supervised-data\" id=\"markdown-toc-23-基于监督数据的转移学习transfer-learning-from-supervised-data\"\u003e2.3 基于监督数据的转移学习（Transfer Learning from Supervised Data）\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#3-bert\" id=\"markdown-toc-3-bert\"\u003e3 BERT\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#30-bert-架构\" id=\"markdown-toc-30-bert-架构\"\u003e3.0 BERT 架构\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#301-bert-模型架构和参数\" id=\"markdown-toc-301-bert-模型架构和参数\"\u003e3.0.1 BERT 模型架构和参数\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#302-输入输出表示\" id=\"markdown-toc-302-输入输出表示\"\u003e3.0.2 输入/输出表示\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#31-预训练-bert\" id=\"markdown-toc-31-预训练-bert\"\u003e3.1 预训练 BERT\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#311-任务-1掩码语言模型masked-lm\" id=\"markdown-toc-311-任务-1掩码语言模型masked-lm\"\u003e3.1.1 任务 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e＃1\u003c/code\u003e：掩码语言模型（Masked LM）\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#312-任务-2下一句预测next-sentence-prediction-nsp\" id=\"markdown-toc-312-任务-2下一句预测next-sentence-prediction-nsp\"\u003e3.1.2 任务 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e＃2\u003c/code\u003e：下一句预测（Next Sentence Prediction, NSP）\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#313-预训练数据集\" id=\"markdown-toc-313-预训练数据集\"\u003e3.1.3 预训练数据集\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#32-微调-bert\" id=\"markdown-toc-32-微调-bert\"\u003e3.2 微调 BERT\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#33-各种场景\" id=\"markdown-toc-33-各种场景\"\u003e3.3 各种场景\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#4-实验\" id=\"markdown-toc-4-实验\"\u003e4 实验\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#41-glue-general-language-understanding-evaluation\" id=\"markdown-toc-41-glue-general-language-understanding-evaluation\"\u003e4.1 GLUE (General Language Understanding Evaluation)\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#411-fine-tune-工作\" id=\"markdown-toc-411-fine-tune-工作\"\u003e4.1.1 Fine-tune 工作\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#412-参数设置\" id=\"markdown-toc-412-参数设置\"\u003e4.1.2 参数设置\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#413-结果\" id=\"markdown-toc-413-结果\"\u003e4.1.3 结果\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#42-squad-stanford-question-answering-dataset-v11\" id=\"markdown-toc-42-squad-stanford-question-answering-dataset-v11\"\u003e4.2 SQuAD (Stanford Question Answering Dataset) v1.1\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#43-squad-v20\" id=\"markdown-toc-43-squad-v20\"\u003e4.3 SQuAD v2.0\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#44-swag-situations-with-adversarial-generations\" id=\"markdown-toc-44-swag-situations-with-adversarial-generations\"\u003e4.4 SWAG (Situations With Adversarial Generations)\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#5-对照研究\" id=\"markdown-toc-5-对照研究\"\u003e5 对照研究\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#51-预训练任务mlmnsp的影响\" id=\"markdown-toc-51-预训练任务mlmnsp的影响\"\u003e5.1 预训练任务（MLM/NSP）的影响\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#511-训练组\" id=\"markdown-toc-511-训练组\"\u003e5.1.1 训练组\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#512-结果对比\" id=\"markdown-toc-512-结果对比\"\u003e5.1.2 结果对比\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#513-与-elmo-的区别\" id=\"markdown-toc-513-与-elmo-的区别\"\u003e5.1.3 与 ELMo 的区别\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#52-模型大小的影响\" id=\"markdown-toc-52-模型大小的影响\"\u003e5.2 模型大小的影响\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#53-bert-基于特征的方式\" id=\"markdown-toc-53-bert-基于特征的方式\"\u003e5.3 BERT 基于特征的方式\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#531-基于特征的方式适用的场景\" id=\"markdown-toc-531-基于特征的方式适用的场景\"\u003e5.3.1 基于特征的方式适用的场景\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#532-实验\" id=\"markdown-toc-532-实验\"\u003e5.3.2 实验\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#533-结果\" id=\"markdown-toc-533-结果\"\u003e5.3.3 结果\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#6-总结\" id=\"markdown-toc-6-总结\"\u003e6 总结\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#附录\" id=\"markdown-toc-附录\"\u003e附录\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#a-additional-details-for-bert\" id=\"markdown-toc-a-additional-details-for-bert\"\u003eA. Additional Details for BERT\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#a1-illustration-of-the-pre-training-tasks\" id=\"markdown-toc-a1-illustration-of-the-pre-training-tasks\"\u003eA.1 Illustration of the Pre-training Tasks\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#a2-pre-training-procedure\" id=\"markdown-toc-a2-pre-training-procedure\"\u003eA.2 Pre-training Procedure\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#a3-fine-tuning-procedure\" id=\"markdown-toc-a3-fine-tuning-procedure\"\u003eA.3 Fine-tuning Procedure\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#a4-comparison-of-bert-elmo-and-openai-gpt\" id=\"markdown-toc-a4-comparison-of-bert-elmo-and-openai-gpt\"\u003eA.4 Comparison of BERT, ELMo ,and OpenAI GPT\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#a5-illustrations-of-fine-tuning-on-different-tasks\" id=\"markdown-toc-a5-illustrations-of-fine-tuning-on-different-tasks\"\u003eA.5 Illustrations of Fine-tuning on Different Tasks\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#b-detailed-experimental-setup\" id=\"markdown-toc-b-detailed-experimental-setup\"\u003eB. Detailed Experimental Setup\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#c-additional-ablation-studies\" id=\"markdown-toc-c-additional-ablation-studies\"\u003eC. Additional Ablation Studies\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#参考文献\" id=\"markdown-toc-参考文献\"\u003e参考文献\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003chr/\u003e\n\n\u003cscript type=\"text/x-mathjax-config\"\u003e\n  \tMathJax.Hub.Config({\n    \textensions: [\"tex2jax.js\"],\n    \tjax: [\"input/TeX\", \"output/HTML-CSS\"],\n    \ttex2jax: {\n      \t\tinlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n      \t\tdisplayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ],\n    \t\tprocessEscapes: true\n\t    },\n\t\t\"HTML-CSS\": {\n\t\t\tavailableFonts: [], preferredFont: null,\n\t\t\twebFont: \"Neo-Euler\",\n\t\t\tmtextFontInherit: true\n\t\t},\n\t\tTeX: {\n\t\t\textensions: [\"color.js\"],\n\t\t\tMacros: {\n\t\t\t\tlgc: [\"{\\\\color{my-light-green} #1}\", 1],\n\t\t\t\tgc: [\"{\\\\color{my-green} #1}\", 1],\n\t\t\t\tlrc: [\"{\\\\color{my-light-red} #1}\", 1],\n\t\t\t\trc: [\"{\\\\color{my-red} #1}\", 1],\n\t\t\t\tlbc: [\"{\\\\color{my-light-blue} #1}\", 1],\n\t\t\t\tbc: [\"{\\\\color{my-blue} #1}\", 1],\n\t\t\t\tkc: [\"{\\\\color{my-gray} #1}\", 1],\n\t\t\t\tloc: [\"{\\\\color{my-light-orange} #1}\", 1],\n\t\t\t\toc: [\"{\\\\color{my-orange} #1}\", 1],\n\n\t\t\t\ta: [\"\\\\mathbf a\"],\n\t\t\t\tA: [\"\\\\mathbf A\"],\n\t\t\t\tb: [\"\\\\mathbf b\"],\n\t\t\t\tB: [\"\\\\mathbf B\"],\n\t\t\t\tc: [\"\\\\mathbf c\"],\n\t\t\t\tC: [\"\\\\mathbf C\"],\n\t\t\t\td: [\"\\\\mathbf d\"],\n\t\t\t\tD: [\"\\\\mathbf D\"],\n\t\t\t\tE: [\"\\\\mathbf E\"],\n\t\t\t\tI: [\"\\\\mathbf I\"],\n\t\t\t\tL: [\"\\\\mathbf L\"],\n\t\t\t\tm: [\"\\\\mathbf m\"],\n\t\t\t\tM: [\"\\\\mathbf M\"],\n\t\t\t\tr: [\"\\\\mathbf r\"],\n\t\t\t\ts: [\"\\\\mathbf s\"],\n\t\t\t\tt: [\"\\\\mathbf t\"],\n\t\t\t\tS: [\"\\\\mathbf S\"],\n\t\t\t\tx: [\"\\\\mathbf x\"],\n\t\t\t\tz: [\"\\\\mathbf z\"],\n\t\t\t\tv: [\"\\\\mathbf v\"],\n\t\t\t\ty: [\"\\\\mathbf y\"],\n\t\t\t\tk: [\"\\\\mathbf k\"],\n\t\t\t\tbp: [\"\\\\mathbf p\"],\n\t\t\t\tP: [\"\\\\mathbf P\"],\n\t\t\t\tq: [\"\\\\mathbf q\"],\n\t\t\t\tQ: [\"\\\\mathbf Q\"],\n\t\t\t\tr: [\"\\\\mathbf r\"],\n\t\t\t\tR: [\"\\\\mathbf R\"],\n\t\t\t\tSig: [\"\\\\mathbf \\\\Sigma\"],\n\t\t\t\tt: [\"\\\\mathbf t\"],\n\t\t\t\tT: [\"\\\\mathbf T\"],\n\t\t\t\te: [\"\\\\mathbf e\"],\n\t\t\t\tX: [\"\\\\mathbf X\"],\n\t\t\t\tu: [\"\\\\mathbf u\"],\n\t\t\t\tU: [\"\\\\mathbf U\"],\n\t\t\t\tv: [\"\\\\mathbf v\"],\n\t\t\t\tV: [\"\\\\mathbf V\"],\n\t\t\t\tw: [\"\\\\mathbf w\"],\n\t\t\t\tW: [\"\\\\mathbf W\"],\n\t\t\t\tY: [\"\\\\mathbf Y\"],\n\t\t\t\tz: [\"\\\\mathbf z\"],\n\t\t\t\tZ: [\"\\\\mathbf Z\"],\n\t\t\t\tp: [\"\\\\,\\\\text{.}\"],\n\t\t\t\ttab: [\"\\\\hspace{0.7cm}\"],\n\n\t\t\t\tsp: [\"^{\\\\small\\\\prime}\"],\n\n\n\t\t\t\tmR: [\"{\\\\mathbb R}\"],\n\t\t\t\tmC: [\"{\\\\mathbb C}\"],\n\t\t\t\tmN: [\"{\\\\mathbb N}\"],\n\t\t\t\tmZ: [\"{\\\\mathbb Z}\"],\n\n\t\t\t\tdeg: [\"{^\\\\circ}\"],\n\n\n\t\t\t\targmin: [\"\\\\underset{#1}{\\\\text{argmin}}\", 1],\n\t\t\t\targmax: [\"\\\\underset{#1}{\\\\text{argmax}}\", 1],\n\n\t\t\t\tco: [\"\\\\;\\\\text{cos}\"],\n\t\t\t\tsi: [\"\\\\;\\\\text{sin}\"]\n\t\t\t}\n\t\t}\n  \t});\n\n  \tMathJax.Hub.Register.StartupHook(\"TeX color Ready\", function() {\n     \tMathJax.Extension[\"TeX/color\"].colors[\"my-green\"] = '#677d00';\n     \tMathJax.Extension[\"TeX/color\"].colors[\"my-light-green\"] = '#acd373';\n     \tMathJax.Extension[\"TeX/color\"].colors[\"my-red\"] = '#b13e26';\n     \tMathJax.Extension[\"TeX/color\"].colors[\"my-light-red\"] = '#d38473';\n     \tMathJax.Extension[\"TeX/color\"].colors[\"my-blue\"] = '#306693';\n       \tMathJax.Extension[\"TeX/color\"].colors[\"my-light-blue\"] = '#73a7d3';\n       \tMathJax.Extension[\"TeX/color\"].colors[\"my-gray\"] = '#999';\n       \tMathJax.Extension[\"TeX/color\"].colors[\"my-orange\"] = '#E69500';\n       \tMathJax.Extension[\"TeX/color\"].colors[\"my-light-orange\"] = '#FFC353';\n\n\n\t});\n\u003c/script\u003e\n\n\u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js\"\u003e\n\u003c/script\u003e\n\n\u003ch1 id=\"摘要\"\u003e摘要\u003c/h1\u003e\n\n\u003cp\u003e本文提出 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eBERT\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e（Bidirectional Encoder Representations from Transformers，\n\u003cstrong\u003e\u003cmark\u003e基于 Transformers 的双向 Encoder 表示\u003c/mark\u003e\u003c/strong\u003e） —— 一种新的语言表示模型\n（language representation model）。\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e与最近的语言表示模型（Peters 等，2018a; Radford 等，2018）不同，\nBERT 利用了\u003cstrong\u003e\u003cmark\u003e所有层中的左右上下文\u003c/mark\u003e\u003c/strong\u003e（both left and right context in all layers），\n在\u003cstrong\u003e\u003cmark\u003e无标签文本\u003c/mark\u003e\u003c/strong\u003e（unlabeled text）上\n\u003cstrong\u003e\u003cmark\u003e预训练深度双向表示\u003c/mark\u003e\u003c/strong\u003e（pretrain deep bidirectional representations）。\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e只需添加一个额外的输出层\u003c/mark\u003e\u003c/strong\u003e，而无需任何 task-specific 架构改动，就可以对预训练的 BERT 模型进行微调，\n创建出用于各种下游任务（例如问答和语言推理）的高效模型。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eBERT 在概念上很简单，实际效果却很强大，在 11 个自然语言处理任务中刷新了目前业界最好的成绩，包括，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eGLUE score to 80.5% (7.7% point absolute improvement)\u003c/li\u003e\n  \u003cli\u003eMultiNLI accuracy to 86.7% (4.6% absolute improvement)\u003c/li\u003e\n  \u003cli\u003eSQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement)\u003c/li\u003e\n  \u003cli\u003eSQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch1 id=\"1-引言\"\u003e1 引言\u003c/h1\u003e\n\n\u003cp\u003e业界已证明，\u003cstrong\u003e\u003cmark\u003e语言模型预训练\u003c/mark\u003e\u003c/strong\u003e（Language model pre-training）\n能\u003cstrong\u003e\u003cmark\u003e显著提高许多自然语言处理（NLP）任务的效果\u003c/mark\u003e\u003c/strong\u003e（Dai 和 Le，2015; Peters 等，2018a; Radford 等，2018; Howard 和 Ruder，2018）。\n这些任务包括：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003esentence-level tasks\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e：例如自然语言\u003cstrong\u003e\u003cmark\u003e推理\u003c/mark\u003e\u003c/strong\u003e（Bowman 等，2015; Williams 等，2018）；\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eparaphrasing\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e（Dolan 和 Brockett，2005）：整体分析句子来预测它们之间的关系；\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003etoken-level tasks\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e：例如 named entity recognition 和\u003cstrong\u003e\u003cmark\u003e问答\u003c/mark\u003e\u003c/strong\u003e，其模型需要完成 token 级别的细粒度输出（Tjong Kim Sang 和 De Meulder，2003; Rajpurkar 等，2016）。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 id=\"11-pre-trained-model-适配具体下游任务的两种方式\"\u003e1.1 Pre-trained model 适配具体下游任务的两种方式\u003c/h2\u003e\n\n\u003cp\u003e将预训练之后的语言表示（pre-trained language representations）应用到下游任务，目前有两种策略：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e基于特征的方式\u003c/mark\u003e\u003c/strong\u003e（feature-based approach）：例如 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eELMo\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e（Peters 等，2018a），\u003cstrong\u003e\u003cmark\u003e使用任务相关的架构，将预训练表示作为附加特征\u003c/mark\u003e\u003c/strong\u003e。\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e微调\u003c/mark\u003e\u003c/strong\u003e（fine-tuning）：例如 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eGenerative Pre-trained Transformer\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e (OpenAI \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eGPT\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e)（Radford 等，2018），\n  引入最少的 task-specific 参数，通过\u003cstrong\u003e\u003cmark\u003e微调所有预训练参数\u003c/mark\u003e\u003c/strong\u003e来训练下游任务。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e这两种方法都是使用\u003cstrong\u003e\u003cmark\u003e单向语言模型\u003c/mark\u003e\u003c/strong\u003e来学习\u003cstrong\u003e\u003cmark\u003e通用语言表示\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003ch2 id=\"12-以-openai-gpt-为代表的单向架构存在的问题\"\u003e1.2 以 OpenAI GPT 为代表的单向架构存在的问题\u003c/h2\u003e\n\n\u003cp\u003e我们认为，以上两种方式（尤其是微调）\u003cstrong\u003e\u003cmark\u003e限制了 pre-trained language representation 的能力\u003c/mark\u003e\u003c/strong\u003e。\n主要是因为其\u003cstrong\u003e\u003cmark\u003e语言模型是单向的\u003c/mark\u003e\u003c/strong\u003e，这\u003cstrong\u003e\u003cmark\u003e限制了预训练期间的架构选择范围\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e例如，OpenAI GPT 使用从左到右的架构（Left-to-Right Model, LRM），因此\nTransformer self-attention 层中的 token 只能关注它前面的 tokens（只能用到前面的上下文）：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e对于\u003cstrong\u003e\u003cmark\u003e句子级别的任务\u003c/mark\u003e\u003c/strong\u003e，这将导致\u003cstrong\u003e\u003cmark\u003e次优\u003c/mark\u003e\u003c/strong\u003e结果；\u003c/li\u003e\n  \u003cli\u003e对 \u003cstrong\u003e\u003cmark\u003etoken 级别的任务\u003c/mark\u003e\u003c/strong\u003e（例如问答）使用 fine-tuning 方式效果可能非常差，\n因为这种场景\u003cstrong\u003e\u003cmark\u003e非常依赖双向上下文\u003c/mark\u003e\u003c/strong\u003e（context from both directions）。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 id=\"13-bert-创新之处\"\u003e1.3 BERT 创新之处\u003c/h2\u003e\n\n\u003cp\u003e本文提出 BERT 来\u003cstrong\u003e\u003cmark\u003e改进基于微调的方式\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e受 Cloze（完形填空）任务（Taylor，1953）启发，BERT 通过一个\u003cstrong\u003e\u003cmark\u003e“掩码语言模型”\u003c/mark\u003e\u003c/strong\u003e（masked language model, MLM）做预训练，\n避免前面提到的\u003cstrong\u003e\u003cmark\u003e单向性带来的问题\u003c/mark\u003e\u003c/strong\u003e，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eMLM \u003cstrong\u003e\u003cmark\u003e随机掩盖输入中的一些 token\u003c/mark\u003e\u003c/strong\u003e ，仅基于上下文来\u003cstrong\u003e\u003cmark\u003e预测被掩盖的单词\u003c/mark\u003e\u003c/strong\u003e（单词用 ID 表示）。\u003c/li\u003e\n  \u003cli\u003e与从左到右语言模型的预训练不同，MLM 能够\u003cstrong\u003e\u003cmark\u003e同时利用左侧和右侧的上下文\u003c/mark\u003e\u003c/strong\u003e，\n从而预训练出一个深度\u003cstrong\u003e\u003cmark\u003e双向\u003c/mark\u003e\u003c/strong\u003e Transformer。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e除了掩码语言模型外，我们还使用\u003cstrong\u003e\u003cmark\u003e“下一句预测”\u003c/mark\u003e\u003c/strong\u003e（next sentence prediction, \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eNSP\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e）\n任务来联合预训练 text-pair representation。\u003c/p\u003e\n\n\u003ch2 id=\"14-本文贡献\"\u003e1.4 本文贡献\u003c/h2\u003e\n\n\u003col\u003e\n  \u003cli\u003e证明了双向预训练对于语言表示的重要性。\n  与 Radford 等（2018）使用单向模型预训练不同，BERT 使用掩码模型来实现预训练的深度双向表示。\n  这也与 Peters 等（2018a）不同，后者使用独立训练的从左到右和从右到左的浅连接。\u003c/li\u003e\n  \u003cli\u003e展示了 pre-trained representations 可以\u003cstrong\u003e\u003cmark\u003e减少\u003c/mark\u003e\u003c/strong\u003e对许多 task-specific 架构的\u003cstrong\u003e\u003cmark\u003e重度工程优化\u003c/mark\u003e\u003c/strong\u003e。\n  BERT 是第一个在大量 sentence-level 和 token-level 任务上达到了 state-of-the-art 性能的\n  \u003cstrong\u003e\u003cmark\u003e基于微调的表示模型\u003c/mark\u003e\u003c/strong\u003e，超过了许多 task-specific 架构。\u003c/li\u003e\n  \u003cli\u003eBERT 刷新了 11 个自然语言处理任务的最好性能。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e代码和预训练模型见 \u003ca href=\"https://github.com/google-research/bert\"\u003egithub.com/google-research/bert\u003c/a\u003e。\u003c/p\u003e\n\n\u003ch1 id=\"2-相关工作\"\u003e2 相关工作\u003c/h1\u003e\n\n\u003cp\u003e（这节不是重点，不翻译了）。\u003c/p\u003e\n\n\u003cp\u003eThere is a long history of pre-training general language representations, and we briefly review the\nmost widely-used approaches in this section.\u003c/p\u003e\n\n\u003ch2 id=\"21-无监督基于特征unsupervised-feature-based的方法\"\u003e2.1 无监督基于特征（Unsupervised Feature-based）的方法\u003c/h2\u003e\n\n\u003cp\u003eLearning widely applicable representations of\nwords has been an active area of research for\ndecades, including non-neural (Brown et al., 1992;\nAndo and Zhang, 2005; Blitzer et al., 2006) and\nneural (Mikolov et al., 2013; Pennington et al.,\n2014) methods. \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003ePre-trained word embeddings\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\nare an integral part of modern NLP systems, offering significant improvements over embeddings\nlearned from scratch (Turian et al., 2010). To pretrain word embedding vectors,\n\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eleft-to-right language modeling\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e objectives have been used (Mnih\nand Hinton, 2009), as well as objectives to discriminate correct from incorrect words in left and\nright context (Mikolov et al., 2013).\u003c/p\u003e\n\n\u003cp\u003eThese approaches have been generalized to coarser granularities, such as\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003esentence embeddings\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e (Kiros et al., 2015; Logeswaran and Lee, 2018)\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eparagraph embeddings\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e (Le and Mikolov, 2014).\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eTo train sentence representations, prior\nwork has used objectives to rank candidate next\nsentences (Jernite et al., 2017; Logeswaran and\nLee, 2018), left-to-right generation of next sentence words given a representation of the previous\nsentence (Kiros et al., 2015), or denoising autoencoder derived objectives (Hill et al., 2016).\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eELMo\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e and its predecessor (Peters et al., 2017,\n2018a) generalize traditional word embedding research along a different dimension. They\n\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eextract context-sensitive features\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e from a left-to-right and a\nright-to-left language model. The contextual representation of each token is the concatenation of\nthe left-to-right and right-to-left representations.\nWhen integrating contextual word embeddings\nwith existing task-specific architectures, ELMo\nadvances the state of the art for several major NLP\nbenchmarks (Peters et al., 2018a) including\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003equestion answering (Rajpurkar et al., 2016)\u003c/li\u003e\n  \u003cli\u003esentiment analysis (Socher et al., 2013)\u003c/li\u003e\n  \u003cli\u003enamed entity recognition (Tjong Kim Sang and De Meulder, 2003)\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eMelamud et al. (2016) proposed learning\ncontextual representations through a task to predict a single word from both left and right context\nusing LSTMs. Similar to ELMo, their model is\nfeature-based and not deeply bidirectional. Fedus\net al. (2018) shows that the cloze task can be used\nto improve the robustness of text generation models.\u003c/p\u003e\n\n\u003ch2 id=\"22-无监督基于微调unsupervised-fine-tuning的方法\"\u003e2.2 无监督基于微调（Unsupervised Fine-tuning）的方法\u003c/h2\u003e\n\n\u003cp\u003eAs with the feature-based approaches, the first\nworks in this direction only pre-trained word embedding parameters from unlabeled text (Collobert and Weston, 2008).\u003c/p\u003e\n\n\u003cp\u003eMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\nfine-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eadvantage of these approaches\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e is that\n\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003efew parameters need to be learned from scratch\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e.\u003c/p\u003e\n\n\u003cp\u003eAt least partly due to this advantage,\n\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eOpenAI GPT\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e (Radford et al., 2018) achieved previously state-of-the-art results on many sentencelevel tasks from the GLUE benchmark (Wang\net al., 2018a). Left-to-right language model\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\u003c/p\u003e\n\n\u003ch2 id=\"23-基于监督数据的转移学习transfer-learning-from-supervised-data\"\u003e2.3 基于监督数据的转移学习（Transfer Learning from Supervised Data）\u003c/h2\u003e\n\n\u003cp\u003eThere has also been work showing effective transfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017).\u003c/p\u003e\n\n\u003cp\u003eComputer vision research has also demonstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to fine-tune models pre-trained with ImageNet (Deng et al., 2009; Yosinski et al., 2014).\u003c/p\u003e\n\n\u003ch1 id=\"3-bert\"\u003e3 BERT\u003c/h1\u003e\n\n\u003cp\u003e本节介绍 BERT 架构及实现。训练一个可用于具体下游任务的 BERT 模型，分为两个步骤：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e预训练：使用\u003cstrong\u003e\u003cmark\u003e不带标签的数据\u003c/mark\u003e\u003c/strong\u003e进行训练，完成多种不同的预训练任务。\u003c/li\u003e\n  \u003cli\u003e微调：首先使用预训练参数进行初始化，然后使用下游任务的数据\u003cstrong\u003e\u003cmark\u003e对所有参数进行微调\u003c/mark\u003e\u003c/strong\u003e。\n每个下游任务最终都得到一个独立的微调模型。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 id=\"30-bert-架构\"\u003e3.0 BERT 架构\u003c/h2\u003e\n\n\u003cp\u003e图 1 是一个问答场景的训练+微调，我们以它为例子讨论架构：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/bert-paper/fig-1.png\" width=\"90%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFigure 1: BERT pre-training 和 fine-tuning 过程。\n预训练模型和微调模型的输出层不一样，除此之外的架构是一样的。\u003cbr/\u003e\n左：用无标注的句子进行预训练，得到一个基础模型（预训练模型）。\u003cbr/\u003e\n右：用同一个基础模型作为起点，针对不同的下游任务进行微调，这会影响模型的所有参数。\u003cbr/\u003e\n\u003cmark\u003e\u003ccode\u003e[CLS]\u003c/code\u003e\u003c/mark\u003e 是加到每个输入开头的一个特殊 token；\n\u003cmark\u003e\u003ccode\u003e[SEP]\u003c/code\u003e\u003c/mark\u003e 是一个特殊的 separator token (e.g. separating questions/answers)\n\u003c/p\u003e\n\n\u003cp\u003eBERT 的一个独特之处是\u003cstrong\u003e\u003cmark\u003e针对不同任务使用统一架构\u003c/mark\u003e\u003c/strong\u003e。\n预训练架构和最终下游架构之间的差异非常小。\u003c/p\u003e\n\n\u003ch3 id=\"301-bert-模型架构和参数\"\u003e3.0.1 BERT 模型架构和参数\u003c/h3\u003e\n\n\u003cp\u003e我们的实现基于 Vaswani 等（2017）的原始实现和我们的库\n\u003ca href=\"https://github.com/tensorflow/tensor2tensor\"\u003etensor2tensor\u003c/a\u003e 。\nTransformer 大家已经耳熟能详，并且我们的实现几乎与原版相同，因此这里不再对架构背景做详细描述，\n需要补课的请参考 Vaswani 等（2017）及网上一些优秀文章，例如 \u003ca href=\"http://nlp.seas.harvard.edu/2018/04/03/attention.html\"\u003eThe Annotated Transformer\u003c/a\u003e。\u003c/p\u003e\n\n\u003cp\u003e本文符号表示，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eL\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 层数（i.e., Transformer blocks）\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eH\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 隐藏层大小（embedding size）\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eA\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e self-attention head 数量\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eIn all cases we set the feed-forward/filter size to be 4H,\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e本文主要给出两种尺寸的模型：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eBERT\u003csub\u003eBASE\u003c/sub\u003e（L=12，H=768，A=12，总参数=\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e110M\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e），参数与 OpenAI \u003cstrong\u003e\u003cmark\u003eGPT 相同\u003c/mark\u003e\u003c/strong\u003e，便于比较；\u003c/li\u003e\n  \u003cli\u003eBERT\u003csub\u003eLARGE\u003c/sub\u003e（L=24，H=1024，A=16，总参数=\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e340M\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e）\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e如果不理解这几个参数表示什么意思，可参考\n\u003ca href=\"/blog/transformers-from-scratch-zh/\"\u003eTransformer 是如何工作的：600 行 Python 代码实现两个（文本分类+文本生成）Transformer（2019）\u003c/a\u003e。\n译注。\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/bert-paper/bert-base-bert-large-encoders.png\" width=\"90%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e两个 size 的 BERT，图中的 encoder 就是 transformer。译注。\u003ca href=\"https://jalammar.github.io/illustrated-bert/\"\u003eImage Source\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003eBERT Transformer 使用双向 self-attention，而 GPT Transformer 使用受限制的 self-attention，\n其中每个 token 只能关注其左侧的上下文。\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eWe note that in the literature the \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003ebidirectional Transformer\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\nis often referred to as a \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e“Transformer encoder”\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e while\nthe left-context-only version is referred to as a \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e“Transformer decoder”\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\nsince it can be used for text generation.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch3 id=\"302-输入输出表示\"\u003e3.0.2 输入/输出表示\u003c/h3\u003e\n\n\u003cp\u003e为了使 BERT 能够处理各种下游任务，在一个 token 序列中，我们的输入要能够明确地区分：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e单个句子（a single sentence）\u003c/li\u003e\n  \u003cli\u003e句子对（a pair of sentences）例如，问题/回答。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e这里，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e“句子”可以是任意一段连续的文本，而不是实际的语言句子。\u003c/li\u003e\n  \u003cli\u003e“序列”是指输入给 BERT 的 token 序列，可以是单个句子或两个句子组合在一起。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e我们使用 30,000 tokens vocabulary 的 \u003ca href=\"https://arxiv.org/abs/1609.08144v2\"\u003eWordPiece\u003c/a\u003e embeddings (Wu et al.,\n2016)。\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e这个 vocabulary 长什么样，可以可以看一下 bert-base-chinese（官方专门针对中文训练的基础模型）：\n\u003ca href=\"https://huggingface.co/google-bert/bert-base-chinese/blob/main/vocab.txt\"\u003ebert-base-chinese/blob/main/vocab.txt\u003c/a\u003e。\n译注。\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e我们 input/output 设计如下：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\n    \u003cp\u003e每个序列的\u003cstrong\u003e\u003cmark\u003e第一个 token\u003c/mark\u003e\u003c/strong\u003e 都是特殊的 classification token \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e[CLS]\u003c/code\u003e；\u003c/p\u003e\n\n    \u003cp\u003e在最终输出中（最上面一行），这个 token (hidden state) 主要用于分类任务，\n 再接一个分类器就能得到一个分类结果（其他的 tokens 全丢弃），如下图所示，\u003c/p\u003e\n\n    \u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/bert-paper/bert-classifier.png\" width=\"70%\"/\u003e\u003c/p\u003e\n    \u003cp align=\"center\"\u003eBERT 用于分类任务，classifier 执行 feed-forward + softmax 操作，译注。\n   \u003ca href=\"http://mccormickml.com/2019/07/22/BERT-fine-tuning/\"\u003eImage Source\u003c/a\u003e\u003c/p\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003e\u003cstrong\u003e\u003cmark\u003e将 sentence-pair 合并成单个序列\u003c/mark\u003e\u003c/strong\u003e。通过两种方式区分，\u003c/p\u003e\n\n    \u003col\u003e\n      \u003cli\u003e使用特殊 token \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e[SEP]\u003c/code\u003e 来分隔句子；\u003c/li\u003e\n      \u003cli\u003e为每个 token 添加一个学习到的 embedding ，标识它属于句子 A 还是句子 B。\u003c/li\u003e\n    \u003c/ol\u003e\n  \u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/bert-paper/fig-1.png\" width=\"90%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFigure 1: BERT pre-training 和 fine-tuning 过程。\n预训练模型和微调模型的输出层不一样，除此之外的架构是一样的。\u003cbr/\u003e\n左：用无标注的句子进行预训练，得到一个基础模型（预训练模型）。\u003cbr/\u003e\n右：用同一个基础模型作为起点，针对不同的下游任务进行微调，这会影响模型的所有参数。\u003cbr/\u003e\n\u003cmark\u003e\u003ccode\u003e[CLS]\u003c/code\u003e\u003c/mark\u003e 是加到每个输入开头的一个特殊 token；\n\u003cmark\u003e\u003ccode\u003e[SEP]\u003c/code\u003e\u003c/mark\u003e 是一个特殊的 separator token (e.g. separating questions/answers)\n\u003c/p\u003e\n\n\u003cp\u003e再回到图 1 所示，我们将\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\n    \u003cp\u003e输入 embedding 表示为 \u003cstrong\u003e\u003cmark\u003e$E$\u003c/mark\u003e\u003c/strong\u003e，\u003c/p\u003e\n\n    \u003cp\u003e对于给定的 token ，它的输入表示是通过将 3 个 embeddings 相加来构建的，如图 2，\u003c/p\u003e\n\n    \u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/bert-paper/fig-2.png\" width=\"90%\"/\u003e\u003c/p\u003e\n    \u003cp align=\"center\"\u003e Figure 2: \u003cmark\u003eBERT input representation\u003c/mark\u003e. \u003c/p\u003e\n\n    \u003col\u003e\n      \u003cli\u003etoken embedding：输入文本经过 tokenizer 之后得到的输出；\u003c/li\u003e\n      \u003cli\u003esegment embedding：表示 token embedding 在这个位置的 token 是属于句子 A 还是句子 B；\u003c/li\u003e\n      \u003cli\u003eposition embedding：token 在 token embedding 中的位置，\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e0,1,2,3...,511\u003c/code\u003e，因为 BERT 最长支持 512 token 输入（除非自己从头开始预训练，可以改参数）。\u003c/li\u003e\n    \u003c/ol\u003e\n  \u003c/li\u003e\n  \u003cli\u003e第 $i$ 个输入 token 的在最后一层的表示（最终隐藏向量）为 \u003cstrong\u003e\u003cmark\u003e$T_i$\u003c/mark\u003e\u003c/strong\u003e，$T_i \\in \\mathbb{R}^H$。\u003c/li\u003e\n  \u003cli\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e[CLS]\u003c/code\u003e token 在最后一层的表示（最终隐藏向量）为 \u003cstrong\u003e\u003cmark\u003e$C$\u003c/mark\u003e\u003c/strong\u003e, $C \\in \\mathbb{R}^{H}$ ，\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 id=\"31-预训练-bert\"\u003e3.1 预训练 BERT\u003c/h2\u003e\n\n\u003cp\u003e图 1 的左侧部分。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/bert-paper/fig-1.png\" width=\"100%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFigure 1: BERT 的 pre-training 和 fine-tuning 过程。\u003c/p\u003e\n\n\u003cp\u003e与 Peters 等（2018a）和 Radford 等（2018）不同，我们不使用传统的从左到右或从右到左的模型来预训练 BERT，\n而是用下面\u003cstrong\u003e\u003cmark\u003e两个无监督任务\u003c/mark\u003e\u003c/strong\u003e（unsupervised tasks）来预训练 BERT。\u003c/p\u003e\n\n\u003ch3 id=\"311-任务-1掩码语言模型masked-lm\"\u003e3.1.1 任务 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e＃1\u003c/code\u003e：掩码语言模型（Masked LM）\u003c/h3\u003e\n\n\u003cp\u003e从直觉上讲，深度双向模型比下面两个模型都更强大：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e从左到右的单向模型（LRM）；\u003c/li\u003e\n  \u003cli\u003e简单拼接（shallow concatenation）了一个左到右模型（LRM）与右到左模型（RLM）的模型。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e不幸的是，标准的条件语言模型（conditional language models）只能从左到右或从右到左进行训练，\n因为 bidirectional conditioning 会使每个单词间接地“看到自己”，模型就可以轻松地在 multi-layered context 中预测目标词。\u003c/p\u003e\n\n\u003cp\u003e为了训练一个深度双向表示，我们简单地\u003cstrong\u003e\u003cmark\u003e随机屏蔽一定比例的输入 tokens\u003c/mark\u003e\u003c/strong\u003e，\n然后再预测这些被屏蔽的 tokens。\n我们将这个过程称为“掩码语言模型”（MLM） —— 这种任务通常也称为 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eCloze\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e（完形填空）（Taylor，1953）。\u003c/p\u003e\n\n\u003cp\u003e在所有实验中，我们随机屏蔽每个序列中 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e15%\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 的 token。\n与 denoising auto-encoders（Vincent 等，2008）不同，我们\u003cstrong\u003e\u003cmark\u003e只预测被屏蔽的单词\u003c/mark\u003e\u003c/strong\u003e，而不是重建整个输入。\u003c/p\u003e\n\n\u003cp\u003e这种方式使我们获得了一个双向预训练模型，但造成了\u003cstrong\u003e\u003cmark\u003e预训练和微调之间的不匹配\u003c/mark\u003e\u003c/strong\u003e，\n因为微调过程中不会出现 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e[MASK] token\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e。\n为了减轻这个问题，我们并不总是用 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e[MASK]\u003c/code\u003e token 替换“掩码”单词：\n训练数据生成器（training data generator）随机选择 15%的 token positions 进行预测。\n如果选择了第 i 个 token ，我们将第 i 个 token 用以下方式替换：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e80% 的概率用 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e[MASK]\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e token 替换，\u003c/li\u003e\n  \u003cli\u003e10% 的概率用 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e随机\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e token 替换，\u003c/li\u003e\n  \u003cli\u003e10% 的概率 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e保持不变\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e然后，使用 $Ti$ 来预测原始 token ，并计算交叉熵损失（cross entropy loss）。\n附录 C.2 中比较了这个过程的几个变种。\u003c/p\u003e\n\n\u003ch3 id=\"312-任务-2下一句预测next-sentence-prediction-nsp\"\u003e3.1.2 任务 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e＃2\u003c/code\u003e：下一句预测（Next Sentence Prediction, NSP）\u003c/h3\u003e\n\n\u003cp\u003e许多重要的下游任务，如问答（Question Answering, QA）\n和自然语言推理（Natural Language Inference, NLI）\n都基于\u003cstrong\u003e\u003cmark\u003e理解两个句子之间的关系\u003c/mark\u003e\u003c/strong\u003e，\n而语言建模（language modeling）并无法直接捕获这种关系。\u003c/p\u003e\n\n\u003cp\u003e为了\u003cstrong\u003e\u003cmark\u003e训练一个能理解句子关系的模型\u003c/mark\u003e\u003c/strong\u003e，我们预先训练了一个二元的下一句预测任务（a binarized next sentence prediction task）：\n给定两个句子 A 和 B，判断 \u003cstrong\u003e\u003cmark\u003eB 是不是 A 的下一句\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/bert-paper/bert-next-sentence-prediction.png\" width=\"70%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eBERT 用于“下一句预测”（NSP）任务，译注。\u003ca href=\"https://jalammar.github.io/illustrated-bert/\"\u003eImage Source\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003e这个任务可以用任何单语语料库（monolingual corpus），具体来说，在选择每个预训练示例的句子 A 和 B 时，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e50％的概率 B 是 A 的下一个句子（labeled as \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eIsNext\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e），\u003c/li\u003e\n  \u003cli\u003e50％的概率 B 是语料库中随机一个句子（labeled as \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eNotNext\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e）。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e再次回到图 1， 这个 yes/no 的判断还是通过 classifier token 的最终嵌入向量 \u003cmark\u003e$C$\u003c/mark\u003e 预测的，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/bert-paper/fig-1.png\" width=\"90%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFigure 1: BERT pre-training 和 fine-tuning 过程。\n预训练模型和微调模型的输出层不一样，除此之外的架构是一样的。\u003cbr/\u003e\n左：用无标注的句子进行预训练，得到一个基础模型（预训练模型）。\u003cbr/\u003e\n右：用同一个基础模型作为起点，针对不同的下游任务进行微调，这会影响模型的所有参数。\u003cbr/\u003e\n\u003cmark\u003e\u003ccode\u003e[CLS]\u003c/code\u003e\u003c/mark\u003e 是加到每个输入开头的一个特殊 token；\n\u003cmark\u003e\u003ccode\u003e[SEP]\u003c/code\u003e\u003c/mark\u003e 是一个特殊的 separator token (e.g. separating questions/answers)\n\u003c/p\u003e\n\n\u003cp\u003e最终我们的模型达到了 97~98% 的准确性。\n尽管它很简单，但我们在第 5.1 节中证明，针对这个任务的预训练对于 QA 和 NLI 都非常有益。\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eThe vector C is not a meaningful sentence representation\nwithout fine-tuning, since it was trained with NSP。\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eNSP 任务与 Jernite 等（2017）和 Logeswaran 和 Lee（2018）使用的 representation learning 有紧密关系。\n但是他们的工作中只将句子 embedding 转移到了下游任务，而 BERT 是将所有参数都转移下游，初始化微调任务用的初始模型。\u003c/p\u003e\n\n\u003ch3 id=\"313-预训练数据集\"\u003e3.1.3 预训练数据集\u003c/h3\u003e\n\n\u003cp\u003e预训练过程跟其他模型的预训练都差不多。对于预训练语料库，我们使用了\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eBooksCorpus (800M words) (Zhu et al., 2015)\u003c/li\u003e\n  \u003cli\u003eEnglish Wikipedia (2,500M words)。只提取文本段落，忽略列表、表格和标题。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e使用文档语料库而不是像 Billion Word Benchmark（Chelba 等，2013）\n这样的 shuffled sentence-level 语料库非常重要，因为方便提取长连续序列。\u003c/p\u003e\n\n\u003ch2 id=\"32-微调-bert\"\u003e3.2 微调 BERT\u003c/h2\u003e\n\n\u003cp\u003eTransformer 中的 self-attention 机制允许 BERT \u003cstrong\u003e\u003cmark\u003e对任何下游任务建模\u003c/mark\u003e\u003c/strong\u003e —— 无论是 single text 还是 text pairs ——\n\u003cstrong\u003e\u003cmark\u003e只需要适当替换输入和输出\u003c/mark\u003e\u003c/strong\u003e，因此对 BERT 进行微调是非常方便的。\u003c/p\u003e\n\n\u003cp\u003e对于 text-pair 类应用，一个常见的模式是在应用 bidirectional cross attention 之前，独立编码 text-pair ，例如 Parikh 等（2016）；Seo 等（2017）。\u003c/p\u003e\n\n\u003cp\u003e但 BERT 使用 self-attention 机制来统一预训练和微调这两个阶段，因为使用 self-attention 对 concatenated text-pair 进行编码，\n有效地包含了两个句子之间的 bidirectional cross attention。\u003c/p\u003e\n\n\u003cp\u003e对于每个任务，只需将任务特定的输入和输出插入到 BERT 中，并对所有参数进行端到端的微调。\n预训练阶段，input 句子 A 和 B 的关系可能是：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003esentence pairs\u003c/li\u003e\n  \u003cli\u003ehypothesis-premise pairs in entailment\u003c/li\u003e\n  \u003cli\u003equestion-passage pairs in question answering\u003c/li\u003e\n  \u003cli\u003e文本分类或序列打标（sequence tagging）中的 degenerate \u003ccode class=\"language-plaintext highlighter-rouge\"\u003etext-? pair\u003c/code\u003e。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e在输出端，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e普通 token representations 送到 token-level 任务的输出层，例如 sequence tagging 或问答，\u003c/li\u003e\n  \u003cli\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e[CLS]\u003c/code\u003e token representation 用于\u003cstrong\u003e\u003cmark\u003e分类\u003c/mark\u003e\u003c/strong\u003e，例如 entailment or sentiment analysis。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e与预训练相比，微调的成本相对较低。从完全相同的预训练模型开始，\n本文中所有结果都可以在最多 1 小时内在单个 Cloud TPU 上复制，或者在 GPU 上几个小时内。\n第 4 节会介绍一些细节。更多细节见附录 A.5。\u003c/p\u003e\n\n\u003ch2 id=\"33-各种场景\"\u003e3.3 各种场景\u003c/h2\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/bert-paper/fig-4.png\" width=\"70%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 4. BERT 用于不同任务场景，来自 paper 附录。\u003cbr/\u003e\n(a) 句子对分类；(b) 单句分类；(c) 问答；(d) 单句打标。\n\u003c/p\u003e\n\n\u003ch1 id=\"4-实验\"\u003e4 实验\u003c/h1\u003e\n\n\u003cp\u003eIn this section, we present BERT fine-tuning results on 11 NLP tasks.\u003c/p\u003e\n\n\u003ch2 id=\"41-glue-general-language-understanding-evaluation\"\u003e4.1 GLUE (General Language Understanding Evaluation)\u003c/h2\u003e\n\n\u003cp\u003eGLUE benchmark (Wang et al., 2018a) 是一个\u003cstrong\u003e\u003cmark\u003e自然语言理解\u003c/mark\u003e\u003c/strong\u003e任务集，\n更多介绍见 Appendix B.1。\u003c/p\u003e\n\n\u003ch3 id=\"411-fine-tune-工作\"\u003e4.1.1 Fine-tune 工作\u003c/h3\u003e\n\n\u003cp\u003e针对 GLUE 进行 fine-tune 所做的工作：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e用第 3 节介绍的方式表示 input sequence (for single sentence or sentence pairs)\u003c/li\u003e\n  \u003cli\u003e用 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003ethe final hidden vector C\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 判断类别；\u003c/li\u003e\n  \u003cli\u003efine-tuning 期间\u003cstrong\u003e\u003cmark\u003e增加的唯一参数\u003c/mark\u003e\u003c/strong\u003e 是分类层的权重\n  $W \\in \\mathbb{R}^{K \\times H}$，其中 $K$ 是 labels 数量。\n  我们用 $C$ 和 $W$ 计算一个标准的 classification loss，例如 $\\log({\\rm softmax}(CW^T))$.\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch3 id=\"412-参数设置\"\u003e4.1.2 参数设置\u003c/h3\u003e\n\n\u003cul\u003e\n  \u003cli\u003ebatch size 32\u003c/li\u003e\n  \u003cli\u003e3 epochs\u003c/li\u003e\n  \u003cli\u003elearning rate: for each task, we selected the best fine-tuning learning rate\n(among \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e5e-5, 4e-5, 3e-5, and 2e-5\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e) on the Dev set.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e另外，我们发现 BERTLARGE \u003cstrong\u003e\u003cmark\u003e在小数据集上 finetuning 有时候不稳定\u003c/mark\u003e\u003c/strong\u003e，\n所以我们会随机重启几次，从得到的模型中选效果最好的。\n随机重启使用相同的 pre-trained checkpoint 但使用\u003cstrong\u003e\u003cmark\u003e不同的数据重排和分类层初始化\u003c/mark\u003e\u003c/strong\u003e\n（data shuffling and classifier layer initialization）。\u003c/p\u003e\n\n\u003ch3 id=\"413-结果\"\u003e4.1.3 结果\u003c/h3\u003e\n\n\u003cp\u003e结果如 Table 1 所示，\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth style=\"text-align: left\"\u003eSystem\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eMNLI-(m/mm)\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eQQP\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eQNLI\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eSST-2\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eCoLA\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eSTS-B\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eMRPC\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eRTE\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eAverage\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e392k\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e363k\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e108k\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e67k\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e8.5k\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e5.7k\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e3.5k\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e2.5k\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e-\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003ePre-OpenAI SOTA\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e80.6/80.1\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e66.1\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e82.3\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e93.2\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e35.0\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e81.0\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e86.0\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e61.7\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e74.0\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eBiLSTM+ELMo+Attn\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e76.4/76.1\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e64.8\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e79.8\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e90.4\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e36.0\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e73.3\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e84.9\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e56.8\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e71.0\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eOpenAI GPT\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e82.1/81.4\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e70.3\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e87.4\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e91.3\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e45.4\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e80.0\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e82.3\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e56.0\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e75.1\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eBERTBASE\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e84.6/83.4\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e71.2\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e90.5\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e93.5\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e52.1\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e85.8\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e88.9\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e66.4\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e79.6\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eBERTLARGE\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e86.7/85.9\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e72.1\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e92.7\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e94.9\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e60.5\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e86.5\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e89.3\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e70.1\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e82.1\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp align=\"center\"\u003e\nTable 1: GLUE Test results, scored by the evaluation server (https://gluebenchmark.com/leaderboard).\nThe number below each task denotes the number of training examples. The “Average” column is slightly different\nthan the official GLUE score, since we exclude the problematic WNLI set.8 BERT and OpenAI GPT are singlemodel, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and accuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components.\n\u003c/p\u003e\n\n\u003cp\u003eBoth\nBERTBASE and BERTLARGE outperform all systems on all tasks by a substantial margin, obtaining\n4.5% and 7.0% respective average accuracy improvement over the prior state of the art. Note that\nBERTBASE and OpenAI GPT are nearly identical\nin terms of model architecture apart from the attention masking. For the largest and most widely\nreported GLUE task, MNLI, BERT obtains a 4.6%\nabsolute accuracy improvement. On the official\nGLUE \u003ca href=\"https://gluebenchmark.com/leaderboard\"\u003eleaderboard\u003c/a\u003e, BERTLARGE obtains a score\nof 80.5, compared to OpenAI GPT, which obtains\n72.8 as of the date of writing.\u003c/p\u003e\n\n\u003cp\u003eWe find that BERTLARGE significantly outperforms BERTBASE across all tasks, especially those\nwith very little training data. The effect of model\nsize is explored more thoroughly in Section 5.2.\u003c/p\u003e\n\n\u003ch2 id=\"42-squad-stanford-question-answering-dataset-v11\"\u003e4.2 SQuAD (Stanford Question Answering Dataset) v1.1\u003c/h2\u003e\n\n\u003cp\u003eSQuAD v1.1 包含了 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e100k crowdsourced question/answer pairs\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e (Rajpurkar et al.,\n2016). Given a question and a passage from\nWikipedia containing the answer, the task is to\npredict the answer text span in the passage.\u003c/p\u003e\n\n\u003cp\u003eAs shown in Figure 1, in the question answering task,\nwe represent the input question and passage as a single packed sequence, with the question using the $A$\nembedding and the passage using the $B$ embedding. We only introduce a start vector $S \\in \\mathbb{R}^H$ and an end vector $E \\in \\mathbb{R}^H$ during fine-tuning.\nThe probability of word $i$ being the start of the answer span is computed as a dot product between $T_i$ and $S$ followed by a softmax over all of the words in the paragraph: $P_i = \\frac{e^{S{\\cdot}T_i}}{\\sum_j e^{S{\\cdot}T_j}}$. The analogous formula is used for the end of the answer span. The score of a candidate span from position $i$ to position $j$ is defined as  $S{\\cdot}T_i + E{\\cdot}T_j$, and the maximum scoring span where $j \\geq i$ is used as a prediction. The training objective is the sum of the log-likelihoods of the correct start and end positions. We fine-tune for 3 epochs with a learning rate of 5e-5 and a batch size of 32.\u003c/p\u003e\n\n\u003cp\u003eTable 2 shows top leaderboard entries as well\nas results from top published systems (Seo et al.,\n2017; Clark and Gardner, 2018; Peters et al.,\n2018a; Hu et al., 2018).\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/bert-paper/table-2.png\" width=\"45%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eTable 2: SQuAD 1.1 results. The BERT ensemble\nis 7x systems which use different pre-training checkpoints and fine-tuning seeds.\n\u003c/p\u003e\n\n\u003cp\u003eThe top results from the\nSQuAD leaderboard do not have up-to-date public\nsystem descriptions available,11 and are allowed to\nuse any public data when training their systems.\nWe therefore use modest data augmentation in\nour system by first fine-tuning on TriviaQA (Joshi\net al., 2017) befor fine-tuning on SQuAD.\nOur best performing system outperforms the top\nleaderboard system by +1.5 F1 in ensembling and\n+1.3 F1 as a single system. In fact, our single\nBERT model outperforms the top ensemble system in terms of F1 score. Without TriviaQA fine-\ntuning data, we only lose 0.1-0.4 F1, still outperforming all existing systems by a wide margin.12\u003c/p\u003e\n\n\u003ch2 id=\"43-squad-v20\"\u003e4.3 SQuAD v2.0\u003c/h2\u003e\n\n\u003cp\u003eThe SQuAD 2.0 task extends the SQuAD 1.1\nproblem definition by allowing for the possibility\nthat no short answer exists in the provided paragraph, making the problem more realistic.\nWe use a simple approach to extend the SQuAD\nv1.1 BERT model for this task. We treat questions that do not have an answer as having an answer span with start and end at the [CLS] token. The probability space for the start and end\nanswer span positions is extended to include the\nposition of the [CLS] token.\u003c/p\u003e\n\n\u003cp\u003eFor prediction, we compare the score of the no-answer span:\n\\(s_{\\tt null} = S{\\cdot}C + E{\\cdot}C\\) to the score of the best non-null span\n$\\hat{s_{i,j}}$ =  \\({\\tt max}_{j \\geq i} S{\\cdot}T_i + E{\\cdot}T_j\\).\nWe predict a non-null answer when  $\\hat{s_{i,j}} \u0026gt; s_{\\tt null} + \\tau$,\nwhere the threshold $\\tau$ is selected on the dev set to maximize F1. We did not use TriviaQA data for this model. We fine-tuned for 2 epochs with a learning rate of 5e-5 and a batch size of 48.\u003c/p\u003e\n\n\u003cp\u003eThe results compared to prior leaderboard entries and top published work (Sun et al., 2018;\nWang et al., 2018b) are shown in Table 3, excluding systems that use BERT as one of their components. We observe a +5.1 F1 improvement over\nthe previous best system.\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/bert-paper/table-3.png\" width=\"45%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eTable 3: SQuAD 2.0 results. We exclude entries that\nuse BERT as one of their components.\n\u003c/p\u003e\n\n\u003ch2 id=\"44-swag-situations-with-adversarial-generations\"\u003e4.4 SWAG (Situations With Adversarial Generations)\u003c/h2\u003e\n\n\u003cp\u003eSWAG dataset contains \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e113k sentence-pair completion examples\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\nthat evaluate grounded commonsense inference (Zellers et al., 2018).\u003c/p\u003e\n\n\u003cp\u003eGiven a sentence, the task is to choose the most plausible continuation among four choices.\nWhen fine-tuning on the SWAG dataset, we\nconstruct four input sequences, each containing\nthe concatenation of the given sentence (sentence\nA) and a possible continuation (sentence B). The\nonly task-specific parameters introduced is a vector whose dot product with the [CLS] token representation C denotes a score for each choice\nwhich is normalized with a softmax layer.\u003c/p\u003e\n\n\u003cp\u003eWe fine-tune the model for 3 epochs with a\nlearning rate of 2e-5 and a batch size of 16. Results are presented in Table 4.\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/bert-paper/table-4.png\" width=\"35%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eTable 4: SWAG Dev and Test accuracies.\nHuman performance is measured with 100 samples, as reported in the SWAG paper.\n\u003c/p\u003e\n\n\u003cp\u003eBERTLARGE outperforms the authors’ baseline ESIM+ELMo system by +27.1% and OpenAI GPT by 8.3%.\u003c/p\u003e\n\n\u003ch1 id=\"5-对照研究\"\u003e5 对照研究\u003c/h1\u003e\n\n\u003cp\u003e本节研究去掉 BERT 的一些功能，看看在不同任务上性能损失多少，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003esentence-level (e.g., SST-2)\u003c/li\u003e\n  \u003cli\u003esentence-pair-level (e.g., MultiNLI)\u003c/li\u003e\n  \u003cli\u003eword-level (e.g., NER)\u003c/li\u003e\n  \u003cli\u003espan-level (e.g., SQuAD)\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e以更好地理解它们的相对重要性。更多相关信息见附录 C。\u003c/p\u003e\n\n\u003ch2 id=\"51-预训练任务mlmnsp的影响\"\u003e5.1 预训练任务（MLM/NSP）的影响\u003c/h2\u003e\n\n\u003ch3 id=\"511-训练组\"\u003e5.1.1 训练组\u003c/h3\u003e\n\n\u003cp\u003e通过以下几组来验证 BERT 深度双向性的重要性，它们使用与 BERTBASE 完全相同的预训练数据、微调方案和超参数：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eNO NSP\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e：即去掉“下一句预测”任务，这仍然是一个双向模型，使用“掩码语言模型”（MLM）进行训练，只是训练时不做 NSP 任务；\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eLTR \u0026amp; NO NSP\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e：不仅去掉 NSP，还使用标准的从左到右（Left-to-Right, LTR）模型进行训练，而非使用双向模型。\n  在微调中也遵从 left-only 约束，否则会导致预训练和微调不匹配，降低下游性能。此外，该模型没有用 NSP 任务进行预训练。\n  这与 OpenAI GPT 直接可比，但我们使用了更大的训练数据集、我们自己的输入表示和我们的微调方案。\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e+ BiLSTM\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e：在 fine-tuning 期间，在 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eLTR \u0026amp; NO NSP\u003c/code\u003e 基础上添加了一个随机初始化的 BiLSTM。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch3 id=\"512-结果对比\"\u003e5.1.2 结果对比\u003c/h3\u003e\n\n\u003cp\u003e结果如表 5，\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth style=\"text-align: left\"\u003eTasks\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eMNLI-m (Acc)\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eQNLI (Acc)\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eMRPC (Acc)\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eSST-2 (Acc)\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eSQuAD (F1)\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eBERTBASE\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e84.4\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e88.4\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e86.7\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e92.7\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e88.5\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eNo NSP\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e83.9\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e84.9\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e86.5\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e92.6\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e87.9\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eLTR \u0026amp; No NSP\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e82.1\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e84.3\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e77.5\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e92.1\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e77.8\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003e+ BiLSTM\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e82.1\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e84.1\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e75.7\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e91.6\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e84.9\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp align=\"center\"\u003eTable 5: Ablation over the pre-training tasks using the BERTBASE architecture.\n\u003c/p\u003e\n\n\u003cp\u003e分析：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e第二组 vs 第一组：\u003cstrong\u003e\u003cmark\u003e去掉 NSP 任务带来的影响\u003c/mark\u003e\u003c/strong\u003e：在 QNLI、MNLI 和 SQuAD 1.1 上性能显著下降。\u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003e第三组 vs 第二组：\u003cstrong\u003e\u003cmark\u003e去掉双向表示带来的影响\u003c/mark\u003e\u003c/strong\u003e：第二行实际上是 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eMLM \u0026amp; NO NSP\u003c/code\u003e，\n  可以看出 LTR 模型在所有任务上的表现都比 MLM 模型差，尤其是 MRPC 和 SQuAD。\u003c/p\u003e\n\n    \u003cul\u003e\n      \u003cli\u003e对于 SQuAD，可以清楚地看到 LTR 模型在 token 预测上表现不佳，因为 token 级别的隐藏状态没有右侧上下文。\u003c/li\u003e\n      \u003cli\u003e为了尝试增强 LTR 系统，我们在其上方添加了一个随机初始化的\u003cstrong\u003e\u003cmark\u003e双向 LSTM\u003c/mark\u003e\u003c/strong\u003e。\n这确实在 SQuAD 上改善了结果，但结果仍远远不及预训练的双向模型。另外，\n双向 LSTM 降低了在 GLUE 上的性能。\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch3 id=\"513-与-elmo-的区别\"\u003e5.1.3 与 ELMo 的区别\u003c/h3\u003e\n\n\u003cp\u003eELMo 训练了单独的从左到右（LTR）和从右到左（RTL）模型，并将每个 token 表示为两个模型的串联。\n然而：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e这比单个双向模型训练成本高一倍；\u003c/li\u003e\n  \u003cli\u003e对于像 QA 这样的任务，这不直观，因为 RTL 模型将无法 condition the answer on the question；\u003c/li\u003e\n  \u003cli\u003e这比深度双向模型弱，因为后者可以在每层使用左右上下文。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch2 id=\"52-模型大小的影响\"\u003e5.2 模型大小的影响\u003c/h2\u003e\n\n\u003cp\u003e为探讨模型大小对微调任务准确性的影响，我们训练了多个 BERT 模型。\n表 6 给出了它们在 GLUE 任务上的结果。\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd\u003eL (层数)\u003c/td\u003e\n      \u003ctd\u003eH (hidden size)\u003c/td\u003e\n      \u003ctd\u003eA (attention head 数)\u003c/td\u003e\n      \u003ctd\u003eLM (ppl)\u003c/td\u003e\n      \u003ctd\u003eMNLI-m\u003c/td\u003e\n      \u003ctd\u003eMRPC\u003c/td\u003e\n      \u003ctd\u003eSST-2\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e3\u003c/td\u003e\n      \u003ctd\u003e768\u003c/td\u003e\n      \u003ctd\u003e12\u003c/td\u003e\n      \u003ctd\u003e5.84\u003c/td\u003e\n      \u003ctd\u003e77.9\u003c/td\u003e\n      \u003ctd\u003e79.8\u003c/td\u003e\n      \u003ctd\u003e88.4\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e6\u003c/td\u003e\n      \u003ctd\u003e768\u003c/td\u003e\n      \u003ctd\u003e3\u003c/td\u003e\n      \u003ctd\u003e5.24\u003c/td\u003e\n      \u003ctd\u003e80.6\u003c/td\u003e\n      \u003ctd\u003e82.2\u003c/td\u003e\n      \u003ctd\u003e90.7\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e6\u003c/td\u003e\n      \u003ctd\u003e768\u003c/td\u003e\n      \u003ctd\u003e12\u003c/td\u003e\n      \u003ctd\u003e4.68\u003c/td\u003e\n      \u003ctd\u003e81.9\u003c/td\u003e\n      \u003ctd\u003e84.8\u003c/td\u003e\n      \u003ctd\u003e91.3\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e12\u003c/td\u003e\n      \u003ctd\u003e768\u003c/td\u003e\n      \u003ctd\u003e12\u003c/td\u003e\n      \u003ctd\u003e3.99\u003c/td\u003e\n      \u003ctd\u003e84.4\u003c/td\u003e\n      \u003ctd\u003e86.7\u003c/td\u003e\n      \u003ctd\u003e92.9\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e12\u003c/td\u003e\n      \u003ctd\u003e1024\u003c/td\u003e\n      \u003ctd\u003e16\u003c/td\u003e\n      \u003ctd\u003e3.54\u003c/td\u003e\n      \u003ctd\u003e85.7\u003c/td\u003e\n      \u003ctd\u003e86.9\u003c/td\u003e\n      \u003ctd\u003e93.3\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e24\u003c/td\u003e\n      \u003ctd\u003e1024\u003c/td\u003e\n      \u003ctd\u003e16\u003c/td\u003e\n      \u003ctd\u003e3.23\u003c/td\u003e\n      \u003ctd\u003e86.6\u003c/td\u003e\n      \u003ctd\u003e87.8\u003c/td\u003e\n      \u003ctd\u003e93.7\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp align=\"center\"\u003e\nTable 6: Ablation over BERT model size.\n“LM (ppl)” is the masked LM \u003cmark\u003eperplexity\u003c/mark\u003e of held-out training data\n\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eIn this table, we report the average Dev Set accuracy from 5 random restarts of fine-tuning.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e可以看到，更大的模型在四个数据集上的准确性都更高 —— 即使对于只有 3,600 个训练示例的 MRPC，\n而且这个数据集与预训练任务差异还挺大的。\n也许令人惊讶的是，在模型已经相对较大的前提下，我们仍然能取得如此显著的改进。例如，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eVaswani 等（2017）尝试的最大 Transformer 是（L=6，H=1024，A=16），编码器参数为 100M，\u003c/li\u003e\n  \u003cli\u003e我们在文献中找到的最大 Transformer 是（L=64，H=512，A=2），具有 235M 参数（Al-Rfou 等，2018）。\u003c/li\u003e\n  \u003cli\u003e相比之下，BERTBASE 包含 110M 参数，BERTLARGE 包含 340M 参数。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e业界早就知道，增加模型大小能持续改进机器翻译和语言建模等大规模任务上的性能，\n表 6 的 perplexity 列也再次证明了这个结果，\n然而，我们认为 BERT 是第一个证明如下结果的研究工作：只要模型得到了充分的预训练，\n那么\u003cstrong\u003e\u003cmark\u003e将模型尺寸扩展到非常大时\u003c/mark\u003e\u003c/strong\u003e（scaling to extreme model sizes），\n\u003cstrong\u003e\u003cmark\u003e对非常小规模的任务\u003c/mark\u003e\u003c/strong\u003e（very small scale tasks）\u003cstrong\u003e\u003cmark\u003e也能带来很大的提升\u003c/mark\u003e\u003c/strong\u003e（large improvements）。\u003c/p\u003e\n\n\u003cp\u003e另外，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003ePeters 等（2018b）研究了将 pre-trained bi-LM size（预训练双向语言模型大小）从两层增加到四层，对下游任务产生的影响，\u003c/li\u003e\n  \u003cli\u003eMelamud 等（2016）提到将隐藏维度从 200 增加到 600 有所帮助，但进一步增加到 1,000 并没有带来更多的改进。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e这两项工作都使用了\u003cstrong\u003e\u003cmark\u003e基于特征的方法\u003c/mark\u003e\u003c/strong\u003e，而我们则是直接在下游任务上进行微调，并仅使用非常少量的随机初始化附加参数，\n结果表明即使下游任务数据非常小，也能从更大、更 expressive 的预训练表示中受益。\u003c/p\u003e\n\n\u003ch2 id=\"53-bert-基于特征的方式\"\u003e5.3 BERT 基于特征的方式\u003c/h2\u003e\n\n\u003cp\u003e到目前为止，本文展示的所有 BERT 结果都使用的微调方式：\n在预训练模型中加一个简单的分类层，针对特定的下游任务对所有参数进行联合微调。\u003c/p\u003e\n\n\u003ch3 id=\"531-基于特征的方式适用的场景\"\u003e5.3.1 基于特征的方式适用的场景\u003c/h3\u003e\n\n\u003cp\u003e不过，基于特征的方法 —— 从预训练模型中提取固定特征（fixed features）—— 在某些场景下有一定的优势，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e首先，\u003cstrong\u003e\u003cmark\u003e不是所有任务都能方便地通过 Transformer encoder 架构表示\u003c/mark\u003e\u003c/strong\u003e，因此这些不适合的任务，都需要添加一个 task-specific model architecture。\u003c/li\u003e\n  \u003cli\u003e其次，昂贵的训练数据表示（representation of the training data）只预训练一次，\n然后在此表示的基础上使用\u003cstrong\u003e\u003cmark\u003e更轻量级的模型\u003c/mark\u003e\u003c/strong\u003e进行多次实验，可以极大节省计算资源。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 id=\"532-实验\"\u003e5.3.2 实验\u003c/h3\u003e\n\n\u003cp\u003e本节通过 BERT 用于 CoNLL-2003 Named Entity Recognition (NER) task (Tjong Kim Sang\nand De Meulder, 2003) 来比较这两种方式。\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eBERT 输入使用保留大小写的 WordPiece 模型，并包含数据提供的 maximal document context。\u003c/li\u003e\n  \u003cli\u003e按照惯例，我们将其作为打标任务（tagging task），但在输出中不使用 CRF 层。\u003c/li\u003e\n  \u003cli\u003e我们将第一个 sub-token 的 representation 作 token-level classifier 的输入，然后在 NER label set 上进行实验。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e为了对比微调方法的效果，我们使用基于特征的方法，对 BERT 参数不做任何微调，\n而是从一个或多个层中提取激活（extracting the activations）。\n这些 contextual embeddings 作为输入，送给一个随机初始化的 two-layer 768-dimensional BiLSTM，\n最后再送到分类层。\u003c/p\u003e\n\n\u003ch3 id=\"533-结果\"\u003e5.3.3 结果\u003c/h3\u003e\n\n\u003cp\u003e结果见表 7。BERTLARGE 与业界最高性能相当，\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd\u003eSystem\u003c/td\u003e\n      \u003ctd\u003eDev F1\u003c/td\u003e\n      \u003ctd\u003eTest F1\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003eELMo (Peters et al., 2018a)\u003c/td\u003e\n      \u003ctd\u003e95.7\u003c/td\u003e\n      \u003ctd\u003e92.2\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003eCVT (Clark et al., 2018)\u003c/td\u003e\n      \u003ctd\u003e-\u003c/td\u003e\n      \u003ctd\u003e92.6\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003eCSE (Akbik et al., 2018)\u003c/td\u003e\n      \u003ctd\u003e-\u003c/td\u003e\n      \u003ctd\u003e93.1\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eFine-tuning approach\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/td\u003e\n      \u003ctd\u003e \u003c/td\u003e\n      \u003ctd\u003e \u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003eBERTLARGE\u003c/td\u003e\n      \u003ctd\u003e96.6\u003c/td\u003e\n      \u003ctd\u003e92.8\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003eBERTBASE\u003c/td\u003e\n      \u003ctd\u003e96.4\u003c/td\u003e\n      \u003ctd\u003e92.4\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eFeature-based approach (BERTBASE)\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/td\u003e\n      \u003ctd\u003e \u003c/td\u003e\n      \u003ctd\u003e \u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003eEmbeddings\u003c/td\u003e\n      \u003ctd\u003e91.0\u003c/td\u003e\n      \u003ctd\u003e-\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003eSecond-to-Last Hidden\u003c/td\u003e\n      \u003ctd\u003e95.6\u003c/td\u003e\n      \u003ctd\u003e-\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003eLast Hidden\u003c/td\u003e\n      \u003ctd\u003e94.9\u003c/td\u003e\n      \u003ctd\u003e-\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003eWeighted Sum Last Four Hidden\u003c/td\u003e\n      \u003ctd\u003e95.9\u003c/td\u003e\n      \u003ctd\u003e-\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003eConcat Last Four Hidden\u003c/td\u003e\n      \u003ctd\u003e96.1\u003c/td\u003e\n      \u003ctd\u003e-\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003eWeighted Sum All 12 Layers\u003c/td\u003e\n      \u003ctd\u003e95.5\u003c/td\u003e\n      \u003ctd\u003e-\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp align=\"center\"\u003e\nTable 7: CoNLL-2003 Named Entity Recognition results. Hyperparameters were selected using the Dev\nset. The reported Dev and Test scores are averaged over\n5 random restarts using those hyperparameters\n\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eThe best performing method concatenates the\ntoken representations from the top four hidden layers of the pre-trained Transformer, which is only\n0.3 F1 behind fine-tuning the entire model.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e这表明 \u003cstrong\u003e\u003cmark\u003e微调和基于特征的方法在 BERT 上都是有效的\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003ch1 id=\"6-总结\"\u003e6 总结\u003c/h1\u003e\n\n\u003cp\u003eRecent empirical improvements due to transfer\nlearning with language models have demonstrated\nthat rich, unsupervised pre-training is an integral\npart of many language understanding systems. In\nparticular, these results enable even low-resource\ntasks to benefit from deep unidirectional architectures.\u003c/p\u003e\n\n\u003cp\u003eOur major contribution is further generalizing these findings to deep\nbidirectional architectures, allowing the same pre-trained model to\nsuccessfully tackle a broad set of NLP tasks.\u003c/p\u003e\n\n\u003ch1 id=\"附录\"\u003e附录\u003c/h1\u003e\n\n\u003ch2 id=\"a-additional-details-for-bert\"\u003eA. Additional Details for BERT\u003c/h2\u003e\n\n\u003ch3 id=\"a1-illustration-of-the-pre-training-tasks\"\u003eA.1 Illustration of the Pre-training Tasks\u003c/h3\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/bert-paper/fig-3.png\" width=\"100%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e\nFigure 3: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT\nuses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-toleft LSTMs to generate features for downstream tasks. Among the three, only BERT representations are jointly\nconditioned on both left and right context in all layers. In addition to the architecture differences, BERT and\nOpenAI GPT are fine-tuning approaches, while ELMo is a feature-based approach.\n\u003c/p\u003e\n\n\u003ch3 id=\"a2-pre-training-procedure\"\u003eA.2 Pre-training Procedure\u003c/h3\u003e\n\n\u003ch3 id=\"a3-fine-tuning-procedure\"\u003eA.3 Fine-tuning Procedure\u003c/h3\u003e\n\n\u003cp\u003eFor fine-tuning, most model hyperparameters are\nthe same as in pre-training, with the exception of\nthe batch size, learning rate, and number of training epochs. The dropout probability was always\nkept at 0.1. The optimal hyperparameter values\nare task-specific, but we found the following range\nof possible values to work well across all tasks:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eBatch size: 16, 32\u003c/li\u003e\n  \u003cli\u003eLearning rate (Adam): 5e-5, 3e-5, 2e-5\u003c/li\u003e\n  \u003cli\u003eNumber of epochs: 2, 3, 4\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eWe also observed that large data sets (e.g.,\n100k+ labeled training examples) were far less\nsensitive to hyperparameter choice than small data\nsets. Fine-tuning is typically very fast, so it is reasonable to simply run an exhaustive search over\nthe above parameters and choose the model that\nperforms best on the development set.\u003c/p\u003e\n\n\u003ch3 id=\"a4-comparison-of-bert-elmo-and-openai-gpt\"\u003eA.4 Comparison of BERT, ELMo ,and OpenAI GPT\u003c/h3\u003e\n\n\u003ch3 id=\"a5-illustrations-of-fine-tuning-on-different-tasks\"\u003eA.5 Illustrations of Fine-tuning on Different Tasks\u003c/h3\u003e\n\n\u003ch2 id=\"b-detailed-experimental-setup\"\u003eB. Detailed Experimental Setup\u003c/h2\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/bert-paper/fig-4.png\" width=\"70%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 4. BERT 用于不同任务场景，来自 paper 附录。\u003cbr/\u003e\n(a) 句子对分类；(b) 单句分类；(c) 问答；(d) 单句打标。\n\u003c/p\u003e\n\n\u003ch2 id=\"c-additional-ablation-studies\"\u003eC. Additional Ablation Studies\u003c/h2\u003e\n\n\u003ch1 id=\"参考文献\"\u003e参考文献\u003c/h1\u003e\n\n\u003col\u003e\n  \u003cli\u003eAlan Akbik, Duncan Blythe, and Roland Vollgraf. 2018. Contextual string embeddings for sequence labeling. In Proceedings of the 27th International Conference on  Computational Linguistics, pages 1638–1649.\u003c/li\u003e\n  \u003cli\u003eRami Al-Rfou, Dokook Choe, Noah Constant, Mandy Guo, and Llion Jones. 2018. Character-level language modeling with deeper self-attention. arXiv preprint arXiv:1808.04444.\u003c/li\u003e\n  \u003cli\u003eRie Kubota Ando and Tong Zhang. 2005. A framework for learning predictive structures from multiple tasks  and unlabeled data. Journal of Machine Learning Research, 6(Nov):1817–1853.\u003c/li\u003e\n  \u003cli\u003eLuisa Bentivogli, Bernardo Magnini, Ido Dagan, Hoa Trang Dang, and Danilo  Giampiccolo. 2009. The fifth PASCAL recognizing textual entailment challenge. In TAC. NIST.\u003c/li\u003e\n  \u003cli\u003eJohn Blitzer, Ryan McDonald, and Fernando Pereira. 2006. Domain adaptation with structural correspondence learning. In Proceedings of the 2006 conference on empirical methods in  natural language processing, pages 120–128. Association for Computational  Linguistics.\u003c/li\u003e\n  \u003cli\u003eSamuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning.  2015. A large annotated corpus for learning natural language inference. In EMNLP. Association for Computational Linguistics.\u003c/li\u003e\n  \u003cli\u003ePeter F Brown, Peter V Desouza, Robert L Mercer, Vincent J Della Pietra, and  Jenifer C Lai. 1992. Class-based n-gram models of natural language. Computational linguistics, 18(4):467–479.\u003c/li\u003e\n  \u003cli\u003eDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-Gazpio, and Lucia Specia.  2017. https://doi.org/10.18653/v1/S17-2001 Semeval-2017 task 1:  Semantic textual similarity multilingual and crosslingual focused  evaluation. In Proceedings of the 11th International Workshop on Semantic  Evaluation (SemEval-2017), pages 1–14, Vancouver, Canada. Association for  Computational Linguistics.\u003c/li\u003e\n  \u003cli\u003eCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp  Koehn, and Tony Robinson. 2013. One billion word benchmark for measuring progress in statistical  language modeling. arXiv preprint arXiv:1312.3005.\u003c/li\u003e\n  \u003cli\u003eZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018. https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs Quora  question pairs.\u003c/li\u003e\n  \u003cli\u003eChristopher Clark and Matt Gardner. 2018. Simple and effective multi-paragraph reading comprehension. In ACL.\u003c/li\u003e\n  \u003cli\u003eKevin Clark, Minh-Thang Luong, Christopher D Manning, and Quoc Le. 2018. Semi-supervised sequence modeling with cross-view training. In Proceedings of the 2018 Conference on Empirical Methods in  Natural Language Processing, pages 1914–1925.\u003c/li\u003e\n  \u003cli\u003eRonan Collobert and Jason Weston. 2008.newblock A unified architecture for natural language processing: Deep neural  networks with multitask learning. In Proceedings of the 25th international conference on Machine  learning, pages 160–167. ACM.\u003c/li\u003e\n  \u003cli\u003eAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u0026#34;ic Barrault, and Antoine  Bordes. 2017. https://www.aclweb.org/anthology/D17-1070 Supervised  learning of universal sentence representations from natural language  inference data. In Proceedings of the 2017 Conference on Empirical Methods in  Natural Language Processing, pages 670–680, Copenhagen, Denmark.  Association for Computational Linguistics.\u003c/li\u003e\n  \u003cli\u003eAndrew M Dai and Quoc V Le. 2015. Semi-supervised sequence learning. In Advances in neural information processing systems, pages  3079–3087.\u003c/li\u003e\n  \u003cli\u003eJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. 2009. ImageNet: A Large-Scale Hierarchical Image Database. In CVPR09.\u003c/li\u003e\n  \u003cli\u003eWilliam B Dolan and Chris Brockett. 2005. Automatically constructing a corpus of sentential paraphrases. In Proceedings of the Third International Workshop on  Paraphrasing (IWP2005).\u003c/li\u003e\n  \u003cli\u003eWilliam Fedus, Ian Goodfellow, and Andrew M Dai. 2018. Maskgan: Better text generation via filling in the_. arXiv preprint arXiv:1801.07736.\u003c/li\u003e\n  \u003cli\u003eDan Hendrycks and Kevin Gimpel. 2016. http://arxiv.org/abs/1606.08415 Bridging nonlinearities and  stochastic regularizers with gaussian error linear units. CoRR, abs/1606.08415.\u003c/li\u003e\n  \u003cli\u003eFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016. Learning distributed representations of sentences from unlabelled  data. In Proceedings of the 2016 Conference of the North American  Chapter of the Association for Computational Linguistics: Human Language  Technologies. Association for Computational Linguistics.\u003c/li\u003e\n  \u003cli\u003eJeremy Howard and Sebastian Ruder. 2018. http://arxiv.org/abs/1801.06146 Universal language model  fine-tuning for text classification. In ACL. Association for Computational Linguistics.\u003c/li\u003e\n  \u003cli\u003eMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu, Furu Wei, and Ming Zhou. 2018. Reinforced mnemonic reader for machine reading comprehension. In IJCAI.\u003c/li\u003e\n  \u003cli\u003eYacine Jernite, Samuel R. Bowman, and David Sontag. 2017. http://arxiv.org/abs/1705.00557 Discourse-based objectives  for fast unsupervised sentence representation learning. CoRR, abs/1705.00557.\u003c/li\u003e\n  \u003cli\u003eMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. 2017. Triviaqa: A large scale distantly supervised challenge dataset for  reading comprehension. In ACL.\u003c/li\u003e\n  \u003cli\u003eRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun,  Antonio Torralba, and Sanja Fidler. 2015. Skip-thought vectors. In Advances in neural information processing systems, pages  3294–3302.\u003c/li\u003e\n  \u003cli\u003eQuoc Le and Tomas Mikolov. 2014. Distributed representations of sentences and documents. In International Conference on Machine Learning, pages  1188–1196.\u003c/li\u003e\n  \u003cli\u003eHector J Levesque, Ernest Davis, and Leora Morgenstern. 2011. The winograd schema challenge. In Aaai spring symposium: Logical formalizations of commonsense  reasoning, volume 46, page 47.\u003c/li\u003e\n  \u003cli\u003eLajanugen Logeswaran and Honglak Lee. 2018. https://openreview.net/forum?id=rJvJXZb0W An efficient  framework for learning sentence representations. In International Conference on Learning Representations.\u003c/li\u003e\n  \u003cli\u003eBryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. 2017. Learned in translation: Contextualized word vectors. In NIPS.\u003c/li\u003e\n  \u003cli\u003eOren Melamud, Jacob Goldberger, and Ido Dagan. 2016. context2vec: Learning generic context embedding with bidirectional  LSTM. In CoNLL.\u003c/li\u003e\n  \u003cli\u003eTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their  compositionality. In Advances in Neural Information Processing Systems 26, pages  3111–3119. Curran Associates, Inc.\u003c/li\u003e\n  \u003cli\u003eAndriy Mnih and Geoffrey E Hinton. 2009. http://papers.nips.cc/paper/3583-a-scalable-hierarchical-distributed-language-model.pdf  A scalable hierarchical distributed language model. In D. Koller, D. Schuurmans, Y. Bengio, and L. Bottou, editors,  Advances in Neural Information Processing Systems 21, pages  1081–1088. Curran Associates, Inc.\u003c/li\u003e\n  \u003cli\u003eAnkur P Parikh, Oscar T\u0026#34;ackstr\u0026#34;om, Dipanjan Das, and Jakob Uszkoreit.  2016. A decomposable attention model for natural language inference. In EMNLP.\u003c/li\u003e\n  \u003cli\u003eJeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. http://www.aclweb.org/anthology/D14-1162 Glove: Global  vectors for word representation. In Empirical Methods in Natural Language Processing (EMNLP),  pages 1532–1543.\u003c/li\u003e\n  \u003cli\u003eMatthew Peters, Waleed Ammar, Chandra Bhagavatula, and Russell Power. 2017. Semi-supervised sequence tagging with bidirectional language models. In ACL.\u003c/li\u003e\n  \u003cli\u003eMatthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark,  Kenton Lee, and Luke Zettlemoyer. 2018\\natexlaba. Deep contextualized word representations. In NAACL.\u003c/li\u003e\n  \u003cli\u003eMatthew Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih.  2018\\natexlabb. Dissecting contextual word embeddings: Architecture and  representation. In Proceedings of the 2018 Conference on Empirical Methods in  Natural Language Processing, pages 1499–1509.\u003c/li\u003e\n  \u003cli\u003eAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding with unsupervised learning. Technical report, OpenAI.\u003c/li\u003e\n  \u003cli\u003ePranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in  Natural Language Processing, pages 2383–2392.\u003c/li\u003e\n  \u003cli\u003eMinjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Bidirectional attention flow for machine comprehension. In ICLR.\u003c/li\u003e\n  \u003cli\u003eRichard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning,  Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over a sentiment  treebank. In Proceedings of the 2013 conference on empirical methods in  natural language processing, pages 1631–1642.\u003c/li\u003e\n  \u003cli\u003eFu Sun, Linyang Li, Xipeng Qiu, and Yang Liu. 2018. U-net: Machine reading comprehension with unanswerable questions. arXiv preprint arXiv:1810.06638.\u003c/li\u003e\n  \u003cli\u003eWilson L Taylor. 1953. “Cloze procedure”: A new tool for measuring readability. Journalism Bulletin, 30(4):415–433.\u003c/li\u003e\n  \u003cli\u003eErik F Tjong Kim Sang and Fien De Meulder. 2003. Introduction to the conll-2003 shared task: Language-independent  named entity recognition. In CoNLL.\u003c/li\u003e\n  \u003cli\u003eJoseph Turian, Lev Ratinov, and Yoshua Bengio. 2010. Word representations: A simple and general method for semi-supervised  learning. In Proceedings of the 48th Annual Meeting of the Association  for Computational Linguistics, ACL ‘10, pages 384–394.\u003c/li\u003e\n  \u003cli\u003eAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,  Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages  6000–6010.\u003c/li\u003e\n  \u003cli\u003ePascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol.  2008. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine  learning, pages 1096–1103. ACM.\u003c/li\u003e\n  \u003cli\u003eAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel  Bowman. 2018\\natexlaba. Glue: A multi-task benchmark and analysis platform for natural  language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP:  Analyzing and Interpreting Neural Networks for NLP, pages 353–355.\u003c/li\u003e\n  \u003cli\u003eWei Wang, Ming Yan, and Chen Wu. 2018\\natexlabb. Multi-granularity hierarchical attention fusion networks for reading  comprehension and question answering. In Proceedings of the 56th Annual Meeting of the Association  for Computational Linguistics (Volume 1: Long Papers). Association for  Computational Linguistics.\u003c/li\u003e\n  \u003cli\u003eAlex Warstadt, Amanpreet Singh, and Samuel R Bowman. 2018. Neural network acceptability judgments. arXiv preprint arXiv:1805.12471.\u003c/li\u003e\n  \u003cli\u003eAdina Williams, Nikita Nangia, and Samuel R Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through  inference. In NAACL.\u003c/li\u003e\n  \u003cli\u003eYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang  Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. 2016. Google’s neural machine translation system: Bridging the gap between  human and machine translation. arXiv preprint arXiv:1609.08144.\u003c/li\u003e\n  \u003cli\u003eJason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson. 2014. How transferable are features in deep neural networks? In Advances in neural information processing systems, pages  3320–3328.\u003c/li\u003e\n  \u003cli\u003eAdams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad  Norouzi, and Quoc V Le. 2018. QANet: Combining local convolution with global self-attention for  reading comprehension. In ICLR.\u003c/li\u003e\n  \u003cli\u003eRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. 2018. Swag: A large-scale adversarial dataset for grounded commonsense  inference. In Proceedings of the 2018 Conference on Empirical Methods in  Natural Language Processing (EMNLP).\u003c/li\u003e\n  \u003cli\u003eYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun,  Antonio Torralba, and Sanja Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by  watching movies and reading books. In Proceedings of the IEEE international conference on computer  vision, pages 19–27.\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003chr/\u003e\n\n\u003cp\u003e\u003ca href=\"https://notbyai.fyi\"\u003e\u003cimg src=\"/assets/img/Written-By-Human-Not-By-AI-Badge-white.svg\" alt=\"Written by Human, Not by AI\"/\u003e\u003c/a\u003e\n\u003ca href=\"https://notbyai.fyi\"\u003e\u003cimg src=\"/assets/img/Written-By-Human-Not-By-AI-Badge-black.svg\" alt=\"Written by Human, Not by AI\"/\u003e\u003c/a\u003e\u003c/p\u003e\n\n\n  \u003c!-- POST NAVIGATION --\u003e\n  \u003cdiv class=\"postNav clearfix\"\u003e\n     \n      \u003ca class=\"prev\" href=\"/blog/k8s-scheduling-plugins-zh/\"\u003e\u003cspan\u003e« K8s 调度框架设计与 scheduler plugins 开发部署示例（2024）\u003c/span\u003e\n      \n    \u003c/a\u003e\n      \n      \n      \u003ca class=\"next\" href=\"/blog/instructgpt-paper-zh/\"\u003e\u003cspan\u003e[译][论文] InstructGPT：基于人类反馈训练语言模型遵从指令的能力（OpenAI，2022） »\u003c/span\u003e\n       \n      \u003c/a\u003e\n     \n  \u003c/div\u003e\n\u003c/div\u003e",
  "Date": "2024-03-10T00:00:00Z",
  "Author": "Arthur Chiao"
}