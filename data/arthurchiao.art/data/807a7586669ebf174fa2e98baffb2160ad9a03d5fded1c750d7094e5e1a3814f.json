{
  "Source": "arthurchiao.art",
  "Title": "[译] Transformer 是如何工作的：600 行 Python 代码实现两个（文本分类+文本生成）Transformer（2019）",
  "Link": "https://arthurchiao.art/blog/transformers-from-scratch-zh/",
  "Content": "\u003cdiv class=\"post\"\u003e\n  \n  \u003ch1 class=\"postTitle\"\u003e[译] Transformer 是如何工作的：600 行 Python 代码实现两个（文本分类+文本生成）Transformer（2019）\u003c/h1\u003e\n  \u003cp class=\"meta\"\u003ePublished at 2023-06-06 | Last Update 2023-08-12\u003c/p\u003e\n  \n  \u003ch3 id=\"译者序\"\u003e译者序\u003c/h3\u003e\n\n\u003cp\u003e本文整理和翻译自 2019 年（最后更新 2023 年）的一篇文章：\n\u003ca href=\"https://peterbloem.nl/blog/transformers\"\u003eTransformers From Scratch\u003c/a\u003e。\u003c/p\u003e\n\n\u003cp\u003e如果对 transformer 的使用场景和所处位置还不清楚，可以先看一下这篇：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003ca href=\"/blog/gpt-as-a-finite-state-markov-chain-zh/\"\u003eGPT 是如何工作的：200 行 Python 代码实现一个极简 GPT（2023）\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e理解本文大部分内容只需要基本的高数知识（矩阵乘法）。\n原文代码见\u003ca href=\"https://github.com/pbloem/former\"\u003e这里\u003c/a\u003e，不过训练代码用到的一些库更新非常快，\n因此跑起来可能有点困难。有兴趣有时间的可以考虑基于较新版本的库重构一下\nself-attention/transformer 及训练代码。\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e译者水平有限，不免存在遗漏或错误之处。如有疑问，敬请查阅原文。\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003e以下是译文。\u003c/p\u003e\n\n\u003chr/\u003e\n\n\u003cul id=\"markdown-toc\"\u003e\n  \u003cli\u003e\u003ca href=\"#译者序\" id=\"markdown-toc-译者序\"\u003e译者序\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#摘要\" id=\"markdown-toc-摘要\"\u003e摘要\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#1-self-attention自注意力模型\" id=\"markdown-toc-1-self-attention自注意力模型\"\u003e1 self-attention（自注意力）模型\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#10-attention注意力名字由来\" id=\"markdown-toc-10-attention注意力名字由来\"\u003e1.0 Attention（注意力）：名字由来\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#11-输入输出vector-to-vector-运算\" id=\"markdown-toc-11-输入输出vector-to-vector-运算\"\u003e1.1 输入输出：vector-to-vector 运算\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#12-权重矩阵计算和归一化\" id=\"markdown-toc-12-权重矩阵计算和归一化\"\u003e1.2 权重矩阵计算和归一化\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#13-直观展示与小结\" id=\"markdown-toc-13-直观展示与小结\"\u003e1.3 直观展示与小结\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#2-self-attention-为什么有效以电影推荐为例\" id=\"markdown-toc-2-self-attention-为什么有效以电影推荐为例\"\u003e2 self-attention 为什么有效？以电影推荐为例\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#21-传统推荐系统特性向量点积用户偏好\" id=\"markdown-toc-21-传统推荐系统特性向量点积用户偏好\"\u003e2.1 传统推荐系统：特性向量\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e点积\u003c/code\u003e用户偏好\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#22-基于-self-attention-的推荐系统\" id=\"markdown-toc-22-基于-self-attention-的推荐系统\"\u003e2.2 基于 self-attention 的推荐系统\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#221-电影特征和用户特征作为模型参数匹配已知的用户偏好\" id=\"markdown-toc-221-电影特征和用户特征作为模型参数匹配已知的用户偏好\"\u003e2.2.1 电影特征和用户特征作为模型参数，匹配已知的用户偏好\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#222-嵌入层对输入进行处理\" id=\"markdown-toc-222-嵌入层对输入进行处理\"\u003e2.2.2 嵌入层：对输入进行处理\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#223-直观解释\" id=\"markdown-toc-223-直观解释\"\u003e2.2.3 直观解释\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#224-self-attention-特殊属性\" id=\"markdown-toc-224-self-attention-特殊属性\"\u003e2.2.4 self-attention 特殊属性\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#3-实现一个基本的-self-attention\" id=\"markdown-toc-3-实现一个基本的-self-attention\"\u003e3. 实现一个基本的 self-attention\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#31-输入的表示tensor多维矩阵\" id=\"markdown-toc-31-输入的表示tensor多维矩阵\"\u003e3.1 输入的表示：tensor（多维矩阵）\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#32-计算权重矩阵输入矩阵--转置矩阵\" id=\"markdown-toc-32-计算权重矩阵输入矩阵--转置矩阵\"\u003e3.2 计算权重矩阵：输入矩阵 * 转置矩阵\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#33-计算输出\" id=\"markdown-toc-33-计算输出\"\u003e3.3 计算输出\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#34-现代-transformer-对-self-attention-的扩展\" id=\"markdown-toc-34-现代-transformer-对-self-attention-的扩展\"\u003e3.4 现代 transformer 对 self-attention 的扩展\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#341-引入控制参数for-queries-keys-and-values\" id=\"markdown-toc-341-引入控制参数for-queries-keys-and-values\"\u003e3.4.1 引入控制参数（for queries, keys and values）\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#342-对点积做缩放处理scaling-the-dot-product\" id=\"markdown-toc-342-对点积做缩放处理scaling-the-dot-product\"\u003e3.4.2 对点积做缩放处理（scaling the dot product）\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#343-引入-multi-head-attention\" id=\"markdown-toc-343-引入-multi-head-attention\"\u003e3.4.3 引入 multi-head attention\u003c/a\u003e            \u003cul\u003e\n              \u003cli\u003e\u003ca href=\"#需求输出中嵌入更多信息\" id=\"markdown-toc-需求输出中嵌入更多信息\"\u003e需求：输出中嵌入更多信息\u003c/a\u003e\u003c/li\u003e\n              \u003cli\u003e\u003ca href=\"#解决方式引入多个-self-attentionmulti-head\" id=\"markdown-toc-解决方式引入多个-self-attentionmulti-head\"\u003e解决方式：引入多个 self-attention（multi-head）\u003c/a\u003e\u003c/li\u003e\n              \u003cli\u003e\u003ca href=\"#提升-multi-head-self-attention-效率querykeyvalue-降维\" id=\"markdown-toc-提升-multi-head-self-attention-效率querykeyvalue-降维\"\u003e提升 multi-head self-attention 效率：query/key/value 降维\u003c/a\u003e\u003c/li\u003e\n              \u003cli\u003e\u003ca href=\"#完整工作流\" id=\"markdown-toc-完整工作流\"\u003e完整工作流\u003c/a\u003e\u003c/li\u003e\n            \u003c/ul\u003e\n          \u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#35-multi-head-vs-single-head-模型参数数量对比\" id=\"markdown-toc-35-multi-head-vs-single-head-模型参数数量对比\"\u003e3.5 multi-head vs. single-head 模型参数数量对比\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#351-single-head\" id=\"markdown-toc-351-single-head\"\u003e3.5.1 single-head\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#352-multi-head\" id=\"markdown-toc-352-multi-head\"\u003e3.5.2 multi-head\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#44-self-attention-主要代码实现\" id=\"markdown-toc-44-self-attention-主要代码实现\"\u003e4.4 self-attention 主要代码实现\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#4-基于-multi-head-self-attention-实现-transformers\" id=\"markdown-toc-4-基于-multi-head-self-attention-实现-transformers\"\u003e4 基于 multi-head self-attention 实现 transformers\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#41-transformer-定义\" id=\"markdown-toc-41-transformer-定义\"\u003e4.1 Transformer 定义\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#42-transformer-block\" id=\"markdown-toc-42-transformer-block\"\u003e4.2 Transformer block\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#43-文本分类text-classificationtransformer\" id=\"markdown-toc-43-文本分类text-classificationtransformer\"\u003e4.3 文本分类（text classification）transformer\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#431-输出单个分类结果\" id=\"markdown-toc-431-输出单个分类结果\"\u003e4.3.1 输出：单个分类结果\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#432-输入词序敏感using-the-positions\" id=\"markdown-toc-432-输入词序敏感using-the-positions\"\u003e4.3.2 输入：词序敏感（using the positions）\u003c/a\u003e            \u003cul\u003e\n              \u003cli\u003e\u003ca href=\"#位置嵌入position-embeddings\" id=\"markdown-toc-位置嵌入position-embeddings\"\u003e位置嵌入（position embeddings）\u003c/a\u003e\u003c/li\u003e\n              \u003cli\u003e\u003ca href=\"#位置编码position-encodings\" id=\"markdown-toc-位置编码position-encodings\"\u003e位置编码（position encodings）\u003c/a\u003e\u003c/li\u003e\n            \u003c/ul\u003e\n          \u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#433-基于-pytorch-实现\" id=\"markdown-toc-433-基于-pytorch-实现\"\u003e4.3.3 基于 Pytorch 实现\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#44-文本生成text-generationtransformer\" id=\"markdown-toc-44-文本生成text-generationtransformer\"\u003e4.4 文本生成（text generation）transformer\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#441-自回归模型和掩码\" id=\"markdown-toc-441-自回归模型和掩码\"\u003e4.4.1 自回归模型和掩码\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#442-训练基于维基百科数据集-enwik8\" id=\"markdown-toc-442-训练基于维基百科数据集-enwik8\"\u003e4.4.2 训练：基于维基百科数据集 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eenwik8\u003c/code\u003e\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#443-文本生成结果分析\" id=\"markdown-toc-443-文本生成结果分析\"\u003e4.4.3 文本生成结果分析\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#45-设计考虑transformer-与-rnn卷积-对比\" id=\"markdown-toc-45-设计考虑transformer-与-rnn卷积-对比\"\u003e4.5 设计考虑：Transformer 与 RNN/卷积 对比\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#5-历史包袱\" id=\"markdown-toc-5-历史包袱\"\u003e5 历史包袱\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#51-为什么叫-self-attention\" id=\"markdown-toc-51-为什么叫-self-attention\"\u003e5.1 为什么叫 self-attention？\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#52-最初的-transformer-encoders-and-decoders\" id=\"markdown-toc-52-最初的-transformer-encoders-and-decoders\"\u003e5.2 最初的 transformer: encoders and decoders\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#6-现代-transformers\" id=\"markdown-toc-6-现代-transformers\"\u003e6 现代 transformers\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#61-google-bert340m-参数\" id=\"markdown-toc-61-google-bert340m-参数\"\u003e6.1 Google \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eBERT\u003c/code\u003e：\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e340M\u003c/code\u003e 参数\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#62-openai-gpt-215b-参数\" id=\"markdown-toc-62-openai-gpt-215b-参数\"\u003e6.2 OpenAI \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eGPT-2\u003c/code\u003e：\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e1.5B\u003c/code\u003e 参数\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#63-transformer-xl\" id=\"markdown-toc-63-transformer-xl\"\u003e6.3 \u003c/a\u003e\u003ca href=\"https://arxiv.org/abs/1901.02860\"\u003eTransformer-XL\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#64-sparse-transformers\" id=\"markdown-toc-64-sparse-transformers\"\u003e6.4 \u003c/a\u003e\u003ca href=\"https://openai.com/blog/sparse-transformer/\"\u003eSparse transformers\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#7-大型模型优化\" id=\"markdown-toc-7-大型模型优化\"\u003e7 大型模型优化\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#71-半精度half-precision\" id=\"markdown-toc-71-半精度half-precision\"\u003e7.1 半精度（half precision）\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#72-梯度积累gradient-accumulation\" id=\"markdown-toc-72-梯度积累gradient-accumulation\"\u003e7.2 梯度积累（gradient accumulation）\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#73-梯度-checkpointgradient-checkpointing\" id=\"markdown-toc-73-梯度-checkpointgradient-checkpointing\"\u003e7.3 梯度 checkpoint（gradient checkpointing）\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#8-结束语\" id=\"markdown-toc-8-结束语\"\u003e8 结束语\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#参考资料\" id=\"markdown-toc-参考资料\"\u003e参考资料\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003chr/\u003e\n\n\u003cscript type=\"text/x-mathjax-config\"\u003e\n  \tMathJax.Hub.Config({\n    \textensions: [\"tex2jax.js\"],\n    \tjax: [\"input/TeX\", \"output/HTML-CSS\"],\n    \ttex2jax: {\n      \t\tinlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n      \t\tdisplayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ],\n    \t\tprocessEscapes: true\n\t    },\n\t\t\"HTML-CSS\": {\n\t\t\tavailableFonts: [], preferredFont: null,\n\t\t\twebFont: \"Neo-Euler\",\n\t\t\tmtextFontInherit: true\n\t\t},\n\t\tTeX: {\n\t\t\textensions: [\"color.js\"],\n\t\t\tMacros: {\n\t\t\t\tlgc: [\"{\\\\color{my-light-green} #1}\", 1],\n\t\t\t\tgc: [\"{\\\\color{my-green} #1}\", 1],\n\t\t\t\tlrc: [\"{\\\\color{my-light-red} #1}\", 1],\n\t\t\t\trc: [\"{\\\\color{my-red} #1}\", 1],\n\t\t\t\tlbc: [\"{\\\\color{my-light-blue} #1}\", 1],\n\t\t\t\tbc: [\"{\\\\color{my-blue} #1}\", 1],\n\t\t\t\tkc: [\"{\\\\color{my-gray} #1}\", 1],\n\t\t\t\tloc: [\"{\\\\color{my-light-orange} #1}\", 1],\n\t\t\t\toc: [\"{\\\\color{my-orange} #1}\", 1],\n\n\t\t\t\ta: [\"\\\\mathbf a\"],\n\t\t\t\tA: [\"\\\\mathbf A\"],\n\t\t\t\tb: [\"\\\\mathbf b\"],\n\t\t\t\tB: [\"\\\\mathbf B\"],\n\t\t\t\tc: [\"\\\\mathbf c\"],\n\t\t\t\tC: [\"\\\\mathbf C\"],\n\t\t\t\td: [\"\\\\mathbf d\"],\n\t\t\t\tD: [\"\\\\mathbf D\"],\n\t\t\t\tE: [\"\\\\mathbf E\"],\n\t\t\t\tI: [\"\\\\mathbf I\"],\n\t\t\t\tL: [\"\\\\mathbf L\"],\n\t\t\t\tm: [\"\\\\mathbf m\"],\n\t\t\t\tM: [\"\\\\mathbf M\"],\n\t\t\t\tr: [\"\\\\mathbf r\"],\n\t\t\t\ts: [\"\\\\mathbf s\"],\n\t\t\t\tt: [\"\\\\mathbf t\"],\n\t\t\t\tS: [\"\\\\mathbf S\"],\n\t\t\t\tx: [\"\\\\mathbf x\"],\n\t\t\t\tz: [\"\\\\mathbf z\"],\n\t\t\t\tv: [\"\\\\mathbf v\"],\n\t\t\t\ty: [\"\\\\mathbf y\"],\n\t\t\t\tk: [\"\\\\mathbf k\"],\n\t\t\t\tbp: [\"\\\\mathbf p\"],\n\t\t\t\tP: [\"\\\\mathbf P\"],\n\t\t\t\tq: [\"\\\\mathbf q\"],\n\t\t\t\tQ: [\"\\\\mathbf Q\"],\n\t\t\t\tr: [\"\\\\mathbf r\"],\n\t\t\t\tR: [\"\\\\mathbf R\"],\n\t\t\t\tSig: [\"\\\\mathbf \\\\Sigma\"],\n\t\t\t\tt: [\"\\\\mathbf t\"],\n\t\t\t\tT: [\"\\\\mathbf T\"],\n\t\t\t\te: [\"\\\\mathbf e\"],\n\t\t\t\tX: [\"\\\\mathbf X\"],\n\t\t\t\tu: [\"\\\\mathbf u\"],\n\t\t\t\tU: [\"\\\\mathbf U\"],\n\t\t\t\tv: [\"\\\\mathbf v\"],\n\t\t\t\tV: [\"\\\\mathbf V\"],\n\t\t\t\tw: [\"\\\\mathbf w\"],\n\t\t\t\tW: [\"\\\\mathbf W\"],\n\t\t\t\tY: [\"\\\\mathbf Y\"],\n\t\t\t\tz: [\"\\\\mathbf z\"],\n\t\t\t\tZ: [\"\\\\mathbf Z\"],\n\t\t\t\tp: [\"\\\\,\\\\text{.}\"],\n\t\t\t\ttab: [\"\\\\hspace{0.7cm}\"],\n\n\t\t\t\tsp: [\"^{\\\\small\\\\prime}\"],\n\n\n\t\t\t\tmR: [\"{\\\\mathbb R}\"],\n\t\t\t\tmC: [\"{\\\\mathbb C}\"],\n\t\t\t\tmN: [\"{\\\\mathbb N}\"],\n\t\t\t\tmZ: [\"{\\\\mathbb Z}\"],\n\n\t\t\t\tdeg: [\"{^\\\\circ}\"],\n\n\n\t\t\t\targmin: [\"\\\\underset{#1}{\\\\text{argmin}}\", 1],\n\t\t\t\targmax: [\"\\\\underset{#1}{\\\\text{argmax}}\", 1],\n\n\t\t\t\tco: [\"\\\\;\\\\text{cos}\"],\n\t\t\t\tsi: [\"\\\\;\\\\text{sin}\"]\n\t\t\t}\n\t\t}\n  \t});\n\n  \tMathJax.Hub.Register.StartupHook(\"TeX color Ready\", function() {\n     \tMathJax.Extension[\"TeX/color\"].colors[\"my-green\"] = '#677d00';\n     \tMathJax.Extension[\"TeX/color\"].colors[\"my-light-green\"] = '#acd373';\n     \tMathJax.Extension[\"TeX/color\"].colors[\"my-red\"] = '#b13e26';\n     \tMathJax.Extension[\"TeX/color\"].colors[\"my-light-red\"] = '#d38473';\n     \tMathJax.Extension[\"TeX/color\"].colors[\"my-blue\"] = '#306693';\n       \tMathJax.Extension[\"TeX/color\"].colors[\"my-light-blue\"] = '#73a7d3';\n       \tMathJax.Extension[\"TeX/color\"].colors[\"my-gray\"] = '#999';\n       \tMathJax.Extension[\"TeX/color\"].colors[\"my-orange\"] = '#E69500';\n       \tMathJax.Extension[\"TeX/color\"].colors[\"my-light-orange\"] = '#FFC353';\n\n\n\t});\n\u003c/script\u003e\n\n\u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js\"\u003e\n\u003c/script\u003e\n\n\u003ch1 id=\"摘要\"\u003e摘要\u003c/h1\u003e\n\n\u003cp\u003eTransformer 是一类非常令人着迷的\u003cstrong\u003e\u003cmark\u003e机器学习架构\u003c/mark\u003e\u003c/strong\u003e（a family of machine learning architectures）。\n之前已经有一些不错的介绍文章（例如 [1, 2]），但过去几年 \u003cstrong\u003e\u003cmark\u003etransformer 变得简单了很多\u003c/mark\u003e\u003c/strong\u003e，\n因此要解释清楚现代架构（modern architectures）是如何工作的，比以前容易多了。\u003c/p\u003e\n\n\u003cp\u003e本文试图丢掉历史包袱，开门见山地解释\u003cstrong\u003e\u003cmark\u003e现代 transformer 的工作原理\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e\u003cstrong\u003e\u003cmark\u003e神经网络和反向传播\u003c/mark\u003e\u003c/strong\u003e（neural networks and backpropagation）的基本知识有助于更好地理解本文，\u003c/p\u003e\n\n  \u003cul\u003e\n    \u003cli\u003e\u003ca href=\"https://mlvu.github.io/lecture06/\"\u003e这个讲座\u003c/a\u003e 介绍了神经网络的基础知识；\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"https://mlvu.github.io/lecture07/\"\u003e这个讲座\u003c/a\u003e 介绍了神经网络如何应用于现代深度学习系统。\u003c/li\u003e\n  \u003c/ul\u003e\n\n  \u003cp\u003e另外，理解本文程序需要\u003ca href=\"https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\"\u003e一点 Pytorch 基础\u003c/a\u003e，\n但没有关系也不大。\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch1 id=\"1-self-attention自注意力模型\"\u003e1 self-attention（自注意力）模型\u003c/h1\u003e\n\n\u003cp\u003eself-attention 运算是\u003cstrong\u003e\u003cmark\u003e所有 transformer 架构的基本运算\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003ch2 id=\"10-attention注意力名字由来\"\u003e1.0 Attention（注意力）：名字由来\u003c/h2\u003e\n\n\u003cp\u003e从最简形式上来说，神经网络是一系列\u003cstrong\u003e\u003cmark\u003e对输入进行加权计算，得到一个输出\u003c/mark\u003e\u003c/strong\u003e的过程。\u003c/p\u003e\n\n\u003cp\u003e具体来说，比如给定一个向量 [1,2,3,4,5] 作为输入，权重矩阵可能是 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e[0, 0, 0, 0.5, 0.5]\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e，\n也就是说最终的 output 实际上只与 input 中的最后两个元素有关系 —— 换句话说，\n这一层神经网络只\u003cstrong\u003e\u003cmark\u003e关注\u003c/mark\u003e\u003c/strong\u003e最后两个元素（\u003cstrong\u003e\u003cmark\u003e注意力\u003c/mark\u003e\u003c/strong\u003e在最后两个元素上），\n其他元素是什么值对结果没有影响 —— 这就是 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eattention\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 这一名字的由来。\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e注意力模型大大降低了神经网络的计算量：经典神经网络是全连接的，而上面的例子中，\n这一层神经网络不需要全连接了，每个输出连接到最后两个输入就行了，也就是从 1x5 维降低到了 1x2 维。\u003c/p\u003e\n\n  \u003cp\u003e图像处理中的卷积神经网络（CNN）也是类似原理：只用一小块图像计算下一层的输出，而不是用整帧图像。\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch2 id=\"11-输入输出vector-to-vector-运算\"\u003e1.1 输入输出：vector-to-vector 运算\u003c/h2\u003e\n\n\u003cp\u003eSelf-attention 是一个 \u003cstrong\u003e\u003cmark\u003esequence-to-sequence 运算\u003c/mark\u003e\u003c/strong\u003e：\n输入一个向量序列（a sequence of vectors），输出另一个向量序列。\u003c/p\u003e\n\n\u003cp\u003e我们用\n\\(\\x_1, \\x_2, \\ldots, \\x_t\\) 表示\u003cstrong\u003e\u003cmark\u003e输入向量\u003c/mark\u003e\u003c/strong\u003e，用\n\\(\\y_1, \\y_2, \\ldots, \\y_t\\) 表示相应的\u003cstrong\u003e\u003cmark\u003e输出向量\u003c/mark\u003e\u003c/strong\u003e，这些向量都是 k 维的。\n要计算输出向量 \\(\\y_\\rc{i}\\) ，self-attention 只需对所有输入向量做\u003cstrong\u003e\u003cmark\u003e加权平均\u003c/mark\u003e\u003c/strong\u003e（weighted average），\u003c/p\u003e\n\n\\[\\y_\\rc{i} = \\sum_{\\gc{j}} w_{\\rc{i}\\gc{j}} \\x_\\gc{j}\\]\n\n\u003cp\u003e在传统神经网络中，权重都是（常量）\u003cstrong\u003e\u003cmark\u003e参数\u003c/mark\u003e\u003c/strong\u003e，\n但这里的权重并不是：\\(w_{\\rc{i}\\gc{j}}\\) 是根据 \\(\\x_\\rc{i}\\) 和 \\(\\x_\\gc{j}\\) 计算出来的。\n计算它有很多种方式（算法），接下来看一种最简单的。\u003c/p\u003e\n\n\u003ch2 id=\"12-权重矩阵计算和归一化\"\u003e1.2 权重矩阵计算和归一化\u003c/h2\u003e\n\n\u003cp\u003e计算权重矩阵的 \u003cstrong\u003e\u003cmark\u003e最简单函数就是点积\u003c/mark\u003e\u003c/strong\u003e（dot product）：\u003c/p\u003e\n\n\\[w\u0026#39;_{\\rc{i}\\gc{j}} = {\\x_\\rc{i}}^T\\x_\\gc{j}\\]\n\n\u003cblockquote\u003e\n  \u003cp\u003e注意到权重矩阵的计算跟它所在的位置 \\((i,j)\\) 直接相关，也就是说，每个位置 \\((i,j)\\) 对应的权重矩阵都不一样。\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e点积得到的结果取值范围是正负无穷，为了使累加和（表示概率）等于 100%，\n需要对它们做\u003cstrong\u003e\u003cmark\u003e归一化\u003c/mark\u003e\u003c/strong\u003e：用 pytorch 术语来说就是 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003esoftmax\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e，\u003c/p\u003e\n\n\\[w_{\\rc{i}\\gc{j}} = \\frac{\\text{exp } w\u0026#39;_{\\rc{i}\\gc{j}}}{\\sum_\\gc{j} \\text{exp }w\u0026#39;_{\\rc{i}\\gc{j}}}\\]\n\n\u003cp\u003e这会将每个权重矩阵归一化到 [0,1]，并且累加和等于 1。\u003c/p\u003e\n\n\u003ch2 id=\"13-直观展示与小结\"\u003e1.3 直观展示与小结\u003c/h2\u003e\n\n\u003cp\u003e以上就是关于 self-attention 的基本运算。总结起来就是两点：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003evector-to-vector 运算\u003c/mark\u003e\u003c/strong\u003e：self-attention 是对 input vector 做\u003cstrong\u003e\u003cmark\u003e矩阵运算\u003c/mark\u003e\u003c/strong\u003e，得到一个\u003cstrong\u003e\u003cmark\u003e加权结果\u003c/mark\u003e\u003c/strong\u003e作为 output vector；\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e加权矩阵计算\u003c/mark\u003e\u003c/strong\u003e：权重矩阵\u003cstrong\u003e\u003cmark\u003e不是常量\u003c/mark\u003e\u003c/strong\u003e，而是跟它所在的位置 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e(i,j)\u003c/code\u003e 直接相关，根据对应位置的 input vector 计算。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e用图来表示如下：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/transformers-from-scratch/self-attention.png\" width=\"65%\" height=\"65%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eself-attention 基本运算\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eoutput vector 中的每个元素 \\(y_j\\) 都是对 \u003cstrong\u003e\u003cmark\u003einput vector 中所有元素\u003c/mark\u003e\u003c/strong\u003e的加权和；\u003c/li\u003e\n  \u003cli\u003e对于 \\(y_j\\)，加权矩阵由 input 元素 \\(x_j\\) 与每个 input 元素计算得到；\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e要构建一个完整的 transformer 还需要一点其他东西，但\u003cstrong\u003e\u003cmark\u003e最核心的运算就是以上这两个了\u003c/mark\u003e\u003c/strong\u003e。\n更重要的是，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e这是整个架构中，唯一在 \u003cstrong\u003e\u003cmark\u003einput \u0026amp; output vector 之间\u003c/mark\u003e\u003c/strong\u003e 所做的运算；\u003c/li\u003e\n  \u003cli\u003eTransformer 架构中的其他运算都是\u003cstrong\u003e\u003cmark\u003e单纯对 input vector 做运算\u003c/mark\u003e\u003c/strong\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch1 id=\"2-self-attention-为什么有效以电影推荐为例\"\u003e2 self-attention 为什么有效？以电影推荐为例\u003c/h1\u003e\n\n\u003cp\u003e上面看到 self-attention 模型非常简单，本质上就一个加权平均公式，那为什么这个加权平均机制的效果这么好呢？\n为了直观解释这个问题，我们看个具体例子 —— 电影推荐 —— 假设你经营着一家电影租赁公司，\n想向用户推荐他们可能会喜欢的电影。接下来看看基于传统方式和 transformer 方式分别是怎么做的。\u003c/p\u003e\n\n\u003ch2 id=\"21-传统推荐系统特性向量点积用户偏好\"\u003e2.1 传统推荐系统：特性向量\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e点积\u003c/code\u003e用户偏好\u003c/h2\u003e\n\n\u003cp\u003e步骤很简单：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e人工设计一些\u003cstrong\u003e\u003cmark\u003e电影特征\u003c/mark\u003e\u003c/strong\u003e，比如浪漫指数、动作指数，\u003c/li\u003e\n  \u003cli\u003e人工设计一些\u003cstrong\u003e\u003cmark\u003e用户特征\u003c/mark\u003e\u003c/strong\u003e，例如他们喜欢浪漫电影或动作片的可能性；\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e有了这两个维度的数据（特征向量）之后，\u003cstrong\u003e\u003cmark\u003e对二者做点积\u003c/mark\u003e\u003c/strong\u003e（dot product），\n得到的就是电影属性与用户喜欢程度之间的\u003cstrong\u003e\u003cmark\u003e匹配程度\u003c/mark\u003e\u003c/strong\u003e，用得分表示，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/transformers-from-scratch/movie-dot-product.png\" width=\"65%\" height=\"65%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e\u003cmark\u003e电影推荐\u003c/mark\u003e：电影特征向量（浪漫、动作、喜剧）与用户特性向量（喜欢浪漫、动作、喜剧的程度）做点积运算\u003c/p\u003e\n\n\u003cp\u003e关于计算结果（得分）：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e如果\u003cstrong\u003e\u003cmark\u003e特征的符号相同\u003c/mark\u003e\u003c/strong\u003e，例如“浪漫电影 \u0026amp;\u0026amp; 用户喜欢浪漫电影”，\n或者“不是浪漫电影 \u0026amp;\u0026amp; 用户不喜欢浪漫电影”，得到的\u003cstrong\u003e\u003cmark\u003e点积就是正数\u003c/mark\u003e\u003c/strong\u003e；反之就是负数；\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e特征值的大小\u003c/mark\u003e\u003c/strong\u003e决定该特征\u003cstrong\u003e\u003cmark\u003e对总分的贡献大小\u003c/mark\u003e\u003c/strong\u003e：\n一部电影可能有点浪漫，但不是很明显，或者用户可能只是不喜欢浪漫，但也没到讨厌的程度。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e这种推荐模型的好处是简单直接，很容易上手；缺点是规模大了很难搞，\n因为对几百万部电影打标的成本非常高，精确标记用户喜欢或不喜欢什么也几乎是不可能的。\u003c/p\u003e\n\n\u003ch2 id=\"22-基于-self-attention-的推荐系统\"\u003e2.2 基于 self-attention 的推荐系统\u003c/h2\u003e\n\n\u003cp\u003e接下来看基于 self-attention 的推荐系统是怎么设计的。\u003c/p\u003e\n\n\u003ch3 id=\"221-电影特征和用户特征作为模型参数匹配已知的用户偏好\"\u003e2.2.1 电影特征和用户特征作为模型参数，匹配已知的用户偏好\u003c/h3\u003e\n\n\u003cp\u003e也是两步：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e电影特征和用户特征\u003c/mark\u003e\u003c/strong\u003e不再直接做点积运算，而是作为\u003cstrong\u003e\u003cmark\u003e模型的参数\u003c/mark\u003e\u003c/strong\u003e（parameters of the model）；\u003c/li\u003e\n  \u003cli\u003e收集少量的\u003cstrong\u003e\u003cmark\u003e用户偏好作为目标\u003c/mark\u003e\u003c/strong\u003e，然后通过\u003cstrong\u003e\u003cmark\u003e优化用户特征和电影特征\u003c/mark\u003e\u003c/strong\u003e（模型参数），\n  使二者的\u003cstrong\u003e\u003cmark\u003e点积匹配已知的用户喜好\u003c/mark\u003e\u003c/strong\u003e。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e这就是 self-attention 的基本原理。注意，\n尽管我们\u003cstrong\u003e\u003cmark\u003e没有告诉模型某个特征意味着什么\u003c/mark\u003e\u003c/strong\u003e（表示什么），\n但实践证明，训练之后的特征确实反映了关于电影内容的合理语义。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/transformers-from-scratch/movie-features.png\" width=\"55%\" height=\"55%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e从一个基本的 matrix factorization 模型学习到的前两个特征。\n模型\u003cmark\u003e只用到了“哪些用户喜欢哪些电影”\u003c/mark\u003e信息，而\u003cmark\u003e没有用到任何电影内容信息\u003c/mark\u003e。\n横轴：从流俗到高雅；纵轴：从小众到主流。信息来自 [4]。\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e这些已经足够说明 dot product 是如何表示对象和它们的关系的。\n更多关于推荐系统的内容，可移步 \u003ca href=\"https://mlvu.github.io/lecture12\"\u003emlvu.github.io/lecture12\u003c/a\u003e。\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch3 id=\"222-嵌入层对输入进行处理\"\u003e2.2.2 嵌入层：对输入进行处理\u003c/h3\u003e\n\n\u003cp\u003e假设我们有一串单词作为输入，原理上只要将其作为 input vector 送到 self-attention 模型。\n但实际上我们需要对这个 input vector 做一下预处理（下一节会解释为什么），生成一个中间表示，\n这就是序列建模中的\u003cstrong\u003e\u003cmark\u003e嵌入层\u003c/mark\u003e\u003c/strong\u003e。\n具体来说，会\u003cstrong\u003e\u003cmark\u003e为每个单词\u003c/mark\u003e\u003c/strong\u003e \\(\\bc{t}\\) \u003cstrong\u003e\u003cmark\u003e分配一个嵌入向量\u003c/mark\u003e\u003c/strong\u003e（embedding vector）\n\\(\\v_\\bc{t}\\)（我们后面将学习到这个值）。\u003c/p\u003e\n\n\u003cp\u003e嵌入层将 input vector：\u003c/p\u003e\n\n\\[\\bc{\\text{the}}, \\bc{\\text{cat}}, \\bc{\\text{walks}}, \\bc{\\text{on}}, \\bc{\\text{the}}, \\bc{\\text{street}}\\]\n\n\u003cp\u003e转换为 \u003cstrong\u003e\u003cmark\u003eembedding vector\u003c/mark\u003e\u003c/strong\u003e（注意：每个单词的维度从 1x1 变成了 1xN）：\u003c/p\u003e\n\n\\[\\v_\\bc{\\text{the}}, \\v_\\bc{\\text{cat}}, \\v_\\bc{\\text{walks}}, \\v_\\bc{\\text{on}}, \\v_\\bc{\\text{the}}, \\v_\\bc{\\text{street}}\\]\n\n\u003cp\u003e将这个 embedding vectors \u003cstrong\u003e\u003cmark\u003e输入 self-attention\u003c/mark\u003e\u003c/strong\u003e 层，得到的就是 output vector：\u003c/p\u003e\n\n\\[\\y_\\bc{\\text{the}}, \\y_\\bc{\\text{cat}}, \\y_\\bc{\\text{walks}}, \\y_\\bc{\\text{on}}, \\y_\\bc{\\text{the}}, \\y_\\bc{\\text{street}}\\]\n\n\u003cp\u003e其中 \\(\\y_\\bc{\\text{cat}}\\)\n是所有嵌入向量的加权和（weighted sum），由它们与 \\(\\v_\\bc{\\text{cat}}\\) 的（归一化）点积加权。\u003c/p\u003e\n\n\u003ch3 id=\"223-直观解释\"\u003e2.2.3 直观解释\u003c/h3\u003e\n\n\u003cp\u003e由于我们正在\u003cstrong\u003e\u003cmark\u003e学习（learning）\u003c/mark\u003e\u003c/strong\u003e \\(\\v_\\bc{t}\\) 的值是什么，两个词的“相关”程度完全由任务决定。\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e在大多数情况下，\u003cstrong\u003e\u003cmark\u003e定冠词 \u0026#34;the\u0026#34;\u003c/mark\u003e\u003c/strong\u003e 与句子中其他单词表示什么意思（the interpretation of the other words）关系不大；\n因此我们最终得到的嵌入层 \\(\\v_\\bc{\\text{the}}\\) 与所有其他单词的点积可能\u003cstrong\u003e\u003cmark\u003e很小或为负数\u003c/mark\u003e\u003c/strong\u003e；\u003c/li\u003e\n  \u003cli\u003e另一方面，要解释这句话中 “walks” 的意思，弄清楚谁在走路是非常有用的。这很可能由名词表达，\n因此对于像 cat 这样的名词和像 walks 这样的动词，我们可能最终学习到的 \\(\\v_\\bc{\\text{cat}}\\) and \\(\\v_\\bc{\\text{walks}}\\) 点积是个\u003cstrong\u003e\u003cmark\u003e较大的正数\u003c/mark\u003e\u003c/strong\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e这就是 self-attention 背后的基本直觉：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e点积表示\u003c/mark\u003e\u003c/strong\u003e输入序列中两个向量的\u003cstrong\u003e\u003cmark\u003e相关程度\u003c/mark\u003e\u003c/strong\u003e，“相关”由学习任务（learning task）定义，\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e输出向量\u003c/mark\u003e\u003c/strong\u003e是整个输入序列的\u003cstrong\u003e\u003cmark\u003e加权和，权重由这些点积决定\u003c/mark\u003e\u003c/strong\u003e。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch3 id=\"224-self-attention-特殊属性\"\u003e2.2.4 self-attention 特殊属性\u003c/h3\u003e\n\n\u003cp\u003e在继续之前，有些特殊属性需要提及一下，因为不同于在一般的 sequence-to-sequence 运算：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\n    \u003cp\u003e\u003cstrong\u003e\u003cmark\u003e到目前为止，我们的 self-attention 模型还没有参数\u003c/mark\u003e\u003c/strong\u003e（\n 虽然下文中，我们还是会为 self-attention 添加几个参数）。\u003c/p\u003e\n\n    \u003cp\u003e换句话说，基本的 self-attention 实际上做什么完全取决于\u003cstrong\u003e\u003cmark\u003e生成输入序列的上游机制\u003c/mark\u003e\u003c/strong\u003e。\n 例如嵌入层这种机制会驱动着 self-attention 学习基于点积的表示。\u003c/p\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eself-attention \u003cstrong\u003e\u003cmark\u003e将输入当做一个集合（set）而不是序列（sequence）\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n    \u003cp\u003e如果我们对输入序列进行重排（permute），输出序列除了也跟着重排，其他方面将完全相同，\n也就是说 self-attention 是\u003cstrong\u003e\u003cmark\u003e排列等变\u003c/mark\u003e\u003c/strong\u003e的（permutation  equivariant）。\n后面会看到，构建完整的 transformer 时，我们还是会引入一些东西来保持输入的顺序信息，\n但要明白 \u003cstrong\u003e\u003cmark\u003eself-attention 本身是不关心输入的顺序属性的\u003c/mark\u003e\u003c/strong\u003e（sequential nature）。\u003c/p\u003e\n  \u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch1 id=\"3-实现一个基本的-self-attention\"\u003e3. 实现一个基本的 self-attention\u003c/h1\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eWhat I cannot create, I do not understand. —— Feynman.\u003c/p\u003e\n\n  \u003cp\u003e纸上得来终觉浅，绝知此事要躬行 —— 陆游《冬夜读书示子聿》\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e接下来我们基于 pytorch 实现前面介绍的最基础 self-attention 模型。\u003c/p\u003e\n\n\u003ch2 id=\"31-输入的表示tensor多维矩阵\"\u003e3.1 输入的表示：tensor（多维矩阵）\u003c/h2\u003e\n\n\u003cp\u003e我们面临的第一个问题是\u003cstrong\u003e\u003cmark\u003e如何用矩阵乘法表示 self-attention\u003c/mark\u003e\u003c/strong\u003e：\n按照定义，直接遍历所有 input vectors 来计算 weight 和 output 就行，\n但显然这种方式效率太低；改进的方式就是用 pytorch 的 tensor 来表示，\n这是一个\u003cstrong\u003e\u003cmark\u003e多维矩阵\u003c/mark\u003e\u003c/strong\u003e数据结构：\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eA \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003etorch.Tensor\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e is a multi-dimensional matrix containing elements of a single data type.\u003c/p\u003e\n\n  \u003cp\u003e\u003ca href=\"https://pytorch.org/docs/stable/tensors.html\"\u003epytorch.org/docs/stable/tensors.html\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cul\u003e\n  \u003cli\u003e输入 \\(\\X\\) 由 \\(t\\) 个 k-维 vector 组成的序列，\u003c/li\u003e\n  \u003cli\u003e引入一个 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003emini-batch\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e dimension \\(b\\)，\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e就得到了一个\u003cstrong\u003e\u003cmark\u003e三维矩阵\u003c/mark\u003e\u003c/strong\u003e \\((b, t, k)\\)，这就是一个 tensor。\u003c/p\u003e\n\n\u003ch2 id=\"32-计算权重矩阵输入矩阵--转置矩阵\"\u003e3.2 计算权重矩阵：输入矩阵 * 转置矩阵\u003c/h2\u003e\n\n\u003cp\u003e接下来计算加权矩阵，它表示的是 \u003cstrong\u003e\u003cmark\u003einput vector 之间的相关性\u003c/mark\u003e\u003c/strong\u003e，\n因此用输入矩阵 \\(\\X\\) 乘以它的转置矩阵（transpose），用 pytorch 库来计算非常方便。\u003c/p\u003e\n\n\u003cdiv class=\"language-python highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003etorch\u003c/span\u003e\n\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003etorch.nn.functional\u003c/span\u003e \u003cspan class=\"k\"\u003eas\u003c/span\u003e \u003cspan class=\"n\"\u003eF\u003c/span\u003e\n\n\u003cspan class=\"c1\"\u003e# 假设我们有一些 tensor x 作为输入，它是 (b, t, k) 维矩阵\n\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e...\u003c/span\u003e\n\n\u003cspan class=\"c1\"\u003e# torch.bmm() 是批量矩阵乘法（batched matrix multiplication）函数，对一批矩阵执行乘法操作\n\u003c/span\u003e\u003cspan class=\"n\"\u003eraw_weights\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ebmm\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003etranspose\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003e然后对权重矩阵进行正值化和归一化，以使得一个 row 内所有权重加起来为 1，\u003c/p\u003e\n\n\u003cdiv class=\"language-python highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"n\"\u003eweights\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eF\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esoftmax\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eraw_weights\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003edim\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003ch2 id=\"33-计算输出\"\u003e3.3 计算输出\u003c/h2\u003e\n\n\u003cp\u003e有了权重矩阵，计算输出就非常简单了：只需要将输入 \\(\\X\\) 和权重矩阵相乘即可，一行代码搞定：\u003c/p\u003e\n\n\u003cdiv class=\"language-python highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"n\"\u003ey\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ebmm\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eweights\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003e输出矩阵 \\(\\Y\\) 就是 size \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e(b, t, k)\u003c/code\u003e 的 tensor，每一行都是对 \\(\\X\\) 的行的加权。\u003c/p\u003e\n\n\u003cp\u003e这就是 \u003cstrong\u003e\u003cmark\u003e最基础的 self-attention 模型\u003c/mark\u003e\u003c/strong\u003e的实现：\n\u003cstrong\u003e\u003cmark\u003e两次矩阵乘法和一次归一化\u003c/mark\u003e\u003c/strong\u003e（softmax）。\u003c/p\u003e\n\n\u003ch2 id=\"34-现代-transformer-对-self-attention-的扩展\"\u003e3.4 现代 transformer 对 self-attention 的扩展\u003c/h2\u003e\n\n\u003cp\u003e现代 transformer 中实际使用的 self-attention 依赖于三个额外技巧。\u003c/p\u003e\n\n\u003ch3 id=\"341-引入控制参数for-queries-keys-and-values\"\u003e3.4.1 引入控制参数（for queries, keys and values）\u003c/h3\u003e\n\n\u003cp\u003e对于位置 \\(i\\) 处的 input vector \\(\\x_\\rc{i}\\)，它在 \u003cstrong\u003e\u003cmark\u003eself-attention 中会被使用三次\u003c/mark\u003e\u003c/strong\u003e，\n根据角色的不同分别称为 queries、keys、values（查询、键和值，后面再解释这些名称的来源），\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003equery\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e：与其他所有 input vector 联合计算 \\(i\\) 位置的 output vector \\(\\y_\\rc{i}\\) 所需的权重；\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003ekey\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e：与 query 类似，与其他所有 input vector 联合计算\n  \\(j\\) 位置的 output vector \\(\\y_\\gc{j}\\) 所需的权重，这里 \\(j \\neq i\\)；\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003evalue\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e：在计算每个 output vector 时，作为输入值参与\u003cstrong\u003e\u003cmark\u003e加权求和\u003c/mark\u003e\u003c/strong\u003e。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e在我们目前的基本 self-attention 中，\u003cstrong\u003e\u003cmark\u003e每个 input vector 必须承担所有三个角色\u003c/mark\u003e\u003c/strong\u003e。\n换句话说，对原始 input vector 应用线性变换，我们就能够为每个角色衍生（derive）出一个新向量，这可以简化 self-attention。\n具体来说，引入三个 \\(k \\times k\\) 权重矩阵 \\(\\W_q\\), \\(\\W_k\\), \\(\\W_v\\)（来自 \u003cstrong\u003e\u003cmark\u003equery/key/value 首字母\u003c/mark\u003e\u003c/strong\u003e）\n对每个输入 \\(x_\\rc{i}\\) 计算三个线性变换，\u003c/p\u003e\n\n\\[\\begin{align*}\n\\q_\\rc{i} \u0026amp;= \\W_q\\x_\\rc{i} \u0026amp;\n\\k_\\rc{i} \u0026amp;= \\W_k\\x_\\rc{i} \u0026amp;\n\\v_\\rc{i} \u0026amp;= \\W_v\\x_\\rc{i}\n\\end{align*}\\]\n\n\u003cp\u003e那么 \\((i,j)\\) 位置处的权重矩阵就可以表示为：\u003c/p\u003e\n\n\\[w\u0026#39;_{\\rc{i}\\gc{j}} = {\\q_\\rc{i}}^T\\k_\\gc{j}\\]\n\n\u003cp\u003e做归一化处理，\u003c/p\u003e\n\n\\[w_{\\rc{i}\\gc{j}} = \\text{softmax}(w\u0026#39;_{\\rc{i}\\gc{j}})\\]\n\n\u003cp\u003e最后，output vector 中位置 \\(j\\) 处的值为：\u003c/p\u003e\n\n\\[\\y_\\rc{i} = \\sum_\\gc{j} w_{\\rc{i}\\gc{j}} \\v_\\gc{j}\\]\n\n\u003cp\u003e这就给 self-attention layer 引入了几个\u003cstrong\u003e\u003cmark\u003e可控制的参数\u003c/mark\u003e\u003c/strong\u003e（controllable parameters, \\(\\W_q\\), \\(\\W_k\\), \\(\\W_v\\)），\n对同一份输入应用不同的线性变换，就可以得到不同角色所需的值，如下图所示，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/transformers-from-scratch/key-query-value.png\" width=\"55%\" height=\"55%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eself-attention key/query/value transformation 的直观解释\u003c/p\u003e\n\n\u003ch3 id=\"342-对点积做缩放处理scaling-the-dot-product\"\u003e3.4.2 对点积做缩放处理（scaling the dot product）\u003c/h3\u003e\n\n\u003cp\u003esoftmax 函数对非常大的输入值敏感。这些 input 会梯度消失，学习变慢甚至完全停止。\n由于点积的平均值随着嵌入维度 \\(k\\) 的增加而增大，因此点积送到 softmax 之前进行缩放有助于缓解这个问题。\u003c/p\u003e\n\n\u003cp\u003e原来执行 softmax 之前的权重矩阵：\u003c/p\u003e\n\n\\[w\u0026#39;_{\\rc{i}\\gc{j}} = {\\q_\\rc{i}}^T\\k_\\gc{j}\\]\n\n\u003cp\u003e现在：\u003c/p\u003e\n\n\\[w\u0026#39;_{\\rc{i}\\gc{j}} = \\frac{{\\q_\\rc{i}}^T\\k_\\gc{j}} {\\sqrt{k}}\\]\n\n\u003cblockquote\u003e\n  \u003cp\u003eWhy \\(\\sqrt{k}\\)? Imagine a vector in \\({\\mathbb R^k}\\) with values all \\(c\\). Its Euclidean length is \\(\\sqrt{k}c\\). Therefore, we are dividing out the amount by which the increase in dimension increases the length of the average vectors\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch3 id=\"343-引入-multi-head-attention\"\u003e3.4.3 引入 multi-head attention\u003c/h3\u003e\n\n\u003cp\u003e最后，需要考虑到，同一个单词随着相邻单词们的不同表示的意思也可能不同。例如下面这个句子：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003emary,gave,roses,to,susan\u003c/p\u003e\n\n\u003cp\u003e我们看到 “gave” 这个词与句子的不同部分有不同的关系：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e“mary” 表示谁在 “gave”，\u003c/li\u003e\n  \u003cli\u003e“roses” 表示 “gave” 的是什么，\u003c/li\u003e\n  \u003cli\u003e“susan” 表示接受者是谁。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch4 id=\"需求输出中嵌入更多信息\"\u003e需求：输出中嵌入更多信息\u003c/h4\u003e\n\n\u003cp\u003e在我们的\u003cstrong\u003e\u003cmark\u003e基本 self-attention 中，所有这些信息是混合在一起的\u003c/mark\u003e\u003c/strong\u003e：\n输入 $\\x_\\bc{\\text{mary}}$ 和 $\\x_\\bc{\\text{susan}}$ 可以不同程度地影响输出 $\\y_\\bc{\\text{gave}}$ ，这取决于它们与 $\\x_\\bc{\\text{gave}}$ 的点积。\u003c/p\u003e\n\n\u003cp\u003e但是，如果我们想以其他方式影响输出，这种模型就不行了。\n例如，如果 “roses” 的给予方和接受方信息都出现在 $\\y_\\bc{\\text{gave}}$ ，但位于不同部分。\n也就是说，\u003cstrong\u003e\u003cmark\u003e基本的 self-attention 欠缺了很多灵活性\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eThis leaves aside how we figure out who gave the roses. We can do that based\non prior knowledge about Mary and Susan, encoded in the embeddings. We can\nalso look at the order of the words, but we’ll look at how to achieve that\nlater.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch4 id=\"解决方式引入多个-self-attentionmulti-head\"\u003e解决方式：引入多个 self-attention（multi-head）\u003c/h4\u003e\n\n\u003cp\u003e要实现这个目的，就需要让我们的模型有\u003cstrong\u003e\u003cmark\u003e更强的辨识力\u003c/mark\u003e\u003c/strong\u003e，一种做法就是\n\u003cstrong\u003e\u003cmark\u003e组合多个 self-attention\u003c/mark\u003e\u003c/strong\u003e（用 \\(\\bc{r}\\) 索引），\n每个对应不同的 query/key/value 参数矩阵 \\(\\W_q^\\bc{r}\\), \\(\\W_k^\\bc{r}\\),\\(\\W_v^\\bc{r}\\)，\n这些就称为 \u003cstrong\u003e\u003cmark\u003eattention heads\u003c/mark\u003e\u003c/strong\u003e（注意力头）。\u003c/p\u003e\n\n\u003cp\u003e对于 input \\(\\x_\\rc{i}\\)，每个 attention head 产生不同的 output vector \\(\\y_\\rc{i}^\\bc{r}\\)（一部分输出）。\n最后再将这些部分输出连接起来，\u003cstrong\u003e\u003cmark\u003e通过线性变换来降维\u003c/mark\u003e\u003c/strong\u003e回 \\(k\\)。\u003c/p\u003e\n\n\u003ch4 id=\"提升-multi-head-self-attention-效率querykeyvalue-降维\"\u003e提升 multi-head self-attention 效率：query/key/value 降维\u003c/h4\u003e\n\n\u003cp\u003e理解 multi-head self-attention 最简单的方法是把它看作\u003cstrong\u003e\u003cmark\u003e多个并行的 self-attention 机制\u003c/mark\u003e\u003c/strong\u003e，\n每个都有自己的键、值和查询转换。\u003c/p\u003e\n\n\u003cp\u003eMulti-head self-attention 的缺点是慢，对于 \\(R\\) 头，\u003cstrong\u003e\u003cmark\u003e慢\u003c/mark\u003e\u003c/strong\u003e \\(R\\) 倍。\n不过有办法优化：我们可以实现这样的 multi-head self-attention，它既能利用多个 self-attention 提升辨识力，\n又与 single-head self-attention 基本一样快。要实现这个目的，每个 head 需要\u003cstrong\u003e\u003cmark\u003e对 query/key/value 降维\u003c/mark\u003e\u003c/strong\u003e。\n如果输入向量有 $k=256$ 维，我们的模型有 $h=4$ 个 attention head，则降维操作包括：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e将输入向量乘以一个 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e256×64\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 矩阵，这会将 input vector \u003cstrong\u003e\u003cmark\u003e从 256 维降到 64 维\u003c/mark\u003e\u003c/strong\u003e；\u003c/li\u003e\n  \u003cli\u003e对于每个 head 需要执行 3 次降维：分别针对 query/key/value 的计算。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e我们甚至只用三次 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003ek×k\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 矩阵乘法就能实现 multi-head 功能，\n唯一需要的额外操作是将生成的 output vector 重新按块排序：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/transformers-from-scratch/kqv-computation.png\" width=\"35%\" height=\"35%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e\nTo compute multi-head attention efficiently, we combine the computation of the projections down to a lower dimensional representation and the computations of the keys, queries and values into three $k \\times k$ matrices.\n\u003c/p\u003e\n\n\u003ch4 id=\"完整工作流\"\u003e完整工作流\u003c/h4\u003e\n\n\u003cp\u003e下图展示了的整个 multi-head self-attention 过程：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/transformers-from-scratch/multi-head.png\" width=\"65%\" height=\"65%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e4-head self-attention 的直观解释。对输入进行降维，针对 key/value/query 分别进行矩阵运算来实现。\u003c/p\u003e\n\n\u003cp\u003e从左到右分为 5 列：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e原始 256-维 input vector；\u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003e输入降维：将 input vector 乘以 256x64 矩阵，降维到 64 维；\u003c/p\u003e\n\n    \u003cp\u003e注意：对每个 input vector 需要分别针对 query/key/value 降维，总共是 3 遍；\u003c/p\u003e\n  \u003c/li\u003e\n  \u003cli\u003e将降维后的 input 分别输入多个并行的 self-attention；\u003c/li\u003e\n  \u003cli\u003e计算得到多个降维之后的 output vector；\u003c/li\u003e\n  \u003cli\u003e对低维度 output vectors 进行拼接，重新回到与 input vectors 一样的维度。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch2 id=\"35-multi-head-vs-single-head-模型参数数量对比\"\u003e3.5 multi-head vs. single-head 模型参数数量对比\u003c/h2\u003e\n\n\u003cp\u003e参数指的是在将 input vector 变成 output vector 过程中用到的那些系数（权重矩阵）。\u003c/p\u003e\n\n\u003cp\u003e我们假设输入的是 k-维 input vectors，接下来分别看下 multi-head 和 single-head\n的参数数量。\u003c/p\u003e\n\n\u003ch3 id=\"351-single-head\"\u003e3.5.1 single-head\u003c/h3\u003e\n\n\u003cul\u003e\n  \u003cli\u003e权重矩阵 \\(w_{\\rc{i}\\gc{j}}\\)，其中 \\(i,j \\in [0,k]\\)；\u003c/li\u003e\n  \u003cli\u003e3 个平面：query/key/value；\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e因此\u003cstrong\u003e\u003cmark\u003e总参数数量\u003c/mark\u003e\u003c/strong\u003e是 $3k^2$。\u003c/p\u003e\n\n\u003ch3 id=\"352-multi-head\"\u003e3.5.2 multi-head\u003c/h3\u003e\n\n\u003cp\u003e假设有 4 个 head，即 \\(h=4\\)，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e每个 head 对应一个 self-attention，每个 self-attention 3 个平面（query/key/value），因此总共 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e3h\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 个平面；\u003c/li\u003e\n  \u003cli\u003e每个平面的权重矩阵 \\(w_{\\rc{i}\\gc{j}}\\)，其中 \\(i \\in [0,k], j \\in [0,k/h]\\)；\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e因此总的参数个数：$3hk\\frac{k}{h} = 3k^2$，\u003cstrong\u003e\u003cmark\u003e与 single-head self-attention 的参数数量相同\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/transformers-from-scratch/multi-head.png\" width=\"65%\" height=\"65%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e4-head self-attention 的直观解释。对输入进行降维，针对 key/value/query 分别进行矩阵运算来实现。\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e唯一的区别是 multi-head self-attention 最后拼接 output vector 时多了一个矩阵 $W_o$。与 single-head 相比，这增加了 $k^2$ 个参数。\n在大多数 Transformer 中，每次 self-attention 之后会紧跟着一个前馈层（feed-forward layer），因此这可能不是绝对必要的。\n但我还未见过能否把 $W_o$ 去掉的严肃讨论。\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch2 id=\"44-self-attention-主要代码实现\"\u003e4.4 self-attention 主要代码实现\u003c/h2\u003e\n\n\u003cp\u003e接下来将我们的 self-attention 实现为一个 python 模块，方便复用：\u003c/p\u003e\n\n\u003cdiv class=\"language-python highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003etorch\u003c/span\u003e\n\u003cspan class=\"kn\"\u003efrom\u003c/span\u003e \u003cspan class=\"nn\"\u003etorch\u003c/span\u003e \u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"n\"\u003enn\u003c/span\u003e\n\u003cspan class=\"kn\"\u003eimport\u003c/span\u003e \u003cspan class=\"nn\"\u003etorch.nn.functional\u003c/span\u003e \u003cspan class=\"k\"\u003eas\u003c/span\u003e \u003cspan class=\"n\"\u003eF\u003c/span\u003e\n\n\u003cspan class=\"k\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eSelfAttention\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003enn\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eModule\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n    \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003e__init__\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ek\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eheads\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mi\"\u003e4\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003emask\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"bp\"\u003eFalse\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n    \u003cspan class=\"nb\"\u003esuper\u003c/span\u003e\u003cspan class=\"p\"\u003e().\u003c/span\u003e\u003cspan class=\"n\"\u003e__init__\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\n    \u003cspan class=\"k\"\u003eassert\u003c/span\u003e \u003cspan class=\"n\"\u003ek\u003c/span\u003e \u003cspan class=\"o\"\u003e%\u003c/span\u003e \u003cspan class=\"n\"\u003eheads\u003c/span\u003e \u003cspan class=\"o\"\u003e==\u003c/span\u003e \u003cspan class=\"mi\"\u003e0\u003c/span\u003e \u003cspan class=\"c1\"\u003e# input vector size 必须是 heads 的整数倍\n\u003c/span\u003e    \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ek\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eheads\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ek\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eheads\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003e然后，初始化几个 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ek*k\u003c/code\u003e 的线性变换矩阵，\n\u003ccode class=\"language-plaintext highlighter-rouge\"\u003enn.Linear(bias=False)\u003c/code\u003e 能实现这个效果，并做了适当的初始化：\u003c/p\u003e\n\n\u003cdiv class=\"language-python highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e    \u003cspan class=\"c1\"\u003e# Compute the queries, keys and values for all heads\n\u003c/span\u003e    \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003etokeys\u003c/span\u003e    \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003enn\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eLinear\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ek\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ek\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ebias\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"bp\"\u003eFalse\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003etoqueries\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003enn\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eLinear\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ek\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ek\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ebias\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"bp\"\u003eFalse\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003etovalues\u003c/span\u003e  \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003enn\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eLinear\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ek\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ek\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ebias\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"bp\"\u003eFalse\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n\t\u003cspan class=\"c1\"\u003e# This will be applied after the multi-head self-attention operation.\n\u003c/span\u003e    \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eunifyheads\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003enn\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eLinear\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ek\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ek\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003e接下来就可以实现了 self-attention 的计算了，在模型中对应的是 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eforward()\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 函数。\u003c/p\u003e\n\n\u003cdiv class=\"language-python highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003eforward\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n    \u003cspan class=\"n\"\u003eb\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003et\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ek\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esize\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n    \u003cspan class=\"n\"\u003eh\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eheads\u003c/span\u003e\n\n    \u003cspan class=\"c1\"\u003e# 首先，为所有 heads 计算 query/key/value，得到的是完整嵌入维度的 k*k 矩阵\n\u003c/span\u003e    \u003cspan class=\"n\"\u003equeries\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003etoqueries\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"n\"\u003ekeys\u003c/span\u003e    \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003etokeys\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"n\"\u003evalues\u003c/span\u003e  \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003etovalues\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n    \u003cspan class=\"c1\"\u003e# 接下来将 queries/keys/values 切块（降维），分别送到不同的 head\n\u003c/span\u003e    \u003cspan class=\"n\"\u003es\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ek\u003c/span\u003e \u003cspan class=\"o\"\u003e//\u003c/span\u003e \u003cspan class=\"n\"\u003eh\u003c/span\u003e\n    \u003cspan class=\"n\"\u003ekeys\u003c/span\u003e    \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ekeys\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eview\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eb\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003et\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eh\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003es\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"n\"\u003equeries\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003equeries\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eview\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eb\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003et\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eh\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003es\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"n\"\u003evalues\u003c/span\u003e  \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003evalues\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eview\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eb\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003et\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eh\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003es\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003e这对 tensors 进行了简单 reshape，现在 tensors 增加了一个 head 维度。\n对于每个 input vector，可以理解为将这个 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003ek*1\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 矩阵变成了一个 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eh * k//h\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 矩阵，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/transformers-from-scratch/reshape.png\" width=\"50%\" height=\"50%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e\u003c/p\u003e\n\n\u003cp\u003e接下来计算点积。每个 head 的点积运算都是一样的，因为我们将 heads fold 到 batch dimention。\n这样我们就可以使用 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003etorch.bmm()\u003c/code\u003e（batch matrix multiplification），而\nkeys, queries and values 可以看做是 batch，只是 batch size 稍大了一点。\u003c/p\u003e\n\n\u003cp\u003e由于 head 和 batch dimension 没有相邻，因此我们在 \u003cstrong\u003e\u003cmark\u003ereshape 之前需要转置\u003c/mark\u003e\u003c/strong\u003e。\n这个操作开销很大，但似乎无法避免：\u003c/p\u003e\n\n\u003cdiv class=\"language-python highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e    \u003cspan class=\"c1\"\u003e# - fold heads into the batch dimension\n\u003c/span\u003e    \u003cspan class=\"n\"\u003ekeys\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ekeys\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003etranspose\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e).\u003c/span\u003e\u003cspan class=\"n\"\u003econtiguous\u003c/span\u003e\u003cspan class=\"p\"\u003e().\u003c/span\u003e\u003cspan class=\"n\"\u003eview\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eb\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"n\"\u003eh\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003et\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003es\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"n\"\u003equeries\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003equeries\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003etranspose\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e).\u003c/span\u003e\u003cspan class=\"n\"\u003econtiguous\u003c/span\u003e\u003cspan class=\"p\"\u003e().\u003c/span\u003e\u003cspan class=\"n\"\u003eview\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eb\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"n\"\u003eh\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003et\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003es\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"n\"\u003evalues\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003evalues\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003etranspose\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e).\u003c/span\u003e\u003cspan class=\"n\"\u003econtiguous\u003c/span\u003e\u003cspan class=\"p\"\u003e().\u003c/span\u003e\u003cspan class=\"n\"\u003eview\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eb\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"n\"\u003eh\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003et\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003es\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eYou can avoid these calls to \u003ccode class=\"language-plaintext highlighter-rouge\"\u003econtiguous()\u003c/code\u003e by using \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ereshape()\u003c/code\u003e instead of\n\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eview()\u003c/code\u003e but I prefer to make it explicit when we are copying a tensor, and\nwhen we are just viewing it. See\n\u003ca href=\"https://github.com/mlvu/worksheets/blob/master/Worksheet%205%2C%20Pytorch.ipynb\"\u003ethis notebook\u003c/a\u003e for an explanation of the difference.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e跟之前一样，点积可以用单个矩阵乘法实现，但现在是 queries 乘以 keys，\u003c/p\u003e\n\n\u003cdiv class=\"language-python highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e    \u003cspan class=\"c1\"\u003e# Get dot product of queries and keys, and scale\n\u003c/span\u003e    \u003cspan class=\"n\"\u003edot\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ebmm\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003equeries\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ekeys\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003etranspose\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e \u003cspan class=\"c1\"\u003e# -- dot has size (b*h, t, t) containing raw weights\n\u003c/span\u003e    \u003cspan class=\"n\"\u003edot\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003edot\u003c/span\u003e \u003cspan class=\"o\"\u003e/\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ek\u003c/span\u003e \u003cspan class=\"o\"\u003e**\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"o\"\u003e/\u003c/span\u003e\u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e                       \u003cspan class=\"c1\"\u003e# scale the dot product\n\u003c/span\u003e    \u003cspan class=\"n\"\u003edot\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eF\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esoftmax\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003edot\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003edim\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e                    \u003cspan class=\"c1\"\u003e# normalize, dot now contains row-wise normalized weights\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003e然后用得到的权重再和 values 做点积，得到的就是每个 attention head 的输出：\u003c/p\u003e\n\n\u003cdiv class=\"language-python highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e    \u003cspan class=\"n\"\u003eout\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ebmm\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003edot\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003evalues\u003c/span\u003e\u003cspan class=\"p\"\u003e).\u003c/span\u003e\u003cspan class=\"n\"\u003eview\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eb\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eh\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003et\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003es\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"c1\"\u003e# apply the self attention to the values\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003e为了将每个 head 的输出重新串联起来得到 k-维的最终输出，我们需要再次转置，然后将转置后的矩阵送到\n\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eunifyheads\u003c/code\u003e layer 做最好的维度变换：\u003c/p\u003e\n\n\u003cdiv class=\"language-python highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e    \u003cspan class=\"c1\"\u003e# swap h, t back, unify heads\n\u003c/span\u003e    \u003cspan class=\"n\"\u003eout\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eout\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003etranspose\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e).\u003c/span\u003e\u003cspan class=\"n\"\u003econtiguous\u003c/span\u003e\u003cspan class=\"p\"\u003e().\u003c/span\u003e\u003cspan class=\"n\"\u003eview\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eb\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003et\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003es\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"n\"\u003eh\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eunifyheads\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eout\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003e至此，一个 multi-head, scaled dot-product self attention 模型就实现好了。\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eThe implementation can be made more concise using \u003ca href=\"https://rockt.github.io/2018/04/30/einsum\"\u003eeinsum notation\u003c/a\u003e (see an example \u003ca href=\"https://github.com/pbloem/former/issues/4\"\u003ehere\u003c/a\u003e).\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch1 id=\"4-基于-multi-head-self-attention-实现-transformers\"\u003e4 基于 multi-head self-attention 实现 transformers\u003c/h1\u003e\n\n\u003ch2 id=\"41-transformer-定义\"\u003e4.1 Transformer 定义\u003c/h2\u003e\n\n\u003cp\u003etransformer 不仅仅是一个 self-attention layer，还是一种\u003cstrong\u003e\u003cmark\u003e架构\u003c/mark\u003e\u003c/strong\u003e（architecture）。\n如何精确地判断一个东西是或者不是 transformer 还不是很明确，本文采用如下的定义：\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e任何设计用来\u003cstrong\u003e\u003cmark\u003e处理一组连接的单元\u003c/mark\u003e\u003c/strong\u003e（例如序列中的 token 或图像中的像素），\n如果\u003cstrong\u003e\u003cmark\u003e单元之间的唯一交互方式是 self-attention\u003c/mark\u003e\u003c/strong\u003e，那这样的架构就称为 transformer。\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e与\u003cstrong\u003e\u003cmark\u003e其他机制（如卷积）\u003c/mark\u003e\u003c/strong\u003e一样，可以基于 self-attention 层构建成更大的网络。但在此之前，\n我们需要将 self-attention 重构为一个可以复用的 block。\u003c/p\u003e\n\n\u003ch2 id=\"42-transformer-block\"\u003e4.2 Transformer block\u003c/h2\u003e\n\n\u003cp\u003e构建基本的 transformer 有几种略微不同的方式，但大多数结构都大致如下：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/transformers-from-scratch/transformer-block.png\" width=\"65%\" height=\"65%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e各块依次执行：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eself-attention 层；\u003c/li\u003e\n  \u003cli\u003e归一化层；\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e前馈层\u003c/mark\u003e\u003c/strong\u003e（feed forward layer），每个 MLP（multi-layer perceptron）分别与每个 input 做运算；\u003c/li\u003e\n  \u003cli\u003e另一个层归一化。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e两次归一化之前都会添加残差连接（residual connections）。\u003c/p\u003e\n\n\u003cp\u003e各组件的顺序并不是只能这样，重要的是\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e将 self-attention 与局部前馈相结合（combine self-attention with a local feedforward），\u003c/li\u003e\n  \u003cli\u003e添加归一化和残差连接。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e归一化和残差连接是常规技巧，用于使深度神经网络的训练更快、更准确。 层归一化仅应用于嵌入维度（layer normalization is applied over the embedding dimension only）。\u003c/p\u003e\n\n\u003cp\u003e实现：\u003c/p\u003e\n\n\u003cdiv class=\"language-python highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"k\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eTransformerBlock\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003enn\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eModule\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n  \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003e__init__\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ek\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eheads\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n    \u003cspan class=\"nb\"\u003esuper\u003c/span\u003e\u003cspan class=\"p\"\u003e().\u003c/span\u003e\u003cspan class=\"n\"\u003e__init__\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\n    \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eattention\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eSelfAttention\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ek\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eheads\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003eheads\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n    \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003enorm1\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003enn\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eLayerNorm\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ek\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003enorm2\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003enn\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eLayerNorm\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ek\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n    \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eff\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003enn\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eSequential\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\n      \u003cspan class=\"n\"\u003enn\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eLinear\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ek\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e4\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"n\"\u003ek\u003c/span\u003e\u003cspan class=\"p\"\u003e),\u003c/span\u003e\n      \u003cspan class=\"n\"\u003enn\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eReLU\u003c/span\u003e\u003cspan class=\"p\"\u003e(),\u003c/span\u003e\n      \u003cspan class=\"n\"\u003enn\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eLinear\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e4\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"n\"\u003ek\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ek\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\n\n  \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003eforward\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n    \u003cspan class=\"n\"\u003eattended\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eattention\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"n\"\u003ex\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003enorm1\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eattended\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n    \u003cspan class=\"n\"\u003efedforward\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eff\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003enorm2\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003efedforward\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003e这里我们选择了让 feed forward 隐藏层比 input/output 大 4 倍，这个倍数的选择是随意的，\n更小的倍数可能也能工作，并且占用内存更少，但最小不能小于 input/output layer 大小。\u003c/p\u003e\n\n\u003ch2 id=\"43-文本分类text-classificationtransformer\"\u003e4.3 文本分类（text classification）transformer\u003c/h2\u003e\n\n\u003cp\u003e我们能构建的最简单 transformer 叫 \u003cstrong\u003e\u003cmark\u003esequence classifier\u003c/mark\u003e\u003c/strong\u003e（顺序分类器）。\n我们用 \u003cstrong\u003e\u003cmark\u003eIMDb\u003c/mark\u003e\u003c/strong\u003e（Internet Movie Database）sentiment classification 数据集：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e数据内容是影评，\u003c/li\u003e\n  \u003cli\u003etoken 化成了单词序列，\u003c/li\u003e\n  \u003cli\u003e分类标签是 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003epositive\u003c/code\u003e 和 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003enegative\u003c/code\u003e（对电影的正面/负面评价）\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cmark\u003e架构的核心部分非常简单，就是一长串 transformer block\u003c/mark\u003e\u003c/strong\u003e。所需做的事情：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e如何将 input sequence feed 给这个长链，\u003c/li\u003e\n  \u003cli\u003e如何对最终 output sequence 进行变换，得到单个分类结果。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 id=\"431-输出单个分类结果\"\u003e4.3.1 输出：单个分类结果\u003c/h3\u003e\n\n\u003cp\u003e从 sequence-to-sequence layers 构建 sequence classifier 的最常见方法是\n对最终输出序列做 global average pooling，并将结果映射到 softmaxed class vector。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/transformers-from-scratch/classifier.png\" width=\"65%\" height=\"65%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e\nOverview of a simple sequence classification transformer. The output sequence is \u003cspan class=\"bc\"\u003eaveraged\u003c/span\u003e to produce a single vector representing the whole sequence. This vector is projected down to a vector with one element per class and softmaxed to produce probabilities.\n\u003c/p\u003e\n\n\u003ch3 id=\"432-输入词序敏感using-the-positions\"\u003e4.3.2 输入：词序敏感（using the positions）\u003c/h3\u003e\n\n\u003cp\u003e前面已经讨论了\u003cstrong\u003e\u003cmark\u003e嵌入层的原理\u003c/mark\u003e\u003c/strong\u003e，接下来我们将用它来表示单词。\u003c/p\u003e\n\n\u003cp\u003e正如前面已经提到的，我们正在\u003cstrong\u003e\u003cmark\u003e堆叠\u003c/mark\u003e\u003c/strong\u003e（stacking）\u003cstrong\u003e\u003cmark\u003e排列等变层\u003c/mark\u003e\u003c/strong\u003e（permutation equivariant layers），\n最终的 global average pooling 是排列不变的（permutation \u003cem\u003ein\u003c/em\u003evariant），\n因此\u003cstrong\u003e\u003cmark\u003e整个网络也是排列不变的\u003c/mark\u003e\u003c/strong\u003e。用白话来说，\n\u003cstrong\u003e\u003cmark\u003e即使我们打乱句子中的单词顺序\u003c/mark\u003e\u003c/strong\u003e，无论我们学到什么权重，\u003cstrong\u003e\u003cmark\u003e都会得到完全相同的分类结果\u003c/mark\u003e\u003c/strong\u003e。\n显然，我们希望这个先进的语言模型至少对词序具有一定的敏感性，因此我们需要解决这个问题。\u003c/p\u003e\n\n\u003cp\u003e解决方案很简单：创建一个与 input 等长的向量记录当前句子中单词的位置，并将其添加到 word embedding 中。\n具体到实现上，有两种选择。\u003c/p\u003e\n\n\u003ch4 id=\"位置嵌入position-embeddings\"\u003e位置嵌入（position embeddings）\u003c/h4\u003e\n\n\u003cp\u003e像嵌入文字一样嵌入位置。就像创建嵌入向量\\(\\v_\\bc{\\text{cat}}\\) 和 \\(\\v_\\bc{\\text{susan}}\\) 一样，\n我们创建嵌入向量\\(\\v_\\bc{\\text{12}}\\) 和 \\(\\v_\\bc{\\text{25}}\\)。\u003c/p\u003e\n\n\u003cp\u003e缺点是在训练期间必须看到每个不同长度的序列，否则相关的位置嵌入得不到训练。\n优点是效果还不错，而且很容易实现。\u003c/p\u003e\n\n\u003ch4 id=\"位置编码position-encodings\"\u003e位置编码（position encodings）\u003c/h4\u003e\n\n\u003cp\u003e位置编码与位置嵌入的工作方式类似，但不学习位置向量，而只是选择一些函数\u003c/p\u003e\n\n\\[f: {\\mathbb N} \\to {\\mathbb R}^k\\]\n\n\u003cp\u003e将位置映射到实值向量，并让网络弄清楚如何解释这些编码。\u003c/p\u003e\n\n\u003cp\u003e好处是，对于精心选择的函数，网络能够处理比训练期间看到的序列更长的序列（在它们上表现应该不会太好，但至少我们可以 check）。\n缺点是编码函数的选择是一个复杂的超参数（a complicated hyperparameter），实现起来有点复杂。\u003c/p\u003e\n\n\u003ch3 id=\"433-基于-pytorch-实现\"\u003e4.3.3 基于 Pytorch 实现\u003c/h3\u003e\n\n\u003cp\u003e简单起见，本文使用位置嵌入（position embeddings）来记录 input 顺序。\u003c/p\u003e\n\n\u003cp\u003e以下就是我们的 text classification transformer 的完整实现：\u003c/p\u003e\n\n\u003cdiv class=\"language-python highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"k\"\u003eclass\u003c/span\u003e \u003cspan class=\"nc\"\u003eTransformer\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003enn\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eModule\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n    \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003e__init__\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ek\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eheads\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003edepth\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eseq_length\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003enum_tokens\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003enum_classes\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n        \u003cspan class=\"nb\"\u003esuper\u003c/span\u003e\u003cspan class=\"p\"\u003e().\u003c/span\u003e\u003cspan class=\"n\"\u003e__init__\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\n        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003enum_tokens\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003enum_tokens\u003c/span\u003e\n        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003etoken_emb\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003enn\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eEmbedding\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003enum_tokens\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ek\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epos_emb\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003enn\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eEmbedding\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eseq_length\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ek\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n        \u003cspan class=\"c1\"\u003e# The sequence of transformer blocks that does all the heavy lifting\n\u003c/span\u003e        \u003cspan class=\"n\"\u003etblocks\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e[]\u003c/span\u003e\n        \u003cspan class=\"k\"\u003efor\u003c/span\u003e \u003cspan class=\"n\"\u003ei\u003c/span\u003e \u003cspan class=\"ow\"\u003ein\u003c/span\u003e \u003cspan class=\"nb\"\u003erange\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003edepth\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n            \u003cspan class=\"n\"\u003etblocks\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eappend\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eTransformerBlock\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ek\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003ek\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eheads\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"n\"\u003eheads\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\n        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003etblocks\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003enn\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eSequential\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"n\"\u003etblocks\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n        \u003cspan class=\"c1\"\u003e# Maps the final output sequence to class logits\n\u003c/span\u003e        \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003etoprobs\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003enn\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003eLinear\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ek\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003enum_classes\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n    \u003cspan class=\"k\"\u003edef\u003c/span\u003e \u003cspan class=\"nf\"\u003eforward\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e):\u003c/span\u003e\n        \u003cspan class=\"s\"\u003e\u0026#34;\u0026#34;\u0026#34;\n        :param x: A (b, t) tensor of integer values representing words (in some predetermined vocabulary).\n        :return: A (b, c) tensor of log-probabilities over the classes (where c is the nr. of classes).\n        \u0026#34;\u0026#34;\u0026#34;\u003c/span\u003e\n        \u003cspan class=\"c1\"\u003e# generate token embeddings\n\u003c/span\u003e        \u003cspan class=\"n\"\u003etokens\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003etoken_emb\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \u003cspan class=\"n\"\u003eb\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003et\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ek\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etokens\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esize\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e\n\n        \u003cspan class=\"c1\"\u003e# generate position embeddings\n\u003c/span\u003e        \u003cspan class=\"n\"\u003epositions\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003earange\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003et\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n        \u003cspan class=\"n\"\u003epositions\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003epos_emb\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003epositions\u003c/span\u003e\u003cspan class=\"p\"\u003e)[\u003c/span\u003e\u003cspan class=\"bp\"\u003eNone\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"p\"\u003e:,\u003c/span\u003e \u003cspan class=\"p\"\u003e:].\u003c/span\u003e\u003cspan class=\"n\"\u003eexpand\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003eb\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003et\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ek\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n        \u003cspan class=\"n\"\u003ex\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etokens\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"n\"\u003epositions\u003c/span\u003e \u003cspan class=\"c1\"\u003e# 为什么文本嵌入和位置嵌入相加，没有理论，可能就是实验下来效果不错。\n\u003c/span\u003e                               \u003cspan class=\"c1\"\u003e# https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/\n\u003c/span\u003e        \u003cspan class=\"n\"\u003ex\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003etblocks\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n        \u003cspan class=\"c1\"\u003e# Average-pool over the t dimension and project to class\n\u003c/span\u003e        \u003cspan class=\"c1\"\u003e# probabilities\n\u003c/span\u003e        \u003cspan class=\"n\"\u003ex\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"bp\"\u003eself\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003etoprobs\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003emean\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003edim\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\n        \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003eF\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003elog_softmax\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003edim\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003e在深度为 6 ，最大序列长度为 512 时，这个 transformer 取得了 85% 的准确度，与 RNN（循环神经网络）模型的结果相当，但训练速度快得多。\n要看到这个 transformer 真正接近人类的性能，就需要在更多数据上训练更深的模型。后文将详细介绍怎么做。\u003c/p\u003e\n\n\u003ch2 id=\"44-文本生成text-generationtransformer\"\u003e4.4 文本生成（text generation）transformer\u003c/h2\u003e\n\n\u003cp\u003e接下来尝试一下\u003cstrong\u003e\u003cmark\u003e自回归模型\u003c/mark\u003e\u003c/strong\u003e（\u003cem\u003eautoregressive\u003c/em\u003e model）：\n训练一个字符级别（\u003cem\u003echaracter\u003c/em\u003e level）的 transformer 来预测序列中的下一个字符。\u003c/p\u003e\n\n\u003ch3 id=\"441-自回归模型和掩码\"\u003e4.4.1 自回归模型和掩码\u003c/h3\u003e\n\n\u003cp\u003e训练方式很简单（并且在 transformer 出现之前就已经\u003ca href=\"http://karpathy.github.io/2015/05/21/rnn-effectiveness/\"\u003e存在\u003c/a\u003e很久了）。\n我们给 sequence-to-sequence 模型一个序列作为输入，然后要求它预测序列中下一个位置的字符。\n换句话说，目标输出是向左移动一个字符的相同序列：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/transformers-from-scratch/generator.png\" width=\"65%\" height=\"65%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e \u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e如果是 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eRNN\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 模型，那这就是我们所需做的所有事情，\n因为它\u003cstrong\u003e\u003cmark\u003e不能往前看\u003c/mark\u003e\u003c/strong\u003e，output \\(i\\) 只依赖 inputs \\(0\\) ~ \\(i\\)。\u003c/li\u003e\n  \u003cli\u003e而对于 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003etransformer\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e，output 取决于\u003cstrong\u003e\u003cmark\u003e整个 input sequence\u003c/mark\u003e\u003c/strong\u003e，\n因此预测下一个单词就简单多了，只需从 input 中挑选。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e要将 self-attention 用作自回归模型，需要确保它\u003cstrong\u003e\u003cmark\u003e不能 look forward input 序列\u003c/mark\u003e\u003c/strong\u003e。\n在 softmax 之前对点积矩阵应用一个\u003cstrong\u003e\u003cmark\u003e掩码\u003c/mark\u003e\u003c/strong\u003e，禁用矩阵对角线之上的所有元素，\n就能帮我们实现这一目的。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/transformers-from-scratch/masked-attention.png\" width=\"65%\" height=\"65%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e对 self-attention 进行 masking 操作，确保 input sequence 中只有当前位置之前的 input elements 能参与计算。\n注意图中的乘法符号其实有一点点误导性：我们实际上是将右上角的元素设置为负无穷大 $-\\infty$ \u003c/p\u003e\n\n\u003cp\u003e由于我们希望这些元素在 softmax 之后全是 0，因此将它们设置为 $-\\infty$。相应的代码：\u003c/p\u003e\n\n\u003cdiv class=\"language-python highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"n\"\u003edot\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003ebmm\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003equeries\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003ekeys\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003etranspose\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e))\u003c/span\u003e\n\n\u003cspan class=\"n\"\u003eindices\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003etorch\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003etriu_indices\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003et\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003et\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003eoffset\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003cspan class=\"n\"\u003edot\u003c/span\u003e\u003cspan class=\"p\"\u003e[:,\u003c/span\u003e \u003cspan class=\"n\"\u003eindices\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e0\u003c/span\u003e\u003cspan class=\"p\"\u003e],\u003c/span\u003e \u003cspan class=\"n\"\u003eindices\u003c/span\u003e\u003cspan class=\"p\"\u003e[\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e]]\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"nb\"\u003efloat\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"s\"\u003e\u0026#39;-inf\u0026#39;\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\n\u003cspan class=\"n\"\u003edot\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003eF\u003c/span\u003e\u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003esoftmax\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003edot\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"n\"\u003edim\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003e这样修改 self-attention 模块之后，模型就不能再 look forward input sequence 了。\u003c/p\u003e\n\n\u003ch3 id=\"442-训练基于维基百科数据集-enwik8\"\u003e4.4.2 训练：基于维基百科数据集 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eenwik8\u003c/code\u003e\u003c/h3\u003e\n\n\u003cp\u003e我们在标准 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eenwik8\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 数据集（取自 \u003ca href=\"http://prize.hutter1.net/\"\u003eHutter prize\u003c/a\u003e）\n上进行训练，该数据集包含 108 个维基百科文本中的字符。在训练期间，通过从数据中随机抽取子序列来生成批次。\u003c/p\u003e\n\n\u003cp\u003e我们使用由 12 个 transformer block 和 256 个嵌入维度组成的 transformer，对长度为 256 的序列进行训练。\n在 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eRTX 2080Ti\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e（大约 170K 个大小为 32 的批次）上训练了大约 24 小时后，\n让模型从 256 个字符的种子开始生成：对于每个字符，输入它前面的 256 个字符，\n然后预测下一个字符。 我们从\u003ca href=\"https://towardsdatascience.com/how-to-sample-from-language-models-682bceb97277\"\u003etemperature\u003c/a\u003e\n为 0.5 的那个开始采样，然后移动到下一个字符。\u003c/p\u003e\n\n\u003cp\u003e输出如下所示：\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e\u003cspan style=\"color: blue;\"\u003e1228X Human \u0026amp; Rousseau.\nBecause many of his stories were originally published in long-forgotten magazines and\n journals, there are a number of [[anthology|anthologies]] by different collators each containing a different selection. His original books ha\u003c/span\u003eve been considered an\n anthologie in the [[Middle Ages]], and were likely to be one of the most common in the\n [[Indian Ocean]] in the [[1st century]]. As a result of his death, the Bible was\n recognised as a counter-attack by the [[Gospel of Matthew]] (1177-1133), and the\n [[Saxony|Saxons]] of the [[Isle of Matthew]] (1100-1138), the third was a topic of the\n [[Saxony|Saxon]] throne, and the [[Roman Empire|Roman]] troops of [[Antiochia]]\n (1145-1148). The [[Roman Empire|Romans]] resigned in [[1148]] and [[1148]] began to\n collapse. The [[Saxony|Saxons]] of the [[Battle of Valasander]] reported the y\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch3 id=\"443-文本生成结果分析\"\u003e4.4.3 文本生成结果分析\u003c/h3\u003e\n\n\u003cp\u003e对于上面的输出，应该注意到，\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e输出的文本中正确使用了维基百科链接标签语法，链接内的文本准确表达了链接主题。\u003c/li\u003e\n  \u003cli\u003e生成的内容也与主题大致一致：生成的文本以圣经和罗马帝国为主题，在不同的地方使用不同的相关术语。\u003c/li\u003e\n  \u003cli\u003e还有一个不那么明显的地方：“Battle of Valasander”，这场“战争”似乎是这个神经网络\u003cstrong\u003e\u003cmark\u003e自己杜撰的\u003c/mark\u003e\u003c/strong\u003e。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e这虽然与 \u003ca href=\"https://openai.com/blog/better-language-models/\"\u003eGPT-2\u003c/a\u003e\n等模型的性能相去甚远，但与 RNN 等模型相比优势已经很明显：更快的训练速度（类似的 RNN 模型需要很多天来训练）和更好的长期一致性。\u003c/p\u003e\n\n\u003cp\u003e另外，该模型在验证集上实现了 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e1.343bit/byte\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 的压缩，\n这与 GPT-2 模型（下文会展开介绍）实现的每字节 0.93 位的相差不远。\u003c/p\u003e\n\n\u003ch2 id=\"45-设计考虑transformer-与-rnn卷积-对比\"\u003e4.5 设计考虑：Transformer 与 RNN/卷积 对比\u003c/h2\u003e\n\n\u003cp\u003e在 \u003cstrong\u003e\u003cmark\u003etransformer 之前，最先进的架构是 RNN\u003c/mark\u003e\u003c/strong\u003e（通常是 LSTM 或 GRU），但它们存在一些问题。\u003c/p\u003e\n\n\u003cp\u003eRNN \u003ca href=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/\"\u003e展开（unrolled）\u003c/a\u003e后长这样：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/transformers-from-scratch/recurrent-connection.png\" width=\"65%\" height=\"65%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n\u003c/p\u003e\n\n\u003cp\u003eRNN 最大的问题是\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e级联\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e（recurrent connection）：\n虽然它使得信息能沿着 input sequence 一路传导，\n但也意味着在计算出 \\(i-1\\) 单元之前，无法计算出 \\(i\\) 单元的输出。\u003c/p\u003e\n\n\u003cp\u003e与 RNN 此对比，\u003cstrong\u003e\u003cmark\u003e一维卷积\u003c/mark\u003e\u003c/strong\u003e（1D convolution）如下：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/transformers-from-scratch/convolutional-connection.png\" width=\"65%\" height=\"65%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n\u003c/p\u003e\n\n\u003cp\u003e在这个模型中，所有输出向量都可以并行计算，因此速度非常快。但缺点是它们\n在 long range dependencies 建模方面非常弱。在一个卷积层中，只有距离比 kernel size\n小的单词之间才能彼此交互。对于更长的依赖，就需要堆叠许多卷积。\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cmark\u003eTransformer 试图兼顾二者的优点\u003c/mark\u003e\u003c/strong\u003e：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e可以像对彼此相邻的单词一样，轻松地对输入序列的整个范围内的依赖关系进行建模（事实上，如果没有位置向量，二者就没有区别）；\u003c/li\u003e\n  \u003cli\u003e同时，避免 recurrent connections，因此整个模型可以用非常高效的 feed forward 方式计算。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eTransformer 的其余设计主要基于一个考虑因素 —— \u003cstrong\u003e\u003cmark\u003e深度\u003c/mark\u003e\u003c/strong\u003e ——\n大多数选择都是训练大量 transformer block 层，例如，transformer 中\u003cstrong\u003e\u003cmark\u003e只有两个非线性的地方\u003c/mark\u003e\u003c/strong\u003e：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eself-attention 中的 softmax；\u003c/li\u003e\n  \u003cli\u003e前馈层中的 ReLU。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e模型的其余部分完全由线性变换组成，\u003cstrong\u003e\u003cmark\u003e完美地保留了梯度\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eI suppose the layer normalization is also nonlinear, but that is one\nnonlinearity that actually helps to keep the gradient stable as it propagates\nback down the network.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch1 id=\"5-历史包袱\"\u003e5 历史包袱\u003c/h1\u003e\n\n\u003cp\u003e如果在网上看一些介绍 transformer 的文章，可能会经注意它们提到的一些概念和术语本文并没有介绍。\n这是因为我认为那些东西并不是理解现代 transformer 所必需的。\n话虽如此，有两个方面还是可以介绍一下，因为它们对于理解网上的那些关于现代 transformer 的文章还是有帮助的。\u003c/p\u003e\n\n\u003ch2 id=\"51-为什么叫-self-attention\"\u003e5.1 为什么叫 self-attention？\u003c/h2\u003e\n\n\u003cp\u003e重点在 \u003cstrong\u003e\u003cmark\u003eattention\u003c/mark\u003e\u003c/strong\u003e 这个单词上。\u003c/p\u003e\n\n\u003cp\u003e在 self-attention 提出之前，sequence models 主要指的是\u003cstrong\u003e\u003cmark\u003e由 recurrent networks 或 convolutions 堆叠（stack）而成的网络\u003c/mark\u003e\u003c/strong\u003e。\n之后人们发现，如果不是将上一层的输出直接 feed 到下一层的输入，\n而是\u003cstrong\u003e\u003cmark\u003e引入一种中间机制来判断输入中的哪些元素与输出中的某个特定单词相关\u003c/mark\u003e\u003c/strong\u003e，\n就能给 sequence models 带来很大改善。具体来说，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e我们把 input 称为 \u003cstrong\u003e\u003cmark\u003evalues\u003c/mark\u003e\u003c/strong\u003e（因为它们是实实在在的值，我们将基于这些值计算输出）；\u003c/li\u003e\n  \u003cli\u003e然后，一些（trainable）机制为\u003cstrong\u003e\u003cmark\u003e每个 value 分配一个 key\u003c/mark\u003e\u003c/strong\u003e；\u003c/li\u003e\n  \u003cli\u003e最后，对每个 output，一些其他机制分配一个 query。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e这些名称源自\u003cstrong\u003e\u003cmark\u003e键值存储（key-value store）数据结构\u003c/mark\u003e\u003c/strong\u003e。\n在 key-value store 场景中，对于每个 query（查询），store 中（最多）只有一个 item 能匹配到，\n这个 item 有唯一的 key，返回这个 key 对应的 value。\u003c/p\u003e\n\n\u003cp\u003eAttention（注意力）模型是 key-value store 模型的宽松版：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003estore 中的每个 key 都能在某种程度上（而不是精确 100% 或 0%）匹配到 query；\u003c/li\u003e\n  \u003cli\u003e另外，query 返回的也不是单个 value，而是所有 value，我们\u003cstrong\u003e\u003cmark\u003e根据每个 key 与 query 匹配的程度对相应 value 取一个加权和\u003c/mark\u003e\u003c/strong\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eself-attention 的重大突破在于，attention 本身就是一种足够强大的机制，能完成所有学习。\n正如作者所说，\u003ca href=\"https://arxiv.org/abs/1706.03762\"\u003eAttention is all you need\u003c/a\u003e。\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eKey/value/query 都来自同一个 input vector（只是各自经过了略微不同的线性变换）；\u003c/li\u003e\n  \u003cli\u003e他们关注自己（attend to themselves），因此叫 self-attention；\u003c/li\u003e\n  \u003cli\u003e这种 self-attention 经过多层堆叠之后，就能提供足够的非线性和表征能力（nonlinearity and representational power）来学习非常复杂的功能。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 id=\"52-最初的-transformer-encoders-and-decoders\"\u003e5.2 最初的 transformer: encoders and decoders\u003c/h2\u003e\n\n\u003cp\u003e当时的 sequence-to-sequence model 的标准结构是带\n\u003ca href=\"https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\"\u003eteacher forcing\u003c/a\u003e\n的 encoder-decoder 架构，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/transformers-from-scratch/encoder-decoder.png\" width=\"65%\" height=\"65%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e encoder-decoder 模型\u003c/p\u003e\n\n\u003cp\u003eEncoder 获取输入序列并将整个 sequence 映射为一个 latent representations，\n这可以是一系列 latent vectors，也可以是如上图中的单个向量。\n然后将该向量传递给 decoder，后者将其解码为期望的目标序列（例如，同一句话的另一种语言表示）。\u003c/p\u003e\n\n\u003cp\u003eTeacher forcing 指的是\u003cstrong\u003e\u003cmark\u003e允许 decoder 访问 input\u003c/mark\u003e\u003c/strong\u003e 的技术 —— 但以自回归（autoregressive）的方式。\n也就是说， decoder 基于 latent vectors 和它自己已经生成的单词，逐单词生成输出句子。\n这减轻了 latent representations 的一些压力：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003edecoder 可以使用逐词采样（word-by-word sampling）来处理语法（syntax and grammar）等低级结构，\u003c/li\u003e\n  \u003cli\u003e而使用 latent vectors 来 capture 更高级别的语义结构（semantic structure）。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e理想情况下，使用相同的 latent representations 进行两次 decoding 会得到两个具有相同含义的不同句子。\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cmark\u003e在后来的 transformer 中\u003c/mark\u003e\u003c/strong\u003e，如 BERT 和 GPT-2， \u003cstrong\u003e\u003cmark\u003eencoder/decoder 被完全去掉了\u003c/mark\u003e\u003c/strong\u003e。\n简单的 transformer block 做堆叠（stack）就足以在许多基于序列的任务中实现最先进的效果。\n这种模型有时被称为 \u003cstrong\u003e\u003cmark\u003edecoder-only transformer\u003c/mark\u003e\u003c/strong\u003e（对于自回归模型）\n或 \u003cstrong\u003e\u003cmark\u003eencoder-only transformer\u003c/mark\u003e\u003c/strong\u003e（对于没有 masking 的模型）。\u003c/p\u003e\n\n\u003ch1 id=\"6-现代-transformers\"\u003e6 现代 transformers\u003c/h1\u003e\n\n\u003cp\u003e来看几个有代表性的现代 transformers。\u003c/p\u003e\n\n\u003ch2 id=\"61-google-bert340m-参数\"\u003e6.1 Google \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eBERT\u003c/code\u003e：\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e340M\u003c/code\u003e 参数\u003c/h2\u003e\n\n\u003cp\u003e\u003ca href=\"https://arxiv.org/abs/1810.04805\"\u003eBERT\u003c/a\u003e (Bidirectional Encoder Representations from Transformers)\n是首批证明 \u003cstrong\u003e\u003cmark\u003etransformer 可以在各种基于语言的任务上\u003c/mark\u003e\u003c/strong\u003e\n（question answering, sentiment classification or classifying whether two sentences naturally follow one another）\n\u003cstrong\u003e\u003cmark\u003e达到人类水平\u003c/mark\u003e\u003c/strong\u003e的模型之一。\u003c/p\u003e\n\n\u003cp\u003eBERT 由一些与本文描述的类似的简单 transformer block 堆叠而成，然后在一个大型通用领域语料库上进行\u003cstrong\u003e\u003cmark\u003e预训练\u003c/mark\u003e\u003c/strong\u003e，\n该语料库由包含 \u003cstrong\u003e\u003cmark\u003e8 亿个（800M）单词\u003c/mark\u003e\u003c/strong\u003e的英文书籍（现代作品，from unpublished authors）\n和包含 25 亿（\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e2.5B\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e）个\u003cstrong\u003e\u003cmark\u003e单词英文\u003c/mark\u003e\u003c/strong\u003e维基百科文章（去掉了 markup）组成。\u003c/p\u003e\n\n\u003cp\u003e预训练由两个任务组成：\u003c/p\u003e\n\n\u003cdl\u003e\n\u003cdt\u003eMasking\u003c/dt\u003e\u003cdd\u003eA certain number of words in the input sequence are: masked out, replaced with a random word or kept as is. The model is then asked to predict, for these words, what the original words were. Note that the model doesn\u0026#39;t need to predict the entire denoised sentence, just the modified words. Since the model doesn\u0026#39;t know which words it will be asked about, it learns a representation for every word in the sequence.\u003c/dd\u003e\n\u003cdt\u003eNext sequence classification\u003c/dt\u003e\u003cdd\u003eTwo sequences of about 256 words are sampled that either (a) follow each other directly in the corpus, or (b) are both taken from random places. The model must then predict whether a or b is the case.\u003c/dd\u003e\n\u003c/dl\u003e\n\n\u003cp\u003eBERT uses WordPiece tokenization, which is somewhere in between word-level and character level sequences. It breaks words like \u003cspan class=\"bc\"\u003ewalking\u003c/span\u003e up into the tokens \u003cspan class=\"bc\"\u003ewalk\u003c/span\u003e and \u003cspan class=\"bc\"\u003e##ing\u003c/span\u003e. This allows the model to make some inferences based on word structure: two verbs ending in -ing have similar grammatical functions, and two verbs starting with walk- have similar semantic function.\u003c/p\u003e\n\n\u003cp\u003eThe input is prepended with a special \u003cspan class=\"bc\"\u003e\u003ccls\u003e\u0026lt;/span\u0026gt; token. The output vector corresponding to this token is used as a sentence representation in sequence classification tasks like the next sentence classification (as opposed to the global average pooling over all vectors that we used in our classification model above).\u003c/cls\u003e\u003c/span\u003e\u003c/p\u003e\n\n\u003cp\u003eAfter pretraining, a single task-specific layer is placed after the body of transformer blocks, which maps the general purpose representation to a task specific output. For classification tasks, this simply maps the first output token to softmax probabilities over the classes. For more complex tasks, a final sequence-to-sequence layer is designed specifically for the task.\u003c/p\u003e\n\n\u003cp\u003eThe whole model is then re-trained to finetune the model for the specific task at hand.\u003c/p\u003e\n\n\u003cp\u003eIn an ablation experiment, 作者展示了与之前的模型相比，最大的改进来自 BERT 的双向特性（bidirectional nature）。\n之前的模型，例如 GPT，使用的是 autoregressive mask，只允许 attention 使用前面的 token。\n在 BERT 中，all attention is over the whole sequence，这是性能提升的主要来源。\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e这也是为什么 “BERT” 中的 B 表示 “bidirectional”。\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e最大的 BERT model 使用了 24 transformer blocks，embedding dimension 1024，16 attention heads，\n\u003cstrong\u003e\u003cmark\u003e总参数数量为 3.4 亿\u003c/mark\u003e\u003c/strong\u003e（340M）。\u003c/p\u003e\n\n\u003ch2 id=\"62-openai-gpt-215b-参数\"\u003e6.2 OpenAI \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eGPT-2\u003c/code\u003e：\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e1.5B\u003c/code\u003e 参数\u003c/h2\u003e\n\n\u003cp\u003eThey show state-of-the art performance on many tasks. On the wikipedia compression task that we tried above, they achieve 0.93 bits per byte.\u003c/p\u003e\n\n\u003cp\u003e\u003ca href=\"https://openai.com/blog/better-language-models/\"\u003eGPT-2\u003c/a\u003e 是第一个真正进入\u003ca href=\"https://www.bbc.com/news/technology-47249163\"\u003e主流新闻\u003c/a\u003e的\ntransformer 模型，原因是 GPT-2 可以生成看起来足够可信的文本，如果 2016 年有这种技术，\n那当年美国总统大选中出现的那种大规模假新闻活动只需要一个人就能完成了。\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e对于 GPT-2，OpenAI 也做出了一个颇受争议的决定 —— 不公布完整模型。\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eGPT-2 第一个技巧是\u003cstrong\u003e\u003cmark\u003e构建一个新的高质量数据集\u003c/mark\u003e\u003c/strong\u003e，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e虽然 BERT 使用了高质量的数据，但数据的来源（精心编写的书籍和维基百科文章）在写作风格上缺乏多样性；\u003c/li\u003e\n  \u003cli\u003e为了在不牺牲质量的前提下收集更多不同的数据，作者使用社交媒体网站 Reddit 上的链接来收集大量文本。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eGPT2 本质上是一个语言\u003cstrong\u003e\u003cmark\u003e生成\u003c/mark\u003e\u003c/strong\u003e模型（language \u003cem\u003egeneration\u003c/em\u003e model），\n因此像我们自己设计的 text generation transformer 一样，它也\u003cstrong\u003e\u003cmark\u003e使用了 masked self-attention\u003c/mark\u003e\u003c/strong\u003e。\n它使用\u003cstrong\u003e\u003cmark\u003e字节对编码\u003c/mark\u003e\u003c/strong\u003e（byte-pair encoding）来 tokenize the language，\n这与 WordPiece encoding 一样\u003cstrong\u003e\u003cmark\u003e将单词拆分为比“比单词短、比单个字母长”的 tokens\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003eGPT2 与我们的 text generation transformer 非常相似，只有很小的层级顺序差异，以及增加了训练深度。\n最大的模型使用 48 个 transformer block，序列长度为 1024，嵌入维度为 1600，总共 \u003cstrong\u003e\u003cmark\u003e15 亿个（1.5B）参数\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003eGPT2 在很多任务上都表现出了最先进的性能。在上面提到的维基百科压缩任务中，它取得了每字节 0.93 位的压缩效率。\u003c/p\u003e\n\n\u003ch2 id=\"63-transformer-xl\"\u003e6.3 \u003ca href=\"https://arxiv.org/abs/1901.02860\"\u003eTransformer-XL\u003c/a\u003e\u003c/h2\u003e\n\n\u003cp\u003eWhile the transformer represents a massive leap forward in modeling long-range dependency, the models we have seen so far are still fundamentally limited by the size of the input. Since the size of the dot-product matrix grows quadratically in the sequence length, this quickly becomes the bottleneck as we try to extend the length of the input sequence. Transformer-XL is one of the first succesful transformer models to tackle this problem.\u003c/p\u003e\n\n\u003cp\u003eDuring training, a long sequence of text (longer than the model could deal with) is broken up into shorter segments. Each segment is processed in sequence, with self-attention computed over the tokens in the curent segment \u003cem\u003eand the previous segment\u003c/em\u003e. Gradients are only computed over the current segment, but information still propagates as the segment window moves through the text. In theory at layer \\(n\\), information may be used from \\(n\\) segments ago.\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eA similar trick in RNN training is called truncated backpropagation through time. We feed the model a very long sequence, but backpropagate only over part of it. The first part of the sequence, for which no gradients are computed, still influences the values of the hidden states in the part for which they are.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eTo make this work, the authors had to let go of the standard position encoding/embedding scheme. Since the position encoding is \u003cem\u003eabsolute\u003c/em\u003e, it would change for each segment and not lead to a consistent embedding over the whole sequence. Instead they use a \u003cem\u003erelative\u003c/em\u003e encoding. For each output vector, a different sequence of position vectors is used that denotes not the absolute position, but the distance to the current output.\u003c/p\u003e\n\n\u003cp\u003eThis requires moving the position encoding into the attention mechanism (which is detailed in the paper). One benefit is that the resulting transformer will likely generalize much better to sequences of unseen length.\u003c/p\u003e\n\n\u003ch2 id=\"64-sparse-transformers\"\u003e6.4 \u003ca href=\"https://openai.com/blog/sparse-transformer/\"\u003eSparse transformers\u003c/a\u003e\u003c/h2\u003e\n\n\u003cp\u003eSparse transformers tackle the problem of quadratic memory use head-on. Instead of computing a dense matrix of attention weights (which grows quadratically), they compute the self-attention only for particular pairs of input tokens, resulting in a \u003cem\u003esparse\u003c/em\u003e attention matrix, with only \\(n\\sqrt{n}\\) explicit elements.\u003c/p\u003e\n\n\u003cp\u003eThis allows models with very large context sizes, for instance for generative modeling over images, with large dependencies between pixels. The tradeoff is that the sparsity structure is not learned, so by the choice of sparse matrix, we are disabling some interactions between input tokens that might otherwise have been useful. However, two units that are not directly related may still interact in higher layers of the transformer (similar to the way a convolutional net builds up a larger receptive field with more convolutional layers).\u003c/p\u003e\n\n\u003cp\u003eBeyond the simple benefit of training transformers with very large sequence lengths, the sparse transformer also allows a very elegant way of designing an inductive bias. We take our input as a collection of units (words, characters, pixels in an image, nodes in a graph) and we specify, through the sparsity of the attention matrix, which units we believe to be related. The rest is just a matter of building the transformer up as deep as it will go and seeing if it trains.\u003c/p\u003e\n\n\u003ch1 id=\"7-大型模型优化\"\u003e7 大型模型优化\u003c/h1\u003e\n\n\u003cp\u003e训练 transformer 的一大瓶颈是 self attention 中的点积矩阵，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e对于序列长度 \\(t\\)，这是一个包含 \\(t^2\\) 个元素的稠密矩阵。\u003c/li\u003e\n  \u003cli\u003e在标准的 32 位精度下，当 \\(t=1000\\) 时，16 矩阵作为一个 batch，这个 batch 占用大约 250Mb 的显存。\u003c/li\u003e\n  \u003cli\u003e由于我们每个 self-attention 操作至少需要四个层（在 softmax 之前和之后，加上它们的梯度），这限制了在标准 12Gb GPU 中最多只能使用 12 层。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e实际上我们能用到的层数更少，因为输入和输出也占用了大量显存（尽管点积占主导地位）。\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e网上有些报道的模型包含\u003ca href=\"https://openai.com/blog/sparse-transformer/\"\u003e超过 12000 的序列长度，有 48 层\u003c/a\u003e，\n使用密实的点积矩阵。这些模型是在集群上训练的，但是单个前向/后向 propagation 仍然只能由单个 GPU 来完成。\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e如何将如此巨大的 transformer 放入 12Gb 内存中？主要有三个技巧。\u003c/p\u003e\n\n\u003ch2 id=\"71-半精度half-precision\"\u003e7.1 半精度（half precision）\u003c/h2\u003e\n\n\u003cp\u003e在现代 GPU 和 TPU 上，tensor 计算可以在 16 位浮点上高效完成。\n但并不是将 tensor 的 dtype 设置为 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003etorch.float16\u003c/code\u003e 那么简单。对于某些部分，如 loss，仍然需要 32 位精度。\n但其中大部分可以通过\u003ca href=\"https://github.com/NVIDIA/apex\"\u003e现有库\u003c/a\u003e相对轻松地搞定。\u003c/p\u003e\n\n\u003cp\u003e半精度优化能使内存占用减半，或者说能使有效内存翻倍。\u003c/p\u003e\n\n\u003ch2 id=\"72-梯度积累gradient-accumulation\"\u003e7.2 梯度积累（gradient accumulation）\u003c/h2\u003e\n\n\u003cp\u003e对于大型模型，我们可能只能对单个实例执行前向/后向传递（forward/backward pass）。\n\u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebatch size = 1\u003c/code\u003e 不太可能产生稳定的学习。\u003c/p\u003e\n\n\u003cp\u003e幸运的是，我们可以对更大 batch size 中的每个实例执行单个前向/后向，并对我们找到的梯度简单地求和\n（这是多元链式法则\u003ca href=\"https://youtu.be/7mTcWrnexkk?t=195\"\u003emultivariate chain rule\u003c/a\u003e的结果）。\n当我们到达 batch 的末尾时，执行单步梯度下降，并将梯度归零（zero out）。\n在 Pytorch 中这非常容易，\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eoptimizer.zero_grad()\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 就行了。\u003c/p\u003e\n\n\u003ch2 id=\"73-梯度-checkpointgradient-checkpointing\"\u003e7.3 梯度 checkpoint（gradient checkpointing）\u003c/h2\u003e\n\n\u003cp\u003e如果模型太大以至于即使是单个 forward/backward 也无法放入内存，那就只能牺牲更多的计算来提高内存效率。\u003c/p\u003e\n\n\u003cp\u003e在 gradient checkpointing 中，将模型分成几个部分（sections）。对每个部分执行单独的\nforward/backward 梯度计算，而无需为其余部分保留中间值。\nPytorch \u003ca href=\"https://pytorch.org/docs/stable/checkpoint.html\"\u003e相关的函数\u003c/a\u003e直接可用。\n更多信息可参考\n\u003ca href=\"https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255\"\u003e这篇博客\u003c/a\u003e。\u003c/p\u003e\n\n\u003ch1 id=\"8-结束语\"\u003e8 结束语\u003c/h1\u003e\n\n\u003cp\u003eTransformer 很可能是未来几十年占主导地位的最简单机器学习架构。作为从业者，有充分的理由关注它们。\u003c/p\u003e\n\n\u003cp\u003e首先，\u003cstrong\u003e\u003cmark\u003e目前的性能瓶颈纯粹在硬件上\u003c/mark\u003e\u003c/strong\u003e。与卷积或 LSTM 不同，\ntransformer 目前的限制完全取决于我们能把多大的模型放到 GPU 内存中，\n以及我们可以在合理的时间内输入多少数据进去。 我毫不怀疑我们最终会达到这样的地步：\n更多层和更多数据不再有帮助，但目前似乎还没有达到这个地步。\u003c/p\u003e\n\n\u003cp\u003e其次，\u003cstrong\u003e\u003cmark\u003etransformer 极其通用\u003c/mark\u003e\u003c/strong\u003e。 到目前为止，transformer 主要在语言建模方面取得了巨大成功，\n在图像和音乐分析方面也取得了一定的成功，但 transformer 具有一定程度的通用性，其他领域的应用还有待开发。\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e基本 transformer 是一个 set-to-set 模型。 只要数据是基本单位组成的集合（a set of units），就可以应用 transformer；\u003c/li\u003e\n  \u003cli\u003e数据的其他信息（如局部结构），可以通过位置嵌入或通过 manipulate 注意力矩阵的结构（使其稀疏或屏蔽部分）来添加，\n这在\u003cstrong\u003e\u003cmark\u003e多模态学习\u003c/mark\u003e\u003c/strong\u003e（multi-modal learning）中特别有用。例如，可以轻松地将带字幕的图像\n分解为像素集合和字符集合，然后设计一些精巧的嵌入和稀疏结构来帮助模型组合和对齐二者。\n如果我们将关于某一领域的全部知识组合成一个关系型结构（relational structure），\n如多模态知识图谱（multi-modal knowledge graph，[3]），那就可以使用简单的 transformer block 在多模态单元之间传播信息，\n然后通过稀疏结构控制与哪些单元直接交互。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e到目前为止，transformer 还主要被视为一种\u003cstrong\u003e\u003cmark\u003e语言模型\u003c/mark\u003e\u003c/strong\u003e。希望随着时间推移，\n我们会看到它在其他领域得到更多采用，不仅是提高这些领域的效率，还包括简化这些领域的现有模型，\n让从业者能更直观地控制他们模型的归纳偏差。\u003c/p\u003e\n\n\u003ch1 id=\"参考资料\"\u003e参考资料\u003c/h1\u003e\n\n\u003col\u003e\n  \u003cli\u003e\u003ca href=\"http://jalammar.github.io/illustrated-transformer/\"\u003eThe illustrated transformer\u003c/a\u003e, Jay Allamar.\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"http://nlp.seas.harvard.edu/2018/04/03/attention.html\"\u003eThe annotated transformer\u003c/a\u003e, Alexander Rush.\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://content.iospress.com/articles/data-science/ds007\"\u003eThe knowledge graph as the default data model for learning on heterogeneous knowledge\u003c/a\u003e Xander Wilcke, Peter Bloem, Victor de Boer\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://datajobs.com/data-science-repo/Recommender-Systems-[Netflix].pdf\"\u003eMatrix factorization techniques for recommender systems\u003c/a\u003e Yehuda Koren et al.\u003c/li\u003e\n\u003c/ol\u003e\n\n\n  \u003c!-- POST NAVIGATION --\u003e\n  \u003cdiv class=\"postNav clearfix\"\u003e\n     \n      \u003ca class=\"prev\" href=\"/blog/gpt-as-a-finite-state-markov-chain-zh/\"\u003e\u003cspan\u003e« [译] GPT 是如何工作的：200 行 Python 代码实现一个极简 GPT（2023）\u003c/span\u003e\n      \n    \u003c/a\u003e\n      \n      \n      \u003ca class=\"next\" href=\"/blog/cnn-intuitive-explanation-zh/\"\u003e\u003cspan\u003e[译] 以图像识别为例，关于卷积神经网络（CNN）的直观解释（2016） »\u003c/span\u003e\n       \n      \u003c/a\u003e\n     \n  \u003c/div\u003e\n\u003c/div\u003e",
  "Date": "2023-06-06T00:00:00Z",
  "Author": "Arthur Chiao"
}