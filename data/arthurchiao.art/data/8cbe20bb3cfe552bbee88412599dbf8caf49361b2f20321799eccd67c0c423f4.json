{
  "Source": "arthurchiao.art",
  "Title": "[译] [论文] Bigtable: A Distributed Storage System for Structured Data (OSDI 2006)",
  "Link": "https://arthurchiao.art/blog/google-bigtable-zh/",
  "Content": "\u003cdiv class=\"post\"\u003e\n  \n  \u003ch1 class=\"postTitle\"\u003e[译] [论文] Bigtable: A Distributed Storage System for Structured Data (OSDI 2006)\u003c/h1\u003e\n  \u003cp class=\"meta\"\u003ePublished at 2019-07-13 | Last Update 2019-07-13\u003c/p\u003e\n  \n  \u003ch3 id=\"译者序\"\u003e译者序\u003c/h3\u003e\n\n\u003cp\u003e本文翻译自 2006 年 Google 的分布式存储经典论文：\u003cstrong\u003eBigtable: A Distributed\nStorage System for Structured Data\u003c/strong\u003e\n(\u003ca href=\"https://research.google.com/archive/bigtable-osdi06.pdf\"\u003ePDF\u003c/a\u003e)。\u003c/p\u003e\n\n\u003cp\u003e标题直译为：\u003cstrong\u003e《Bigtable： 适用于结构化数据的分布式存储系统》\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e本文对排版做了一些调整，以更适合网页阅读。\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e翻译仅供个人学习交流。由于译者水平有限，本文不免存在遗漏或错误之处。如有疑问，\n请查阅原文。\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003e以下是译文。\u003c/p\u003e\n\n\u003chr/\u003e\n\n\u003ch2 id=\"摘要\"\u003e摘要\u003c/h2\u003e\n\n\u003cp\u003eBigtable 是一个用于管理\u003cstrong\u003e结构化数据\u003c/strong\u003e（structured data）的\u003cstrong\u003e分布式存储系统\u003c/strong\u003e，设\n计可以扩展到非常大的规模：由几千个通用服务器（commodity servers）组成的 PB 级存\n储。\u003c/p\u003e\n\n\u003cp\u003e很多 Google 产品，包括 web index、Google Earth 和 Google Finance，都将数据存储在\nBigtable 中。不过，这些应用对 Bigtable 的要求有很大差异，不管是从数据大小（从\nURL 到网页到卫星图像）还是从延迟（从后台批量处理到实时数据服务）考虑。但是，\nBigtable 仍然给这些产品提供了一个灵活、高性能的解决方案，它提供的简单数据模型可以\n使\u003cstrong\u003e客户端动态控制\u003c/strong\u003e数据的\u003cstrong\u003e布局和格式\u003c/strong\u003e（layout and format）。\u003c/p\u003e\n\n\u003cp\u003e本文介绍 Bigtable 的设计与实现。\u003c/p\u003e\n\n\u003ch2 id=\"1-引言\"\u003e1 引言\u003c/h2\u003e\n\n\u003cp\u003e在过去的两年半中，我们设计、实现并部署了一个称为 Bigtable 的分布式存储\n系统，用于管理 Google 的结构化数据。\n设计中 Bigtable 能可靠地扩展到 \u003cstrong\u003ePB 级数据，上千个节点\u003c/strong\u003e。现在已经实现了广\n泛的应用场景支持、可扩展性、高性能，以及高可用性等设计目标。\u003c/p\u003e\n\n\u003cp\u003e目前 Bigtable 已经被超过 60 个 Google 产品和项目所使用，其中包括 Google\nAnalytics、Google Finance、Orkut、Personalized Search、Writely、以及 Google\nEarth。这些产品的使用场景差异很大，从面向吞吐的批处理任务，到延迟敏感的终端用户\n数据服务。不同产品使用的 Bigtable 集群配置差异也很大，有的集群只有几台节点，有的\n有几千台，存储几百 TB 的数据。\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e从某些方面看，Bigtable 像是一个数据库\u003c/strong\u003e：它的很多实现策略（implementation\nstrategies）确实和数据库类似。\u003cstrong\u003e并行数据库\u003c/strong\u003e [14]（Parallel databases）和\u003cstrong\u003e主存\n数据库\u003c/strong\u003e [13]（main-memory databases）已经在可扩展性和高性能方面取得了很大成功，\n（Bigtable 也关注这两方面，但除此之外，）Bigtable 提供的接口与它们不同。\u003c/p\u003e\n\n\u003cp\u003eBigtable 不支持完整的关系型数据模型（full relational data model）；它提供给客户\n端的是一个\u003cstrong\u003e简单数据模型\u003c/strong\u003e（simple data model），支持\u003cstrong\u003e动态控制数据的布局和\n格式\u003c/strong\u003e（layout and format），并允许客户端推测\u003cstrong\u003e数据在底层存储中的 locality（本地性）\n特性\u003c/strong\u003e。数据使用\u003cstrong\u003e行名和列名\u003c/strong\u003e（row and column names）进行索引，这些名字可以是任\n意字符串（strings）。\u003c/p\u003e\n\n\u003cp\u003eBigtable \u003cstrong\u003e不理解数据的内容\u003c/strong\u003e（将数据视为 uninterpreted strings），虽然很多字符\n串都是客户端将各种结构化和半结构化数据（structured and semi-structured data）序\n列化而来的。客户端可以通过精心\u003cstrong\u003e选择 schema 来控制数据的 locality\u003c/strong\u003e。schema 参数\n还可以让客户端动态控制数据是从内存还是磁盘读取（serve）。\u003c/p\u003e\n\n\u003ch2 id=\"2-数据模型\"\u003e2 数据模型\u003c/h2\u003e\n\n\u003cp\u003e一个 Bigtable 就是一个\u003cstrong\u003e稀疏、分布式、持久\u003c/strong\u003e的\u003cstrong\u003e多维有序\u003c/strong\u003e映射表（map），数据通\n过\u003cstrong\u003e行键、列键和一个时间戳\u003c/strong\u003e进行索引，表中的每个数据项都是\u003cstrong\u003e不作理解的字节数组\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eA Bigtable is a sparse, distributed, persistent multidimensional sorted map.\nThe map is indexed by a row key, column key, and a timestamp; each value in\nthe map is an uninterpreted array of bytes.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e\u003cstrong\u003e映射\u003c/strong\u003e：\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e(row:string, column:string, time:int64) -\u0026gt; string\u003c/code\u003e\u003c/p\u003e\n\n\u003cp\u003e我们首先评估了类似 Bigtable 这样的系统有哪些潜在的使用场景，然后才确定了数据模型。\n举个具体例子，这个例子也影响了 Bigtable 的一些设计：我们想保存大量的网页\n和网页相关的元数据，这些数据会被不同的项目使用，这里将这张表称为 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eWebtable\u003c/code\u003e。\u003c/p\u003e\n\n\u003cp\u003e在 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eWebtable\u003c/code\u003e 中，我们用网页的 URL 作为行键，网页某些信息作为列键，将网页内容存\n储在 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003econtents:\u003c/code\u003e 列，并记录抓取网页时对应的时间戳，最终存储布局如图 1 所示。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/google-bigtable/1.png\" width=\"90%\" height=\"90%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e图 1 存储网页的 bigtable 的一个切片（slice）\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e行索引：\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eURL\u003c/code\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003econtents:\u003c/code\u003e 列：存储页面内容（page content）\u003c/li\u003e\n  \u003cli\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eanchor:\u003c/code\u003e 开头的列：存储引用了这个页面的 anchor（HTML 锚点）的文本（text of\nthe anchors that reference this page）\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e图中可以看出，CNN 主页被 Sports Illustrated（\u003ccode class=\"language-plaintext highlighter-rouge\"\u003ecnnsi.com\u003c/code\u003e）和 MY-look 主页（\n\u003ccode class=\"language-plaintext highlighter-rouge\"\u003emy.look.ca\u003c/code\u003e）引用了，因此会有 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eanchor:cnnsi.com\u003c/code\u003e 和 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eanchor:my.look.ca\u003c/code\u003e 两列，其\n中每列一个版本；\u003ccode class=\"language-plaintext highlighter-rouge\"\u003econtents:\u003c/code\u003e 列有三个版本，时间戳分别为 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003et3\u003c/code\u003e、\u003ccode class=\"language-plaintext highlighter-rouge\"\u003et5\u003c/code\u003e 和 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003et6\u003c/code\u003e。\u003c/p\u003e\n\n\u003ch3 id=\"21-行row\"\u003e2.1 行（Row）\u003c/h3\u003e\n\n\u003cp\u003e行键（row key）可以是\u003cstrong\u003e任意字符串\u003c/strong\u003e（目前最大支持 64KB，大部分用户使用的\nkey 都在 10-100 字节之间）。\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e单行数据的读/写操作是原子的\u003c/strong\u003e（不管该行有多少列参与读/写），这样的设计使得多\n个客户端并发更新同一行时，更容易推断系统的行为。\u003c/p\u003e\n\n\u003cp\u003eBigtable 中的数据是根据\u003cstrong\u003e行键的词典顺序（lexicographic order）\u003c/strong\u003e组织的，并\u003cstrong\u003e动态\n地对行范围（row range）进行切分\u003c/strong\u003e（partition）。\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e每个行范围称为一个 tablet\u003c/strong\u003e，这是\u003cstrong\u003e请求分散和负载均衡的单位\u003c/strong\u003e（unit of\ndistribution and load balancing）。因此，\u003cstrong\u003e读取一个较小的行范围（short row\nranges）是很高效的\u003c/strong\u003e，通常情况下只需要和很少的几台机器通信。客户端可以利用这个特\n性，通过合理的选择行键来在访问数据时获得更好 locality。\u003c/p\u003e\n\n\u003cp\u003e举个例子，在 Webtable 中，将 URL 的 hostname 字段进行翻转，来自相同域（domain）\n的页面在存储时就会变成连续的行。例如 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003emaps.google.com/index.html\u003c/code\u003e 页面在存储时行\n键就是 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ecom.google.maps/index.html\u003c/code\u003e。\u003cstrong\u003e来自相同域的页面存储到连续的行\u003c/strong\u003e，会使那\n些针对主机和域的分析（host and domain analyses）非常高效。\u003c/p\u003e\n\n\u003ch3 id=\"22-column-families列族\"\u003e2.2 Column Families（列族）\u003c/h3\u003e\n\n\u003cp\u003e多个\u003cstrong\u003e列键\u003c/strong\u003e（column keys）可以组织成 \u003cstrong\u003ecolumn families\u003c/strong\u003e（列族）。\n\u003cstrong\u003ecolumn family 是访问控制（access control）的基本单位\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e一般来说，存储在\u003cstrong\u003e同一 column family 内\n的数据，其类型都是相同的\u003c/strong\u003e（我们会将同一 column family 内的数据压缩到一起）。\u003c/p\u003e\n\n\u003cp\u003e必须\u003cstrong\u003e先创建一个 column family\u003c/strong\u003e，才能向这个 column family 内的列写入数据；创建\n完成后，就可以在这个 family 内使用任何的列键。\n我们有意使得\u003cstrong\u003e每个 table 内的 column family 数量尽量少\u003c/strong\u003e（最多几百个），并且在随后\n的过程中 family 很少有变化。\u003c/p\u003e\n\n\u003cp\u003e但另一方面，每个 table 的\u003cstrong\u003e列数量并没有限制\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e列键的格式\u003c/strong\u003e：\u003ccode class=\"language-plaintext highlighter-rouge\"\u003efamily:qualifier\u003c/code\u003e，其中 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003efamily\u003c/code\u003e 必须为可打印的（printable）字\n符串，但 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003equalifier\u003c/code\u003e（修饰符）可以为任意字符串。例如，Webtable 中有一个 colum\nfamily 是\u003cstrong\u003e语言（language）\u003c/strong\u003e，用来标记每个网页分别是用什么语言写的。在这个\ncolumn family 中我们只用了一个列键，其中存储的是每种语言的 ID。\nWebtable 中的另一个 column family 是 anchor，在这个 family 中每一个列键都表示一\n个独立的 anchor，如图 1 所示，其中的修饰符（qualifier）是引用这个网页的 anchor\n名字，对应的数据项内容是链接的文本（link text）。\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e访问控制（access control）和磁盘及内存记账（accounting）都是在 column\nfamily 层做的\u003c/strong\u003e。还是以 Webtable 为例，这种级别的控制可以使我们管理几种不同类型的\n应用：有的只添加新的基础数据进来，有的读取基础数据后创建衍生的 column family，有\n的只允许查看当前的数据（甚至可以根据保密程度只允许查看一部分 column family）。\u003c/p\u003e\n\n\u003ch3 id=\"23-时间戳\"\u003e2.3 时间戳\u003c/h3\u003e\n\n\u003cp\u003eBigtable 中的\u003cstrong\u003e每个数据都可以存储多个版本\u003c/strong\u003e，不同版本用时间戳索引。\u003c/p\u003e\n\n\u003cp\u003e时间戳是 64 位整数，可以由 Bigtable 指定，这种情况下就是毫秒（ms）级的真实时间戳\n；也可以由客户端应用指定。如果是应用指定，那为了避免冲突，应用必须保证时间戳的唯\n一性。\u003c/p\u003e\n\n\u003cp\u003e同一数据的不同版本以时间戳\u003cstrong\u003e降序\u003c/strong\u003e（decreasing timestamp order）的方式存储，这样\n首先读到的都是最新的版本。\u003c/p\u003e\n\n\u003cp\u003e为避免版本化数据的管理过于繁琐，我们提供了两个配置参数可以让 Bigtable 自动进行垃\n圾回收（GC）。客户端可以指定：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e保留最后的 N 个版本\u003c/li\u003e\n  \u003cli\u003e保留最近的某段时间内的版本（例如，只保留过去 7 天写入的版本）\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e在 Webtable 中，每个页面的时间戳是该页面被爬取时的时间，我们设置只保留最后的 3\n个版本。\u003c/p\u003e\n\n\u003ch2 id=\"3-api\"\u003e3 API\u003c/h2\u003e\n\n\u003cp\u003eBigtable API 提供了创建、删除 table 和 column family 的功能。另外，它还提供了更\n改集群、table 和 column family 元数据的能力，例如访问控制权限。\u003c/p\u003e\n\n\u003cp\u003e客户端应用可以读/写 Bigtable 中的值，从指定行中查找值，或者对 table 内的一个数据\n子集进行遍历。\u003c/p\u003e\n\n\u003cp\u003e图 2 是向 Bigtable 写数据的一段 C++ 代码，使用了 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eRowMutation\u003c/code\u003e 抽象来执行一系列\n更新操作。为保持代码简洁，例子中去掉了一些无关的技术细节。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/google-bigtable/2.png\" width=\"50%\" height=\"50%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e图 2 Writing to Bigtable \u003c/p\u003e\n\n\u003cp\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eApply()\u003c/code\u003e 向 Webtable 执行一次原子操作，其中包括：添加一个 anchor 到\n\u003ccode class=\"language-plaintext highlighter-rouge\"\u003ewww.cnn.com\u003c/code\u003e，删除另一个 anchor。\u003c/p\u003e\n\n\u003cp\u003e图 3 是另一个例子，使用一个 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eScanner\u003c/code\u003e 抽象对一行内的所有 anchor 进行遍历。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/google-bigtable/3.png\" width=\"50%\" height=\"50%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e图 3 Reading from Bigtable\u003c/p\u003e\n\n\u003cp\u003e客户端可以在多个 column family 上进行遍历，这里有几种限制 scan 产生的行、列和时\n间戳的机制。\n例如，可以指定以上 scan 只产生列键能匹配正则表达式 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eanchor:*.cnn.com\u003c/code\u003e 的 anchors，\n或者时间戳在最近 10 天内的 anchor。\u003c/p\u003e\n\n\u003cp\u003eBigtable 还提供其他的一些特性，使得用户可以对数据进行更复杂的控制。\u003c/p\u003e\n\n\u003cp\u003e首先，提供了\u003cstrong\u003e单行事务\u003c/strong\u003e（single-row transaction），可以对单行内的数据执行原子的\n“读-修改-写”（read-modify-write）序列操作。但 Bigtable 目前并不支持通用的跨行事\n务（general transactions across row keys），虽然它提供了在客户端侧进行\u003cstrong\u003e跨行批量\n写\u003c/strong\u003e（batching writes across row keys）的接口。\u003c/p\u003e\n\n\u003cp\u003e第二，允许将 cell（table 中的一个格子）当整型计数器用。\u003c/p\u003e\n\n\u003cp\u003e最后，支持在服务端执行由客户端提供的脚本。脚本使用的是 Google 为数据处理开发的\n称为 Aawzall [28] 的语言。目前这套基于 Sawzall 的 API 不允许客户端脚本将数据回写\n到 Bigtable，但它们可以进行各种形式的数据变换、计算、求和等等。\u003c/p\u003e\n\n\u003cp\u003eBigtable 可以和 MapReduce [12] 一起使用，后者是 Google 开发的一个大规模并行计算框架。\n我们写了一些封装函数，将 Bigtable 用作 MapReduce job 的输入源和输出目标。\u003c/p\u003e\n\n\u003ch2 id=\"4-外部系统依赖building-blocks\"\u003e4 外部系统依赖（Building Blocks）\u003c/h2\u003e\n\n\u003cp\u003eBigtable 构建在其他几个 Google 的基础设施之上。\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eGFS\u003c/li\u003e\n  \u003cli\u003eSSTable\u003c/li\u003e\n  \u003cli\u003eChubby\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 id=\"gfs\"\u003eGFS\u003c/h3\u003e\n\n\u003cp\u003eBigtable 使用分布式文件系统 GFS（Google File System）[17] 存储日志和数据文件。\u003c/p\u003e\n\n\u003cp\u003eBigtable 集群通常\u003cstrong\u003e和其他一些分布式应用共享一个服务器资源池\u003c/strong\u003e（pool of\nmachines），而且 \u003cstrong\u003eBigtable 进程经常和其他应用混跑在同一台机器上\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003eBigtable 依赖一个集群管理系统来调度任务、管理共享的机器上的资源、处理机器故障，\n以及监控机器状态。\u003c/p\u003e\n\n\u003ch3 id=\"sstable\"\u003eSSTable\u003c/h3\u003e\n\n\u003cp\u003eBigtable \u003cstrong\u003e内部使用 Google 的 SSTable 格式存储数据\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003eSSTable 是一个\u003cstrong\u003e持久化的、有序的、不可变的\u003c/strong\u003e映射表（map），其中的\u003cstrong\u003e键和值都可以\n是任意字节字符串\u003c/strong\u003e。它提供了按 key 查询和对指定的 key range 进行遍历的操作。\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eAn SSTable provides a persistent, ordered immutable map from keys to values,\nwhere both keys and values are arbitrary byte strings.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e在内部，每个 SSTable 都包含一系列的 \u003cstrong\u003eblocks\u003c/strong\u003e（通常每个 block 64KB，但这个参数\n可配置）。\u003c/p\u003e\n\n\u003cp\u003eblock 用 \u003cstrong\u003eblock index\u003c/strong\u003e（存储在 SSTable 的末尾）来定位，block index\n会在打开 SSTable 的时候加载到\u003cstrong\u003e内存\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e一次查询操作只需要一次磁盘寻址\u003c/strong\u003e（disk seek）：首先在\u003cstrong\u003e内存\u003c/strong\u003e中通过二分查找（\nbinary search）找到 block index，然后定位到 block 在\u003cstrong\u003e磁盘\u003c/strong\u003e中的位置，从\u003cstrong\u003e磁盘\u003c/strong\u003e\n读取相应的数据。另外，也可以\u003cstrong\u003e将整个 SSTable 映射到内存\u003c/strong\u003e，这样查询就完全不需要\n磁盘操作了。\u003c/p\u003e\n\n\u003ch3 id=\"chuby\"\u003eChuby\u003c/h3\u003e\n\n\u003cp\u003eBigtable 依赖 Chubby —— 一个高可用、持久的分布式锁服务（a highly-available and\npersistent distributed lock service） [8]。\u003c/p\u003e\n\n\u003cp\u003e一个 Chubby 服务由 \u003cstrong\u003e5 个活跃副本\u003c/strong\u003e（active replicas）组成，其中一个会被选举为\nmaster，并负责处理请求。只有大多数副本都活着，并且互相之间可以通信时，这个服务才\n算活着（live）。\u003c/p\u003e\n\n\u003cp\u003e在遇到故障时，Chubby 使用 Paxos 算法 [9, 23] 保证副本之间的一致性。\u003c/p\u003e\n\n\u003cp\u003eChubby 提供了一个包含\u003cstrong\u003e目录和小文件\u003c/strong\u003e的命名空间（namespace），\u003cstrong\u003e每个目录或文件都\n可以作为一个锁\u003c/strong\u003e，读或写一个文件是\u003cstrong\u003e原子的\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eChubby 客户端库维护了一份这些文件的一致性缓存\u003c/strong\u003e（consistent caching）。每个\nChubby 客户端都会和 Chubby 服务维持一个 session。当一个客户端的租约（lease）到期\n并且无法续约（renew）时，这个 session 就失效了。\u003cstrong\u003esession 失效后会失去它之前的锁\n和打开的文件句柄\u003c/strong\u003e（handle）。Chubby 客户端还可以在 Chubby 文件和目录上注册回调\n函数，当文件/目录有变化或者 session 过期时，就会收到通知。\u003c/p\u003e\n\n\u003cp\u003eBigtable 使用 Chubby 完成很多不同类型的工作：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e保证任何时间最多只有一个 active master\u003c/li\u003e\n  \u003cli\u003e存储 Bigtable 数据的 bootstrap location（见 5.1）\u003c/li\u003e\n  \u003cli\u003etablet 服务发现和服务终止清理工作（见 5.2）\u003c/li\u003e\n  \u003cli\u003e存储 Bigtable schema 信息（每个 table 的 column family 信息）\u003c/li\u003e\n  \u003cli\u003e存储访问控制列表\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e\u003cstrong\u003e如果 Chubby 服务不可用超过一段时间，Bigtable 也将变得不可用\u003c/strong\u003e。我们近期对 14 个\nBigtable 集群（总共依赖 11 个 Chubby 集群）的测量显示，由于 Chubby 不可用（网络\n或 Chubby 本身问题引起的） 导致的 Bigtable 不可用时间（数据在 Bigtable 中但无法访\n问）百分比平均为 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e0.0047%\u003c/code\u003e，受影响最大的那个集群为 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e0.0326%\u003c/code\u003e。\u003c/p\u003e\n\n\u003ch2 id=\"5-实现\"\u003e5 实现\u003c/h2\u003e\n\n\u003cp\u003eBigtable 主要由三个组件构成：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e一个客户端库，会链接到每个客户端\u003c/li\u003e\n  \u003cli\u003e一个 master server\u003c/li\u003e\n  \u003cli\u003e多个 tablet server\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e可以根据系统负载动态地向集群添加或删除 tablet server。\u003c/p\u003e\n\n\u003cp\u003emaster 负责：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e将 tablet 分配给 tablet server\u003c/li\u003e\n  \u003cli\u003e检测 tablet server 的过期（expiration）及新加（addition）事件\u003c/li\u003e\n  \u003cli\u003e平衡 tablet server 负载\u003c/li\u003e\n  \u003cli\u003e垃圾回收（GC）\u003c/li\u003e\n  \u003cli\u003e处理 schema 变动，例如 table 和 column family 的创建\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e\u003cstrong\u003e每个 tablet server 管理一组 tablets\u003c/strong\u003e（一般 10～1000 个）。tablet server 管理\n这些 tablet 的读写请求，并且当 tablet 太大时，\u003cstrong\u003e还负责对它们进行切分\u003c/strong\u003e（split）\n。\u003c/p\u003e\n\n\u003cp\u003e和很多单 master（single master）分布式存储系统一样 [17, 21]，\u003cstrong\u003e客户端数据不经过\nmaster 节点\u003c/strong\u003e：\u003cstrong\u003e读写请求直接到 tablet server\u003c/strong\u003e。\n由于\u003cstrong\u003e客户端不依赖 master 就能确定 tablet 位置信息\u003c/strong\u003e，因此大部分客户端从来不和\nmaster 通信。因此，实际中 master 节点的负载很低。\u003c/p\u003e\n\n\u003cp\u003e每个 Bigtable 集群会有很多张 table，每张 table 会有很多 tablets，每个 tablets 包\n含一个 row range（行键范围）内的全部数据。\n初始时每个 table 只包含一个 tablet。当 table 逐渐变大时，它会自动分裂成多个\ntablets，\u003cstrong\u003e默认情况下每个 tablet 大约 100-200MB\u003c/strong\u003e。\u003c/p\u003e\n\n\u003ch3 id=\"51-tablet-位置\"\u003e5.1 Tablet 位置\u003c/h3\u003e\n\n\u003ch4 id=\"服务端\"\u003e服务端\u003c/h4\u003e\n\n\u003cp\u003e我们使用一个和 B+ 树 [10] 类似的\u003cstrong\u003e三级结构\u003c/strong\u003e（three level hierarchy）来存储\ntablet 位置信息，如图 4 所示。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/google-bigtable/4.png\" width=\"60%\" height=\"60%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e图 4 Tablet location hierarchy\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e第一级：\u003cstrong\u003eChubby 中的一个文件\u003c/strong\u003e\u003c/li\u003e\n  \u003cli\u003e第二级：\u003cstrong\u003eMETADATA tables\u003c/strong\u003e（第一个 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eMETADATA\u003c/code\u003e table 比较特殊，所以在图中单独画\n出，但它其实和其他 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eMETADATA\u003c/code\u003e table 都属于第二级）\u003c/li\u003e\n  \u003cli\u003e第三级：\u003cstrong\u003euser tablets\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eMETADATA\u003c/code\u003e 是一个特殊的 tablet，其中的第一个 tablet 称为 \u003cstrong\u003eroot tablet\u003c/strong\u003e。root\ntablet 和 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eMETADATA\u003c/code\u003e 内其他 tablet 不同之处在于：它\u003cstrong\u003e永远不会分裂\u003c/strong\u003e（split），这\n样就可以\u003cstrong\u003e保证 tablet location 层级不会超过三层\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e三级间的关系：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eChubby 中的文件保存了 root tablet 的位置\u003c/li\u003e\n  \u003cli\u003eroot tablet 保存了 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eMETADATA\u003c/code\u003e table 内所有其他 table 的位置\u003c/li\u003e\n  \u003cli\u003e每个 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eMETADATA\u003c/code\u003e tablet（root tablet 除外）保存了一组 user tablet 的位置\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eMETADATA\u003c/code\u003e table 存储 user tablet 位置信息的方式\u003c/strong\u003e（假设 user table 名为 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eUserTableX\u003c/code\u003e）：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003evalue：\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eUserTableX\u003c/code\u003e 的位置\u003c/li\u003e\n  \u003cli\u003ekey（row key）：\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eUserTableX\u003c/code\u003e 的 table ID 和它的最后一行的某种\u003cstrong\u003e编码\u003c/strong\u003e（encoding）\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eThe METADATA table stores the location of a tablet under a row key that is an\nencoding of the tablet’s table identifier and its end row.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eMETADATA\u003c/code\u003e 的\u003cstrong\u003e每行数据在内存中大约占 1KB\u003c/strong\u003e。如果将 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eMETADATA\u003c/code\u003e tablet 限制在\n\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e128MB\u003c/code\u003e 这样一个中等大小，这种三级位置方案就可以存储高达 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e2^34\u003c/code\u003e 个 tablets（\n\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e128MB\u003c/code\u003e = \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e2^17 * 1KB\u003c/code\u003e，即 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eMETADATA\u003c/code\u003e table 可以指向 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e2^17\u003c/code\u003e 个 user table，每个\nuser table 同样是 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e128MB\u003c/code\u003e 的话，就有 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e2^17 * 2^17 = 2^34\u003c/code\u003e 个 tablets，译者注）。\n如果每个 tablet 128 MB 大小，那总数据量就高达 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e2^61\u003c/code\u003e 字节（\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e128MB = 2^27 Byte\u003c/code\u003e，\n\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e2^34 * 2^27 = 2^61\u003c/code\u003e，即 \u003cstrong\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e2000PB\u003c/code\u003e\u003c/strong\u003e）。\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eWith a modest limit of 128 MB METADATA tablets, our three-level location\nscheme is sufficient to address 234 tablets (or 2^61 bytes in 128 MB tablets).\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch4 id=\"客户端\"\u003e客户端\u003c/h4\u003e\n\n\u003cp\u003e\u003cstrong\u003e客户端库会缓存 tablet 位置信息\u003c/strong\u003e。\n如果客户端不知道 tablet 的位置，或者发现缓存的位置信息不对，它就会去访问 table\nlocation 层级结构，逐层向上（recursively moves up）。\u003c/p\u003e\n\n\u003cp\u003e如果客户端的缓存是空的，位置算法需要三个网络往返（round trip），其中包括一次\nChubby 读取。如果客户端缓存过期了，位置算法需要最多六次网络往返，因为只会在\ncache miss 的时候才会检测缓存是否过期（假设 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eMETADATA\u003c/code\u003e tablets 移动不是非常频繁\n）。\u003c/p\u003e\n\n\u003cp\u003e虽然 tablet 位置放在内存，不需要 GFS 操作，但是，我们可以通过\u003cstrong\u003e客户端预取\u003c/strong\u003e（\nprefetch）的方式继续减少这里的开销：\u003cstrong\u003e每次从 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eMETADATA\u003c/code\u003e table 读取的时候，都读取\n多个 tablet 的元数据\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e另外，我们还在 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eMETADATA\u003c/code\u003e table 中存储了其他一些次要信息，包括每个 tablet 上的事件\n的日志（例如使用这个 tablet 的服务是何时启动的），这些信息对 debug 和性能分析很\n有用。\u003c/p\u003e\n\n\u003ch3 id=\"52-tablet-分配\"\u003e5.2 Tablet 分配\u003c/h3\u003e\n\n\u003cp\u003e\u003cstrong\u003e每个 tablet 每次只会分配给一个 tablet server\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003emaster 会跟踪活着的 tablet server 以及当前 tablet 和 tablet server 的分配关系，\n其中包括哪些 tablet 是还没有被分配出去的。当一个 tablet 还没有分配出去，并且找到\n了一个有空闲资源的 tablet server，master 就会向这个 server \u003cstrong\u003e发送一个 tablet 加载\n请求\u003c/strong\u003e（load request），将这个 tablet 分配给它。\u003c/p\u003e\n\n\u003cp\u003eBigtable \u003cstrong\u003e使用 Chubby 跟踪 tablet servers\u003c/strong\u003e。当一个 tablet server 启动后，它会\n\u003cstrong\u003e在特定的 Chubby 目录下创建和获取一个名字唯一的独占锁\u003c/strong\u003e（exclusive lock）。\nmaster 通过\u003cstrong\u003e监听这个目录\u003c/strong\u003e（the \u003cem\u003eservers directory\u003c/em\u003e）来\u003cstrong\u003e发现 tablet servers\u003c/strong\u003e\n。\u003c/p\u003e\n\n\u003cp\u003e如果一个 tablet server \u003cstrong\u003e失去了这个独占锁\u003c/strong\u003e，例如由于网络分裂导致 Chubby session\n断了，那这个 server 会\u003cstrong\u003e停止服务这个 tablet\u003c/strong\u003e。（Chubby 提供了一种高效机制使得\ntablet server 无需产生网络流量就可以判断它自己是否还拥有锁）。\u003c/p\u003e\n\n\u003cp\u003etablet server 失去锁之后，如果锁文件还在，它会尝试重新去获取这个锁；如果锁\n文件不在了，tablet server 会\u003cstrong\u003e自杀（kill itself）\u003c/strong\u003e，因为它无法为这个 tablet 提\n供服务了。\u003c/p\u003e\n\n\u003cp\u003etablet server 终止时（例如，由于集群管理系统将 tablet server 所在的机器移\n除集群）会将它持有的锁释放，这样 master 就可以及时将对应的 tablets 分配给其他\ntablet server。\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003emaster 负责检测 tablet server 是否工作正常，以及及时重新分配 tablets。\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003e为了检测 tablet server 是否正常工作，master 会定期地询问每个 tablet server 的锁\n的状态。如果一个 server 汇报说锁丢失了，或者如果 master 连续 N 次无法连接到这个\nserver，master 就会\u003cstrong\u003e尝试亲自去获取这个锁文件\u003c/strong\u003e。如果获取锁成功，说明\nChubby 是活着的，那 master 就可以确定：要么是 tablet server 挂了，要么是它无法连\n接到 Chubby，然后 master 就会删掉这个锁文件，以保证这个 tablet server 不会再为这\n个 tablet 提供服务。删除后，master 就将原来分配给这个 tablet server\n的 tablets 标记为未分配的（unassigned）。\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e为了保证 Bigtable 不受 master 和 Chubby 之间的网络问题的影响，master 会在它的\nChubby session 过期时自杀。\u003c/strong\u003e但如前面所描述的，\u003cstrong\u003emaster 挂掉不会影响 tablets 的\n分配\u003c/strong\u003e。\u003c/p\u003e\n\n\u003ch4 id=\"master-启动流程\"\u003emaster 启动流程\u003c/h4\u003e\n\n\u003cp\u003e当一个 master 被集群管理系统启动后，它必须先查看当前的 tablet 分配情况，然后才能\n去修改。\u003c/p\u003e\n\n\u003cp\u003emaster 启动后所做的事情如下：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e从 Chubby 获取一个唯一的 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003emaster\u003c/code\u003e 锁，这样为了避免并发的 master 实例化（instantiation）\u003c/li\u003e\n  \u003cli\u003e扫描 Chubby 中的 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eservers\u003c/code\u003e 目录，查看当前有哪些活着的 server\u003c/li\u003e\n  \u003cli\u003e和每个活着的 tablet server 通信，查看（discover）当前分别给这些 tablet server 分\n配了哪些 tablets\u003c/li\u003e\n  \u003cli\u003e扫描 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eMETADATA\u003c/code\u003e table，查看当前有哪些 tablets（全部 tablets 都在这里）；扫描\n过程中发现的还未被分配出去的 tablets，会添加到一个未分配 tables 集合，后面就\n可以被重新分配出去\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch4 id=\"难点\"\u003e难点\u003c/h4\u003e\n\n\u003cp\u003e以上过程的一个难点是：\u003cstrong\u003e在扫描 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eMETADATA\u003c/code\u003e table 之前，必须保证 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eMETADATA\u003c/code\u003e\ntablets 自己已经被分配出去了\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eOne complication is that the scan of the METADATA table cannot happen until\nthe METADATA tablets have been assigned.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e因此，如果在步骤 3 中发现 root tablet 还没有被分配出去，那 master 就要先将它放到\n未分配 tablets 集合，然后去执行步骤 4。\n这样就保证了 root tablet 将会被分配出去。\u003c/p\u003e\n\n\u003ch4 id=\"tablet-分裂和分裂后的新-tablet-发现\"\u003etablet 分裂和分裂后的新 tablet 发现\u003c/h4\u003e\n\n\u003cp\u003e因为 root tablet 包含了所有 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eMETADATA\u003c/code\u003e tablet 的名字，因此 master 扫描 root tablet\n之后就知道了当前有哪些 tablets。\u003c/p\u003e\n\n\u003cp\u003e只有在发生以下情况时，当前的 tablets 集合才会有变化：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e创建或删除一个 table\u003c/li\u003e\n  \u003cli\u003e两个 tablets 合并成一个更大的，或者一个 tablet 分裂成两个小的\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003emaster 能够跟踪这些变化，因为除了 tablet 分裂之外，其他流程都是由 master\n处理的。\u003cstrong\u003etablet 分裂\u003c/strong\u003e比较特殊，因为它是\u003cstrong\u003e由 tablet server 发起\u003c/strong\u003e的。\u003c/p\u003e\n\n\u003cp\u003etablet server 将新的 tablet 信息记录到 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eMETADATA\u003c/code\u003e table，然后提交这次分裂。提交\n后，master 会收到通知。如果通知丢失（由于 tablet server 或 master 挂掉），master\n会在它下次要求一个 tablet server 加载 tablets 时发现。这个 tablet server 会将这\n次分裂信息通知给 master，因为它在 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eMETADATA\u003c/code\u003e table 中发现的 tablets 项只覆盖\nmaster 要求它加载的 tablets 的了一部分。\u003c/p\u003e\n\n\u003ch3 id=\"53-为-tablet-提供服务tablet-serving\"\u003e5.3 为 tablet 提供服务（Tablet Serving）\u003c/h3\u003e\n\n\u003cp\u003e\u003cstrong\u003etablet 的持久状态存储在 GFS 中\u003c/strong\u003e，如图 5 所示。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/google-bigtable/5.png\" width=\"60%\" height=\"60%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e图 5 Reading from Bigtable\u003c/p\u003e\n\n\u003cp\u003e更新（update）会提交到一个 commit log 文件，其中保存了 redo 记录。\n最近的几次更新会存储在\u003cstrong\u003e内存中\u003c/strong\u003e一个称为 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003esstable\u003c/code\u003e 的有序缓冲区（\nsorted buffer）中；其他老一些的更新存储在 SSTable 中。\u003c/p\u003e\n\n\u003ch4 id=\"tablet-恢复\"\u003etablet 恢复\u003c/h4\u003e\n\n\u003cp\u003e恢复一个 tablet 时，tablet server 需要从 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eMETADATA\u003c/code\u003e table 读取它的元数据。\u003c/p\u003e\n\n\u003cp\u003e这里的元数据包括：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e组成这个 tablet 的 SSTable 列表\u003c/li\u003e\n  \u003cli\u003e一系列 redo 点，指向 commit log 中 tablet 的数据\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003etablet server \u003cstrong\u003e将 SSTable 索引读到内存，然后应用 redo 点之后提交的所有更新\u003c/strong\u003e，\n就可以重建 memtable。\u003c/p\u003e\n\n\u003ch4 id=\"写操作\"\u003e写操作\u003c/h4\u003e\n\n\u003cp\u003e当一个写操作到达 tablet server 时，它会检查写操作是否格式正确（well-formed），以\n及发送者是否有权限执行这次操作。\u003c/p\u003e\n\n\u003cp\u003e鉴权的实现方式是：\u003cstrong\u003e从 Chubby 文件读取允许的写者列表\u003c/strong\u003e（writer list）（在绝大多\n数情况下，这次读都会命中 Chubby 客户端的缓存）。\u003c/p\u003e\n\n\u003cp\u003e一次合法的写操作会记录到 commit log。为了提高小文件写入的吞吐，我们使用了\u003cstrong\u003e批量\n提交\u003c/strong\u003e（group commit）技术 [13, 16]。\u003cstrong\u003e写操作被提交后，它的内容（数据）就会/才会\n插入到 memtable\u003c/strong\u003e。\u003c/p\u003e\n\n\u003ch4 id=\"读操作\"\u003e读操作\u003c/h4\u003e\n\n\u003cp\u003e一次读操作到达 tablet server 时，也会执行类似的格式检查和鉴权。\u003c/p\u003e\n\n\u003cp\u003e合法的读操作是在 SSTable 和 memtable 的合并视图上进行的（executed on a merged\nview of the sequence of SSTables and the memtable）。\n由于 SSTable 和 memtable 都是按词典顺序排序的，因此合并视图的创建很高效。\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e在 tablet 分裂或合并时，读或写操作仍然是可以进行的\u003c/strong\u003e。\u003c/p\u003e\n\n\u003ch3 id=\"54-压缩compactions\"\u003e5.4 压缩（Compactions）\u003c/h3\u003e\n\n\u003cul\u003e\n  \u003cli\u003eminor compaction\u003c/li\u003e\n  \u003cli\u003emajor compaction\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e随着写操作的增多，memtable 在不断变大。\u003cstrong\u003ememtable 超过一定大小时会被冻结\u003c/strong\u003e（\nfrozen），然后创建一个新的 memtable 来接受写入，冻结的 memtable 会\u003cstrong\u003e转化成\nSSTable 写入 GFS\u003c/strong\u003e，这称为 \u003cstrong\u003eminor compaction\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003eminor compaction 有两个目的：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e减少 tablet server 占用的内存\u003c/li\u003e\n  \u003cli\u003etablet server 挂掉之后恢复时，减少从 commit log 读取的数据量\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e\u003cstrong\u003e在 compaction 的过程中，读和写操作是可以正常进行的。\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003e每次 minor compaction 都会创建一个新 SSTable，如果不加额外处理，后面的读操作可能\n就需要将多个 SSTable 进行合并才能读到需要的内容。\u003c/p\u003e\n\n\u003cp\u003e因此，我们在后台定期地执行一个 \u003cstrong\u003emerge compaction\u003c/strong\u003e，这样就可以保证文件（SSTable\n）数量保持在一个范围内。合并压缩读取若干个 SSTable 和 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ememtable\u003c/code\u003e 的内容，然后写到\n一个新的 SSTable。写入完成后，原来的 SSTable 和 memtable 的内容就可以删掉了。这\n种将多个 SSTable 重写成一个的 merge compaction 就称为 \u003cstrong\u003emajor compaction\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e非 major compaction 产生的 SSTable 会包含特殊的删除信息\u003c/strong\u003e（deletion entries）\n，用于标记其中已经被删除的数据 —— 实际上这些数据还没有被真正删除，只是标记为已删\n除。而 \u003cstrong\u003emajor compaction 产生的 SSTable 不会包含这些删除信息或者已删除的数据\u003c/strong\u003e\n（deletion information or deleted data）。\u003c/p\u003e\n\n\u003cp\u003eBigtable 定期地遍历所有 tablets，执行 major compaction 操作。这使得 Bigtable 可\n以\u003cstrong\u003e及时回收已（被标记为）删除的数据占用的资源\u003c/strong\u003e，而且可以\u003cstrong\u003e保证已（被标记为）删除\n的数据及时从系统中消失\u003c/strong\u003e，这对于存储敏感数据的服务来说是很重要的。\u003c/p\u003e\n\n\u003ch2 id=\"6-改进refinements\"\u003e6 改进（Refinements）\u003c/h2\u003e\n\n\u003cp\u003e以上描述的实现需要一些改进才能满足我们的用户所需的高性能、可用性和可靠性。\u003c/p\u003e\n\n\u003cp\u003e本节将更深入地介绍几个实现部分，以此来展示这些需求。\u003c/p\u003e\n\n\u003ch3 id=\"61-locality-groups\"\u003e6.1 Locality groups\u003c/h3\u003e\n\n\u003cp\u003e客户端可以将多个 column family 组织到一个 locality group。\n每个 tablet 会\u003cstrong\u003e为每个 locality group 生成一个单独的 SSTable\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e将一般\u003cstrong\u003e不会一起访问的 column family 划分到不同的 locality group 会提升读性能\u003c/strong\u003e\n。例如，Webtable 中的页面元数据（例如语言和校验和）可以放到同一个 locality group\n，而将页面内容放到另一个 locality group：应用读取元数据的时候就不需要再读取整个\n页面内容。\u003c/p\u003e\n\n\u003cp\u003e此外，还可以\u003cstrong\u003e基于 locality group 维度对某些参数进行调优\u003c/strong\u003e。例如，可以声明一个\nlocality group 是\u003cstrong\u003e驻留内存的\u003c/strong\u003e（in-memory）。驻留内存的 locality group 对应的\nSSTable 会被惰性加载到 tablet server 的内存。 一旦加载，这类 column family 的读\n操作就不再需要访问磁盘。这个特性对访问频繁的小文件非常有用：\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eMETADATA\u003c/code\u003e table 的\n\u003ccode class=\"language-plaintext highlighter-rouge\"\u003elocation\u003c/code\u003e column family 内部用的就是这种类型。\u003c/p\u003e\n\n\u003ch3 id=\"62-压缩compression\"\u003e6.2 压缩（Compression）\u003c/h3\u003e\n\n\u003cp\u003e客户端可以控制 SSTable 是否需要压缩，以及用什么格式压缩。\u003c/p\u003e\n\n\u003ch4 id=\"压缩的粒度和算法\"\u003e压缩的粒度和算法\u003c/h4\u003e\n\n\u003cp\u003e压缩的\u003cstrong\u003e基本单位是 SSTable block\u003c/strong\u003e（大小可以由 locality group 的参数控制）。\n虽然 block 级别的压缩（相对于更大的数据级别）损失了一些压缩效率，但在只需读取\n部分内容时，我们不需要解压整个文件，从而\u003cstrong\u003e提高了读效率\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e我们的很多客户端都使用一种自定义的双通（two-pass）压缩算法：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e先使用 Bentley-McIlroy 算法 [6] 压缩大窗口内的长公共前缀（long common strings across a large window）\u003c/li\u003e\n  \u003cli\u003e再使用一个快速算法压缩 16KB 窗口内的重复字符串\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e在现代计算机上，这两个算法都非常快，\u003cstrong\u003e压缩速度可以达到 100~200 MB/s，解压可以达到\n400~1000 MB/s\u003c/strong\u003e。\u003c/p\u003e\n\n\u003ch4 id=\"压缩的速度和效率\"\u003e压缩的速度和效率\u003c/h4\u003e\n\n\u003cp\u003e虽然相比于压缩效率我们更看重压缩速度，但令人惊奇的是，我们的双通压缩算法效率非常\n好。\u003c/p\u003e\n\n\u003cp\u003e例如，在 Webtable 中，我们存储了大量的页面进行了一次实验。实验中每个页面只存储了\n一个版本。结果显示，这个算法的压缩比达到了 10:1，而典型情况下 Gzip 压缩 HTML 页\n面只有 3:1 或 4:1 的效率。\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e这么高的压缩效率来自 Webtable 的行（row）组织方式\u003c/strong\u003e：来自相同域名（host）的页\n面都存储在一起。这些页面有着很多类似内容（模板），非常适合 Bentley-McIlroy 算法\n。不止是 Webtable，很多应用都根据行名（row names）将相似的数据组织到一起进行存储\n，因此可以取得非常好的压缩比。如果数据是存储了多个版本而不是一个版本，那压缩比会\n更高。\u003c/p\u003e\n\n\u003ch3 id=\"63-读缓存\"\u003e6.3 读缓存\u003c/h3\u003e\n\n\u003cp\u003e为了提高读性能，tablet server 使用了两级缓存：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eScan Cache\n    \u003cul\u003e\n      \u003cli\u003e高层缓存\u003c/li\u003e\n      \u003cli\u003e存储 SSTable 返回给 tablet server 的 \u003cstrong\u003ekey-value pair\u003c/strong\u003e\u003c/li\u003e\n      \u003cli\u003e适用于\u003cstrong\u003e频繁访问相同数据\u003c/strong\u003e的应用\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003eBlock Cache\n    \u003cul\u003e\n      \u003cli\u003e低层缓存\u003c/li\u003e\n      \u003cli\u003e存储从 GFS 读取的 \u003cstrong\u003eSSTable blocks\u003c/strong\u003e\u003c/li\u003e\n      \u003cli\u003e适用于\u003cstrong\u003e连续访问相邻（相近）数据\u003c/strong\u003e的应用。例如顺序读，或者在热点行（hot\nrow）中相同 locality group 内不同列的随机读\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 id=\"64-bloom-过滤器\"\u003e6.4 Bloom 过滤器\u003c/h3\u003e\n\n\u003cp\u003e5.3 介绍过，一次读操作必须要对组成一个 tablet 状态的所有 SSTable 都进行读取。如\n果这些 SSTable 没有在内存，我们就要进行多次磁盘访问。我们允许客户端在一个特殊的\nlocality group 内指定要\u003cstrong\u003e对 SSTable 创建 Bloom 过滤器\u003c/strong\u003e [7]，通过这种方式就可以\n减少这种磁盘访问。\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eBloom 过滤器可以判断一个 SSTable 是否包含指定行/列对（row/column pair）对应的\n数据\u003c/strong\u003e。对于特定的应用来说，给 tablet server 增加少量内存用于存储 Bloom 过滤器，就\n可以\u003cstrong\u003e极大地减少读操作的磁盘访问\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e我们的实际使用也显示，大部分对不存在的行或列的访问都无需涉及磁盘操作（在 Bloom\n过滤器这一层就判断不存在了，无需再查找磁盘）。\u003c/p\u003e\n\n\u003ch3 id=\"65-commit-log-实现\"\u003e6.5 Commit-log 实现\u003c/h3\u003e\n\n\u003ch4 id=\"每个-tablet-还是每个-tablet-server-一个-log-文件\"\u003e每个 tablet 还是每个 tablet server 一个 log 文件\u003c/h4\u003e\n\n\u003cp\u003e如果为每个 tablet 维护一个单独的 log 文件，那会导致底层 GFS 大量文件的并发写。考\n虑到 GFS 的具体实现，这些并发写进而会导致大量的磁盘访问，以完成不同物理文件的并\n发写入。另外，每个 tablet 一个 log 文件的设计还会降低组提交（group commit，批量\n提交）优化的有效性，因为每个组（group）都会很小。\u003c/p\u003e\n\n\u003cp\u003e因此，为了克服以上问题，我们为\u003cstrong\u003e每个 tablet server 维护一个 commit log\u003c/strong\u003e，将属于\n这个 tablet server 的不同的 tablet 操作都写入这同一个物理上的 log 文件 [18, 20]。\u003c/p\u003e\n\n\u003ch4 id=\"恢复过程变复杂\"\u003e恢复过程变复杂\u003c/h4\u003e\n\n\u003cp\u003e这种方式使得常规操作（normal operations）的性能得到了很大提升，但是，它使 tablet\n恢复过程变得复杂。\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e当一个 tablet server 挂掉后，它负责的那些 tablets 就会重新分配给其他（大量）的\ntablet servers\u003c/strong\u003e：通常情况下每个 tablet server 只会分到其中的一小部分。恢复一个\ntablet 的状态时，新的 tablet server 需要从原 tablet server 的 commit log 里重新\n应用（reapply）这个 tablet 的修改（mutation）。然而，\u003cstrong\u003e这些 tablet 的 mutation 都\n混在同一个物理的 log 文件内\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e一种方式是每个新的 tablet server 都去读完整的 commit log，将自己需要的部分过滤出\n来。但是，如果有 100 个机器分到了 tablet 的话，这个 log 文件就要被读 100 次。\u003c/p\u003e\n\n\u003ch4 id=\"优化两个写线程和两份-commit-log\"\u003e优化：两个写线程和两份 commit log\u003c/h4\u003e\n\n\u003cp\u003e为了避免这种重复读，我们\u003cstrong\u003e将 commit log 内容\u003c/strong\u003e以 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e(table; row name; log sequence\nnumber)\u003c/code\u003e 为键（key）\u003cstrong\u003e进行排序\u003c/strong\u003e。\u003cstrong\u003e在排序后的 commit log 中，每个 tablet 的所有\nmutation 都是连续的\u003c/strong\u003e，因此可以实现高效的读取：\u003cstrong\u003e只需一次磁盘寻址加随后的顺序读\u003c/strong\u003e。\n为了加速排序过程，我们还将 commit log 分割成 64 MB 的段（segment），分散到多个\ntablet server 上并发地进行排序。\u003c/p\u003e\n\n\u003cp\u003e这个排序过程是由 \u003cstrong\u003emaster 协调（coordinate）、tablet server 触发\u003c/strong\u003e的：\ntablet server 向 master 汇报说需要从一些 commit log 中恢复一些 mutation。\u003c/p\u003e\n\n\u003cp\u003e写提交记录到 GFS 有时会遇到性能卡顿，这可能有多方面原因。例如，负责写操作的 GFS\nserver 挂了，或者到三个指定的 GFS master 的网络发生了拥塞或过载。为了减少这些\nGFS 导致的延迟抖动，\u003cstrong\u003e每个 tablet server 为 commit log 使用了两个写线程\u003c/strong\u003e：每个\n线程写到各自的 log 文件，但同时只会有一个线程是活跃的。\n如果当前的活跃线程写性能非常差，写操作就会切换到另一个线程，由这个新线程负责之后\n的写。\u003c/p\u003e\n\n\u003cp\u003elog 中的记录（entry）都有序列号，恢复的时候可以根据序列号过滤由于 log 切换导致\n的重复数据。\u003c/p\u003e\n\n\u003ch3 id=\"66-加速-tablet-恢复过程\"\u003e6.6 加速 tablet 恢复过程\u003c/h3\u003e\n\n\u003cp\u003e如果 master 将一个 tablet 从一个 tablet server 移动到另一个，源\ntablet server 会先对这个 tablet 进行一次 minor compaction。\n这会对 commit log 里还未压缩的状态进行一次压缩，减少恢复时需要读取的数据量。\n这次压缩完成后，源 tablet server 停止为这个 tablet 提供服务。\u003c/p\u003e\n\n\u003cp\u003e源 tablet server 在真正卸载（unload）这个 tablet 之前会再进行一次（通常非常快的\n）minor compaction，对第一次 minor compaction 到当前时刻内新进来的未压缩状态进行\n压缩。这次压缩做完之后，这个 tablet 就可以被其他的 tablet server 加载（load），\n而无需恢复任何 log 记录。\u003c/p\u003e\n\n\u003ch3 id=\"67-利用不可变性exploiting-immutability\"\u003e6.7 利用不可变性（Exploiting immutability）\u003c/h3\u003e\n\n\u003cp\u003e除了 SSTable 缓存之外，Bigtable 系统其他一些部分也因 SSTable 的不可变性而得到简\n化。例如，从 SSTable 读取数据时，对文件系统的访问不需要任何同步。因此，对行的并\n发控制可以实现地非常高效。\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e读和写操作涉及的唯一可变数据结构是 memtable\u003c/strong\u003e。为减少 memtable 的读竞争，我们\n将 memtable 的行（row）设计为\u003cstrong\u003e写时复制\u003c/strong\u003e（copy-on-write），这样读和写就可以并行\n进行。\u003c/p\u003e\n\n\u003cp\u003e因为 SSTable 是不可变的，所以\u003cstrong\u003e彻底删除数据\u003c/strong\u003e（permanently removing deleted data\n）的问题就变成了\u003cstrong\u003e对过期的 SSTable 进行垃圾回收\u003c/strong\u003e（garbage collecting obsolete\nSSTables）。\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e每个 tablet 的 SSTable 会注册到 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eMETADATA\u003c/code\u003e table\u003c/strong\u003e。master 会对过期的 SSTable\n进行\u003cstrong\u003e“先标记后清除”\u003c/strong\u003e（mark-and-sweep） [25]，其中 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eMETADATA\u003c/code\u003e table 记录了这些\nSSTable 的对应的 tablet 的 root。\u003c/p\u003e\n\n\u003cp\u003e最后，\u003cstrong\u003eSSTable 的不可变性使得 tablet 分裂过程更快\u003c/strong\u003e。我们直接让子 tablet 共享\n父 tablet 的 SSTable ，而不是为每个子 tablet 分别创建一个新的 SSTable。\u003c/p\u003e\n\n\u003ch2 id=\"7-性能评估\"\u003e7 性能评估\u003c/h2\u003e\n\n\u003ch3 id=\"测试环境\"\u003e测试环境\u003c/h3\u003e\n\n\u003cp\u003e我们在一套有 N 个 tablet server 的 Bigtable 集群进行测试，测量 N 变化时 Bigtable\n的性能和可扩展性。\u003c/p\u003e\n\n\u003cp\u003e每个 tablet server 使用 1GB 内存，写到由 1786 台节点组成的 GFS 集群，其中每个节\n点配备了两个 400GB 的 IDE 硬盘。\u003c/p\u003e\n\n\u003cp\u003eN 个客户端生成 Bigtable 负载用于测试（用和 tablet server 同样数量的客户端是\n为了保证客户端不会称为性能瓶颈）。\u003c/p\u003e\n\n\u003cp\u003e每个机器有两个双核 Opteron 2 GHz 处理器，足够的物理内存，以及一个 1Gbps 以太网链\n路。所有机器连接到一个\u003cstrong\u003e两级树状交换网络\u003c/strong\u003e（two-level tree-shaped switched\nnetwork），网络根节点有 100-200 Gbps 的聚合带宽。所有机器都在同一个物理基础设施\n中，因此机器间的时延小于 1ms。\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003etablet server、master、测试用的客户端，以及 GFS server 都运行在相同的一组机器上\n。本实验是在一个正常使用中的集群上进行的\u003c/strong\u003e，因此：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e每个机器都运行了一个 GFS server\u003c/li\u003e\n  \u003cli\u003e有的机器运行了一个 tablet server，或者一个客户端进程，或者其他与本实验\n无关的工作任务\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch3 id=\"性能指标\"\u003e性能指标\u003c/h3\u003e\n\n\u003cp\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eR\u003c/code\u003e 是测试中 Bigtable 的不重复行键（row key）数量。\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eR\u003c/code\u003e 的选择使得每个基准测试\n中每个 tablet server 读或写大约 1GB 数据。\u003c/p\u003e\n\n\u003cp\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003esequential write\u003c/code\u003e（顺序写）将行空间等分成 10N 份，通过一个中心调度器分配给 N 个\n客户端，每个客户端都是先拿到一份进行处理，完成后调度器会再分给它一份，这种动态分\n配可以减少客户端所在机器上的其他进程对实验的影响。每一个行键对应写一个字符串，字\n符串是随机生产的，因此无法压缩（uncompressible）。另外，不同行键对应的字符串是不\n同的，因此也是无法跨行压缩的。\u003c/p\u003e\n\n\u003cp\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003erandom write\u003c/code\u003e（随机写）基准测试与顺序写类似，除了行键在写之前是对 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eR\u003c/code\u003e 取模的（\nrow key was hashed modulo R），因此写操作可以在整个测试期间都均匀地分散到整个行\n空间。\u003c/p\u003e\n\n\u003cp\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003esequential read\u003c/code\u003e（顺序读）生产行键的方式与顺序写类似，读的也是顺序写测试写入的\n数据。\u003c/p\u003e\n\n\u003cp\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003erandom read\u003c/code\u003e（随机读）与随机写类似。\u003c/p\u003e\n\n\u003cp\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003escan\u003c/code\u003e（扫描）和顺序读类似，但利用了 Bigtable 提供的\u003cstrong\u003e扫描给定行范围内的所有值\u003c/strong\u003e\n的 API。使用这个 API 可以减少 RPC 的次数，因为一次 RPC 就可以从 tablet server 取\n到大量的值。\u003c/p\u003e\n\n\u003cp\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003erandom read (mem)\u003c/code\u003e 和顺序读类似，但测试数据的 locality group 标记为驻留内存型（\nin-memory），因此会从 tablet server 的内存而不是 GFS 读取。在这个测试中，我们将\n每个 tablet 的测试数据从 1GB 降到了 100MB，以充分保证它们能落到 tablet server 的\n内存中。\u003c/p\u003e\n\n\u003cp\u003e图 6 以两种视图展示了读/写 1000 字节的值到 Bigtable 时的性能。\n左侧是每个 tablet server 每秒的操作数；右侧是聚合之后的每秒操作数。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/google-bigtable/6.png\" width=\"100%\" height=\"100%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e图 6 读/写 1000 字节的值到 Bigtable 时的性能\u003c/p\u003e\n\n\u003ch3 id=\"71-单-tablet-server-性能\"\u003e7.1 单 tablet-server 性能\u003c/h3\u003e\n\n\u003cp\u003e首先看单个 tablet server 的性能。\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e随机读比其他的操作都要慢一个数量级\u003c/strong\u003e甚至更多。\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e每次随机读都需要将 64KB 的 SSTable block 从 GFS 通过网络传输到 tablet server，\n而其中仅仅包含了一个 1000 字节的值\u003c/strong\u003e。\u003cstrong\u003etablet server 每秒大约 1200 次读操作，折\n算约为 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e75 MB/s\u003c/code\u003e 从 GFS 读数据。\u003c/strong\u003e考虑到网络栈、SSTable 解析、Bigtable 代码等开\n销，这个带宽足以使 tablet server 的 CPU 达到饱和了，也足以使机器的网络链路饱和了\n（75 MB/s = 600 Mbps，系统总共 1Gbps 带宽）。大部分这种访问类型的 Bigtable 应用\n会\u003cstrong\u003e将 block size 设置的更小\u003c/strong\u003e，一般设为 8 KB。\u003c/p\u003e\n\n\u003cp\u003e从内存的随机读会快很多，因为每个 1000 字节的读都是从 tablet server 的本地内存读\n取的，不需要从 GFS 访问 64KB 的 block。\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e随机和顺序写的性能都要比随机读好\u003c/strong\u003e，因为每个 tablet server 会将所有写操作追加\n到同一个 commit log 然后执行\u003cstrong\u003e批量提交\u003c/strong\u003e（group commit），从而高效地写入到 GFS。\n\u003cstrong\u003e随机写和顺序写的性能并没有明显差异\u003c/strong\u003e，因为两种情况下，所有到 tablet server 的\n写最后都是到了同一个 commit log。\u003c/p\u003e\n\n\u003cp\u003e顺序读的性能远好于随机读，因为每个从 GFS \u003cstrong\u003e预取\u003c/strong\u003e（prefetch）的 64KB SSTable\nblock 都存储到了 blcok 缓存，下一次 64 读请求就会用到。\u003c/p\u003e\n\n\u003cp\u003e扫描的性能更好，因为客户端的一次 RPC 请求就可以从 tablet server 拿到大量的值，因\n此 RPC 开销被平摊了。\u003c/p\u003e\n\n\u003ch3 id=\"72-扩展性scaling\"\u003e7.2 扩展性（scaling）\u003c/h3\u003e\n\n\u003cp\u003e当我们将系统中 tablet server 的数量从 1 增加到 500 时，\n聚合吞吐量（aggregate throughput）的增长非常明显，超过了 100 倍。\n例如，当 tablet server 数量增加到 500 倍时，\u003ccode class=\"language-plaintext highlighter-rouge\"\u003erandom read (mem)\u003c/code\u003e 增长了几乎 300\n倍。这是因为这个基准测试的性能瓶颈在 tablet server 的 CPU。\u003c/p\u003e\n\n\u003cp\u003e但是，\u003cstrong\u003e性能并没有线性增长\u003c/strong\u003e。对于大部分基准测试，在 tablet server 从 1 增加到\n500 的过程中，单台 server 的吞吐量都有一个明显的下降（图 6 左边的表）。这个下降\n是\u003cstrong\u003e由不同 server 配置导致的负载不均衡引起的，大部分情况下是由于机器上的其他进程\n在竞争 CPU 和网络资源\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e我们的负载均衡算法就是想解决这个问题，但由于两个主要原因无法做到完美：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e减少 tablet 的移动会引起 rebalancing 的抖动（tablet 在移动的时候会有很短的一\n段时间不可用，一般在 1 秒以下）\u003c/li\u003e\n  \u003cli\u003e基准测试生成的负载会随着测试的进行而不断漂移（shifts around）\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e随机读基准测试的扩展性最差（server 增加 500 倍时，它的聚合吞吐量只增加了 100 倍）。\n前面解释过，造成这个问题的原因是对于每个 1000 字节的值，我们都需要通过网络传输一\n个 64KB 的 block。这个数据量使得我们与其他进程共享的 1Gbps 网络带宽达到饱和，因\n此随着机器数量的增加，每节点平均吞吐量（per-server throughput）下降非常明显。\u003c/p\u003e\n\n\u003ch2 id=\"8-真实应用\"\u003e8 真实应用\u003c/h2\u003e\n\n\u003cp\u003e截至 2006 年 8 月，Google 总共运行着 388 个非测试的 Bigtable 集群，分布在不同的\n数据中心，加起来有 24,500 个 tablet server。\u003c/p\u003e\n\n\u003cp\u003e表 1 展示了这些集群中 tablet server 数量的大致分布：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e表 1 Bigtable 集群中 tablet server 数量分布\u003c/p\u003e\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/google-bigtable/table-1.png\" width=\"45%\" height=\"45%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e其中一些集群是用于开发目的，因此会有较长时间的空闲状态。\u003c/p\u003e\n\n\u003cp\u003e我们挑选了 14 个活跃集群，总共包含 8069 个 tablet server，提供了如下聚合性能：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e120 万次请求/秒（QPS）\u003c/li\u003e\n  \u003cli\u003e741 MB/s RPC 入流量\u003c/li\u003e\n  \u003cli\u003e16 GB/s RPC 出流量\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e图 2 给出了目前在用的几个 table 的一些数据。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e表 2 生产环境 Bigtable 的一些数据\u003c/p\u003e\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/google-bigtable/table-2.png\" width=\"100%\" height=\"100%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e一些 table 存储的是给用户使用的数据，另外一些存储的是批处理用的数据。table 的\n大小、平均 cell 大小、内存中数据（served from memory）所占的比例、table schema\n的复杂度等等差异都很大。在本节接下来的内容中，我们将简要介绍产品团队是如何使用\nBigtable 的。\u003c/p\u003e\n\n\u003ch3 id=\"81-google-analytics\"\u003e8.1 Google Analytics\u003c/h3\u003e\n\n\u003cp\u003eGoogle Analytics (analytics.google.com) 是一个帮助网站管理员分析网站流量的服务。\u003c/p\u003e\n\n\u003cp\u003e它提供了很多聚合统计数据，例如每天的独立访问量和每个 URL 每天的访问量，以及网站\n跟踪报告，例如给定一组之前浏览了某个页面的用户，它可以给出实际发生了购买行为的用\n户比例。\u003c/p\u003e\n\n\u003cp\u003e为了实现这些功能，网络管理员需要在他们的网页上嵌入一段 JavaScript 代码。\n这样每当这个网页被访问时，这段程序就会被激活。它会记录很多的信息，例如用户 ID 以\n及页面信息，发送给 Google Analytics，Google Analytics 会对这些信息进行汇总，最后\n呈现给网站管理员。\u003c/p\u003e\n\n\u003cp\u003e这里简要介绍 Google Analytics 使用的两个 table。\u003c/p\u003e\n\n\u003cp\u003e原始点击（raw click）table（~200 TB）为每个用户维护了一个（数据）行。行名是网站\n名和 session 创建时间组成的一个元组（tuple）。这样的 schema 保证了访问网站的\nsession 按照时间顺序（chronologically）是连续的。这个 table 压缩到了原始大小的\n14%。\u003c/p\u003e\n\n\u003cp\u003e汇总（summary）table（~20TB）存储了每个网站的一些预定义的汇总。这个 table 是通过\n定期的 MapReduce 任务对原始点击表进行计算得到的。每个 MapReduce 任务会从原始点击\n表中提取最近的 session 数据，系统整体的吞吐受限于 GFS 的吞吐。这个表压缩到了原始\n大小的 29%。\u003c/p\u003e\n\n\u003ch3 id=\"82-google-earth\"\u003e8.2 Google Earth\u003c/h3\u003e\n\n\u003cp\u003eGoogle 提供了地球高精度卫星图服务给用户，可以通过基于网页的 Google Maps 接口（\nmaps.google.com）或客户端软件 Google Earth（earth.google.com）访问。这些产品允许\n用户在任何分辨率的卫星图上游走，停留、查看和标注。\u003c/p\u003e\n\n\u003cp\u003e这个系统使用了一个表来做数据预处理，另外很多表来服务客户端数据。预处理 pipeline\n使用一个表来存储原始图像。预处理过程会将图像进行清洗和合并（clean and\nconsolidate），变成可以提供服务的数据。这个表存储了大约 70 TB 的数据，因此是放在\n磁盘上的。另外这些图像都已经高效地压缩过了，因此 Bigtable 的压缩是关闭的。表的每\n一行代表一个 geographic segment（地理位置）。行名的设计使得地理上相邻的 segment\n在存储的时候也是相邻的。另外，这个表还包含一个 column family，用来跟踪每个\nsegment 的数据来源（sources of data for each segment）。这个 column family 有大\n量的列：基本上每个原始数据图像（raw data image）都有一列。因为每个 segment 都是\n用少量几张图像合成的，因此这个 column family 非常稀疏。\u003c/p\u003e\n\n\u003cp\u003e预处理 pipeline 强烈依赖 MapReduce 对 Bigtable 内的数据进行变换。部分 MapReduce\njob 进行时，系统整体可以达到每个 tablet server 1MB/s 以上的数据处理速度。\u003c/p\u003e\n\n\u003cp\u003e服务系统使用一个表来索引存储在 GFS 中的数据。这个表相对比较小（~500GB），但它必\n须保证每个数据中心每秒几万次请求（QPS）的负载下，仍然保持很低的延迟。因此，这个\n表同时分散到了几百个 tablet server 上进行处理，并且还包含了驻留内存的 column\nfamily。\u003c/p\u003e\n\n\u003ch3 id=\"83-personalized-search\"\u003e8.3 Personalized Search\u003c/h3\u003e\n\n\u003cp\u003ePersonalized Search（个性化搜索）(www.google.com/psearch)是一个自选的服务，它会\n记录用户的搜索关键词和在各种 Google 服务上的点击，例如网页搜索、图像和新闻等等。\n用户可以通过浏览自己的搜索关键词和点击记录来查看他们的搜索历史，可以要求根据\n自己过去的 Google 使用习惯来向他们提供个性化搜索结果。\u003c/p\u003e\n\n\u003cp\u003e个性化搜索将用户数据存储到 Bigtable。每个用户有一个唯一的用户 ID，并根据这个 ID\n分配一个行名。所有的用户动作存储在另一个表，每种类型的动作会占用一个 column\nfamily（例如，有一个 column family 存储所有的网页查询）。每个数据用动作发生的时\n刻作为它在 Bigtable 中的时间戳。\u003c/p\u003e\n\n\u003cp\u003e个性化搜索利用 MapReduce 在 Bigtable 上进行运算，为每个用户生成一个 profile。\n这些 profile 就会用来做个性化的实时搜索。\u003c/p\u003e\n\n\u003cp\u003e个性化搜索的数据会在几个 Bigtable 之间做复制，以提高可用性，减少客户端距离导致的\n延迟。这个团队最初在 Bigtable 之上开发了自己的一套客户端侧复制机制，以保证所有副\n本的最终一致性。现在，复制子系统已经集成到服务端。\u003c/p\u003e\n\n\u003cp\u003e个性化搜索存储系统的设计允许其他团队在他们各自的列中添加用户级别的（per-user）信\n息，这个系统现在被很多 Google 其他产品在使用，存储他们自己的用户级别的（per-user\n）配置选项和设置。但在多个开发团队之间共享一个表会导致数量异常庞大的 column\nfamily。\u003c/p\u003e\n\n\u003cp\u003e为了帮助共享，我们给 Bigtable 添加了一个简单的配额（quota）机制，限制单一客户端\n在一个共享表中所占的存储大小。对于那些多个产品团队使用 Bigtable 存储用户级别信息\n的场景，这种机制提供了一定的隔离性。\u003c/p\u003e\n\n\u003ch2 id=\"9-从中所学lessons\"\u003e9 从中所学（Lessons）\u003c/h2\u003e\n\n\u003cp\u003e在设计、实现、维护和支持 Bigtable 的过程中，我们得到了很多有用的经验，也学习到了\n很多有趣的教训。\u003c/p\u003e\n\n\u003ch3 id=\"故障源远比你想象中多\"\u003e故障源远比你想象中多\u003c/h3\u003e\n\n\u003cp\u003e首先我们认识到，大型分布式系统在很多方面的故障面前都很脆弱，不仅仅是很多分布式协\n议所假设的网络分裂和出错后停止服务（fail-stop failures）。例如，我们就遇到过如下\n场景引起的故障：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e内存和网络损坏\u003c/li\u003e\n  \u003cli\u003e很大的时钟偏差（clock skew）\u003c/li\u003e\n  \u003cli\u003e机器死机（hung）\u003c/li\u003e\n  \u003cli\u003e更复杂的和非对称的网络分裂\u003c/li\u003e\n  \u003cli\u003e依赖的基础服务的 bug（例如 Chubby）\u003c/li\u003e\n  \u003cli\u003eGFS 配额溢出（overflow）\u003c/li\u003e\n  \u003cli\u003e计划及非计划的硬件维护\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e随着对这一问题的了解的深入，我们开始修改各种的协议来应对这一问题。例如，我们给\nRPC 机制添加了校验和。\u003c/p\u003e\n\n\u003cp\u003e另外，我们还去掉了系统的一个部分对另一部分的假设。例如，我们不再假设一次 Chubby\n操作只会返回固定的几种错误。\u003c/p\u003e\n\n\u003ch3 id=\"避免过早添加使用场景不明确的新特性\"\u003e避免过早添加使用场景不明确的新特性\u003c/h3\u003e\n\n\u003cp\u003e我们得到的另一重要经验是：如果还不是非常清楚一个新特性将被如何使用，那就不要着急\n添加到系统中。\u003c/p\u003e\n\n\u003cp\u003e例如，我们最初有计划在 API 中支持广义事物模型（general-purpose transaction）。但\n因为当时没有迫切的使用场景，因此没有立即去实现。现在有了很多真实应用跑在 Bigtable\n之后，我们审视了这些应用的真实需求，发现大部分应用其实只需要单行事务（single-row\ntransaction）。\u003c/p\u003e\n\n\u003cp\u003e对于真的有分布式事务需求的人，我们发现他们最核心的需求其实是维护二级索引（\nsecondary indices），因此我们计划通过添加一个特殊的机制来满足这个需求。这个机制\n没有分布式事务通用，但性能会更好（尤其是跨上百行以上的更新），而且对于乐观跨数据\n中心复制（optimistic cross-data-center replication）来说，和我们系统的集成会更好。\u003c/p\u003e\n\n\u003ch3 id=\"系统级监控非常重要\"\u003e系统级监控非常重要\u003c/h3\u003e\n\n\u003cp\u003e在日常支持 Bigtable 中学到的实际一课是：合理的系统级监控（例如监控 Bigtable 本身\n，以及使用 Bigtable 的客户端）非常重要。\u003c/p\u003e\n\n\u003cp\u003e例如，我们扩展了我们的 RPC 系统，可以记录重要动作的详细跟踪信息。这个特性帮助我\n们检测和解决了很多问题，包括：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003etablet 数据结构上的锁竞争\u003c/li\u003e\n  \u003cli\u003e提交 Bigtable mutation 时 GFS 写很慢\u003c/li\u003e\n  \u003cli\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eMETADATA\u003c/code\u003e tablets 不可用时访问 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eMETADATA\u003c/code\u003e 表时被卡住（stuck）\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e监控的另一个例子是每个 Bigtable 集群都注册到了 Chubby。这使得我们可以跟踪所有的集\n群，看到集群有多大，各自运行的是什么版本，接收到的流量有多大，是否有异常的大延迟\n等等。\u003c/p\u003e\n\n\u003ch3 id=\"保持设计的简洁\"\u003e保持设计的简洁\u003c/h3\u003e\n\n\u003cp\u003e\u003cstrong\u003e我们学到的最重要经验是：简单设计带来的价值\u003c/strong\u003e（the value of simple designs）。\u003c/p\u003e\n\n\u003cp\u003e考虑到我们的系统规模（10 万行代码，不包括测试），以及代码都会随着时间以难以\n意料的方式演进，我们发现代码和设计的简洁性对代码的维护和 debug 有着巨大的帮助。\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eGiven both the size of our system (about 100,000 lines of non-test code), as\nwell as the fact that code evolves over time in unexpected ways, we have found\nthat code and design clarity are of immense help in code maintenance and\ndebugging.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e一个例子是我们的 tablet server 成员（membership）协议。我们的第一版非常简单：\nmaster 定期向 tablet server 提供租约，如果一个 tablet server 的租约到期，它就自\n杀。不幸的是，这个协议在发生网络问题时可用性非常差，而且对 master 恢复时间也很敏感。\u003c/p\u003e\n\n\u003cp\u003e接下来我们重新设计了好几版这个协议，直到它令我们满意。但是，这时的协议已经变得过\n于复杂，而且依赖了一些很少被其他应用使用的 Chubby 特性。最后发现我们花了大量的时\n间来 debug 怪异的边界场景，不仅仅是 Bigtable 代码，还包括 Chubby 代码。\u003c/p\u003e\n\n\u003cp\u003e最终，我们放弃了这个版本，重新回到了一个新的更简单的协议，只依赖使用广泛的 Chubby\n特性。\u003c/p\u003e\n\n\u003ch2 id=\"10-相关工作\"\u003e10 相关工作\u003c/h2\u003e\n\n\u003cp\u003eThe Boxwood project [24] has components that overlap\nin some ways with Chubby, GFS, and Bigtable, since it\nprovides for distributed agreement, locking, distributed\nchunk storage, and distributed B-tree storage. In each\ncase where there is overlap, it appears that the Boxwood’s\ncomponent is targeted at a somewhat lower level\nthan the corresponding Google service. The Boxwood\nproject’s goal is to provide infrastructure for building\nhigher-level services such as file systems or databases,\nwhile the goal of Bigtable is to directly support client\napplications that wish to store data.\u003c/p\u003e\n\n\u003cp\u003eMany recent projects have tackled the problem of providing\ndistributed storage or higher-level services over\nwide area networks, often at “Internet scale.” This includes\nwork on distributed hash tables that began with\nprojects such as CAN [29], Chord [32], Tapestry [37],\nand Pastry [30]. These systems address concerns that do\nnot arise for Bigtable, such as highly variable bandwidth,\nuntrusted participants, or frequent reconfiguration; decentralized\ncontrol and Byzantine fault tolerance are not\nBigtable goals.\u003c/p\u003e\n\n\u003cp\u003eIn terms of the distributed data storage model that one\nmight provide to application developers, we believe the\nkey-value pair model provided by distributed B-trees or\ndistributed hash tables is too limiting. Key-value pairs\nare a useful building block, but they should not be the\nonly building block one provides to developers. The\nmodel we chose is richer than simple key-value pairs,\nand supports sparse semi-structured data. Nonetheless,\nit is still simple enough that it lends itself to a very ef\ncient  representation, and it is transparent enough\n(via locality groups) to allow our users to tune important\nbehaviors of the system.\u003c/p\u003e\n\n\u003cp\u003eSeveral database vendors have developed parallel\ndatabases that can store large volumes of data. Oracle’s\nReal Application Cluster database [27] uses shared disks\nto store data (Bigtable uses GFS) and a distributed lock\nmanager (Bigtable uses Chubby). IBM’s DB2 Parallel\nEdition [4] is based on a shared-nothing [33] architecture\nsimilar to Bigtable. Each DB2 server is responsible for\na subset of the rows in a table which it stores in a local\nrelational database. Both products provide a complete\nrelational model with transactions.\u003c/p\u003e\n\n\u003cp\u003eBigtable locality groups realize similar compression\nand disk read performance benets observed for other\nsystems that organize data on disk using column-based\nrather than row-based storage, including C-Store [1, 34]\nand commercial products such as Sybase IQ [15, 36],\nSenSage [31], KDB+ [22], and the ColumnBM storage\nlayer in MonetDB/X100 [38]. Another system that does\nvertical and horizontal data partioning into  and\nachieves good data compression ratios is AT\u0026amp;T’s Daytona\ndatabase [19]. Locality groups do not support CPUcache-\nlevel optimizations, such as those described by\nAilamaki [2].\u003c/p\u003e\n\n\u003cp\u003eThe manner in which Bigtable uses memtables and\nSSTables to store updates to tablets is analogous to the\nway that the Log-Structured Merge Tree [26] stores updates\nto index data. In both systems, sorted data is\nbuffered in memory before being written to disk, and\nreads must merge data from memory and disk.\u003c/p\u003e\n\n\u003cp\u003eC-Store and Bigtable share many characteristics: both\nsystems use a shared-nothing architecture and have two\ndifferent data structures, one for recent writes, and one\nfor storing long-lived data, with a mechanism for moving\ndata from one form to the other. The systems differ\nsignificantly in their API: C-Store behaves like a\nrelational database, whereas Bigtable provides a lower\nlevel read and write interface and is designed to support\nmany thousands of such operations per second per server.\nC-Store is also a “read-optimized relational DBMS”,\nwhereas Bigtable provides good performance on both\nread-intensive and write-intensive applications.\u003c/p\u003e\n\n\u003cp\u003eBigtable’s load balancer has to solve some of the same\nkinds of load and memory balancing problems faced by\nshared-nothing databases (e.g., [11, 35]). Our problem is\nsomewhat simpler: (1) we do not consider the possibility\nof multiple copies of the same data, possibly in alternate\nforms due to views or indices; (2) we let the user tell us\nwhat data belongs in memory and what data should stay\non disk, rather than trying to determine this dynamically;\n(3) we have no complex queries to execute or optimize.\u003c/p\u003e\n\n\u003ch2 id=\"11-总结\"\u003e11 总结\u003c/h2\u003e\n\n\u003cp\u003e我们在 Google 设计了 Bigtable，一个存储\u003cstrong\u003e结构化数据\u003c/strong\u003e的分布式系统。\u003c/p\u003e\n\n\u003cp\u003eBigtable 从 2005 年 4 月开始用于生产环境，而在此之前，我们花了大约 \u003cstrong\u003e7 个人年\u003c/strong\u003e\n（person-year）的时间在设计和实现上。到 2006 年 8 月，已经有超过 60 个项目在使用\nBigtable。\u003c/p\u003e\n\n\u003cp\u003e我们的用户很喜欢 Bigtable 提供的性能和高可用性，当集群面临的负载不断增加时\n，他们只需简单地向集群添加更多的节点就可以扩展 Bigtable 的容量。\u003c/p\u003e\n\n\u003cp\u003e考虑到 Bigtable 的接口不是太常规（unusual），一个有趣的问题就是，我们的用户需要\n花多长时间去适应 Bigtable。新用户有时不太确定如何使用 Bigtable 最合适，尤其是如\n果之前已经习惯了关系型数据库提供的广义事务。然后，很多 Google 产品成功地使用了\nBigtable 还是说明了，我们的设计在实际使用中还是非常不错的。\u003c/p\u003e\n\n\u003cp\u003e当前我们正在添加一些新的特性，例如支持 secondary indices，以及构建跨数据中心复制\n的、有多个 master 副本的 Bigtable。我们还在做的是将 Bigtable 作为一个服务提供给\n各产品组，以后每个组就不需要自己维护他们的集群。随着服务集群的扩展，我们将\n需要处理更多 Bigtable 内部的资源共享问题 [3, 5]。\u003c/p\u003e\n\n\u003cp\u003e最后，我们发现\u003cstrong\u003e构建我们自己的存储解决方案可以带来非常大的优势\u003c/strong\u003e。为 Bigtable 设\n计自己的数据模型已经给我们带来非常多的便利性。另外，我们\u003cstrong\u003e对 Bigtable 的实现，以\n及 Bigtable 所依赖的其他 Google 基础设施有足够的控制权\u003c/strong\u003e，因此任何一个地方有瓶颈\n了，我们都可以及时解决。\u003c/p\u003e\n\n\u003ch2 id=\"acknowledgements\"\u003eAcknowledgements\u003c/h2\u003e\n\n\u003cp\u003eWe thank the anonymous reviewers, David Nagle, and\nour shepherd Brad Calder, for their feedback on this paper.\nThe Bigtable system has benefited greatly from the\nfeedback of our many users within Google. In addition,\nwe thank the following people for their contributions to\nBigtable: Dan Aguayo, Sameer Ajmani, Zhifeng Chen,\nBill Coughran, Mike Epstein, Healfdene Goguen, Robert\nGriesemer, Jeremy Hylton, Josh Hyman, Alex Khesin,\nJoanna Kulik, Alberto Lerner, Sherry Listgarten, Mike\nMaloney, Eduardo Pinheiro, Kathy Polizzi, Frank Yellin,\nand Arthur Zwiegincew.\u003c/p\u003e\n\n\u003ch2 id=\"参考文献\"\u003e参考文献\u003c/h2\u003e\n\n\u003col\u003e\n  \u003cli\u003eABADI, et al. Integrating compression and execution in columnoriented database systems. SIGMOD 2006\u003c/li\u003e\n  \u003cli\u003eAILAMAKI, et al. Weaving relations for cache performance. In The VLDB Journal 2001\u003c/li\u003e\n  \u003cli\u003eBANGA, et al. Resource containers: A new facility for resource management in server systems. OSDI 1999\u003c/li\u003e\n  \u003cli\u003eBARU, et al. DB2 parallel edition. IBM Systems Journal 1995\u003c/li\u003e\n  \u003cli\u003eBAVIER, et al. Operating system support for planetary-scale network services. NSDI 2004\u003c/li\u003e\n  \u003cli\u003eBENTLEY, et al. Data compression using long common strings. In Data Compression Conference 1999\u003c/li\u003e\n  \u003cli\u003eBLOOM, et al. Space/time trade-offs in hash coding with allowable errors. CACM 1970\u003c/li\u003e\n  \u003cli\u003eBURROWS, M. \u003cstrong\u003eThe Chubby lock service for loosely coupled distributed systems\u003c/strong\u003e. OSDI 2006\u003c/li\u003e\n  \u003cli\u003eCHANDRA, et al. Paxos made live An engineering perspective. PODC 2007\u003c/li\u003e\n  \u003cli\u003eCOMER, D. Ubiquitous B-tree. Computing Surveys, 1979\u003c/li\u003e\n  \u003cli\u003eCOPELAND, et al. Data placement in Bubba. SIGMOD (1988)\u003c/li\u003e\n  \u003cli\u003eDEAN, et al. \u003cstrong\u003eMapReduce: Simplified data processing on large clusters\u003c/strong\u003e. OSDI 2004\u003c/li\u003e\n  \u003cli\u003eDEWITT, et al. Implementation techniques for main memory database systems. SIGMOD 1984\u003c/li\u003e\n  \u003cli\u003eDEWITT, et al. Parallel database systems: The future of high performance database systems. CACM, 1992\u003c/li\u003e\n  \u003cli\u003eFRENCH, C. D. One size fits all database architectures do not work for DSS. SIGMOD 1995\u003c/li\u003e\n  \u003cli\u003eGAWLICK, D., AND KINKADE, D. Varieties of concurrency control in IMS/VS fast path. Database Engineering Bulletin, 1985\u003c/li\u003e\n  \u003cli\u003eGHEMAWAT, et al. \u003cstrong\u003eThe Google file system\u003c/strong\u003e. SOSP, 2003\u003c/li\u003e\n  \u003cli\u003eGRAY, J. Notes on database operating systems. In Operating Systems - An Advanced Course, 1978\u003c/li\u003e\n  \u003cli\u003eGREER, R. Daytona and the fourth-generation language Cymbal. SIGMOD, 1999\u003c/li\u003e\n  \u003cli\u003eHARTMAN, J. H., AND OUSTERHOUT, J. K. The Zebra striped network file system. SOSP, 1993\u003c/li\u003e\n  \u003cli\u003eKX.COM. kx.com/products/database.php. Product page.\u003c/li\u003e\n  \u003cli\u003eLAMPORT, L. The part-time parliament. ACM TOCS, 1998\u003c/li\u003e\n  \u003cli\u003eMACCORMICK, et al. Boxwood: Abstractions as the foundation for storage infrastructure. OSDI 2004\u003c/li\u003e\n  \u003cli\u003eMCCARTHY, J. Recursive functions of symbolic expressions and their computation by machine. CACM, 1960\u003c/li\u003e\n  \u003cli\u003eO’NEIL, et al. \u003cstrong\u003eThe log-structured merge-tree (LSM-tree)\u003c/strong\u003e. Acta Inf, 1996\u003c/li\u003e\n  \u003cli\u003eORACLE.COM. www.oracle.com/technology/products/- database/clustering/index.html. Product page.\u003c/li\u003e\n  \u003cli\u003ePIKE, et al. Interpreting the data: Parallel analysis with Sawzall. Scientific Programming Journal, 2005\u003c/li\u003e\n  \u003cli\u003eRATNASAMY, et al. A scalable content-addressable network. SIGCOMM 2001\u003c/li\u003e\n  \u003cli\u003eROWSTRON, et al: Scalable, distributed object location and routing for largescale peer-to-peer systems. Middleware 2001\u003c/li\u003e\n  \u003cli\u003eSENSAGE.COM. sensage.com/products-sensage.htm.  Product page.\u003c/li\u003e\n  \u003cli\u003eSTOICA, et al. Chord: A scalable peer-to-peer lookup service for Internet applications. SIGCOMM 2001\u003c/li\u003e\n  \u003cli\u003eSTONEBRAKER, M. The case for shared nothing. Database Engineering Bulletin, 1986\u003c/li\u003e\n  \u003cli\u003eSTONEBRAKER, et al. C-Store: A columnoriented DBMS. VLDB 2005\u003c/li\u003e\n  \u003cli\u003eSTONEBRAKER, et al. Mariposa: A new architecture for distributed data. the Tenth ICDE, 1994\u003c/li\u003e\n  \u003cli\u003eSYBASE.COM. www.sybase.com/products/databaseservers/ sybaseiq. Product page.\u003c/li\u003e\n  \u003cli\u003eZHAO, et al. Tapestry: An infrastructure for fault-tolerant wide-area location and routing, 2001\u003c/li\u003e\n  \u003cli\u003eZUKOWSKI, et al. MonetDB/X100 - A DBMS in the CPU cache. IEEE Data Eng, 2005\u003c/li\u003e\n\u003c/ol\u003e\n\n\n  \u003c!-- POST NAVIGATION --\u003e\n  \u003cdiv class=\"postNav clearfix\"\u003e\n     \n      \u003ca class=\"prev\" href=\"/blog/amazon-dynamo-zh/\"\u003e\u003cspan\u003e« [译] [论文] Dynamo: Amazon\u0026#39;s Highly Available Key-value Store（SOSP 2007）\u003c/span\u003e\n      \n    \u003c/a\u003e\n      \n      \n      \u003ca class=\"next\" href=\"/blog/ceph-osdi-zh/\"\u003e\u003cspan\u003e[译] [论文] Ceph: A Scalable, High-Performance Distributed File System (OSDI 2006) »\u003c/span\u003e\n       \n      \u003c/a\u003e\n     \n  \u003c/div\u003e\n\u003c/div\u003e",
  "Date": "2019-07-13T00:00:00Z",
  "Author": "Arthur Chiao"
}