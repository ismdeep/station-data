{
  "Source": "arthurchiao.art",
  "Title": "[译] 什么是 GPT？Transformer 工作原理的动画展示（2024）",
  "Link": "https://arthurchiao.art/blog/visual-intro-to-transformers-zh/",
  "Content": "\u003cdiv class=\"post\"\u003e\n  \n  \u003ch1 class=\"postTitle\"\u003e[译] 什么是 GPT？Transformer 工作原理的动画展示（2024）\u003c/h1\u003e\n  \u003cp class=\"meta\"\u003ePublished at 2024-05-12 | Last Update 2024-05-12\u003c/p\u003e\n  \n  \u003ch3 id=\"译者序\"\u003e译者序\u003c/h3\u003e\n\n\u003cp\u003e本文翻译自 2024 年的一个视频（前半部分），这是原作者 Deep Learning 系列的第 5 章，强烈推荐原视频：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eYoutube：\u003ca href=\"https://www.youtube.com/watch?v=wjZofJX0v4M\"\u003eBut what is a GPT? Visual intro to transformers\u003c/a\u003e；\u003c/li\u003e\n  \u003cli\u003eB 站：\u003ca href=\"https://www.bilibili.com/video/BV13z421U7cs\"\u003e官方搬运\u003c/a\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/visual-intro-to-transformers/transformer-modules.gif\" width=\"100%\" height=\"100%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eTransformer 预测下一个单词四部曲。MLP 也称为 feed-forward。\u003c/p\u003e\n\n\u003cp\u003e作者以深厚的技术积累，将一些复杂系统以可视化的方式讲给普通人，这种能力是极其难得的。\n本译文希望通过“文字+动图”这种可视化又方便随时停下来思考的方式介绍 Transformer 的内部工作原理。\n如果想进一步从技术和实现上了解 Transformer/GPT/LLM，可参考：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003ca href=\"/blog/gpt-as-a-finite-state-markov-chain-zh/\"\u003eGPT 是如何工作的：200 行 Python 代码实现一个极简 GPT（2023）\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"/blog/transformers-from-scratch-zh/\"\u003eTransformer 是如何工作的：600 行 Python 代码实现 self-attention 和两类 Transformer（2019）\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"/blog/instructgpt-paper-zh/\"\u003eInstructGPT：基于人类反馈训练语言模型遵从指令的能力（OpenAI，2022）\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"/blog/llm-practical-guide-zh/\"\u003e大语言模型（LLM）综述与实用指南（Amazon，2023）\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"/blog/how-to-train-a-gpt-assistant-zh/\"\u003e如何训练一个企业级 GPT 助手（OpenAI，2023）\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e水平及维护精力所限，译文不免存在错误或过时之处，如有疑问，请查阅原视频。\n\u003cstrong\u003e\u003cmark\u003e传播知识，尊重劳动，年满十八周岁，转载请注明\u003ca href=\"https://arthurchiao.art\"\u003e出处\u003c/a\u003e\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e以下是译文。\u003c/p\u003e\n\n\u003chr/\u003e\n\n\u003cul id=\"markdown-toc\"\u003e\n  \u003cli\u003e\u003ca href=\"#译者序\" id=\"markdown-toc-译者序\"\u003e译者序\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#1-图解-generative-pre-trained-transformergpt\" id=\"markdown-toc-1-图解-generative-pre-trained-transformergpt\"\u003e1 图解 “Generative Pre-trained Transformer”（GPT）\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#11-generative生成式\" id=\"markdown-toc-11-generative生成式\"\u003e1.1 Generative：生成式\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#111-可视化\" id=\"markdown-toc-111-可视化\"\u003e1.1.1 可视化\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#112-生成式-vs-判别式译注\" id=\"markdown-toc-112-生成式-vs-判别式译注\"\u003e1.1.2 生成式 vs. 判别式（译注）\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#12-pre-trained预训练\" id=\"markdown-toc-12-pre-trained预训练\"\u003e1.2 Pre-trained：预训练\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#121-可视化\" id=\"markdown-toc-121-可视化\"\u003e1.2.1 可视化\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#122-预训练-vs-增量训练微调\" id=\"markdown-toc-122-预训练-vs-增量训练微调\"\u003e1.2.2 预训练 vs. 增量训练（微调）\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#13-transformer一类神经网络架构\" id=\"markdown-toc-13-transformer一类神经网络架构\"\u003e1.3 Transformer：一类神经网络架构\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#14-小结\" id=\"markdown-toc-14-小结\"\u003e1.4 小结\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#2-transformer-起源与应用\" id=\"markdown-toc-2-transformer-起源与应用\"\u003e2 Transformer 起源与应用\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#21-attention-is-all-you-need-google-2017机器翻译\" id=\"markdown-toc-21-attention-is-all-you-need-google-2017机器翻译\"\u003e2.1 Attention Is All You Need, Google, 2017，机器翻译\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#22-generative-transformer\" id=\"markdown-toc-22-generative-transformer\"\u003e2.2 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eGenerative\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e Transformer\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#23-gpt-2gpt-3-生成效果文本续写预览\" id=\"markdown-toc-23-gpt-2gpt-3-生成效果文本续写预览\"\u003e2.3 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eGPT-2/GPT-3\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 生成效果（文本续写）预览\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#24-chatgpt-等交互式大模型\" id=\"markdown-toc-24-chatgpt-等交互式大模型\"\u003e2.4 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eChatGPT\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 等交互式大模型\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#25-小结\" id=\"markdown-toc-25-小结\"\u003e2.5 小结\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#3-transformer-数据处理四部曲\" id=\"markdown-toc-3-transformer-数据处理四部曲\"\u003e3 Transformer 数据处理四部曲\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#31-embedding分词与向量表示\" id=\"markdown-toc-31-embedding分词与向量表示\"\u003e3.1 Embedding：分词与向量表示\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#311-token-的向量表示\" id=\"markdown-toc-311-token-的向量表示\"\u003e3.1.1 token 的向量表示\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#312-向量表示的直观解释\" id=\"markdown-toc-312-向量表示的直观解释\"\u003e3.1.2 向量表示的直观解释\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#32-attentionembedding-向量间的语义交流\" id=\"markdown-toc-32-attentionembedding-向量间的语义交流\"\u003e3.2 Attention：embedding 向量间的语义交流\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#321-语义交流\" id=\"markdown-toc-321-语义交流\"\u003e3.2.1 语义交流\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#322-例子machine-learning-model--fashion-model\" id=\"markdown-toc-322-例子machine-learning-model--fashion-model\"\u003e3.2.2 例子：”machine learning \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003emodel\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e” / “fashion \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003emodel\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e”\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#33-feed-forward--mlp向量之间无交流\" id=\"markdown-toc-33-feed-forward--mlp向量之间无交流\"\u003e3.3 Feed-forward / MLP：向量之间无交流\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#331-针对所有向量做一次性变换\" id=\"markdown-toc-331-针对所有向量做一次性变换\"\u003e3.3.1 针对所有向量做一次性变换\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#332-直观解释\" id=\"markdown-toc-332-直观解释\"\u003e3.3.2 直观解释\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#333-重复-attention--feed-forward-模块组成多层网络\" id=\"markdown-toc-333-重复-attention--feed-forward-模块组成多层网络\"\u003e3.3.3 重复 Attention + Feed-forward 模块，组成多层网络\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#34-unembedding概率\" id=\"markdown-toc-34-unembedding概率\"\u003e3.4 Unembedding：概率\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#341-最后一层-feed-forward-输出中的最后一个向量\" id=\"markdown-toc-341-最后一层-feed-forward-输出中的最后一个向量\"\u003e3.4.1 最后一层 feed-forward 输出中的最后一个向量\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#342-下一个单词的选择\" id=\"markdown-toc-342-下一个单词的选择\"\u003e3.4.2 下一个单词的选择\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#35-小结\" id=\"markdown-toc-35-小结\"\u003e3.5 小结\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#4-gpt---chatgpt从文本补全到交互式聊天助手\" id=\"markdown-toc-4-gpt---chatgpt从文本补全到交互式聊天助手\"\u003e4 GPT -\u0026gt; ChatGPT：从文本补全到交互式聊天助手\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#41-系统提示词伪装成聊天\" id=\"markdown-toc-41-系统提示词伪装成聊天\"\u003e4.1 系统提示词，伪装成聊天\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#42-如何训练一个企业级-gpt-助手译注\" id=\"markdown-toc-42-如何训练一个企业级-gpt-助手译注\"\u003e4.2 如何训练一个企业级 GPT 助手（译注）\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#5-总结\" id=\"markdown-toc-5-总结\"\u003e5 总结\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003chr/\u003e\n\n\u003ch1 id=\"1-图解-generative-pre-trained-transformergpt\"\u003e1 图解 “Generative Pre-trained Transformer”（GPT）\u003c/h1\u003e\n\n\u003cp\u003eGPT 是 Generative Pre-trained Transformer 的缩写，直译为“生成式预训练 transformer”，\n我们先从字面上解释一下它们分别是什么意思。\u003c/p\u003e\n\n\u003ch2 id=\"11-generative生成式\"\u003e1.1 Generative：生成式\u003c/h2\u003e\n\n\u003cp\u003e“Generative”（\u003cstrong\u003e\u003cmark\u003e生成式\u003c/mark\u003e\u003c/strong\u003e）意思很直白，就是给定一段输入（例如，最常见的文本输入），\n模型就能\u003cstrong\u003e\u003cmark\u003e续写\u003c/mark\u003e\u003c/strong\u003e（“编”）下去。\u003c/p\u003e\n\n\u003ch3 id=\"111-可视化\"\u003e1.1.1 可视化\u003c/h3\u003e\n\n\u003cp\u003e下面是个例子，给定 \u003cmark\u003e“The most effective way to learn computer science is”\u003c/mark\u003e 作为输入，\n模型就开始续写后面的内容了。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/visual-intro-to-transformers/generative-meaning.gif\" width=\"90%\" height=\"90%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e“Generative”：生成（续写）文本的能力。\u003c/p\u003e\n\n\u003ch3 id=\"112-生成式-vs-判别式译注\"\u003e1.1.2 生成式 vs. 判别式（译注）\u003c/h3\u003e\n\n\u003cp\u003e文本续写这种生成式模型，区别于 BERT 那种\u003cstrong\u003e\u003cmark\u003e判别式\u003c/mark\u003e\u003c/strong\u003e模型（用于分类、完形填空等等），\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003ca href=\"/blog/bert-paper-zh/\"\u003e\u003cmark\u003eBERT：预训练深度双向 Transformers 做语言理解\u003c/mark\u003e（Google，2019）\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 id=\"12-pre-trained预训练\"\u003e1.2 Pre-trained：预训练\u003c/h2\u003e\n\n\u003cp\u003e“Pre-trained”（预训练）指的是模型是\u003cstrong\u003e\u003cmark\u003e用大量数据训练出来的\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003ch3 id=\"121-可视化\"\u003e1.2.1 可视化\u003c/h3\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/visual-intro-to-transformers/pre-trained-meaning.gif\" width=\"90%\" height=\"90%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e“Pre-trained”：用大量数据进行训练。\u003cbr/\u003e\n图中的大量旋钮/仪表盘就是所谓的\u003cmark\u003e“模型参数”\u003c/mark\u003e，训练过程就是在不断优化这些参数，后面会详细介绍。\u003c/p\u003e\n\n\u003ch3 id=\"122-预训练-vs-增量训练微调\"\u003e1.2.2 预训练 vs. 增量训练（微调）\u003c/h3\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cmark\u003e“预”\u003c/mark\u003e\u003c/strong\u003e这个字也暗示了模型还有在特定任务中\u003cstrong\u003e\u003cmark\u003e进一步训练\u003c/mark\u003e\u003c/strong\u003e的可能 ——\n也就是我们常说的\u003cstrong\u003e\u003cmark\u003e“微调”\u003c/mark\u003e\u003c/strong\u003e（finetuning）。\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e如何对预训练模型进行微调：\n\u003ca href=\"/blog/instructgpt-paper-zh/\"\u003eInstructGPT：基于人类反馈训练语言模型遵从指令的能力（OpenAI，2022）\u003c/a\u003e。\n译注。\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch2 id=\"13-transformer一类神经网络架构\"\u003e1.3 Transformer：一类神经网络架构\u003c/h2\u003e\n\n\u003cp\u003e“GPT” 三个词中最重要的其实是最后一个词 Transformer。\nTransformer 是一类\u003cstrong\u003e\u003cmark\u003e神经网络\u003c/mark\u003e\u003c/strong\u003e/机器学习模型，作为近期 AI 领域的核心创新，\n推动着这个领域近几年的极速发展。\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eTransformer 直译为\u003cstrong\u003e\u003cmark\u003e“变换器”\u003c/mark\u003e\u003c/strong\u003e或“转换器”，通过数学运算不断对输入数据进行变换/转换。另外，变压器、变形金刚也是这个词。\n译注。\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/visual-intro-to-transformers/transformer-detailed-1.gif\" width=\"90%\" height=\"90%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eTransformer：一类神经网络架构的统称。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/visual-intro-to-transformers/transformer-detailed-2.gif\" width=\"90%\" height=\"90%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eTransformer 最后的输出层。后面还会详细介绍\u003c/p\u003e\n\n\u003ch2 id=\"14-小结\"\u003e1.4 小结\u003c/h2\u003e\n\n\u003cp\u003e如今已经可以基于 Transformer 构建许多不同类型的模型，不限于文本，例如，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e语音转文字\u003c/li\u003e\n  \u003cli\u003e文字转语音\u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003e文生图（text-to-image）：DALL·E、MidJourney 等在 2022 年风靡全球的工具，都是基于 Transformer。\u003c/p\u003e\n\n    \u003cp\u003e\u003ca href=\"/blog/rise-of-diffusion-based-models-zh/\"\u003e文生图（text-to-image）简史：扩散模型（diffusion models）的崛起与发展（2022）\u003c/a\u003e\u003c/p\u003e\n\n    \u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/visual-intro-to-transformers/dalle-pi.gif\" width=\"85%\" height=\"85%\"/\u003e\u003c/p\u003e\n    \u003cp align=\"center\"\u003e虽然无法让模型真正理解 \u0026#34;物种 π\u0026#34;是什么（本来就是瞎编的），但它竟然能生成出来，而且效果很惊艳。\u003c/p\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e本文希望通过“文字+动图”这种可视化又方便随时停下来思考的方式，解释 Transformer 的内部工作原理。\u003c/p\u003e\n\n\u003ch1 id=\"2-transformer-起源与应用\"\u003e2 Transformer 起源与应用\u003c/h1\u003e\n\n\u003ch2 id=\"21-attention-is-all-you-need-google-2017机器翻译\"\u003e2.1 Attention Is All You Need, Google, 2017，机器翻译\u003c/h2\u003e\n\n\u003cp\u003eTransformer 是 Google 2017 年在 \u003ca href=\"https://arxiv.org/abs/1706.03762\"\u003eAttention Is All You Need\u003c/a\u003e paper 中提出的，\n当时主要用于\u003cstrong\u003e\u003cmark\u003e文本翻译\u003c/mark\u003e\u003c/strong\u003e：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/visual-intro-to-transformers/machine-translation.gif\" width=\"85%\" height=\"85%\"/\u003e\u003c/p\u003e\n\n\u003ch2 id=\"22-generative-transformer\"\u003e2.2 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eGenerative\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e Transformer\u003c/h2\u003e\n\n\u003cp\u003e之后，Transformer 的应用场景扩展到了多个领域，例如 ChatGPT 背后也是 Transformer，\n这种 Transformer 接受一段文本（或图像/音频）作为输入，然后就能预测接下来的内容。\n以预测下一个单词为例，如下图所示，下一个单词有多种可能，各自的概率也不一样：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/visual-intro-to-transformers/generative-transformer.gif\" width=\"85%\" height=\"85%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e但有了一个这样的\u003cstrong\u003e\u003cmark\u003e预测下一个单词\u003c/mark\u003e\u003c/strong\u003e模型，就能通过如下步骤让它生成更长的文字，非常简单：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e将\u003cstrong\u003e\u003cmark\u003e初始文本\u003c/mark\u003e\u003c/strong\u003e输入模型；\u003c/li\u003e\n  \u003cli\u003e模型\u003cstrong\u003e\u003cmark\u003e预测出下一个可能的单词列表及其概率\u003c/mark\u003e\u003c/strong\u003e，然后通过某种算法（不一定挑概率最大的）\n  从中选一个作为下一个单词，这个过程称为\u003cstrong\u003e\u003cmark\u003e采样\u003c/mark\u003e\u003c/strong\u003e（sampling）；\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e将新单词追加到文本结尾\u003c/mark\u003e\u003c/strong\u003e，然后将整个文本\u003cstrong\u003e\u003cmark\u003e再次输入模型\u003c/mark\u003e\u003c/strong\u003e；转 2；\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e以上 step 2 \u0026amp; 3 不断重复，得到的句子就越来越长。\u003c/p\u003e\n\n\u003ch2 id=\"23-gpt-2gpt-3-生成效果文本续写预览\"\u003e2.3 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eGPT-2/GPT-3\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 生成效果（文本续写）预览\u003c/h2\u003e\n\n\u003cp\u003e来看看生成的效果，这里拿 GPT-2 和 GPT-3 作为例子。\u003c/p\u003e\n\n\u003cp\u003e下面是在我的笔记本电脑上运行 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eGPT-2\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e，不断预测与采样，逐渐补全为一个故事。\n但\u003cstrong\u003e\u003cmark\u003e结果比较差\u003c/mark\u003e\u003c/strong\u003e，生成的故事基本上没什么逻辑可言：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/visual-intro-to-transformers/gpt2-output-1.gif\" width=\"75%\" height=\"75%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e下面是换成 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eGPT-3\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e（模型不再开源，所以是通过 API），\nGPT-3 和 GPT-2 基本架构一样，只是规模更大，\n但\u003cstrong\u003e\u003cmark\u003e效果突然变得非常好\u003c/mark\u003e\u003c/strong\u003e，\n生成的故事不仅合乎逻辑，甚至还暗示 “物种 π” 居住在一个数学和计算王国：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/visual-intro-to-transformers/gpt3-output-1.gif\" width=\"75%\" height=\"75%\"/\u003e\u003c/p\u003e\n\n\u003ch2 id=\"24-chatgpt-等交互式大模型\"\u003e2.4 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eChatGPT\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 等交互式大模型\u003c/h2\u003e\n\n\u003cp\u003e以上这个不断重复“预测+选取”来生成文本的过程，就是 ChatGPT 或其他类似大语言模型（LLM）\n的底层工作原理 —— \u003cstrong\u003e\u003cmark\u003e逐单词（token）生成文本\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003ch2 id=\"25-小结\"\u003e2.5 小结\u003c/h2\u003e\n\n\u003cp\u003e以上是对 GPT 及其背后的 Transformer 的一个感性认识。接下来我们就深入到 Transformer 内部，\n看看它是如何根据给定输入来预测（计算）出下一个单词的。\u003c/p\u003e\n\n\u003ch1 id=\"3-transformer-数据处理四部曲\"\u003e3 Transformer 数据处理四部曲\u003c/h1\u003e\n\n\u003cp\u003e为理解 Transformer 的内部工作原理，本节从端到端（从最初的用户输入，到最终的模型输出）的角度看看数据是如何在 Transformer 中流动的。\n从宏观来看，输入数据在 Transformer 中经历如下四个处理阶段：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/visual-intro-to-transformers/transformer-modules.gif\" width=\"100%\" height=\"100%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eTransformer 数据处理四部曲\u003c/p\u003e\n\n\u003cp\u003e下面分别来看。\u003c/p\u003e\n\n\u003ch2 id=\"31-embedding分词与向量表示\"\u003e3.1 Embedding：分词与向量表示\u003c/h2\u003e\n\n\u003cp\u003e首先，输入内容会被拆分成许多小片段（这个过程称为 tokenization），这些小片段称为 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003etoken\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e对于文本：token 通常是单词、词根、标点符号，或者其他常见的字符组合；\u003c/li\u003e\n  \u003cli\u003e对于图片：token 可能是一小块像素区域；\u003c/li\u003e\n  \u003cli\u003e对于音频：token 可能是一小段声音。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e然后，将每个 token 用一个\u003cstrong\u003e\u003cmark\u003e向量（一维数组）\u003c/mark\u003e\u003c/strong\u003e来表示。\u003c/p\u003e\n\n\u003ch3 id=\"311-token-的向量表示\"\u003e3.1.1 token 的向量表示\u003c/h3\u003e\n\n\u003cp\u003e这实际上是以某种方式在\u003cstrong\u003e\u003cmark\u003e编码\u003c/mark\u003e\u003c/strong\u003e该 token；\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/visual-intro-to-transformers/embedding-1.gif\" width=\"80%\" height=\"80%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eEmbedding：每个 token 对应一个 \u003ccode\u003eN*1\u003c/code\u003e 维度的数值格式表示的向量。\u003c/p\u003e\n\n\u003ch3 id=\"312-向量表示的直观解释\"\u003e3.1.2 向量表示的直观解释\u003c/h3\u003e\n\n\u003cp\u003e如果把这些向量看作是在\u003cstrong\u003e\u003cmark\u003e高维空间中的坐标\u003c/mark\u003e\u003c/strong\u003e， 那么含义相似的单词在这个高维空间中是\u003cstrong\u003e\u003cmark\u003e相邻的\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/visual-intro-to-transformers/embedding-2.gif\" width=\"80%\" height=\"80%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e词义相近的四个单词 “leap/jump/skip/hop” 在向量空间中是相邻的\u003c/p\u003e\n\n\u003cp\u003e将输入进行 tokenization 并转成向量表示之后，输入就从一个句子就变成了一个\u003cstrong\u003e\u003cmark\u003e向量序列\u003c/mark\u003e\u003c/strong\u003e。\n接下来，这个向量序列会进行一个称为 attention 的运算。\u003c/p\u003e\n\n\u003ch2 id=\"32-attentionembedding-向量间的语义交流\"\u003e3.2 Attention：embedding 向量间的语义交流\u003c/h2\u003e\n\n\u003ch3 id=\"321-语义交流\"\u003e3.2.1 语义交流\u003c/h3\u003e\n\n\u003cp\u003eattention 使得\u003cstrong\u003e\u003cmark\u003e向量之间能够相互“交流”信息\u003c/mark\u003e\u003c/strong\u003e。这个交流是双向的，在这个过程中，每个向量都会更新自身的值。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/visual-intro-to-transformers/attention-1.gif\" width=\"80%\" height=\"80%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e这种信息“交流”是有上下文和语义理解能力的。\u003c/p\u003e\n\n\u003ch3 id=\"322-例子machine-learning-model--fashion-model\"\u003e3.2.2 例子：”machine learning \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003emodel\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e” / “fashion \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003emodel\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e”\u003c/h3\u003e\n\n\u003cp\u003e例如，“model” 这个词在 “machine learning model”（机器学习模型）和在 “fashion model”（时尚模特）中的意思就完全不一样，\n因此\u003cstrong\u003e\u003cmark\u003e虽然是同一个单词（token），但对应的 embedding 向量是不同的\u003c/mark\u003e\u003c/strong\u003e，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/visual-intro-to-transformers/attention-2.gif\" width=\"80%\" height=\"80%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eAttention 模块的作用就是确定上下文中哪些词之间有语义关系，以及如何准确地理解这些含义（更新相应的向量）。\n这里说的“含义”（meaning），指的是编码在向量中的信息。\u003c/p\u003e\n\n\u003ch2 id=\"33-feed-forward--mlp向量之间无交流\"\u003e3.3 Feed-forward / MLP：向量之间无交流\u003c/h2\u003e\n\n\u003cp\u003eAttention 模块让输入向量们彼此充分交换了信息（例如，单词 “model” 指的应该是“模特”还是“模型”），\n然后，这些向量会进入第三个处理阶段：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/visual-intro-to-transformers/mlp-1.gif\" width=\"80%\" height=\"80%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e第三阶段：多层感知机（\u003cmark\u003e\u003ccode\u003emulti-layer perceptron\u003c/code\u003e\u003c/mark\u003e），也称为前馈层（\u003cmark\u003e\u003ccode\u003efeed-forward layer\u003c/code\u003e\u003c/mark\u003e）。\u003c/p\u003e\n\n\u003ch3 id=\"331-针对所有向量做一次性变换\"\u003e3.3.1 针对所有向量做一次性变换\u003c/h3\u003e\n\n\u003cp\u003e这个阶段，向量之间没有互相“交流”，而是并行地经历同一处理：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/visual-intro-to-transformers/mlp-2.gif\" width=\"80%\" height=\"80%\"/\u003e\u003c/p\u003e\n\n\u003ch3 id=\"332-直观解释\"\u003e3.3.2 直观解释\u003c/h3\u003e\n\n\u003cp\u003e后面会看，从直观上来说，这个步骤有点像\u003cstrong\u003e\u003cmark\u003e对每个向量都提出一组同样的问题\u003c/mark\u003e\u003c/strong\u003e，然后\u003cstrong\u003e\u003cmark\u003e根据得到的回答来更新对应的向量\u003c/mark\u003e\u003c/strong\u003e：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/visual-intro-to-transformers/mlp-3.gif\" width=\"80%\" height=\"80%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e以上解释中省略了归一化等一些中间步骤，但已经可以看出：\nattention 和 feed-forward 本质上都是\u003cstrong\u003e\u003cmark\u003e大量的矩阵乘法\u003c/mark\u003e\u003c/strong\u003e，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/visual-intro-to-transformers/matmul-1.gif\" width=\"80%\" height=\"80%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e本文的一个目的就是让读者理解这些矩阵乘法的直观意义。\u003c/p\u003e\n\n\u003ch3 id=\"333-重复-attention--feed-forward-模块组成多层网络\"\u003e3.3.3 重复 Attention + Feed-forward 模块，组成多层网络\u003c/h3\u003e\n\n\u003cp\u003eTransformer 基本上是不断复制 Attention 和 Feed-forward 这两个基本结构，\n这两个模块的组合成为神经网络的一层。在每一层，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/visual-intro-to-transformers/repeat-blocks.gif\" width=\"100%\" height=\"100%\"/\u003e\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e输入向量通过 attention 更新彼此；\u003c/li\u003e\n  \u003cli\u003efeed-forward 模块将这些更新之后的向量做统一变换，得到这一层的输出向量；\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 id=\"34-unembedding概率\"\u003e3.4 Unembedding：概率\u003c/h2\u003e\n\n\u003ch3 id=\"341-最后一层-feed-forward-输出中的最后一个向量\"\u003e3.4.1 最后一层 feed-forward 输出中的最后一个向量\u003c/h3\u003e\n\n\u003cp\u003e如果一切顺利，最后一层 feed-forward 输出中的最后一个向量（the very last vector in the sequence），\n就已经包含了句子的核心意义（essential meaning of the passage）。对这个向量进行 unembedding 操作（也是一次性矩阵运算），\n得到的就是下一个单词的备选列表及其概率：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/visual-intro-to-transformers/last-vector.gif\" width=\"80%\" height=\"80%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e图：原始输入为 \u0026#34;To date, the cleverest thinker of all time was\u0026#34;，让模型预测下一个 token。经过多层 attention + feed-forward 之后，\n最后一层输出的\u003cmark\u003e最后一个向量已经学习到了输入句子表达的意思\u003c/mark\u003e，（经过简单转换之后）就能作为\u003cmark\u003e下一个单词的概率\u003c/mark\u003e。\u003c/p\u003e\n\n\u003ch3 id=\"342-下一个单词的选择\"\u003e3.4.2 下一个单词的选择\u003c/h3\u003e\n\n\u003cp\u003e根据一定的规则选择一个 token，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e注意这里不一定选概率最大的，根据工程经验，一直选概率最大的，生成的文本会比较呆板；\u003c/li\u003e\n  \u003cli\u003e实际上由一个称为 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003etemperature\u003c/code\u003e 的参数控制；\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 id=\"35-小结\"\u003e3.5 小结\u003c/h2\u003e\n\n\u003cp\u003e以上就是 Transformer 内部的工作原理。\u003c/p\u003e\n\n\u003cp\u003e前面已经提到，有了一个这样的\u003cstrong\u003e\u003cmark\u003e预测下一个单词\u003c/mark\u003e\u003c/strong\u003e模型，就能通过如下步骤让它生成更长的文字，非常简单：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e将\u003cstrong\u003e\u003cmark\u003e初始文本\u003c/mark\u003e\u003c/strong\u003e输入模型；\u003c/li\u003e\n  \u003cli\u003e模型\u003cstrong\u003e\u003cmark\u003e预测出下一个可能的单词列表及其概率\u003c/mark\u003e\u003c/strong\u003e，然后通过某种算法（不一定挑概率最大的）\n  从中选一个作为下一个单词，这个过程称为\u003cstrong\u003e\u003cmark\u003e采样\u003c/mark\u003e\u003c/strong\u003e（sampling）；\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e将新单词追加到文本结尾\u003c/mark\u003e\u003c/strong\u003e，然后将整个文本\u003cstrong\u003e\u003cmark\u003e再次输入模型\u003c/mark\u003e\u003c/strong\u003e；转 2；\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch1 id=\"4-gpt---chatgpt从文本补全到交互式聊天助手\"\u003e4 GPT -\u0026gt; ChatGPT：从文本补全到交互式聊天助手\u003c/h1\u003e\n\n\u003cp\u003eGPT-3 的早期演示就是这样的：给 GPT-3 一段起始文本，它就自动补全（续写）故事和文章。\n这正式以上介绍的 Transformer 的基本也是核心功能。\u003c/p\u003e\n\n\u003cp\u003eChatGPT 的核心是 GPT 系列（GPT 3/3.5/4），但它怎么实现聊天这种工作方式的呢？\u003c/p\u003e\n\n\u003ch2 id=\"41-系统提示词伪装成聊天\"\u003e4.1 系统提示词，伪装成聊天\u003c/h2\u003e\n\n\u003cp\u003e其实很简单，将输入文本稍作整理，弄成聊天内容，然后把这样的文本再送到 GPT/Transformer，\n它就会把这个当前是聊天内容，续写下去。最后只需要把它续写的内容再抽出来返回给用户，\n对用户来说，就是在聊天。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/visual-intro-to-transformers/system-prompt.gif\" width=\"70%\" height=\"70%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e这段文本设定用户是在与一个 AI 助手交互的场景，这就是所谓的系统提示词（\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003esystem prompt\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e）。\u003c/p\u003e\n\n\u003ch2 id=\"42-如何训练一个企业级-gpt-助手译注\"\u003e4.2 如何训练一个企业级 GPT 助手（译注）\u003c/h2\u003e\n\n\u003cp\u003eOpenAI 官方对 GPT-\u0026gt;ChatGPT 有过专门分享：\u003ca href=\"/blog/how-to-train-a-gpt-assistant-zh/\"\u003e如何训练一个企业级 GPT 助手（OpenAI，2023）\u003c/a\u003e\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e\u003cstrong\u003e\u003cmark\u003e基础模型不是助手\u003c/mark\u003e\u003c/strong\u003e，它们\u003cstrong\u003e\u003cmark\u003e不想回答问题，只想补全文档\u003c/mark\u003e\u003c/strong\u003e。\n因此，如果让它们“写一首关于面包和奶酪的诗”，它们不仅不“听话”，反而会有样学样，列更多的任务出来，像下面左图这样，\u003c/p\u003e\n\n  \u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/how-to-train-a-gpt-assistant/base-model-not-assistant.png\" width=\"80%\" height=\"80%\"/\u003e\u003c/p\u003e\n\n  \u003cp\u003e这是因为它只是在\u003cstrong\u003e\u003cmark\u003e忠实地补全文档\u003c/mark\u003e\u003c/strong\u003e。\n但如果你能\u003cstrong\u003e\u003cmark\u003e成功地提示它\u003c/mark\u003e\u003c/strong\u003e，例如，\u003cstrong\u003e\u003cmark\u003e开头就说“这是一首关于面包和奶酪的诗”\u003c/mark\u003e\u003c/strong\u003e，\n那它接下来就会真的补全一首这样的诗出来，如右图。\u003c/p\u003e\n\n  \u003cp\u003e我们还可以通过 few-shot 来\u003cstrong\u003e\u003cmark\u003e进一步“欺骗”它\u003c/mark\u003e\u003c/strong\u003e。把你想问的问题整理成一个\u003cstrong\u003e\u003cmark\u003e“提问+回答”的文档格式\u003c/mark\u003e\u003c/strong\u003e，\n前面给一点正常的论述，然后突然来个问题，它以为自己还是在补全文档，其实已经把问题回答了：\u003c/p\u003e\n\n  \u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/how-to-train-a-gpt-assistant/base-model-not-assistant-2.png\" width=\"80%\" height=\"80%\"/\u003e\u003c/p\u003e\n\n  \u003cp\u003e这就是把基础模型\u003cstrong\u003e\u003cmark\u003e调教成一个 AI 助手\u003c/mark\u003e\u003c/strong\u003e的过程。\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch1 id=\"5-总结\"\u003e5 总结\u003c/h1\u003e\n\n\u003cp\u003e本文整理翻译了原视频的前半部分，通过可视化方式解释 GPT/Transformer 的内部工作原理。\n原视频后面的部分是关于 general deep learning, machine learning 等等的基础，想继续学习的，强烈推荐。\u003c/p\u003e\n\n\u003chr/\u003e\n\n\u003cp\u003e\u003ca href=\"https://notbyai.fyi\"\u003e\u003cimg src=\"/assets/img/Written-By-Human-Not-By-AI-Badge-white.svg\" alt=\"Written by Human, Not by AI\"/\u003e\u003c/a\u003e\n\u003ca href=\"https://notbyai.fyi\"\u003e\u003cimg src=\"/assets/img/Written-By-Human-Not-By-AI-Badge-black.svg\" alt=\"Written by Human, Not by AI\"/\u003e\u003c/a\u003e\u003c/p\u003e\n\n\n  \u003c!-- POST NAVIGATION --\u003e\n  \u003cdiv class=\"postNav clearfix\"\u003e\n     \n      \u003ca class=\"prev\" href=\"/blog/meta-ai-infra-zh/\"\u003e\u003cspan\u003e« [译] Meta/Facebook 超大规模 AI/GPU 基础设施设计（2024）\u003c/span\u003e\n      \n    \u003c/a\u003e\n      \n      \n      \u003ca class=\"next\" href=\"/blog/practical-storage-hierarchy/\"\u003e\u003cspan\u003ePractical Storage Hierarchy and Performance: From HDDs to On-chip Caches（2024） »\u003c/span\u003e\n       \n      \u003c/a\u003e\n     \n  \u003c/div\u003e\n\u003c/div\u003e",
  "Date": "2024-05-12T00:00:00Z",
  "Author": "Arthur Chiao"
}