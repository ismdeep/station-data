{
  "Source": "arthurchiao.art",
  "Title": "[译] 如何训练一个企业级 GPT 助手（OpenAI，2023）",
  "Link": "https://arthurchiao.art/blog/how-to-train-a-gpt-assistant-zh/",
  "Content": "\u003cdiv class=\"post\"\u003e\n  \n  \u003ch1 class=\"postTitle\"\u003e[译] 如何训练一个企业级 GPT 助手（OpenAI，2023）\u003c/h1\u003e\n  \u003cp class=\"meta\"\u003ePublished at 2023-09-01 | Last Update 2023-09-01\u003c/p\u003e\n  \n  \u003ch3 id=\"译者序\"\u003e译者序\u003c/h3\u003e\n\n\u003cp\u003e本文来自 OpenAI 的 Andrej Karpathy 在 Microsoft Build 2023 大会的分享：\n\u003ca href=\"https://build.microsoft.com/en-US/sessions/db3f4859-cd30-4445-a0cd-553c3304f8e2\"\u003eState of GPT\u003c/a\u003e。\n原分享包括两部分，\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e如何训练一个 GPT 助手；\u003c/li\u003e\n  \u003cli\u003e如何有效地将这些助手 apply 到应用程序中。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e本文翻译了其中第一部分。\n作者之前还有一篇“如何训练一个乞丐级 GPT”：\n\u003ca href=\"/blog/gpt-as-a-finite-state-markov-chain-zh/\"\u003eGPT 是如何工作的：200 行 Python 代码实现一个极简 GPT（2023）\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e译者水平有限，不免存在遗漏或错误之处。如有疑问，敬请查阅原文。\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003e以下是译文。\u003c/p\u003e\n\n\u003chr/\u003e\n\n\u003cul id=\"markdown-toc\"\u003e\n  \u003cli\u003e\u003ca href=\"#译者序\" id=\"markdown-toc-译者序\"\u003e译者序\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#0-引言\" id=\"markdown-toc-0-引言\"\u003e0 引言\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#1-预训练\" id=\"markdown-toc-1-预训练\"\u003e1 预训练\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#11-数据集\" id=\"markdown-toc-11-数据集\"\u003e1.1 数据集\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#12-文本-token-化\" id=\"markdown-toc-12-文本-token-化\"\u003e1.2 文本 token 化\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#13-超参数gpt-3-vs-llama\" id=\"markdown-toc-13-超参数gpt-3-vs-llama\"\u003e1.3 超参数：GPT-3 vs. LLaMA\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#131-词汇表大小上下文长度参数数量\" id=\"markdown-toc-131-词汇表大小上下文长度参数数量\"\u003e1.3.1 词汇表大小、上下文长度、参数数量\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#132-硬件环境和成本\" id=\"markdown-toc-132-硬件环境和成本\"\u003e1.3.2 硬件环境和成本\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#14-开始训练\" id=\"markdown-toc-14-开始训练\"\u003e1.4 开始训练\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#141-根据-batch-size-和上下文长度-token-化输入文本\" id=\"markdown-toc-141-根据-batch-size-和上下文长度-token-化输入文本\"\u003e1.4.1 根据 batch size 和上下文长度 token 化输入文本\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#142-预测下一个-token\" id=\"markdown-toc-142-预测下一个-token\"\u003e1.4.2 预测下一个 token\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#143-损失函数\" id=\"markdown-toc-143-损失函数\"\u003e1.4.3 损失函数\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#15-基础模型的功能\" id=\"markdown-toc-15-基础模型的功能\"\u003e1.5 基础模型的功能\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#151-分类gpt-1\" id=\"markdown-toc-151-分类gpt-1\"\u003e1.5.1 分类（GPT-1）\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#152-提示工程--文档补全gpt-2\" id=\"markdown-toc-152-提示工程--文档补全gpt-2\"\u003e1.5.2 提示工程 + 文档补全（GPT-2）\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#16-基础模型不是助手\" id=\"markdown-toc-16-基础模型不是助手\"\u003e1.6 基础模型不是助手\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#161-大模型进化树\" id=\"markdown-toc-161-大模型进化树\"\u003e1.6.1 大模型进化树\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#162-文档提示效果不佳\" id=\"markdown-toc-162-文档提示效果不佳\"\u003e1.6.2 文档提示：效果不佳\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#2-监督微调sft\" id=\"markdown-toc-2-监督微调sft\"\u003e2 监督微调（SFT）\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#21-收集高质量人工标注数据\" id=\"markdown-toc-21-收集高质量人工标注数据\"\u003e2.1 收集高质量人工标注数据\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#22-sft-训练\" id=\"markdown-toc-22-sft-训练\"\u003e2.2 SFT 训练\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#3-奖励建模\" id=\"markdown-toc-3-奖励建模\"\u003e3 奖励建模\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#31-例子评估-chatgpt-编程的好坏\" id=\"markdown-toc-31-例子评估-chatgpt-编程的好坏\"\u003e3.1 例子：评估 ChatGPT 编程的好坏\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#32-奖励\" id=\"markdown-toc-32-奖励\"\u003e3.2 奖励\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#33-奖励模型的特点\" id=\"markdown-toc-33-奖励模型的特点\"\u003e3.3 奖励模型的特点\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#4-强化学习rlhf\" id=\"markdown-toc-4-强化学习rlhf\"\u003e4 强化学习（RLHF）\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#41-rlhf-训练\" id=\"markdown-toc-41-rlhf-训练\"\u003e4.1 RLHF 训练\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#42-为什么要使用-rlhf\" id=\"markdown-toc-42-为什么要使用-rlhf\"\u003e4.2 为什么要使用 RLHF？\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#43-模型的熵\" id=\"markdown-toc-43-模型的熵\"\u003e4.3 模型的熵\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#5-总结\" id=\"markdown-toc-5-总结\"\u003e5 总结\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003chr/\u003e\n\n\u003ch1 id=\"0-引言\"\u003e0 引言\u003c/h1\u003e\n\n\u003cp\u003e人工智能领域正在经历翻天覆地的变化，因此这里讲的只是到目前为止训练 GPT 助手的方法。\n如下图所示，大致分为\u003cstrong\u003e\u003cmark\u003e四个阶段\u003c/mark\u003e\u003c/strong\u003e（从左往右）：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/how-to-train-a-gpt-assistant/training-pipeline.png\" width=\"100%\" height=\"100%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e训练一个 GPT 助手的流程\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e预训练\u003c/mark\u003e\u003c/strong\u003e（pre-training）\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e监督微调\u003c/mark\u003e\u003c/strong\u003e（supervised fine tuning, SFT）\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e奖励建模\u003c/mark\u003e\u003c/strong\u003e（reward modeling）\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e强化学习\u003c/mark\u003e\u003c/strong\u003e（reinforcement learning）\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e每个阶段又分为\u003cstrong\u003e\u003cmark\u003e三个部分\u003c/mark\u003e\u003c/strong\u003e（从上到下）：\u003cstrong\u003e\u003cmark\u003e数据集\u003c/mark\u003e\u003c/strong\u003e、\u003cstrong\u003e\u003cmark\u003e算法\u003c/mark\u003e\u003c/strong\u003e和输出的\u003cstrong\u003e\u003cmark\u003e模型\u003c/mark\u003e\u003c/strong\u003e。\n另外，每个阶段分别有哪些代表性的模型、训练周期和成本、模型是否能独立部署等等，下面也做了个简单说明：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/how-to-train-a-gpt-assistant/training-pipeline-notes.png\" width=\"100%\" height=\"100%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e关于每个训练阶段的一些说明\u003c/p\u003e\n\n\u003cp\u003e下面分别介绍下每个阶段的工作。\u003c/p\u003e\n\n\u003ch1 id=\"1-预训练\"\u003e1 预训练\u003c/h1\u003e\n\n\u003cp\u003e这个阶段占了整个过程（四个阶段）\u003cstrong\u003e\u003cmark\u003e绝大部分算力\u003c/mark\u003e\u003c/strong\u003e，例如占据了 99% 的训练计算时间和浮点运算。\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e处理的是互联网规模的数据集，使用数千个 GPU 训练，\u003c/li\u003e\n  \u003cli\u003e可能需要数月的时间。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e其他三个阶段是\u003cstrong\u003e\u003cmark\u003e微调阶段\u003c/mark\u003e\u003c/strong\u003e，只需要使用\u003cstrong\u003e\u003cmark\u003e较少的 GPU 训练几个小时或几天\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e来看一下预训练阶段，如何产生一个基础模型。\u003c/p\u003e\n\n\u003ch2 id=\"11-数据集\"\u003e1.1 数据集\u003c/h2\u003e\n\n\u003cp\u003e首先需要收集大量的数据。例如，下面是 Meta \u003ca href=\"/blog/llama-paper-zh/\"\u003e训练 LLaMA 所用的数据集\u003c/a\u003e，\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd\u003e数据集\u003c/td\u003e\n      \u003ctd\u003e占比\u003c/td\u003e\n      \u003ctd\u003e迭代次数（Epochs）\u003c/td\u003e\n      \u003ctd\u003e数据集大小（Disk size）\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003eCommonCrawl\u003c/td\u003e\n      \u003ctd\u003e67.0%\u003c/td\u003e\n      \u003ctd\u003e1.10\u003c/td\u003e\n      \u003ctd\u003e3.3 TB\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003eC4\u003c/td\u003e\n      \u003ctd\u003e15.0%\u003c/td\u003e\n      \u003ctd\u003e1.06\u003c/td\u003e\n      \u003ctd\u003e783 GB\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003eGithub\u003c/td\u003e\n      \u003ctd\u003e4.5%\u003c/td\u003e\n      \u003ctd\u003e0.64\u003c/td\u003e\n      \u003ctd\u003e328 GB\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003eWikipedia\u003c/td\u003e\n      \u003ctd\u003e4.5%\u003c/td\u003e\n      \u003ctd\u003e2.45\u003c/td\u003e\n      \u003ctd\u003e83 GB\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003eBooks\u003c/td\u003e\n      \u003ctd\u003e4.5%\u003c/td\u003e\n      \u003ctd\u003e2.23\u003c/td\u003e\n      \u003ctd\u003e85 GB\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003eArXiv\u003c/td\u003e\n      \u003ctd\u003e2.5%\u003c/td\u003e\n      \u003ctd\u003e1.06\u003c/td\u003e\n      \u003ctd\u003e92 GB\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003eStackExchange\u003c/td\u003e\n      \u003ctd\u003e2.0%\u003c/td\u003e\n      \u003ctd\u003e1.03\u003c/td\u003e\n      \u003ctd\u003e78 GB\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp align=\"center\"\u003e表 1：\u003cmark\u003eLLaMA 预训练数据\u003c/mark\u003e。\u003cbr/\u003e\n其中 epochs 是用 1.4T tokens 预训练时的迭代次数。用 1T tokens 预训练时也是用的这个数据集比例。\n\u003c/p\u003e\n\n\u003cp\u003e可以大致看到这些数据集的类型。它们混合在一起，然后根据比例进行采样，得到 GPT 神经网络的训练集。\u003c/p\u003e\n\n\u003ch2 id=\"12-文本-token-化\"\u003e1.2 文本 token 化\u003c/h2\u003e\n\n\u003cp\u003e在实际训练这些数据之前，需要经过一个\u003cstrong\u003e\u003cmark\u003e预处理步骤\u003c/mark\u003e\u003c/strong\u003e，即 \u003cstrong\u003e\u003cmark\u003etoken 化\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\n    \u003cp\u003e将\u003cstrong\u003e\u003cmark\u003e原始文本\u003c/mark\u003e\u003c/strong\u003e翻译成\u003cstrong\u003e\u003cmark\u003e整数序列\u003c/mark\u003e\u003c/strong\u003e，后者是 GPT 的表示方式。\u003c/p\u003e\n\n    \u003cul\u003e\n      \u003cli\u003e一个 token 可能是一个单词、一个词根、标点、标点+单词等等；\u003c/li\u003e\n      \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e每个 token 平均对应 0.75 个单词\u003c/mark\u003e\u003c/strong\u003e；\u003c/li\u003e\n      \u003cli\u003e所有的独立 token 组成一个词典（词汇表），典型的词典大小：\u003cstrong\u003e\u003cmark\u003e10k~100k tokens\u003c/mark\u003e\u003c/strong\u003e；\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003e这种\u003cstrong\u003e\u003cmark\u003e文本/token 转换是无损的\u003c/mark\u003e\u003c/strong\u003e，有很多算法，例如常用的字节对编码。\u003c/p\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e下图是个例子，可以很清楚地看出如何将句子切割成 token，然后再用整数表示的：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/how-to-train-a-gpt-assistant/tokenizer.png\" width=\"65%\" height=\"65%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e将文本 token 化\u003c/p\u003e\n\n\u003cp\u003e最后得到的这个\u003cstrong\u003e\u003cmark\u003e整数序列，就是实际输入到 transformer 的东西\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003ch2 id=\"13-超参数gpt-3-vs-llama\"\u003e1.3 超参数：GPT-3 vs. LLaMA\u003c/h2\u003e\n\n\u003cp\u003e接下来需要考虑控制阶段的超参数。这里拿两个具体模型 GPT-3/LLaMA 作为例子，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eGPT-4 的训练信息公开比较少，所以这里使用 GPT-3 的数据，注意 GPT-3 已经是三年前的模型了。\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"/blog/llama-paper-zh/\"\u003eLLaMA\u003c/a\u003e 是 Meta 最近发布的一个开源模型，数据比较新，信息比较全。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 id=\"131-词汇表大小上下文长度参数数量\"\u003e1.3.1 词汇表大小、上下文长度、参数数量\u003c/h3\u003e\n\n\u003cp\u003e预训练处理的数量级大致如下：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e词汇表大小\u003c/mark\u003e\u003c/strong\u003e通常为 10K 个 token。\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e上下文长度\u003c/mark\u003e\u003c/strong\u003e通常为 2k/4k，有时甚至 100k。这决定了 GPT 在预测序列中下一个整数时所能查看的最大整数数量。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/how-to-train-a-gpt-assistant/gpt3-vs-llama.png\" width=\"100%\" height=\"100%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eGPT-3 vs. LLaMA 超参数对比\u003c/p\u003e\n\n\u003cp\u003e可以看到，GPT-3 的最大参数是 175b，而 LLaMA 的最大参数只有 65b。虽然参数少了将近 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e2/3\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e，\n但 LLaMA 比 GPT-3 更强大，直观上是因为 \u003cstrong\u003e\u003cmark\u003eLLaMA 的训练时间更长\u003c/mark\u003e\u003c/strong\u003e —— 在 1.4 万亿个 token 上进行训练，而 GPT-3 仅仅在 0.3 万亿个 token 上训练。\n所以\u003cstrong\u003e\u003cmark\u003e不能仅凭模型的参数数量\u003c/mark\u003e\u003c/strong\u003e来评判其性能。\u003c/p\u003e\n\n\u003ch3 id=\"132-硬件环境和成本\"\u003e1.3.2 硬件环境和成本\u003c/h3\u003e\n\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth style=\"text-align: left\"\u003e \u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eGPU\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003e训练时长\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003e训练成本\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eGPT-3\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e约\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e一万张 V100\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e30 天左右\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e$100 万 ~ $1000 万\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eLLaMA\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e两千张 A100\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e21 天\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e$500 万\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eV100/A100 \u003ca href=\"/blog/gpu-prices/\"\u003e算力对比参考\u003c/a\u003e。\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e这些都是在预训练阶段应该考虑的。\u003c/p\u003e\n\n\u003ch2 id=\"14-开始训练\"\u003e1.4 开始训练\u003c/h2\u003e\n\n\u003ch3 id=\"141-根据-batch-size-和上下文长度-token-化输入文本\"\u003e1.4.1 根据 batch size 和上下文长度 token 化输入文本\u003c/h3\u003e\n\n\u003cp\u003e输入给 Transformer 的是 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e(B,T)\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 维度的矩阵，其中，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eB 表示批次大小（\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003ebatch size\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e），\u003c/li\u003e\n  \u003cli\u003eT 表示最大\u003cstrong\u003e\u003cmark\u003e上下文长度\u003c/mark\u003e\u003c/strong\u003e，\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e另外，输入会整理成行（row），每个输入\u003cstrong\u003e\u003cmark\u003e序列的结尾\u003c/mark\u003e\u003c/strong\u003e用一个特殊的 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e\u0026lt;|endoftext|\u0026gt;\u003c/code\u003e token 来标记。\n下面是一个具体例子，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/how-to-train-a-gpt-assistant/hyperparams.png\" width=\"100%\" height=\"100%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e其中，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e上面的 4 行文本就是输入，每个输入序列都用特殊 token 结束；\u003c/li\u003e\n  \u003cli\u003e下面的表格就是 \u003cstrong\u003e\u003cmark\u003etoken 化之后的表示\u003c/mark\u003e\u003c/strong\u003e（基本上每个单词对应一个 token）；\u003c/li\u003e\n  \u003cli\u003e这里 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e(B,T) = (4,10)\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e，即每个批次输入 4 行文本，最大上下文长度是 10 个 token。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 id=\"142-预测下一个-token\"\u003e1.4.2 预测下一个 token\u003c/h3\u003e\n\n\u003cp\u003e在预测每个位置的下一个 token 时，\n只能用到\u003cstrong\u003e\u003cmark\u003e当前行\u003c/mark\u003e\u003c/strong\u003e中当前位置\u003cstrong\u003e\u003cmark\u003e前面的最多 T（上下文长度）个 token\u003c/mark\u003e\u003c/strong\u003e。\n对照下图，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/how-to-train-a-gpt-assistant/hyperparams-2.png\" width=\"80%\" height=\"80%\"/\u003e\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003cspan style=\"color:green; font-weight:bold\"\u003e绿色\u003c/span\u003e的是当前正在处理的 token；\u003c/li\u003e\n  \u003cli\u003e前面\u003cspan style=\"color:yellow; font-weight:bold\"\u003e黄色\u003c/span\u003e的 tokens 是它可以用到的上下文（context），会输入到神经网络；\u003c/li\u003e\n  \u003cli\u003e后面紧挨着的\u003cspan style=\"color:red; font-weight:bold\"\u003e红色\u003c/span\u003e token 就是 Transformer 预测的下一个 token（target）。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e这里无法展开介绍 Transformer 的神经网络架构，只需要知道它是一个大型的神经网络模块，通常有数百亿个参数。\n调整这些参数，下一个 token 的\u003cstrong\u003e\u003cmark\u003e预测分布\u003c/mark\u003e\u003c/strong\u003e 就会发生变化。\n例如，如果词汇表大小为 50,257 个 token，那么每个位置的输出将有 1/50257 种可能，它们的概率服从某种分布。\n在上面这个例子中，下一个 token 是 513。\n我们把它作为“正确答案”，反过来调整 Transformer 的权重，让它的预测与输入尽量接近。\n我们在每个单元格上都应用这个方法，并不断输入新的批次，让 Transformer 在序列中正确预测下一个 token。\u003c/p\u003e\n\n\u003cp\u003e现在看个更真实的训练，《纽约时报》团队在莎士比亚数据集上训练了一个小型 GPT。\n下面是一小段莎士比亚文本和训练之后的采样效果：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/how-to-train-a-gpt-assistant/training-process.png\" width=\"100%\" height=\"100%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e采样的方式是预测下一个 token，可以看到：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e左下角：开始时，GPT 的权重是完全随机的，因此也会得到完全随机的采样输出。\u003c/li\u003e\n  \u003cli\u003e右边：随着 GPT 训练时间越来越长，会得到越来越一致和连贯的采样输出。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 id=\"143-损失函数\"\u003e1.4.3 损失函数\u003c/h3\u003e\n\n\u003cp\u003e训练一段时间之后，你会发现 Transformer 已经学会了单词以及在哪里放空格和逗号等等。\n因此，随着时间的推移，我们可以得到越来越一致的预测。\n下面是随着训练的推移，损失函数的变化。值越小意味着 Transformer 预测下一个 token 越准确（更高的概率）。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/how-to-train-a-gpt-assistant/training-curves.png\" width=\"90%\" height=\"90%\"/\u003e\u003c/p\u003e\n\n\u003ch2 id=\"15-基础模型的功能\"\u003e1.5 基础模型的功能\u003c/h2\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/how-to-train-a-gpt-assistant/gpt-1.png\" width=\"100%\" height=\"100%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e如果用了一个月时间训练出一个\u003cstrong\u003e\u003cmark\u003e基座大模型\u003c/mark\u003e\u003c/strong\u003e，那接下来可以做什么呢？\n我们在这个领域注意到的第一件事是：这些模型在语言建模的过程中\u003cstrong\u003e\u003cmark\u003e学习到了非常强大的通用表示\u003c/mark\u003e\u003c/strong\u003e，\n能够非常高效地\u003cstrong\u003e\u003cmark\u003e针对任意下游任务进行微调\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003ch3 id=\"151-分类gpt-1\"\u003e1.5.1 分类（GPT-1）\u003c/h3\u003e\n\n\u003cp\u003e如果对情感分类感兴趣，可以用微调之后的基础大模型来完成这个功能。\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e以前的方法是收集大量正面和负面的样本，然后训练某种 NLP 模型。\u003c/li\u003e\n  \u003cli\u003e现在的做法是忽略情感分类，进行大规模的语言模型预训练，训练一个大型 Transformer。\n即便只有很少的样本，也可以非常高效地为这个任务微调你的模型。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e这在实践中非常有效。\n原因是 Transformer 在语言建模任务中为预测下一个 token 而进行了大量的训练，\n这个过程中\u003cstrong\u003e\u003cmark\u003e对文本的结构和其中的各种概念\u003c/mark\u003e\u003c/strong\u003e有了很深的理解。\u003c/p\u003e\n\n\u003cp\u003e这就是 GPT-1 能做的事情。\u003c/p\u003e\n\n\u003ch3 id=\"152-提示工程--文档补全gpt-2\"\u003e1.5.2 提示工程 + 文档补全（GPT-2）\u003c/h3\u003e\n\n\u003cp\u003e在 GPT-2 时代，人们注意到\u003cstrong\u003e\u003cmark\u003e比微调更好的方法\u003c/mark\u003e\u003c/strong\u003e是\u003cstrong\u003e\u003cmark\u003e给模型以有效的提示\u003c/mark\u003e\u003c/strong\u003e。\n\u003cstrong\u003e\u003cmark\u003e语言模型功能其实非常单一，它们只想要补全文档\u003c/mark\u003e\u003c/strong\u003e（预测下一个 token 的高级形式），换句话说，\n如果你想让它们完成其他任务，就要通过某些方式\u003cstrong\u003e\u003cmark\u003e骗一下它们\u003c/mark\u003e\u003c/strong\u003e，让它们以为自己在补全文档就行了。\u003c/p\u003e\n\n\u003cp\u003e比如下面这个例子，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/how-to-train-a-gpt-assistant/gpt-2.png\" width=\"65%\" height=\"65%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e首先有一些段落，然后我们把其中一些内容整理成\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e问题：xxx\n回答：xxx\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003e的形式（称为 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003efew-shot\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 提示）。\n如果我们以提示的形式，向 transforer 提出一个问题，那它接下来做的事情仍然是它认为的“补全文档”，\n但实际上已经回答了我们的问题。这是针对基础模型做\u003cstrong\u003e\u003cmark\u003e提示工程\u003c/mark\u003e\u003c/strong\u003e的例子：\n让它相信自己在补全（模仿）一个文档，而实际上是回答了我们的问题。\u003c/p\u003e\n\n\u003cp\u003e我认为 \u003cstrong\u003e\u003cmark\u003eGPT-2 开启了提示时代\u003c/mark\u003e\u003c/strong\u003e，\n下图可以看到，提示工程在很多问题上非常有效，甚至不需要训练任何神经网络或微调。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/how-to-train-a-gpt-assistant/gpt-2-2.png\" width=\"75%\" height=\"75%\"/\u003e\u003c/p\u003e\n\n\u003ch2 id=\"16-基础模型不是助手\"\u003e1.6 基础模型不是助手\u003c/h2\u003e\n\n\u003ch3 id=\"161-大模型进化树\"\u003e1.6.1 大模型进化树\u003c/h3\u003e\n\n\u003cp\u003eGPT-2 之后，我们看到了一个完整的基础模型的进化树。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/llm-practical-guide/fig-1.png\" width=\"90%\" height=\"90%\"/\u003e\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e\u003ca href=\"/blog/llm-practical-guide-zh/\"\u003e大语言模型（LLM）综述与实用指南（Amazon，2023）\u003c/a\u003e\u003c/p\u003e\n\n  \u003col\u003e\n    \u003cli\u003eGPT Improving Language Understanding by Generative Pre-Training. 2018.\u003c/li\u003e\n    \u003cli\u003eGPT-2 Language Models are Unsupervised Multitask Learners. 2018.\u003c/li\u003e\n    \u003cli\u003eGPT-3 “Language Models are Few-Shot Learners”. NeurlPS 2020.\u003c/li\u003e\n    \u003cli\u003eOPT “OPT: Open Pre-trained Transformer Language Models”. 2022.\u003c/li\u003e\n    \u003cli\u003ePaLM “PaLM: Scaling Language Modeling with Pathways”. Aakanksha Chowdhery et al. arXiv 2022.\u003c/li\u003e\n    \u003cli\u003eBLOOM “BLOOM: A 176B-Parameter Open-Access Multilingual Language Model”. 2022.\u003c/li\u003e\n    \u003cli\u003eMT-NLG “Using DeepSpeed and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale GenerativeLanguage Model”.2021.\u003c/li\u003e\n    \u003cli\u003eGLaM “GLaM: Efficient Scaling of Language Models with Mixture-of-Experts”. ICML 2022.\u003c/li\u003e\n    \u003cli\u003eGopher “Scaling Language Models: Methods, Analysis \u0026amp; Insights from Training Gopher”. 2021.\u003c/li\u003e\n    \u003cli\u003echinchilla “Training Compute-Optimal Large Language Models”. 2022.\u003c/li\u003e\n    \u003cli\u003eLaMDA “LaMDA: Language Models for Dialog Applications”2021.\u003c/li\u003e\n    \u003cli\u003eLLaMA “LLaMA: Open and Efficient Foundation Language Models”. 2023.\u003c/li\u003e\n    \u003cli\u003eGPT-4 “GPT-4 Technical Report”. 2023.\u003c/li\u003e\n    \u003cli\u003eBloombergGPT BloombergGPT: A Large Language Model for Finance, 2023,\u003c/li\u003e\n    \u003cli\u003eGPT-NeoX-20B: “GPT-NeoX-20B: An Open-Source Autoregressive Language Model”.2022.\u003c/li\u003e\n  \u003c/ol\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e但注意，并非图中所有的模型都是公开的。例如，GPT-4 基础模型从未发布过。\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003eGPT-4 API 背后也并不是 GPT-4 基础模型，而是一个助手模型\u003c/mark\u003e\u003c/strong\u003e。\u003c/li\u003e\n  \u003cli\u003eGPT-3 基础模型可以通过 API 使用，模型名为 DaVinci。\u003c/li\u003e\n  \u003cli\u003eGPT-2 基础模型的权重在我们的 GitHub repo 上。\u003c/li\u003e\n  \u003cli\u003e目前最好的可用基础模型可能是 Meta 的 LLaMA 系列。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 id=\"162-文档提示效果不佳\"\u003e1.6.2 文档提示：效果不佳\u003c/h3\u003e\n\n\u003cp\u003e需要再次说明的是：\u003cstrong\u003e\u003cmark\u003e基础模型不是助手\u003c/mark\u003e\u003c/strong\u003e，它们\u003cstrong\u003e\u003cmark\u003e不想回答问题，只想补全文档\u003c/mark\u003e\u003c/strong\u003e。\n因此，如果让它们“写一首关于面包和奶酪的诗”，它们不仅不“听话”，反而会有样学样，列更多的任务出来，像下面左图这样，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/how-to-train-a-gpt-assistant/base-model-not-assistant.png\" width=\"80%\" height=\"80%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e这是因为它只是在\u003cstrong\u003e\u003cmark\u003e忠实地补全文档\u003c/mark\u003e\u003c/strong\u003e。\n但如果你能\u003cstrong\u003e\u003cmark\u003e成功地提示它\u003c/mark\u003e\u003c/strong\u003e，例如，\u003cstrong\u003e\u003cmark\u003e开头就说“这是一首关于面包和奶酪的诗”\u003c/mark\u003e\u003c/strong\u003e，\n那它接下来就会真的补全一首这样的诗出来，如右图。\u003c/p\u003e\n\n\u003cp\u003e我们还可以通过 few-shot 来\u003cstrong\u003e\u003cmark\u003e进一步“欺骗”它\u003c/mark\u003e\u003c/strong\u003e。把你想问的问题整理成一个\u003cstrong\u003e\u003cmark\u003e“提问+回答”的文档格式\u003c/mark\u003e\u003c/strong\u003e，\n前面给一点正常的论述，然后突然来个问题，它以为自己还是在补全文档，其实已经把问题回答了：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/how-to-train-a-gpt-assistant/base-model-not-assistant-2.png\" width=\"80%\" height=\"80%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e这就是把基础模型\u003cstrong\u003e\u003cmark\u003e调教成一个 AI 助手\u003c/mark\u003e\u003c/strong\u003e的过程。\n不过，这种方式虽然可行，但不是很可靠，在实践中效果也不是特别好。\u003c/p\u003e\n\n\u003cp\u003e有没有更好的办法呢？有 —— 监督微调。\u003c/p\u003e\n\n\u003ch1 id=\"2-监督微调sft\"\u003e2 监督微调（SFT）\u003c/h1\u003e\n\n\u003ch2 id=\"21-收集高质量人工标注数据\"\u003e2.1 收集高质量人工标注数据\u003c/h2\u003e\n\n\u003cp\u003e在监督微调阶段，首先需要收集\u003cstrong\u003e\u003cmark\u003e小但高质量的数据集\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/how-to-train-a-gpt-assistant/sft-dataset.png\" width=\"100%\" height=\"100%\"/\u003e\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\n    \u003cp\u003e通常是通过供应商的形式收集，格式是\u003cstrong\u003e\u003cmark\u003e“提示 + 理想回答”\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n    \u003cp\u003e这里面包括了一些随机的提示，例如“Can you write a short introduction about the relevance of the term monopsony”，然后承包商（人类标注员）会写出一个理想的回答。\n  写出这些回答时，需要遵循详细的规范（上图右边。你可能看不懂，我也看不懂），并要求回答是有帮助的、真实的和无害的。\u003c/p\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003e通常收集\u003cstrong\u003e\u003cmark\u003e数万条\u003c/mark\u003e\u003c/strong\u003e这样的数据。\u003c/p\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 id=\"22-sft-训练\"\u003e2.2 SFT 训练\u003c/h2\u003e\n\n\u003cp\u003e然后在这些数据上再次进行语言建模。\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e算法还是一样\u003c/mark\u003e\u003c/strong\u003e，只是更换了训练集。\u003c/li\u003e\n  \u003cli\u003e预训练是互联网文档，这是数量庞大但质量较低的数据，现在是 QA 类型的提示回答类数据，数量不多但质量很高。\u003c/li\u003e\n  \u003cli\u003e这个阶段只需要\u003cstrong\u003e\u003cmark\u003e百来片 GPU\u003c/mark\u003e\u003c/strong\u003e，\u003cstrong\u003e\u003cmark\u003e训练几天时间\u003c/mark\u003e\u003c/strong\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e在这个阶段的训练后，我们得到了一个 \u003cstrong\u003e\u003cmark\u003eSFT 模型\u003c/mark\u003e\u003c/strong\u003e。例子：\u003ccode class=\"language-plaintext highlighter-rouge\"\u003evicuna-13b\u003c/code\u003e。\n实际上可以部署这些模型，它们是\u003cstrong\u003e\u003cmark\u003e真正的助手\u003c/mark\u003e\u003c/strong\u003e。\n但要想效果更好，还需要一些改进，从人类反馈中学习（RLHF）。\u003c/p\u003e\n\n\u003ch1 id=\"3-奖励建模\"\u003e3 奖励建模\u003c/h1\u003e\n\n\u003cp\u003eRLHF 包括奖励建模和强化学习。\u003c/p\u003e\n\n\u003cp\u003e在奖励建模阶段，会将数据收集转变为比较（comparison）的形式。下面看个例子。\u003c/p\u003e\n\n\u003ch2 id=\"31-例子评估-chatgpt-编程的好坏\"\u003e3.1 例子：评估 ChatGPT 编程的好坏\u003c/h2\u003e\n\n\u003cp\u003e基于上一步已经训练好的 SFT 模型，让它写一个检查给定字符串是否为回文的程序或函数。\n我们重复三次，每次都给完全相同的提示，得到三个回答。\u003c/p\u003e\n\n\u003cp\u003e第一次：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/how-to-train-a-gpt-assistant/chatgpt-1.png\" width=\"60%\" height=\"60%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e第二次：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/how-to-train-a-gpt-assistant/chatgpt-2.png\" width=\"60%\" height=\"60%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e第三次：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/how-to-train-a-gpt-assistant/chatgpt-3.png\" width=\"60%\" height=\"60%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e然后我们要求\u003cstrong\u003e\u003cmark\u003e人对这三个结果进行排名\u003c/mark\u003e\u003c/strong\u003e。\n实际来说，这些结果很难进行比较，因为好坏差异可能并没有那么大；\n但假设我们认为其中必然有一个结果比其他的好得多。\u003c/p\u003e\n\n\u003cp\u003e这样我们就可以得到一个结果排名，然后我们可以进行类似于二元分类（binary classification）的操作，\n对这些回答的所有可能组合对进行比较。\u003c/p\u003e\n\n\u003ch2 id=\"32-奖励\"\u003e3.2 奖励\u003c/h2\u003e\n\n\u003cp\u003e现在来看一下如何对奖励进行建模。\u003c/p\u003e\n\n\u003cp\u003e将三次的提示+回答按行排列，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/how-to-train-a-gpt-assistant/rm-training.png\" width=\"80%\" height=\"80%\"/\u003e\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003cspan style=\"color:blue; font-weight:bold\"\u003e蓝色\u003c/span\u003e的是提示（prompt tokens），每行都一样；\u003c/li\u003e\n  \u003cli\u003e\u003cspan style=\"color:yellow; font-weight:bold\"\u003e黄色\u003c/span\u003e的是 SFT 模型基于 prompt 产生的补全（completion tokens），每次都不同；\u003c/li\u003e\n  \u003cli\u003e\u003cspan style=\"color:green; font-weight:bold\"\u003e绿色\u003c/span\u003e的是特殊的 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e\u0026lt;|reward|\u0026gt;\u003c/code\u003e token。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e这些数据一起作为新的输入，再训练一个 transforer 模型，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e输入：蓝色+黄色 tokens，即原始 prompt + SFT 模型补全\u003c/li\u003e\n  \u003cli\u003e输出：绿色 token，即奖励（分数）\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e也就是说，这个模型用“原始问题 + SFT 模型补全结果”来预测“SFT 模型补全结果”的好坏。\n换句话说，\u003cstrong\u003e\u003cmark\u003e对每个 SFT 模型的补全质量进行预测\u003c/mark\u003e\u003c/strong\u003e。这个预测用数值表示结果的好坏，\n我们将这个转化为一个\u003cstrong\u003e\u003cmark\u003e损失函数\u003c/mark\u003e\u003c/strong\u003e，并训练我们的模型使得奖励预测与人工给出的 comparison 基准一致。\u003c/p\u003e\n\n\u003cp\u003e这就是\u003cstrong\u003e\u003cmark\u003e训练奖励模型的方法\u003c/mark\u003e\u003c/strong\u003e，这使我们能够对补全的结果好坏进行评分。\u003c/p\u003e\n\n\u003ch2 id=\"33-奖励模型的特点\"\u003e3.3 奖励模型的特点\u003c/h2\u003e\n\n\u003cp\u003e跟基座模型、SFT 模型以及后面将介绍的强化学习模型相比，奖励模型的最大特点是\u003cstrong\u003e\u003cmark\u003e不能独立部署\u003c/mark\u003e\u003c/strong\u003e，\n也就是说不能单独部署这样的一个模型，然后接受用户提示（输入），给出有意义的输出（补全）。\u003c/p\u003e\n\n\u003cp\u003e为什么呢？上一节的原理其实已经给出答案了：奖励模型要求的输入是“问题+回答”，它的功能是对其中的“回答”进行评分，判断其好坏。\n因此它只是一个完整系统中的模块，而并不是一个可以直接面向用户的模型。\u003c/p\u003e\n\n\u003ch1 id=\"4-强化学习rlhf\"\u003e4 强化学习（RLHF）\u003c/h1\u003e\n\n\u003cp\u003e奖励模型虽然不能单独部署，但对接下来的强化学习阶段非常有用。\n因为有了它，我们就能\u003cstrong\u003e\u003cmark\u003e对任意提示的任意补全的质量进行评分\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003ch2 id=\"41-rlhf-训练\"\u003e4.1 RLHF 训练\u003c/h2\u003e\n\n\u003cp\u003e现在我们获取了一大批提示，接下来\u003cstrong\u003e\u003cmark\u003e基于奖励模型进行强化学习\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e针对\u003cstrong\u003e\u003cmark\u003e给定提示的每个补全\u003c/mark\u003e\u003c/strong\u003e，奖励模型能够预测这些补全的质量；\u003c/li\u003e\n  \u003cli\u003e评分过程（或称损失函数）其实也是根据给定的一串 tokens（SFT 模型的输出）来预测下一个 token（分数），因此也是一个语言建模过程，跟预训练建模并没有本质区别。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e举个例子，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/how-to-train-a-gpt-assistant/rl-training.png\" width=\"80%\" height=\"80%\"/\u003e\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e第一行：奖励模型判断这是个\u003cstrong\u003e\u003cmark\u003e高质量\u003c/mark\u003e\u003c/strong\u003e的补全。这一行中的所有 token 都将得到加强，在未来将获得更高的出现概率。\u003c/li\u003e\n  \u003cli\u003e第二行：奖励模型判断这是个\u003cstrong\u003e\u003cmark\u003e不符合要求\u003c/mark\u003e\u003c/strong\u003e的补全，给负分。这一行中的每个 token 在未来只能获得稍高的出现概率。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e在许多提示和批次上反复进行这个过程，符合人类偏好的 SFT 补全（黄色 token）就会得到高分。\u003c/p\u003e\n\n\u003cp\u003e这就是 \u003cstrong\u003e\u003cmark\u003eRLHF 的训练过程\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e这个阶段结束后，得到是一个可以部署的模型。例如，\u003cstrong\u003e\u003cmark\u003eChatGPT 就是一个 RLHF 模型\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003ch2 id=\"42-为什么要使用-rlhf\"\u003e4.2 为什么要使用 RLHF？\u003c/h2\u003e\n\n\u003cp\u003e简单回答是：\u003cstrong\u003e\u003cmark\u003e效果好\u003c/mark\u003e\u003c/strong\u003e。\n下图来自 InstructGPT 论文，其中 PPO 模型就是 RLHF 的。\n从人类的反馈来看，质量从高到低依次为：RLHF 模型、SFT 模型、基座模型。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/how-to-train-a-gpt-assistant/why-rlhf-1.png\" width=\"70%\" height=\"70%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e那么，为什么 RLHF 效果这么好呢？\u003cstrong\u003e\u003cmark\u003e社区并没有一个公认的解释\u003c/mark\u003e\u003c/strong\u003e，\n但这里我可以提供一个可能的原因：\u003cstrong\u003e\u003cmark\u003e比较（comparison）和生成（generation）在计算上的不对称性\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e以生成一个俳句为例。假设让一个模型写一个关于回形针的俳句，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/how-to-train-a-gpt-assistant/why-rlhf-2.png\" width=\"90%\" height=\"90%\"/\u003e\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e如果你是一个承包商，为 SFT 收集数据，那你应该如何为回形针创作一个好的俳句呢？这\u003cstrong\u003e\u003cmark\u003e很难\u003c/mark\u003e\u003c/strong\u003e；\u003c/li\u003e\n  \u003cli\u003e另一方面，但如果给你一些俳句的例子，让你对它们的好坏进行比较（评分），这个就\u003cstrong\u003e\u003cmark\u003e简单\u003c/mark\u003e\u003c/strong\u003e多了；\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e因此，判断比生成要容易的多。这种\u003cstrong\u003e\u003cmark\u003e不对称性使得 comparison 成为一种潜在的更好方式\u003c/mark\u003e\u003c/strong\u003e（好落地，实操性强），\n可以利用人的判断力来创建一个更好的模型。\u003c/p\u003e\n\n\u003ch2 id=\"43-模型的熵\"\u003e4.3 模型的熵\u003c/h2\u003e\n\n\u003cp\u003e某些情况下，RLHF 模型并不是基础模型的简单改进。特别是，我们注意到 RLHF 模型会\u003cstrong\u003e\u003cmark\u003e丢失一些熵\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e这意味着它们会给出\u003cstrong\u003e\u003cmark\u003e更加确定性的结果\u003c/mark\u003e\u003c/strong\u003e；相比基础模型，RLHF 模型的输出变化更少；\u003c/li\u003e\n  \u003cli\u003e基础模型熵比较大，会给出很多不同的输出。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/how-to-train-a-gpt-assistant/mode-collapse.png\" width=\"90%\" height=\"90%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e在以下情况下，我仍然喜欢使用基础模型：已经有 N 个东西，想生成更多类似的东西时。\n例如下图，给出了 7 个 pokeman 名字，想得到更多类似的名字，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/how-to-train-a-gpt-assistant/mode-collapse-2.png\" width=\"80%\" height=\"80%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e后面给出的这些名字看着都是虚构的（没去验证）。我认为这种任务基础模型很擅长，\n因为它熵比较大，因此能给出多样的、酷炫的、与之前给出的东西相似的输出。\u003c/p\u003e\n\n\u003ch1 id=\"5-总结\"\u003e5 总结\u003c/h1\u003e\n\n\u003cp\u003e下图是目前市面上可用的助手模型。\n伯克利大学的一个团队对它们进行了排名，并给出了 ELO 评分。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/how-to-train-a-gpt-assistant/assistant-models.png\" width=\"80%\" height=\"80%\"/\u003e\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e目前最好的模型是 GPT-4。\u003c/li\u003e\n  \u003cli\u003e前三个都是 RLHF 模型，\u003c/li\u003e\n  \u003cli\u003e其他的都是 SFT 模型。\u003c/li\u003e\n\u003c/ul\u003e\n\n\n  \u003c!-- POST NAVIGATION --\u003e\n  \u003cdiv class=\"postNav clearfix\"\u003e\n     \n      \u003ca class=\"prev\" href=\"/blog/understanding-gpu-performance/\"\u003e\u003cspan\u003e« Understanding NVIDIA GPU Performance: Utilization vs. Saturation (2023)\u003c/span\u003e\n      \n    \u003c/a\u003e\n      \n      \n      \u003ca class=\"next\" href=\"/blog/gpu-advanced-notes-1-zh/\"\u003e\u003cspan\u003eGPU 进阶笔记（一）：高性能 GPU 服务器硬件拓扑与集群组网（2023） »\u003c/span\u003e\n       \n      \u003c/a\u003e\n     \n  \u003c/div\u003e\n\u003c/div\u003e",
  "Date": "2023-09-01T00:00:00Z",
  "Author": "Arthur Chiao"
}