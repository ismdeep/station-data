{
  "Source": "arthurchiao.art",
  "Title": "[译] Cloudflare 边缘网络架构：无处不在的 BPF（2019）",
  "Link": "https://arthurchiao.art/blog/cloudflare-arch-and-bpf-zh/",
  "Content": "\u003cdiv class=\"post\"\u003e\n  \n  \u003ch1 class=\"postTitle\"\u003e[译] Cloudflare 边缘网络架构：无处不在的 BPF（2019）\u003c/h1\u003e\n  \u003cp class=\"meta\"\u003ePublished at 2019-06-12 | Last Update 2019-06-12\u003c/p\u003e\n  \n  \u003ch3 id=\"译者序\"\u003e译者序\u003c/h3\u003e\n\n\u003cp\u003e本文翻译自 2019 年的一篇英文博客 \u003ca href=\"https://blog.cloudflare.com/cloudflare-architecture-and-how-bpf-eats-the-world/\"\u003eCloudflare architecture and how BPF eats the\nworld\u003c/a\u003e\n。\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e由于译者水平有限，本文不免存在遗漏或错误之处。如有疑问，请查阅原文。\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003e以下是译文。\u003c/p\u003e\n\n\u003chr/\u003e\n\n\u003cul id=\"markdown-toc\"\u003e\n  \u003cli\u003e\u003ca href=\"#译者序\" id=\"markdown-toc-译者序\"\u003e译者序\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#边缘网络\" id=\"markdown-toc-边缘网络\"\u003e边缘网络\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#ddos-mitigation\" id=\"markdown-toc-ddos-mitigation\"\u003eDDos Mitigation\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#负载均衡\" id=\"markdown-toc-负载均衡\"\u003e负载均衡\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#tcpudp-socket-dispatch\" id=\"markdown-toc-tcpudp-socket-dispatch\"\u003eTCP/UDP Socket Dispatch\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#sockmap\" id=\"markdown-toc-sockmap\"\u003eSOCKMAP\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#prometheus---ebpf_exporter\" id=\"markdown-toc-prometheus---ebpf_exporter\"\u003ePrometheus - ebpf_exporter\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#无处不在的-ebpf\" id=\"markdown-toc-无处不在的-ebpf\"\u003e无处不在的 eBPF\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003chr/\u003e\n\n\u003ch1 id=\"边缘网络\"\u003e边缘网络\u003c/h1\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cloudflare-arch-and-bpf/1.jpg\" width=\"80%\" height=\"80%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eCloudflare 的服务器运行 Linux 系统。\u003c/p\u003e\n\n\u003cp\u003e我们的数据中心分为两类：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e大的“核心”数据中心：用于处理日志、分析攻击、统计分析数据\u003c/li\u003e\n  \u003cli\u003e小的“边缘”数据中心（服务器集群）：分布在全球 180 个站点，用于分发用户内容\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e本文将关注于\u003cstrong\u003e边缘服务器\u003c/strong\u003e部分。特别地，我们在这些服务器中使用了最新的 Linux 特\n性，进行了针对性的性能优化，并特别关注 DoS 弹性。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cloudflare-arch-and-bpf/2.png\" width=\"80%\" height=\"80%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e特殊的网络配置——\u003cstrong\u003e大量使用任播路由\u003c/strong\u003e（anycast routing）——导致我们的边缘服务器也\n很特殊。任播意味着，我们\u003cstrong\u003e所有的数据中心都通告相同的一组 IP 地址\u003c/strong\u003e，如上图所示。\u003c/p\u003e\n\n\u003cp\u003e这种设计可以带来很多好处：\u003c/p\u003e\n\n\u003cp\u003e首先，可以\u003cstrong\u003e保证终端用户的访问速度最优\u003c/strong\u003e。不管用户在全球什么位置，都会匹配到最近\n的数据中心；\u003c/p\u003e\n\n\u003cp\u003e其次，可以\u003cstrong\u003e分散 DoS 攻击\u003c/strong\u003e。当发生攻击时，每个边缘节点只会收到全部流量的一\n小部分，这使得流量过滤更容易。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cloudflare-arch-and-bpf/3.jpg\" width=\"80%\" height=\"80%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e另外，任播使得所有边缘数据中心的\u003cstrong\u003e网络配置都是一致的\u003c/strong\u003e（uniform）。因此，当我们\n将同一设计应用于所有数据中心时，这些边缘服务器中的软件栈也都是一致的。即，\u003cstrong\u003e所有服\n务器运行的软件都是相同的\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e理论上来说，每台机器都可以处理任何任务（every machine can handle every task）。\n我们在机器上运行了多种不同功能和不同需求的任务。我们有完整的 HTTP 协议栈、神奇的\nCloudflare Workers、两种 DNS —— 权威 DNS 和解析 DNS，以及其他对外的服务，例如\nSpectrum 和 Warp。\u003c/p\u003e\n\n\u003cp\u003e即使每台服务器都部署了以上所有软件，典型情况下，请求还是经过多台机器最终穿过我们\n的软件栈的。例如，一个 HTTP 请求的 5 个处理阶段如下图所示：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cloudflare-arch-and-bpf/4.png\" width=\"80%\" height=\"80%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003einbound packet（入向包）的几个处理阶段：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e包到达\u003cstrong\u003e路由器\u003c/strong\u003e，路由器通过 ECMP 将包转发给下一级服务器。我们用 ECMP 在后端\n机器（至少 16 个）之间分发数据包，这是\u003cstrong\u003e最底层的（rudimentary）的负载均衡技术\u003c/strong\u003e\u003c/li\u003e\n  \u003cli\u003e在\u003cstrong\u003e服务器\u003c/strong\u003e上，通过 XDP eBPF 收包。XDP 层做两件事情，首先是运行 \u003cstrong\u003eDoS\nmitigations\u003c/strong\u003e，识别来自大规模三层攻击的包并将其丢弃\u003c/li\u003e\n  \u003cli\u003e然后通过\u003cstrong\u003e四层负载均衡\u003c/strong\u003e（也是在 XDP），将正常的包重定向到下一级应用服务器，\n这些服务器上跑的是应用。这样的设计使得我们有比 ECMP 更细粒度的负载均衡，并\n且有能力将异常的应用机器优雅地移出集群\u003c/li\u003e\n  \u003cli\u003e包到达\u003cstrong\u003e应用宿主机\u003c/strong\u003e，进入 Linux 网络栈、经过 iptables 防火墙，最后分发到相应\n的 socket\u003c/li\u003e\n  \u003cli\u003e最后，包\u003cstrong\u003e被应用程序接收\u003c/strong\u003e。例如 HTTP 连接会由“协议”服务器处理，这种服务器负\n责执行 TLS 加密、处理 HTTP、HTTP/2 以及 QUIC 协议等等\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e在以上过程中我们使用了最新最酷的 Linux 特性。这些非常有用的现代功能可以分为三类：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eDoS Handling\u003c/li\u003e\n  \u003cli\u003eLoad balancing\u003c/li\u003e\n  \u003cli\u003eSocket dispatch\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch1 id=\"ddos-mitigation\"\u003eDDos Mitigation\u003c/h1\u003e\n\n\u003cp\u003e来更深入地看一下 DDoS 处理。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cloudflare-arch-and-bpf/5.png\" width=\"80%\" height=\"80%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e前面说到，ECMP 路由之后会到 Linux XDP，这里会做一些包括 DDoS 防御在内的处理。\u003c/p\u003e\n\n\u003cp\u003e我们传统的抵御大规模 DDoS 攻击的代码是基于 BPF 和 iptables 的。最近，我们将这些\n代码迁移到了 XDP eBPF。这项工作非常有挑战性，有兴趣可以参考我们的下列分享：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003ca href=\"https://blog.cloudflare.com/l4drop-xdp-ebpf-based-ddos-mitigations/\"\u003eL4Drop: XDP DDoS Mitigations\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://blog.cloudflare.com/xdpcap/\"\u003exdpcap: XDP Packet Capture\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://netdevconf.org/0x13/session.html?talk-XDP-based-DDoS-mitigation\"\u003eXDP based DoS mitigation\u003c/a\u003e talk by Arthur Fabre\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://netdevconf.org/2.1/papers/Gilberto_Bertin_XDP_in_practice.pdf\"\u003eXDP in practice: integrating XDP into our DDoS mitigation pipeline\u003c/a\u003e (PDF)\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e在这个过程中，我们遇到了很多 eBPF/XDP 的限制，其中之一就是缺少并发原语（\nconcurrency primitives）。实现 race-free token bucket （无竞争令牌桶）之类的东西\n是非常难的。后来我们发现 \u003ca href=\"http://vger.kernel.org/lpc-bpf2018.html#session-9\"\u003eFacebook 的工程师 Julia\nKartseva\u003c/a\u003e 也面临同样的问题。\n在二月份引入了 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebpf_spin_lock\u003c/code\u003e 辅助函数之后，这个问题得到了解决。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cloudflare-arch-and-bpf/6.png\" width=\"80%\" height=\"80%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e虽然我们的现代大规模 DoS 防御系统是在 XDP 层做的，但我们的 7 层（应用层）防御目前\n还是依赖 iptables。在应用层，更高级别防火墙的一些特性会带来很大帮助：connlimit、\nhashlimits 和 ipsets。我们也用到了 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ext_bpf\u003c/code\u003e iptables 模块，以在 iptables 内运行\ncBPF 匹配 packet payload。下面两个分享中介绍了这部分工作：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003ca href=\"https://speakerdeck.com/majek04/lessons-from-defending-the-indefensible\"\u003eLessons from defending the indefensible\u003c/a\u003e (PPT)\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://blog.cloudflare.com/introducing-the-bpf-tools/\"\u003eIntroducing the BPF tools\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cloudflare-arch-and-bpf/7.png\" width=\"80%\" height=\"80%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eXDP 和 iptables 之后，我们还有\u003cstrong\u003e位于内核的最后一层 DoS 防御层\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e考虑 UDP mitigation 失败的情况。在这种情况下，大量的 UDP 包会直接到达应用的 UDP\nsocket，可能导致 socket 溢出而丢包 —— 期望的包和不期望的包都丢了。对于 DNS 之类的\n应用来说（DNS 基于 UDP）这将是灾难性的。以前我们应付这种问题的方式：\u003cstrong\u003e每个 IP 只\n运行一个 socket\u003c/strong\u003e，如果一个 IP （或机器）被攻击了，至少不会影响其他 IP 的流量。\u003c/p\u003e\n\n\u003cp\u003e现在这一方式很难奏效了：我们有 30K+ DNS IP，运行同样数量的 UDP socket 显然不合理\n。因此，我们的新方案是：使用 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eSO_ATTACH_BPF\u003c/code\u003e 选项，\u003cstrong\u003e在每个 UDP socket 上运行一\n个（复杂的）eBPF 过滤器\u003c/strong\u003e 。关于在 socket 上运行 eBPF 的内容，参考我们的分享：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003ca href=\"https://blog.cloudflare.com/epbf_sockets_hop_distance/\"\u003eeBPF, Sockets, Hop Distance and manually writing eBPF assembly\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://blog.cloudflare.com/sockmap-tcp-splicing-of-the-future/\"\u003eSOCKMAP - TCP splicing of the future\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e前面提到，\u003cstrong\u003eeBPF 可以对包做限速\u003c/strong\u003e。它的实现方式是在 eBPF map 中维护了状态信息 ——\n包的数量。这样可以保证单个被攻击的 IP 不会影响其他流量。这项工作目前运行良好，但\n在过程中我们发现了 eBPF 校验器的一个 bug：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003ca href=\"https://blog.cloudflare.com/ebpf-cant-count/\"\u003eeBPF can’t count?!\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e我猜可能是因为在 UDP socket 上运行 eBPF 还是一件比较少见的事情，所以 bug 之前没\n有被及时发现。\u003c/p\u003e\n\n\u003ch1 id=\"负载均衡\"\u003e负载均衡\u003c/h1\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cloudflare-arch-and-bpf/8.png\" width=\"80%\" height=\"80%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e除了 DoS 之外，我们还在 XDP 中做 \u003cstrong\u003e4 层负载均衡\u003c/strong\u003e。这是个新项目，因此我们对外的\n分享还不多。简单来说（做这件事情）是因为：在特定的情况下，我们要能通过 XDP 查询\nsocket。\u003c/p\u003e\n\n\u003cp\u003e问题比较简单：从包中提取五元组（5-tuple），根据五元组查找内核数据结构 “socket”。\n这还是比较容易实现的，因为可以利用 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebpf_sk_lookup\u003c/code\u003e 辅助函数；但其中也有复杂之处\n，例如：当 SYN-cookie 功能打开时，无法验证收到的某个 ACK 是否来自\n一个合法的三次握手。我的同事 Lorenz Bauer 正在致力于为这个边界场景添加支持。\u003c/p\u003e\n\n\u003ch1 id=\"tcpudp-socket-dispatch\"\u003eTCP/UDP Socket Dispatch\u003c/h1\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cloudflare-arch-and-bpf/9.png\" width=\"80%\" height=\"80%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e经过了 DoS 和 LB 层之后，数据包来到了常规的 Linux TCP/UDP 协议栈。这里是进行\nsocket dispatch 的地方，例如，端口是 53 的包会被发送到 DNS 服务器。\u003c/p\u003e\n\n\u003cp\u003e我们尽量使用常规的 Linux 特性，但当服务器（集群）上有上千个 IP 地址时，情况变得\n复杂起来。\u003c/p\u003e\n\n\u003cp\u003e如果使用 \u003ca href=\"https://blog.cloudflare.com/how-we-built-spectrum\"\u003e“AnyIP” trick\u003c/a\u003e，那\n让 Linux 正确地对包进行路由是很容易的。但要将包正确地发送到对应的应用，则是另外一回事\n。不幸的是，Linux 原生的 socket dispatch 逻辑对我们的需求来说不够灵活。我们希\n望 80 这样的常用端口可以被不同应用共享，每个应用运行在不同的 IP 地址范围。Linux\n本身并不支持这种功能，\u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebind()\u003c/code\u003e 只能针对单个具体 IP，或者所有 IP（\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e0.0.0.0\u003c/code\u003e）。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cloudflare-arch-and-bpf/10.png\" width=\"80%\" height=\"80%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e为了满足这种需求，我们开发了一个内核补丁，添加了 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eSO_BINDTOPREFIX\u003c/code\u003e \u003ca href=\"http://patchwork.ozlabs.org/patch/602916/\"\u003esocket 选项\n\u003c/a\u003e。从名字上可以看出，它允许对\u003cstrong\u003e给定的\nIP 前缀\u003c/strong\u003e调用 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebind()\u003c/code\u003e，这就使得多个应用可以共享某些常用端口，例如 53 和 80。\u003c/p\u003e\n\n\u003cp\u003e接着我们遇到了另一问题：我们的 Spectrum 产品需要监听所有的 65535 个端口。监听这\n么多的 socket 并不是一个好主意（可以查看\u003ca href=\"https://blog.cloudflare.com/revenge-listening-sockets/\"\u003e我们之前的博客\n\u003c/a\u003e），所以得寻找其他\n方式。经过一翻探索，我们找到了一个冷门的 iptables 模块：TPROXY，来解决这个问题。\n进一步阅读：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003ca href=\"https://blog.cloudflare.com/how-we-built-spectrum/\"\u003eAbusing Linux’s firewall: the hack that allowed us to build\nSpectrum\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e这种配置可以工作，但我们不想要那些额外的防火墙规则。我们正在尝试解决这个问题\n—— 事实上是\u003cstrong\u003e扩展 socket dispatch 逻辑\u003c/strong\u003e。你猜对了 —— 我们想基于 eBPF 来做。敬请\n期待我们的补丁。\u003c/p\u003e\n\n\u003ch1 id=\"sockmap\"\u003eSOCKMAP\u003c/h1\u003e\n\n\u003cp\u003e一种基于 eBPF 优化应用的方式。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cloudflare-arch-and-bpf/11.png\" width=\"95%\" height=\"95%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e近期我们对基于 SOCKMAP 做 TCP splicing 非常感兴趣：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003ca href=\"https://blog.cloudflare.com/sockmap-tcp-splicing-of-the-future/\"\u003eSOCKMAP - TCP splicing of the\nfuture\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e这项技术有望用于优化我们软件栈中几个部分的尾延迟。虽然当前 SOCKMAP 的功能尚不\n完备，但其潜力巨大。\u003c/p\u003e\n\n\u003cp\u003e类似的，新的 \u003ca href=\"https://netdevconf.org/2.2/papers/brakmo-tcpbpf-talk.pdf\"\u003eTCP-BPF，也就是\nBPF_SOCK_OPS\u003c/a\u003e hooks 提\n供了强大的方式检查 TCP flow 的性能参数，对我们性能团队来说非常有用。\u003c/p\u003e\n\n\u003ch1 id=\"prometheus---ebpf_exporter\"\u003ePrometheus - ebpf_exporter\u003c/h1\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cloudflare-arch-and-bpf/12.jpg\" width=\"60%\" height=\"60%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eLinux 自带的一些统计计数很棒，但数据的粒度过粗，无法满足现代监控的需求，例如，\n\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eTcpExtListenDrops\u003c/code\u003e 和 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eTcpExtListenOverflows\u003c/code\u003e 都是全局计数器，而我们需要的是应\n用级别的计数。\u003c/p\u003e\n\n\u003cp\u003e我们的解决方案是：\u003cstrong\u003e使用 eBPF probes 直接从内核获取这些数据\u003c/strong\u003e。我的同事 Ivan\nBabrou 写了一个名为 “ebpf_exporter” 的 Prometheus metrics exporter 来完成这项工\n作，见：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003ca href=\"https://blog.cloudflare.com/introducing-ebpf_exporter/\"\u003eIntroducing ebpf_exporter\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://blog.cloudflare.com/introducing-ebpf_exporter/\"\u003egithub.com/cloudflare/ebpf_exporter\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch1 id=\"无处不在的-ebpf\"\u003e无处不在的 eBPF\u003c/h1\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cloudflare-arch-and-bpf/13.png\" width=\"80%\" height=\"80%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e最后总结一下，本文介绍了 eBPF 在我们的边缘服务器里 6 个不同层次的应用：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\u003cstrong\u003e大规模 DoS mitigation\u003c/strong\u003e：XDP eBPF\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e防御应用层攻击\u003c/strong\u003e：iptables \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ext_bpf\u003c/code\u003e cBPF\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eUDP socket 限速\u003c/strong\u003e：\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eSO_ATTACH_BPF\u003c/code\u003e\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e负载均衡器\u003c/strong\u003e：XDP\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eTCP socket splicing；TCP-BPF：TCP 测量\u003c/strong\u003e：SOCKMAP\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e细粒度监控采集\u003c/strong\u003e：ebpf_exporter\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e这仅仅是开始！\u003c/p\u003e\n\n\u003cp\u003e接下来我们会进一步改进基于 eBPF 的 socket dispatch，基于 eBPF 的\nLinux TC（Traffic Control），以及与 cgroup eBPF hook 的更多集成。我们的 SRE 团队\n也维护着越来越多的 \u003ca href=\"https://github.com/iovisor/bcc\"\u003eBCC 脚本\u003c/a\u003e，它们对 debug 很有\n帮助。\u003c/p\u003e\n\n\u003cp\u003eLinux 似乎已经停止开发新的 API，所有新功能都是以 eBPF hook 和辅助函数的方式实现\n的。这可以带来很多优势，升级 eBPF 程序要比重新编译内核模块方便和安全得多。其中一\n些东西，例如 TCP-BPF、导出大量性能跟踪数据等，没有 eBPF 可能就无法完成。\u003c/p\u003e\n\n\u003cp\u003e有人说“软件正在吞噬世界”（software is eating the world），而我要说：“BPF 正\n在吞噬软件”（BPF is eating the software）。\u003c/p\u003e\n\n\n  \u003c!-- POST NAVIGATION --\u003e\n  \u003cdiv class=\"postNav clearfix\"\u003e\n     \n      \u003ca class=\"prev\" href=\"/blog/transparent-chaos-testing-with-envoy-cilium-ebpf-zh/\"\u003e\u003cspan\u003e« [译] 基于 Envoy、Cilium 和 eBPF 实现透明的混沌测试（KubeCon, 2019）\u003c/span\u003e\n      \n    \u003c/a\u003e\n      \n      \n      \u003ca class=\"next\" href=\"/blog/building-microservices-notes-zh/\"\u003e\u003cspan\u003e[笔记] Building Microservices（O\u0026#39;Reily 2015） »\u003c/span\u003e\n       \n      \u003c/a\u003e\n     \n  \u003c/div\u003e\n\u003c/div\u003e",
  "Date": "2019-06-12T00:00:00Z",
  "Author": "Arthur Chiao"
}