{
  "Source": "arthurchiao.art",
  "Title": "[译][论文] InstructGPT：基于人类反馈训练语言模型遵从指令的能力（OpenAI，2022）",
  "Link": "https://arthurchiao.art/blog/instructgpt-paper-zh/",
  "Content": "\u003cdiv class=\"post\"\u003e\n  \n  \u003ch1 class=\"postTitle\"\u003e[译][论文] InstructGPT：基于人类反馈训练语言模型遵从指令的能力（OpenAI，2022）\u003c/h1\u003e\n  \u003cp class=\"meta\"\u003ePublished at 2024-03-24 | Last Update 2024-03-24\u003c/p\u003e\n  \n  \u003ch3 id=\"译者序\"\u003e译者序\u003c/h3\u003e\n\n\u003cp\u003e本文翻译自 2022 年 OpenAI 的论文：\n\u003ca href=\"https://arxiv.org/abs/2203.02155\"\u003eTraining language models to follow instructions with human feedback\u003c/a\u003e，\n整理翻译了其中感兴趣的部分。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/llm-practical-guide/fig-1.png\" width=\"90%\" height=\"90%\"/\u003e\u003c/p\u003e\n\u003cp\u003e大模型进化树，可以看到 InstructGPT 所处的年代和位置。来自 \u003ca href=\"/blog/llm-practical-guide-zh/\"\u003e大语言模型（LLM）综述与实用指南（Amazon，2023）\u003c/a\u003e。\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eGPT -\u0026gt; InstructGPT -\u0026gt; ChatGPT\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 的过程，可参考\n\u003ca href=\"/blog/how-to-train-a-gpt-assistant-zh/\"\u003e如何训练一个企业级 GPT 助手（OpenAI，2023）\u003c/a\u003e。\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e译者水平有限，不免存在遗漏或错误之处。如有疑问，敬请查阅原文。\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003e以下是译文。\u003c/p\u003e\n\n\u003chr/\u003e\n\n\u003cul id=\"markdown-toc\"\u003e\n  \u003cli\u003e\u003ca href=\"#译者序\" id=\"markdown-toc-译者序\"\u003e译者序\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#摘要\" id=\"markdown-toc-摘要\"\u003e摘要\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#1-引言\" id=\"markdown-toc-1-引言\"\u003e1 引言\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#11-大模型存在的问题\" id=\"markdown-toc-11-大模型存在的问题\"\u003e1.1 大模型存在的问题\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#12-语言模型建模偏差预测下一个-token-vs-有益且安全地遵循用户指令\" id=\"markdown-toc-12-语言模型建模偏差预测下一个-token-vs-有益且安全地遵循用户指令\"\u003e1.2 语言模型建模偏差：预测下一个 token \u003ccode class=\"language-plaintext highlighter-rouge\"\u003evs.\u003c/code\u003e 有益且安全地遵循用户指令\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#13-常规解决方式及评估标准\" id=\"markdown-toc-13-常规解决方式及评估标准\"\u003e1.3 常规解决方式及评估标准\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#14-本文方法基于人类反馈微调来对齐\" id=\"markdown-toc-14-本文方法基于人类反馈微调来对齐\"\u003e1.4 本文方法：基于人类反馈+微调来对齐\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#15-模型尺寸及架构\" id=\"markdown-toc-15-模型尺寸及架构\"\u003e1.5 模型尺寸及架构\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#16-主要发现\" id=\"markdown-toc-16-主要发现\"\u003e1.6 主要发现\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#161-标注员明显更喜欢-instructgpt-而非-gpt-3-的输出\" id=\"markdown-toc-161-标注员明显更喜欢-instructgpt-而非-gpt-3-的输出\"\u003e1.6.1 标注员明显更喜欢 InstructGPT 而非 GPT-3 的输出\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#162-instructgpt-相比-gpt-3-在真实性方面有所改进\" id=\"markdown-toc-162-instructgpt-相比-gpt-3-在真实性方面有所改进\"\u003e1.6.2 InstructGPT 相比 GPT-3 在真实性方面有所改进\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#163-instructgpt-相比-gpt-3-毒性略微下降但偏见未下降\" id=\"markdown-toc-163-instructgpt-相比-gpt-3-毒性略微下降但偏见未下降\"\u003e1.6.3 InstructGPT 相比 GPT-3 毒性略微下降，但偏见未下降\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#164-通过修改-rlhf-微调过程可以最小化在公开-nlp-数据集上的性能退化\" id=\"markdown-toc-164-通过修改-rlhf-微调过程可以最小化在公开-nlp-数据集上的性能退化\"\u003e1.6.4 通过修改 RLHF 微调过程，可以最小化在公开 NLP 数据集上的性能退化\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#165-在公开-nlp-数据集上微调不如在人类偏好数据上微调的效果好\" id=\"markdown-toc-165-在公开-nlp-数据集上微调不如在人类偏好数据上微调的效果好\"\u003e1.6.5 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e在公开 NLP 数据集上微调\u003c/code\u003e不如\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e在人类偏好数据上微调\u003c/code\u003e的效果好\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#166-instructgpt-对-rlhf-微调之外的指令有良好的泛化能力\" id=\"markdown-toc-166-instructgpt-对-rlhf-微调之外的指令有良好的泛化能力\"\u003e1.6.6 InstructGPT 对 RLHF 微调之外的指令有良好的泛化能力\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#2-相关工作\" id=\"markdown-toc-2-相关工作\"\u003e2 相关工作\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#21-对齐alignment与人类反馈学习learning-from-human-feedback研究\" id=\"markdown-toc-21-对齐alignment与人类反馈学习learning-from-human-feedback研究\"\u003e2.1 对齐（alignment）与人类反馈学习（learning from human feedback）研究\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#211-rlhf来自游戏领域\" id=\"markdown-toc-211-rlhf来自游戏领域\"\u003e2.1.1 RLHF：来自游戏领域\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#212-instructgpt基于-rlhf-在更广泛的语言任务上对齐-llm\" id=\"markdown-toc-212-instructgpt基于-rlhf-在更广泛的语言任务上对齐-llm\"\u003e2.1.2 InstructGPT：基于 RLHF 在更广泛的语言任务上对齐 LLM\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#213-语言模型对齐意味着什么\" id=\"markdown-toc-213-语言模型对齐意味着什么\"\u003e2.1.3 语言模型对齐意味着什么\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#22-训练模型遵循指令follow-instructions\" id=\"markdown-toc-22-训练模型遵循指令follow-instructions\"\u003e2.2 训练模型遵循指令（follow instructions）\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#23-评估语言模型的危害\" id=\"markdown-toc-23-评估语言模型的危害\"\u003e2.3 评估语言模型的危害\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#24-修改模型行为降低危害\" id=\"markdown-toc-24-修改模型行为降低危害\"\u003e2.4 修改模型行为，降低危害\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#3-方法论与实验详情\" id=\"markdown-toc-3-方法论与实验详情\"\u003e3 方法论与实验详情\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#31-high-level-方法论\" id=\"markdown-toc-31-high-level-方法论\"\u003e3.1 High-level 方法论\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#311-准备工作\" id=\"markdown-toc-311-准备工作\"\u003e3.1.1 准备工作\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#312-instructgpt-训练三部曲\" id=\"markdown-toc-312-instructgpt-训练三部曲\"\u003e3.1.2 InstructGPT 训练三部曲\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#32-数据集\" id=\"markdown-toc-32-数据集\"\u003e3.2 数据集\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#321-主要来自-openai-api-用户数据\" id=\"markdown-toc-321-主要来自-openai-api-用户数据\"\u003e3.2.1 主要来自 OpenAI API 用户数据\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#322-去重\" id=\"markdown-toc-322-去重\"\u003e3.2.2 去重\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#323-冷启动第一版-instructgpt\" id=\"markdown-toc-323-冷启动第一版-instructgpt\"\u003e3.2.3 冷启动（第一版 InstructGPT）\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#323-三种-promptplainfew-shotuser-based\" id=\"markdown-toc-323-三种-promptplainfew-shotuser-based\"\u003e3.2.3 三种 prompt：plain/few-shot/user-based\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#324-三个-prompts-数据集及大小\" id=\"markdown-toc-324-三个-prompts-数据集及大小\"\u003e3.2.4 三个 prompts 数据集及大小\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#325-prompts-类别分布及占比\" id=\"markdown-toc-325-prompts-类别分布及占比\"\u003e3.2.5 Prompts 类别分布及占比\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#326-几个-prompt-例子\" id=\"markdown-toc-326-几个-prompt-例子\"\u003e3.2.6 几个 prompt 例子\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#33-训练任务\" id=\"markdown-toc-33-训练任务\"\u003e3.3 训练任务\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#34-人工数据收集\" id=\"markdown-toc-34-人工数据收集\"\u003e3.4 人工数据收集\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#341-标注员筛选\" id=\"markdown-toc-341-标注员筛选\"\u003e3.4.1 标注员筛选\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#342-对齐冲突的处理\" id=\"markdown-toc-342-对齐冲突的处理\"\u003e3.4.2 对齐冲突的处理\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#343-对照度标注员验证泛华能力\" id=\"markdown-toc-343-对照度标注员验证泛华能力\"\u003e3.4.3 对照度标注员：验证泛华能力\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#35-models模型\" id=\"markdown-toc-35-models模型\"\u003e3.5 Models（模型）\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#351-supervised-fine-tuning-sft\" id=\"markdown-toc-351-supervised-fine-tuning-sft\"\u003e3.5.1 Supervised fine-tuning (SFT)\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#352-reward-modeling-rm\" id=\"markdown-toc-352-reward-modeling-rm\"\u003e3.5.2 Reward modeling (RM)\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#353-reinforcement-learning-rl\" id=\"markdown-toc-353-reinforcement-learning-rl\"\u003e3.5.3 Reinforcement learning (RL)\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#354-性能比较基线\" id=\"markdown-toc-354-性能比较基线\"\u003e3.5.4 性能比较基线\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#36-性能评估\" id=\"markdown-toc-36-性能评估\"\u003e3.6 性能评估\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#361-指标\" id=\"markdown-toc-361-指标\"\u003e3.6.1 指标\u003c/a\u003e            \u003cul\u003e\n              \u003cli\u003e\u003ca href=\"#helpful\" id=\"markdown-toc-helpful\"\u003ehelpful\u003c/a\u003e\u003c/li\u003e\n              \u003cli\u003e\u003ca href=\"#honest--truthfulness\" id=\"markdown-toc-honest--truthfulness\"\u003ehonest / truthfulness\u003c/a\u003e\u003c/li\u003e\n              \u003cli\u003e\u003ca href=\"#harmless\" id=\"markdown-toc-harmless\"\u003eharmless\u003c/a\u003e\u003c/li\u003e\n            \u003c/ul\u003e\n          \u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#362-定量评估\" id=\"markdown-toc-362-定量评估\"\u003e3.6.2 定量评估\u003c/a\u003e            \u003cul\u003e\n              \u003cli\u003e\u003ca href=\"#在-openai-api-真实用户的-prompts-上的表现\" id=\"markdown-toc-在-openai-api-真实用户的-prompts-上的表现\"\u003e在 OpenAI API 真实用户的 prompts 上的表现\u003c/a\u003e\u003c/li\u003e\n              \u003cli\u003e\u003ca href=\"#在公开-nlp-数据集上的表现\" id=\"markdown-toc-在公开-nlp-数据集上的表现\"\u003e在公开 NLP 数据集上的表现\u003c/a\u003e\u003c/li\u003e\n            \u003c/ul\u003e\n          \u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#4-结果\" id=\"markdown-toc-4-结果\"\u003e4 结果\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#5-问题讨论\" id=\"markdown-toc-5-问题讨论\"\u003e5 问题讨论\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#参考文献\" id=\"markdown-toc-参考文献\"\u003e参考文献\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#附录-a-prompt-数据详情\" id=\"markdown-toc-附录-a-prompt-数据详情\"\u003e附录 A: Prompt 数据详情\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#a1-labeler-written-prompts\" id=\"markdown-toc-a1-labeler-written-prompts\"\u003eA.1 Labeler-written prompts\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#a2-api-user-prompts\" id=\"markdown-toc-a2-api-user-prompts\"\u003eA.2 API user prompts\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#a21-从-instructgpt-api-playground-收集上来的-user-prompts-示例\" id=\"markdown-toc-a21-从-instructgpt-api-playground-收集上来的-user-prompts-示例\"\u003eA.2.1 从 InstructGPT API (Playground) 收集上来的 user prompts 示例\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#a22-从-gpt-3-api-收集上来的-user-prompts-示例\" id=\"markdown-toc-a22-从-gpt-3-api-收集上来的-user-prompts-示例\"\u003eA.2.2 从 GPT-3 API 收集上来的 user prompts 示例\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#a3-数据集大小sft-15k--rm-50k--ppo-47k\" id=\"markdown-toc-a3-数据集大小sft-15k--rm-50k--ppo-47k\"\u003eA.3 数据集大小：\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eSFT 15k / RM 50k / PPO 47k\u003c/code\u003e\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#a4-数据多样性\" id=\"markdown-toc-a4-数据多样性\"\u003eA.4 数据多样性\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#附录-badditional-human-data-collection-details\" id=\"markdown-toc-附录-badditional-human-data-collection-details\"\u003e附录 B：Additional human data collection details\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#附录-c一些模型细节\" id=\"markdown-toc-附录-c一些模型细节\"\u003e附录 C：一些模型细节\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#c1-sft-训练细节\" id=\"markdown-toc-c1-sft-训练细节\"\u003eC.1 SFT 训练细节\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#c2-rm-训练细节\" id=\"markdown-toc-c2-rm-训练细节\"\u003eC.2 RM 训练细节\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#c3-rlhf-的初始化模型initialization-models细节\" id=\"markdown-toc-c3-rlhf-的初始化模型initialization-models细节\"\u003eC.3 RLHF 的初始化模型（initialization models）细节\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#c4-rlhf-训练细节\" id=\"markdown-toc-c4-rlhf-训练细节\"\u003eC.4 RLHF 训练细节\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#c5-flan-和-t0-模型\" id=\"markdown-toc-c5-flan-和-t0-模型\"\u003eC.5 FLAN 和 T0 模型\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#附录-dautomatic-evaluation-details\" id=\"markdown-toc-附录-dautomatic-evaluation-details\"\u003e附录 D：Automatic evaluation details\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#附录-eadditional-results\" id=\"markdown-toc-附录-eadditional-results\"\u003e附录 E：Additional results\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#附录-fmodel-samples\" id=\"markdown-toc-附录-fmodel-samples\"\u003e附录 F：Model samples\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003chr/\u003e\n\n\u003cscript type=\"text/x-mathjax-config\"\u003e\n    MathJax.Hub.Config({\n      extensions: [\"tex2jax.js\"],\n      jax: [\"input/TeX\", \"output/HTML-CSS\"],\n      tex2jax: {\n          inlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n          displayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ],\n        processEscapes: true\n      },\n    \"HTML-CSS\": {\n      availableFonts: [], preferredFont: null,\n      webFont: \"Neo-Euler\",\n      mtextFontInherit: true\n    },\n    TeX: {\n      extensions: [\"color.js\"],\n      Macros: {\n        lgc: [\"{\\\\color{my-light-green} #1}\", 1],\n        gc: [\"{\\\\color{my-green} #1}\", 1],\n        lrc: [\"{\\\\color{my-light-red} #1}\", 1],\n        rc: [\"{\\\\color{my-red} #1}\", 1],\n        lbc: [\"{\\\\color{my-light-blue} #1}\", 1],\n        bc: [\"{\\\\color{my-blue} #1}\", 1],\n        kc: [\"{\\\\color{my-gray} #1}\", 1],\n        loc: [\"{\\\\color{my-light-orange} #1}\", 1],\n        oc: [\"{\\\\color{my-orange} #1}\", 1],\n\n        a: [\"\\\\mathbf a\"],\n        A: [\"\\\\mathbf A\"],\n        b: [\"\\\\mathbf b\"],\n        B: [\"\\\\mathbf B\"],\n        c: [\"\\\\mathbf c\"],\n        C: [\"\\\\mathbf C\"],\n        d: [\"\\\\mathbf d\"],\n        D: [\"\\\\mathbf D\"],\n        E: [\"\\\\mathbf E\"],\n        I: [\"\\\\mathbf I\"],\n        L: [\"\\\\mathbf L\"],\n        m: [\"\\\\mathbf m\"],\n        M: [\"\\\\mathbf M\"],\n        r: [\"\\\\mathbf r\"],\n        s: [\"\\\\mathbf s\"],\n        t: [\"\\\\mathbf t\"],\n        S: [\"\\\\mathbf S\"],\n        x: [\"\\\\mathbf x\"],\n        z: [\"\\\\mathbf z\"],\n        v: [\"\\\\mathbf v\"],\n        y: [\"\\\\mathbf y\"],\n        k: [\"\\\\mathbf k\"],\n        bp: [\"\\\\mathbf p\"],\n        P: [\"\\\\mathbf P\"],\n        q: [\"\\\\mathbf q\"],\n        Q: [\"\\\\mathbf Q\"],\n        r: [\"\\\\mathbf r\"],\n        R: [\"\\\\mathbf R\"],\n        Sig: [\"\\\\mathbf \\\\Sigma\"],\n        t: [\"\\\\mathbf t\"],\n        T: [\"\\\\mathbf T\"],\n        e: [\"\\\\mathbf e\"],\n        X: [\"\\\\mathbf X\"],\n        u: [\"\\\\mathbf u\"],\n        U: [\"\\\\mathbf U\"],\n        v: [\"\\\\mathbf v\"],\n        V: [\"\\\\mathbf V\"],\n        w: [\"\\\\mathbf w\"],\n        W: [\"\\\\mathbf W\"],\n        Y: [\"\\\\mathbf Y\"],\n        z: [\"\\\\mathbf z\"],\n        Z: [\"\\\\mathbf Z\"],\n        p: [\"\\\\,\\\\text{.}\"],\n        tab: [\"\\\\hspace{0.7cm}\"],\n\n        sp: [\"^{\\\\small\\\\prime}\"],\n\n\n        mR: [\"{\\\\mathbb R}\"],\n        mC: [\"{\\\\mathbb C}\"],\n        mN: [\"{\\\\mathbb N}\"],\n        mZ: [\"{\\\\mathbb Z}\"],\n\n        deg: [\"{^\\\\circ}\"],\n\n\n        argmin: [\"\\\\underset{#1}{\\\\text{argmin}}\", 1],\n        argmax: [\"\\\\underset{#1}{\\\\text{argmax}}\", 1],\n\n        co: [\"\\\\;\\\\text{cos}\"],\n        si: [\"\\\\;\\\\text{sin}\"]\n      }\n    }\n    });\n\n    MathJax.Hub.Register.StartupHook(\"TeX color Ready\", function() {\n       MathJax.Extension[\"TeX/color\"].colors[\"my-green\"] = '#677d00';\n       MathJax.Extension[\"TeX/color\"].colors[\"my-light-green\"] = '#acd373';\n       MathJax.Extension[\"TeX/color\"].colors[\"my-red\"] = '#b13e26';\n       MathJax.Extension[\"TeX/color\"].colors[\"my-light-red\"] = '#d38473';\n       MathJax.Extension[\"TeX/color\"].colors[\"my-blue\"] = '#306693';\n         MathJax.Extension[\"TeX/color\"].colors[\"my-light-blue\"] = '#73a7d3';\n         MathJax.Extension[\"TeX/color\"].colors[\"my-gray\"] = '#999';\n         MathJax.Extension[\"TeX/color\"].colors[\"my-orange\"] = '#E69500';\n         MathJax.Extension[\"TeX/color\"].colors[\"my-light-orange\"] = '#FFC353';\n\n\n  });\n\u003c/script\u003e\n\n\u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js\"\u003e\n\u003c/script\u003e\n\n\u003ch1 id=\"摘要\"\u003e摘要\u003c/h1\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cmark\u003e增大模型尺寸\u003c/mark\u003e\u003c/strong\u003e未必就能提高它\u003cstrong\u003e\u003cmark\u003e对用户意图的理解能力\u003c/mark\u003e\u003c/strong\u003e。\n例如，一些大模型可能会生成不真实、有毒或对用户并无帮助（untruthful, toxic, or simply not helpful）的输出。\n换句话说，这些模型与它们的用户\u003cstrong\u003e\u003cmark\u003e没有对齐\u003c/mark\u003e\u003c/strong\u003e（not aligned）。\u003c/p\u003e\n\n\u003cp\u003e本文展示了一种\u003cstrong\u003e\u003cmark\u003e基于人类反馈进行微调\u003c/mark\u003e\u003c/strong\u003e（fine-tuning with human feedback），\n从而在各种任务上\u003cstrong\u003e\u003cmark\u003e将语言模型与用户意图对齐\u003c/mark\u003e\u003c/strong\u003e的方法。简单来说，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e先收集一组\u003cstrong\u003e\u003cmark\u003e“预期的模型行为应该是什么样”\u003c/mark\u003e\u003c/strong\u003e的数据集，\n然后使用\u003cstrong\u003e\u003cmark\u003e监督学习来微调 GPT-3\u003c/mark\u003e\u003c/strong\u003e（SFT），\u003c/li\u003e\n  \u003cli\u003e接着，收集一组排名形式组织的\u003cstrong\u003e\u003cmark\u003e模型输出\u003c/mark\u003e\u003c/strong\u003e（rankings of model outputs）作为数据集，\n使用\u003cstrong\u003e\u003cmark\u003e人类反馈强化学习\u003c/mark\u003e\u003c/strong\u003e（RLHF）进一步微调上一步得到的模型。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e我们将最终得到的这种模型称为 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eInstructGPT\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e175b GPT-3 vs. 1.3b InstructGPT\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 的\u003cstrong\u003e\u003cmark\u003e人工测评\u003c/mark\u003e\u003c/strong\u003e显示，\n大家更喜欢后者，尽管它的参数不到前者的 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e1%\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e。\u003c/li\u003e\n  \u003cli\u003eInstructGPT 在真实性（truthfulness）方面也有所改进，减少了有毒输出，\n在公开 NLP 数据集上的性能退化也很小。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e尽管 InstructGPT 仍然会犯一些简单的错误，但我们的研究结果表明，\n\u003cstrong\u003e\u003cmark\u003e基于人类反馈进行微调\u003c/mark\u003e\u003c/strong\u003e（fine-tuning with human feedback）是一个很有前途的\n\u003cstrong\u003e\u003cmark\u003e将语言模型与人类意图对齐\u003c/mark\u003e\u003c/strong\u003e的方向。\u003c/p\u003e\n\n\u003ch1 id=\"1-引言\"\u003e1 引言\u003c/h1\u003e\n\n\u003cp\u003e给定一些任务示例（examples of the task）作为输入，大语言模型（LLMs）可以被 “prompt” 去执行一系列自然语言处理（NLP）任务。\u003c/p\u003e\n\n\u003ch2 id=\"11-大模型存在的问题\"\u003e1.1 大模型存在的问题\u003c/h2\u003e\n\n\u003cp\u003e然而，这些模型经常会出现一些意外的行为，比如编造事实、生成有偏见或有毒的文本，\n或者忽视用户的指示（Bender 等，2021；Bommasani 等，2021；Kenton 等，2021；\nWeidinger 等，2021；Tamkin 等，2021；Gehman 等，2020）。\u003c/p\u003e\n\n\u003ch2 id=\"12-语言模型建模偏差预测下一个-token-vs-有益且安全地遵循用户指令\"\u003e1.2 语言模型建模偏差：预测下一个 token \u003ccode class=\"language-plaintext highlighter-rouge\"\u003evs.\u003c/code\u003e 有益且安全地遵循用户指令\u003c/h2\u003e\n\n\u003cp\u003e出现以上现象，\n是因为许多近期的 LLM 建模目标都是（基于互联网数据训练）\u003cstrong\u003e\u003cmark\u003e预测下一个 token\u003c/mark\u003e\u003c/strong\u003e ——\n而并不是\u003cstrong\u003e\u003cmark\u003e“有益且安全地遵循用户的指令”\u003c/mark\u003e\u003c/strong\u003e（Radford 等，2019；Brown 等，2020；Fedus 等，2021；Rae 等，2021；Thoppilan 等，2022）。\n也就是说，\u003cstrong\u003e\u003cmark\u003e语言建模目标有偏差\u003c/mark\u003e\u003c/strong\u003e（the language modeling objective is misaligned）。\u003c/p\u003e\n\n\u003cp\u003e由于 LLM 已经部署在大量实际应用中，因此解决大模型的这些非预期行为非常重要。\u003c/p\u003e\n\n\u003ch2 id=\"13-常规解决方式及评估标准\"\u003e1.3 常规解决方式及评估标准\u003c/h2\u003e\n\n\u003cp\u003e通过训练语言模型按照用户意图行事（Leike 等，2018）来推进语言模型的对齐。\n这里的意图包括\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e明确的意图，如遵循指示，\u003c/li\u003e\n  \u003cli\u003e隐含的意图，如保持真实、无偏见、无毒及无害性。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e使用 Askell 等（2021）的术语，我们希望语言模型是\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e有帮助的（\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003ehelpful\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e，应该帮助用户解决任务），\u003c/li\u003e\n  \u003cli\u003e诚实的（\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003ehonest\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e，不应该捏造信息或误导用户），\u003c/li\u003e\n  \u003cli\u003e无害的（\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eharmless\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e，不应该对人或环境造成身体、心理或社会伤害）。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e我们在第 3.6 节中详细阐述了这些标准的评估。\u003c/p\u003e\n\n\u003ch2 id=\"14-本文方法基于人类反馈微调来对齐\"\u003e1.4 本文方法：基于人类反馈+微调来对齐\u003c/h2\u003e\n\n\u003cp\u003e本文专注于\u003cstrong\u003e\u003cmark\u003e通过微调方法来对齐语言模型\u003c/mark\u003e\u003c/strong\u003e。具体来说，\n使用人类反馈强化学习（RLHF；Christiano 等，2017；Stiennon 等，2020）\n来微调 GPT-3，以便它能遵循类型广泛的各种用户指令。具体过程如图 2 所示，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/instructgpt-paper/fig-2.png\" width=\"100%\"/\u003e\u003c/p\u003e\n\u003cp alige=\"center\"\u003e Figure 2: \u003cmark\u003eInstructGPT 三部曲\u003c/mark\u003e：(1) \u003cmark\u003eSFT\u003c/mark\u003e, (2)\n\u003cmark\u003eRM training\u003c/mark\u003e, (3) \u003cmark\u003eRLHF via proximal policy optimization (PPO)\u003c/mark\u003e on RM.\u003cbr/\u003e\n蓝色箭头表示相应的数据用于训练模型。Step 2 中 A-D 是模型输出的采样，然后标注员对它们进行排序。详见 Section 3。\n\u003c/p\u003e\n\n\u003cp\u003e三个步骤：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\n    \u003cp\u003e收集示例数据（\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003edemonstration data\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e），训练一个监督策略（\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003esupervised policy\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e）。\u003c/p\u003e\n\n    \u003cp\u003e对于给定的输入，\u003cstrong\u003e\u003cmark\u003e标注员给出期望的行为\u003c/mark\u003e\u003c/strong\u003e (详见 3.2 节)。然后，使用监督学习（supervised learning）对一个预训练的 GPT-3 模型进行微调。\u003c/p\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003e收集对比数据（\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003ecomparison data\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e），训练一个奖励模型（\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eRM\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e）。\u003c/p\u003e\n\n    \u003cp\u003e对给定输入，收集两个输出，标注员给出他们的偏好（which output they prefer）。然后，训练一个奖励模型来预测人类偏好输出（human-preferred output）。\u003c/p\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003e针对奖励模型，使用 PPO 对策略进行优化（optimize a policy）。\u003c/p\u003e\n\n    \u003cp\u003e将 RM 的输出作为一个标量奖励。通过 PPO 算法 (Schulman 等，2017) 对监督策略进行微调（fine-tune the supervised policy），以优化这一奖励。\u003c/p\u003e\n  \u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e步骤 2 和 3 可以持续迭代；在当前最佳策略上收集更多的对比数据，这些数据又用于训练新的 RM 和新的策略。\n实际上，大部分对比数据来自于我们的 supervised policies，一小部分来自于我们的 PPO policies。\u003c/p\u003e\n\n\u003cp\u003e这个过程将 GPT-3 的行为与特定人群的偏好（stated preferences of a specific group of people，大多是我们的标注员和研究人员），\n而非任何更广泛的“人类价值观”对齐；5.2 节将进一步讨论这个问题。\u003c/p\u003e\n\n\u003ch2 id=\"15-模型尺寸及架构\"\u003e1.5 模型尺寸及架构\u003c/h2\u003e\n\n\u003cp\u003e我们训练了三种尺寸的 InstructGPT 模型：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e1.3B\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e6B\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e175B\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e所有模型都使用了 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eGPT-3\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 架构。\u003c/p\u003e\n\n\u003ch2 id=\"16-主要发现\"\u003e1.6 主要发现\u003c/h2\u003e\n\n\u003cp\u003e我们的主要发现如下。\u003c/p\u003e\n\n\u003ch3 id=\"161-标注员明显更喜欢-instructgpt-而非-gpt-3-的输出\"\u003e1.6.1 标注员明显更喜欢 InstructGPT 而非 GPT-3 的输出\u003c/h3\u003e\n\n\u003cp\u003e我们将 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e175b GPT-3 vs. 1.3b InstructGPT\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 的输出进行了\u003cstrong\u003e\u003cmark\u003e人工测评\u003c/mark\u003e\u003c/strong\u003e，\n大家明显更喜欢后者，尽管它的参数不到前者的 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e1%\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e这两类模型具有相同的架构，唯一的区别是 \u003cstrong\u003e\u003cmark\u003eInstructGPT 在人工数据上进行了微调\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e作为对比，我们给 GPT-3 添加了一个 few-shot prompt 以使其更好地遵循指令（变成了一个\u003cstrong\u003e\u003cmark\u003e提示词调优过的 GPT-3\u003c/mark\u003e\u003c/strong\u003e），\n但效果仍赶不上 InstructGPT：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e175B InstructGPT 在 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e85 ± 3%\u003c/code\u003e 的结果中优于 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e175B GPT-3\u003c/code\u003e，\u003c/li\u003e\n  \u003cli\u003e175B InstructGPT 在 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e71 ± 4%\u003c/code\u003e 的结果中优于 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003efew-shot 175B GPT-3\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e根据标注员的反馈，InstructGPT 模型的输出更符合 prompt ，并更可靠地遵循指令中的明确约束。\u003c/p\u003e\n\n\u003ch3 id=\"162-instructgpt-相比-gpt-3-在真实性方面有所改进\"\u003e1.6.2 InstructGPT 相比 GPT-3 在真实性方面有所改进\u003c/h3\u003e\n\n\u003cp\u003e在 TruthfulQA 基准测试中，InstructGPT 生成 truthful \u0026amp; informative 答案的概率比 GPT-3 高约一倍。\u003c/p\u003e\n\n\u003cp\u003e对于“封闭域”（closed-domain）任务（\u003cstrong\u003e\u003cmark\u003e输出不应包含输入中不存在的信息\u003c/mark\u003e\u003c/strong\u003e，例如摘要和封闭域的问答测试，\nInstructGPT 的信息虚构率（\u003cstrong\u003e\u003cmark\u003e编造输入中不存在的信息\u003c/mark\u003e\u003c/strong\u003e）只有 GPT-3 的一半（\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e21% vs. 41%\u003c/code\u003e）。\u003c/p\u003e\n\n\u003ch3 id=\"163-instructgpt-相比-gpt-3-毒性略微下降但偏见未下降\"\u003e1.6.3 InstructGPT 相比 GPT-3 毒性略微下降，但偏见未下降\u003c/h3\u003e\n\n\u003cp\u003e为了衡量毒性，我们使用了 RealToxicityPrompts 数据集（Gehman 等，2020），并进行了自动和人工评估。\n当提示模型需要 respectful 时（prompted to be respectful），InstructGPT 生成的有毒输出比 GPT-3 少约 25%。\u003c/p\u003e\n\n\u003cp\u003e在 Winogender（Rudinger 等，2018）和 CrowSPairs（Nangia 等，2020）数据集上，\nInstructGPT 相比 GPT-3 没有明显改进。\u003c/p\u003e\n\n\u003ch3 id=\"164-通过修改-rlhf-微调过程可以最小化在公开-nlp-数据集上的性能退化\"\u003e1.6.4 通过修改 RLHF 微调过程，可以最小化在公开 NLP 数据集上的性能退化\u003c/h3\u003e\n\n\u003cp\u003e在 RLHF 微调过程中，我们观察到在某些公开 NLP 数据集上 InstructGPT 相比 GPT-3 存在性能下降，\n尤其是 SQuAD（Rajpurkar 等，2018）、DROP（Dua 等，2019）、HellaSwag（Zellers 等，2019）和 WMT 2015 法英翻译（Bojar 等，2015）。\u003c/p\u003e\n\n\u003cp\u003e这是一个\u003cstrong\u003e\u003cmark\u003e“对齐税”\u003c/mark\u003e\u003c/strong\u003e（alignment tax）的例子 —— 对齐可能会牺牲在某些任务上的性能。\n在不降低标注员偏好分数的前提下，我们通过混合 PPO updates 与 PPO-ptx updates（增加预训练分布的对数似然），\n大大减少了在这些数据集上的性能下降。\u003c/p\u003e\n\n\u003cp\u003eInstructGPT 可以推广到那些\u003cstrong\u003e\u003cmark\u003e未参与编写训练数据的标注员\u003c/mark\u003e\u003c/strong\u003e（held-out labelers）。\n为测试 InstructGPT 的\u003cstrong\u003e\u003cmark\u003e泛化能力\u003c/mark\u003e\u003c/strong\u003e而进行的初步实验结果表明，与参与训练的标注员（training labelers）一样，\n未参与训练的标注员也更喜欢 InstructGPT 而不是 GPT-3 的输出。\n当然，还需要进一步研究这些模型在更广泛的用户群体上的表现，\n以及它们在人们所期望的行为存在分歧的输入上的表现（inputs where humans disagree about the desired behavior）。\u003c/p\u003e\n\n\u003ch3 id=\"165-在公开-nlp-数据集上微调不如在人类偏好数据上微调的效果好\"\u003e1.6.5 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e在公开 NLP 数据集上微调\u003c/code\u003e不如\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e在人类偏好数据上微调\u003c/code\u003e的效果好\u003c/h3\u003e\n\n\u003cp\u003e我们比较了两个微调的 GPT-3：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e在\u003cstrong\u003e\u003cmark\u003e人类偏好数据\u003c/mark\u003e\u003c/strong\u003e上微调的 GPT-3（即 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eInstructGPT\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e）；\u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003e在两个\u003cstrong\u003e\u003cmark\u003e公开 NLP 任务\u003c/mark\u003e\u003c/strong\u003e（FLAN（Wei 等，2021）和 T0/T0++（Sanh 等，2021）上微调的 GPT-3。\u003c/p\u003e\n\n    \u003cp\u003e这两个数据集包含多种 NLP 任务，以及每个任务的自然语言指令（natural language instructions）。\u003c/p\u003e\n  \u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e标注员明显更喜欢 InstructGPT 的输出。相比基线，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eInstructGPT 的胜率为 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e73.4 ± 2％\u003c/code\u003e，\u003c/li\u003e\n  \u003cli\u003eT0 和 FLAN fine-tuned GPT-3 分别为 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e26.8 ± 2％\u003c/code\u003e 和 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e29.8 ± 2％\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 id=\"166-instructgpt-对-rlhf-微调之外的指令有良好的泛化能力\"\u003e1.6.6 InstructGPT 对 RLHF 微调之外的指令有良好的泛化能力\u003c/h3\u003e\n\n\u003cp\u003e我们对 InstructGPT 的能力进行了定性探究，发现它能够遵循如下指令：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e总结代码，\u003c/li\u003e\n  \u003cli\u003e回答关于代码的问题，\u003c/li\u003e\n  \u003cli\u003e有时还能遵循不同语言的指令，尽管这些指令在训练数据中非常少。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e相比之下，GPT-3 虽然也可以执行这些任务，但需要更精心设计的 prompt ，并且遵循这些领域指令的效果欠佳。\u003c/p\u003e\n\n\u003cp\u003e这个结果很令人兴奋，因为它表明 InstructGPT 能够推广“遵循指令”的概念。\n即使在只有非常少的直接监督信号（SFT 训练样本）的任务上，它们仍然具备了一定的对齐性。\u003c/p\u003e\n\n\u003cp\u003eInstructGPT 仍然会犯一些简单的错误。例如，可能无法遵循指令，捏造事实，对简单问题给出冗长的回答，或无法检测出有错误前提的指令。\n但总体而言，我们的结果表明，使用人类偏好微调大语言模型可以显著改善它们在各种任务上的行为，\n相应的，也需要更多工作提高它们的安全性和可靠性。\u003c/p\u003e\n\n\u003ch1 id=\"2-相关工作\"\u003e2 相关工作\u003c/h1\u003e\n\n\u003ch2 id=\"21-对齐alignment与人类反馈学习learning-from-human-feedback研究\"\u003e2.1 对齐（alignment）与人类反馈学习（learning from human feedback）研究\u003c/h2\u003e\n\n\u003ch3 id=\"211-rlhf来自游戏领域\"\u003e2.1.1 RLHF：来自游戏领域\u003c/h3\u003e\n\n\u003cp\u003eInstructGPT 建立在前人的技术基础上，特别是用人类反馈强化学习\u003cstrong\u003e\u003cmark\u003e（RLHF）来对齐模型\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003eRLHF 最初是为了在模拟环境（simulated environments）和 Atari 游戏中\u003cstrong\u003e\u003cmark\u003e训练简单机器人\u003c/mark\u003e\u003c/strong\u003e而开发的（Christiano 等，2017; Ibarz 等，2018），\n最近被用于\u003cstrong\u003e\u003cmark\u003e微调语言模型\u003c/mark\u003e\u003c/strong\u003e来\u003cstrong\u003e\u003cmark\u003e总结文本\u003c/mark\u003e\u003c/strong\u003e（Ziegler 等，2019; Stiennon 等，2020; Böhm 等，2019; Wu 等，2021）。\u003c/p\u003e\n\n\u003ch3 id=\"212-instructgpt基于-rlhf-在更广泛的语言任务上对齐-llm\"\u003e2.1.2 InstructGPT：基于 RLHF 在更广泛的语言任务上对齐 LLM\u003c/h3\u003e\n\n\u003cp\u003e这项工作还受到了下列类似工作的影响：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e对话（Jaques 等，2019; Yi 等，2019; Hancock 等，2019）\u003c/li\u003e\n  \u003cli\u003e翻译（Kreutzer 等，2018; Bahdanau 等，2016）\u003c/li\u003e\n  \u003cli\u003e语义解析（Lawrence 和 Riezler，2018）\u003c/li\u003e\n  \u003cli\u003e故事生成（Zhou 和 Xu，2020）\u003c/li\u003e\n  \u003cli\u003e评论生成（Cho 等，2018）\u003c/li\u003e\n  \u003cli\u003e证据提取（Perez 等，2019）\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eMadaan 等（2022）使用人类反馈来增强 prompts，以提高 GPT-3 的性能。\n在基于文本的环境中，使用带有 4a normative prior 的 RL 来对齐 agents（Nahian 等，2021）。\u003c/p\u003e\n\n\u003cp\u003e我们的工作可以看作是\u003cstrong\u003e\u003cmark\u003e用 RLHF 在更广泛的语言任务上对齐语言模型\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003ch3 id=\"213-语言模型对齐意味着什么\"\u003e2.1.3 语言模型对齐意味着什么\u003c/h3\u003e\n\n\u003cp\u003e近期，“语言模型对齐意味着什么”这一问题备受关注（Gabriel，2020）。\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eKenton 等（2021）列出了由于不对齐而导致的模型行为问题，包括产生有害内容和游戏中的错误目标。\u003c/li\u003e\n  \u003cli\u003e同一时间，Askell 等（2021）提出将语言助手作为对齐研究的测试对象，研究了一些简单的基线和它们的扩展特性。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 id=\"22-训练模型遵循指令follow-instructions\"\u003e2.2 训练模型遵循指令（follow instructions）\u003c/h2\u003e\n\n\u003cp\u003eOur work is also related to research on crosstask generalization in language models, where LMs are fine-tuned on a broad range of public NLP\ndatasets (usually prefixed with an appropriate instruction) and evaluated on a different set of NLP\ntasks. There has been a range of work in this domain (Yi et al., 2019; Mishra et al., 2021; Wei\net al., 2021; Khashabi et al., 2020; Sanh et al., 2021; Aribandi et al., 2021), which differ in training\nand evaluation data, formatting of instructions, size of pretrained models, and other experimental\ndetails. A consistent finding across studies is that fine-tuning LMs on a range of NLP tasks, with\ninstructions, improves their downstream performance on held-out tasks, both in the zero-shot and\nfew-shot settings.\u003c/p\u003e\n\n\u003cp\u003eThere is also a related line of work on \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003einstruction following for navigation\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e, where models are trained\nto follow natural language instructions to navigate in a simulated environment (Bahdanau et al., 2018;\nAbramson et al., 2020; Zhao et al., 2021).\u003c/p\u003e\n\n\u003ch2 id=\"23-评估语言模型的危害\"\u003e2.3 评估语言模型的危害\u003c/h2\u003e\n\n\u003cp\u003eA goal of modifying the behavior of language models\nis to mitigate the harms of these models when they’re deployed in the real world. These risks have\nbeen extensively documented (Bender et al., 2021; Bommasani et al., 2021; Kenton et al., 2021;\nWeidinger et al., 2021; Tamkin et al., 2021). Language models can produce biased outputs (Dhamala\net al., 2021; Liang et al., 2021; Manela et al., 2021; Caliskan et al., 2017; Kirk et al., 2021), leak\nprivate data (Carlini et al., 2021), generate misinformation (Solaiman et al., 2019; Buchanan et al.,\n2021), and be used maliciously; for a thorough review we direct the reader to Weidinger et al. (2021).\nDeploying language models in specific domains gives rise to new risks and challenges, for example in\ndialog systems (Henderson et al., 2018; Xu et al., 2020; Dinan et al., 2019b). There is a nascent but\ngrowing field that aims to build benchmarks to concretely evaluate these harms, particularly around\ntoxicity (Gehman et al., 2020), stereotypes (Nadeem et al., 2020), and social bias (Dhamala et al.,\n2021; Nangia et al., 2020; Rudinger et al., 2018). Making significant progress on these problems is\nhard since well-intentioned interventions on LM behavior can have side-effects (Welbl et al., 2021;\nBlodgett et al., 2020); for instance, efforts to reduce the toxicity of LMs can reduce their ability to\nmodel text from under-represented groups, due to prejudicial correlations in the training data (Xu\net al., 2021).\u003c/p\u003e\n\n\u003ch2 id=\"24-修改模型行为降低危害\"\u003e2.4 修改模型行为，降低危害\u003c/h2\u003e\n\n\u003cp\u003eThere are many ways to change\nthe generation behavior of language models. Solaiman and Dennison (2021) fine-tune LMs on a\nsmall, value-targeted dataset, which improves the models’ ability to adhere to these values on a\nquestion answering task. Ngo et al. (2021) filter the pretraining dataset by removing documents on\nwhich a language model has a high conditional likelihood of generating a set of researcher-written\ntrigger phrases. When trained on this filtered dataset, their LMs generate less harmful text, at the cost\nof a slight decrease in language modeling performance. Xu et al. (2020) use a variety of approaches\nto improve the safety of chatbots, including data filtering, blocking certain words or n-grams during\ngeneration, safety-specific control tokens (Keskar et al., 2019; Dinan et al., 2019a), and human-in-theloop data collection (Dinan et al., 2019b). Other approaches for mitigating the generated bias by LMs\nuse word embedding regularization (Liu et al., 2019; Huang et al., 2019), data augmentation (Liu\net al., 2019; Dinan et al., 2019a; Sheng et al., 2019), null space projection to make the distribution\nover sensitive tokens more uniform (Liang et al., 2021), different objective functions (Qian et al.,\n2019), or causal mediation analysis (Vig et al., 2020). There is also work on steering the generation\nof language models using a second (usually smaller) language model (Dathathri et al., 2019; Krause\net al., 2020), and variants of this idea have been applied to reducing language model toxicity (Schick\net al., 2021)\u003c/p\u003e\n\n\u003ch1 id=\"3-方法论与实验详情\"\u003e3 方法论与实验详情\u003c/h1\u003e\n\n\u003ch2 id=\"31-high-level-方法论\"\u003e3.1 High-level 方法论\u003c/h2\u003e\n\n\u003cp\u003e我们延用了 Ziegler 等 (2019) 和 Stiennon 等 (2020) 在 stylistic continuation and summarization 领域应用的方法，\u003c/p\u003e\n\n\u003ch3 id=\"311-准备工作\"\u003e3.1.1 准备工作\u003c/h3\u003e\n\n\u003cp\u003e如下基础准备：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e一个预训练的语言模型，即 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eGPT-3\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e (Radford 等，2019; Brown 等，2020; Fedus 等，2021; Rae 等，2021; Thoppilan 等，2022)\u003c/li\u003e\n  \u003cli\u003e一个 prompt 类别分布（a distribution of prompts，希望模型输出对齐到这些领域）\u003c/li\u003e\n  \u003cli\u003e一个经过培训的人工标注团队 (详见 3.4 节)。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 id=\"312-instructgpt-训练三部曲\"\u003e3.1.2 InstructGPT 训练三部曲\u003c/h3\u003e\n\n\u003cp\u003e按照以下三个步骤开始训练，如图 2 所示，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/instructgpt-paper/fig-2.png\" width=\"100%\"/\u003e\u003c/p\u003e\n\u003cp alige=\"center\"\u003e Figure 2: \u003cmark\u003eInstructGPT 三部曲\u003c/mark\u003e：(1) \u003cmark\u003eSFT\u003c/mark\u003e, (2)\n\u003cmark\u003eRM training\u003c/mark\u003e, (3) \u003cmark\u003eRLHF via proximal policy optimization (PPO)\u003c/mark\u003e on RM.\u003cbr/\u003e\n蓝色箭头表示相应的数据用于训练模型。Step 2 中 A-D 是模型输出的采样，然后标注员对它们进行排序。详见 Section 3。\n\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\n    \u003cp\u003e收集示范数据（\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003edemonstration data\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e），训练一个监督策略（\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003esupervised policy\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e）。\u003c/p\u003e\n\n    \u003cp\u003e对于给定的输入，\u003cstrong\u003e\u003cmark\u003e标注员给出期望的行为\u003c/mark\u003e\u003c/strong\u003e (详见 3.2 节)。然后，使用监督学习（supervised learning）对一个预训练的 GPT-3 模型进行微调。\u003c/p\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003e收集对比数据（\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003ecomparison data\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e），训练一个奖励模型（\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eRM\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e）。\u003c/p\u003e\n\n    \u003cp\u003e对给定输入，收集两个输出，标注员给出他们的偏好（which output they prefer）。然后，训练一个奖励模型来预测人类偏好输出（human-preferred output）。\u003c/p\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003e针对奖励模型，使用 PPO 对策略进行优化（optimize a policy）。\u003c/p\u003e\n\n    \u003cp\u003e将 RM 的输出作为一个标量奖励。通过 PPO 算法 (Schulman 等，2017) 对监督策略进行微调（fine-tune the supervised policy），以优化这一奖励。\u003c/p\u003e\n  \u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e步骤 2 和 3 可以持续迭代；每次在当前最佳策略上收集更多的对比数据，这些数据又用于训练新的 RM 和新的策略。\n实际上，大部分对比数据来自于我们的 supervised policies，一小部分来自于我们的 PPO policies。\u003c/p\u003e\n\n\u003ch2 id=\"32-数据集\"\u003e3.2 数据集\u003c/h2\u003e\n\n\u003ch3 id=\"321-主要来自-openai-api-用户数据\"\u003e3.2.1 主要来自 OpenAI API 用户数据\u003c/h3\u003e\n\n\u003cp\u003e我们的 prompts 数据集主要来自\u003cstrong\u003e\u003cmark\u003e用户提交给 OpenAI API 的文本 prompts\u003c/mark\u003e\u003c/strong\u003e，\n尤其是用户通过 OpenAI \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003ePlayground\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e interface 提交的那些 prompts ——\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e这个环境背后运行的是我们用 SFT + 我们自己的一部分示例数据训练出来的\u003cstrong\u003e\u003cmark\u003e初期 InstructGPT models\u003c/mark\u003e\u003c/strong\u003e。\u003c/li\u003e\n  \u003cli\u003e用户每次通过 Playground 接口用到 InstructGPT 时，我们都会告知他们，他们的数据可能会被用于训练下一步的模型。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e本文并没有用到生产环境 OpenAI API 的用户数据。\u003c/p\u003e\n\n\u003ch3 id=\"322-去重\"\u003e3.2.2 去重\u003c/h3\u003e\n\n\u003cp\u003e我们的去重比较简单，有共同长前缀的 prompt 就认为是重复的，并将每个用户 ID 的 prompt 数量限制为 200 个。\u003c/p\u003e\n\n\u003cp\u003e我们还基于用户 ID 创建\u003cstrong\u003e\u003cmark\u003e训练、验证和测试集\u003c/mark\u003e\u003c/strong\u003e（train, validation, and test splits）。\n为了避免模型学习到客户信息，我们过滤掉了训练数据集中包含个人身份信息（PII）的 prompts。\u003c/p\u003e\n\n\u003ch3 id=\"323-冷启动第一版-instructgpt\"\u003e3.2.3 冷启动（第一版 InstructGPT）\u003c/h3\u003e\n\n\u003cp\u003e为了训练最初的 InstructGPT 模型，我们要求\u003cstrong\u003e\u003cmark\u003e标注员自己编写 prompt\u003c/mark\u003e\u003c/strong\u003e。\n这是因为我们需要一些\u003cstrong\u003e\u003cmark\u003e初始的指令式的 prompts 来启动这个过程\u003c/mark\u003e\u003c/strong\u003e，\n而这类数据很难从 GTP-3 API 的用户数据中获得，用户通常不会提交这些格式的 prompts。\u003c/p\u003e\n\n\u003ch3 id=\"323-三种-promptplainfew-shotuser-based\"\u003e3.2.3 三种 prompt：plain/few-shot/user-based\u003c/h3\u003e\n\n\u003cp\u003e我们要求标注员编写三种类型的 prompt ：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003ePlain\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e: 标注员提出任意的任务，确保任务具有足够的多样性就行。\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eFew-shot\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e: 标注员提出一条指令，并为该指令提供多个查询/响应对（query/response pairs）。\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eUser-based\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e: OpenAI API 的 waitlist applications 中我们列了一些使用案例。我们要求标注员提供与这些使用案例相关的 prompts。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e详见附录 A。\u003c/p\u003e\n\n\u003ch3 id=\"324-三个-prompts-数据集及大小\"\u003e3.2.4 三个 prompts 数据集及大小\u003c/h3\u003e\n\n\u003cp\u003e根据以上 prompts，我们生成了三个不同的数据集用于不同的微调过程，如表 6 所示，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003eTable 6: Dataset sizes, in terms of number of prompts.\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth style=\"text-align: left\"\u003esplit\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003esource\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003esize\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eSFT train\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003elabeler\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e11,295\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eSFT train\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003ecustomer\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1,430\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eSFT valid\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003elabeler\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1,550\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eSFT valid\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003ecustomer\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e103\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003e\u003cmark\u003eSFT 总计\u003c/mark\u003e\u003c/strong\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e~15k\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eRM train\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003elabeler\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e6,623\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eRM train\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003ecustomer\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e26,584\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eRM valid\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003elabeler\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e3,488\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eRM valid\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003ecustomer\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e14,399\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003e\u003cmark\u003eRM 总计\u003c/mark\u003e\u003c/strong\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e~50k\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003ePPO train\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003ecustomer\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e31,144\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003ePPO valid\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003ecustomer\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e16,185\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003e\u003cmark\u003ePPO 总计\u003c/mark\u003e\u003c/strong\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e~47k\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003col\u003e\n  \u003cli\u003eSFT 数据集（来自 API 和标注员）：用于训练 SFT 模型，包含约 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e13k\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e training prompts\u003c/li\u003e\n  \u003cli\u003eRM 数据集（来自 API 和标注员）：标注员对模型输出的排名数据，用于训练 RM 模型，有 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e33k\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e training prompts\u003c/li\u003e\n  \u003cli\u003ePPO 数据集（仅来自 API）：\u003cstrong\u003e\u003cmark\u003e没有任何人工标签\u003c/mark\u003e\u003c/strong\u003e，用作 RLHF 微调的输入，有 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e31k\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e training prompts。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch3 id=\"325-prompts-类别分布及占比\"\u003e3.2.5 Prompts 类别分布及占比\u003c/h3\u003e\n\n\u003cp\u003e表 1 中展示了 API prompt（尤其是 RM 数据集）的类别分布，这些类别由我们的承包商标注。\n可以看到，\u003cstrong\u003e\u003cmark\u003e占比最大的是文本生成\u003c/mark\u003e\u003c/strong\u003e，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003eTable 1: API prompt dataset 中 \u003cmark\u003euse case 类别及占比\u003c/mark\u003e\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth style=\"text-align: left\"\u003eUse-case\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003e(%)\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eGeneration\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e45.6%\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eOpen QA\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e12.4%\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eBrainstorming\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e11.2%\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eChat\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e8.4%\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eRewrite\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e6.6%\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eSummarization\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e4.2%\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eClassification\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e3.5%\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eOther\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e3.5%\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eClosed QA\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e2.6%\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eExtract\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1.9%\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003ch3 id=\"326-几个-prompt-例子\"\u003e3.2.6 几个 prompt 例子\u003c/h3\u003e\n\n\u003cp\u003e表 2 展示了几个 prompt 示例（由研究人员编写，提交给 InstructGPT 的格式），\u003c/p\u003e\n\n\u003cp align=\"center\"\u003eTable 2: API prompt \u003cmark\u003e具体例子\u003c/mark\u003e。\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth style=\"text-align: left\"\u003eUse-case\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003ePrompt\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eBrainstorming\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eList five ideas for how to regain enthusiasm for my career\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eGeneration\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eWrite a short story where a bear goes to the beach, makes friends with a seal, and then returns home.\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eRewrite\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eThis is the summary of a Broadway play:\u003cbr/\u003e “”“\u003cbr/\u003e {summary}\u003cbr/\u003e “”“\u003cbr/\u003e This is the outline of the commercial for that play:\u003cbr/\u003e “”“\u003cbr/\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp\u003e更多信息：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e提交给 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eInstructGPT\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 的 prompts 见附录 A.2.1，\u003c/li\u003e\n  \u003cli\u003e提交给 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eGPT-3\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 的 prompts（做对比） 见附录 A.2.2，\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch2 id=\"33-训练任务\"\u003e3.3 训练任务\u003c/h2\u003e\n\n\u003cp\u003e我们的训练任务有两个来源：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e标注员编写的 prompt 数据集，\u003c/li\u003e\n  \u003cli\u003e提交给早期 InstructGPT 模型的 prompt 数据集。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e这些 prompt 种类繁多，包括生成、问答、对话、摘要、提取和其他自然语言任务 (见表 1)。\n我们的数据集中 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e96%+\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 是英文，但在 4.3 节中，\n我们也探讨了 InstructGPT 对其他语言指令的响应能力以及完成代码任务的能力。\u003c/p\u003e\n\n\u003cp\u003e对于每个自然语言 prompt，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e任务通常\u003cstrong\u003e\u003cmark\u003e通过自然语言指令直接指定\u003c/mark\u003e\u003c/strong\u003e，例如，\n“Write a story about a wise frog”（“写一个关于一只聪明的青蛙的故事”），\u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003e也可以\u003cstrong\u003e\u003cmark\u003e间接指定\u003c/mark\u003e\u003c/strong\u003e\u003c/p\u003e\n\n    \u003cul\u003e\n      \u003cli\u003e通过 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003efew-shot examples\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e，例如，提供两个关于青蛙的故事作为示例，prompt 模型生成一个新的故事，\u003c/li\u003e\n      \u003cli\u003e通过 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eimplicit continuation\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e，例如，提供一个关于青蛙的故事的开头，让模型续写。\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e在每种情况下，我们都要求标注员\u003cstrong\u003e\u003cmark\u003e尽力推断每个 prompt 背后的用户意图\u003c/mark\u003e\u003c/strong\u003e，\n并要求他们跳过那些任务非常模糊的 prompt。\n此外，标注员还会根据我们提供的指导 (见附录 B) 和他们自己的判断，\n思考其中隐含的意图（implicit intentions），例如回答的真实性，潜在的有偏见或有毒输出。\u003c/p\u003e\n\n\u003ch2 id=\"34-人工数据收集\"\u003e3.4 人工数据收集\u003c/h2\u003e\n\n\u003cp\u003e为了生成示范和对比数据，以及进行结果评估，我们通过 Upwork 和 ScaleAI 雇了大约 \u003cstrong\u003e\u003cmark\u003e40 名外包人员\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e与之前关于摘要任务的人类偏好数据收集工作 (Ziegler 等，2019; Stiennon 等，2020; Wu 等，2021) 相比，\n我们的输入数据涵盖了\u003cstrong\u003e\u003cmark\u003e范围更广泛的任务\u003c/mark\u003e\u003c/strong\u003e，甚至还包括有争议和敏感的主题。\u003c/p\u003e\n\n\u003ch3 id=\"341-标注员筛选\"\u003e3.4.1 标注员筛选\u003c/h3\u003e\n\n\u003cp\u003e我们的目标是选择一组标注员，他们对不同人口分布的\u003cstrong\u003e\u003cmark\u003e偏好很敏锐\u003c/mark\u003e\u003c/strong\u003e，并\u003cstrong\u003e\u003cmark\u003e擅长识别潜在的有害输出\u003c/mark\u003e\u003c/strong\u003e。\n因此，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e我们进行了一个\u003cstrong\u003e\u003cmark\u003e筛选测试\u003c/mark\u003e\u003c/strong\u003e（screening test）来衡量标注员在这些方面的表现；\u003c/li\u003e\n  \u003cli\u003e最后选出在这个测试中表现良好的标注员。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e相关的选择过程和标注员分布信息，见附录 B.1。\u003c/p\u003e\n\n\u003ch3 id=\"342-对齐冲突的处理\"\u003e3.4.2 对齐冲突的处理\u003c/h3\u003e\n\n\u003cp\u003e在训练和评估过程中，我们的对齐标准可能会发生冲突：例如，当用户请求一个潜在有害的响应时。\n针对这种情况，我们采取如下方式，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e训练阶段\u003c/mark\u003e\u003c/strong\u003e：\u003cstrong\u003e\u003cmark\u003e优先考虑对用户的有用性\u003c/mark\u003e\u003c/strong\u003e (否则就需要做出一些艰难的设计决策，我们留给未来的工作；更多讨论见 5.4 节)。\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e最终评估阶段\u003c/mark\u003e\u003c/strong\u003e：要求标注员\u003cstrong\u003e\u003cmark\u003e优先考虑真实性和无害性\u003c/mark\u003e\u003c/strong\u003e (因为这是我们真正关心的)。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e与 Stiennon 等 (2020) 一样，我们在项目过程中与标注员密切合作。我们有一个入职流程，对标注员进行培训，\n为每个任务编写详细的说明 (见附录 B.2)，并在群聊中回答标注员的问题。\u003c/p\u003e\n\n\u003ch3 id=\"343-对照度标注员验证泛华能力\"\u003e3.4.3 对照度标注员：验证泛华能力\u003c/h3\u003e\n\n\u003cp\u003e为了解 InstructGPT 推广到其他标注员的偏好时表现有多好，\n我们\u003cstrong\u003e\u003cmark\u003e雇佣了另一组独立的标注员\u003c/mark\u003e\u003c/strong\u003e，他们不参与编写任何训练数据。\n这些标注员来自相同的供应商，但没有经过前面的筛选过程。\u003c/p\u003e\n\n\u003cp\u003eDespite the complexity of the task, we find that inter-annotator agreement rates are quite high:\ntraining labelers agree with each-other 72:6 ± 1:5% of the time, while for held-out labelers this\nnumber is 77:3 ± 1:3%. For comparison, in the summarization work of Stiennon et al. (2020)\nresearcher-researcher agreement was 73 ± 4%.\u003c/p\u003e\n\n\u003ch2 id=\"35-models模型\"\u003e3.5 Models（模型）\u003c/h2\u003e\n\n\u003cp\u003e我们从 GPT-3 预训练模型开始微调。GPT-3 在大量互联网数据上进行了训练，适用于各种下游任务，\n但其行为尚未充分符合人类需求。基于 GPT-3，我们使用三种不同技术进行了模型微调。\u003c/p\u003e\n\n\u003ch3 id=\"351-supervised-fine-tuning-sft\"\u003e3.5.1 Supervised fine-tuning (SFT)\u003c/h3\u003e\n\n\u003cp\u003e使用监督学习的方式，在我们的示范数据上对 GPT-3 进行微调。\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e16 epoch\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/li\u003e\n  \u003cli\u003ea cosine learning rate decay\u003c/li\u003e\n  \u003cli\u003eresidual dropout \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e0.2\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e得到很多个 SFT 模型。最后根据 validation set 上的 RM 分数选择最终的 SFT 模型。\u003c/p\u003e\n\n\u003cp\u003e与 Wu 等（2021）类似，我们发现我们的 SFT 模型在 1 个 epoch 后在 validation loss 上就会过拟合（overfit）；\n但是，同时我们发现，尽管存在过拟合问题，但\u003cstrong\u003e\u003cmark\u003e更多 epoch 对 RM 分数和人类偏好得分都有帮助\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003ch3 id=\"352-reward-modeling-rm\"\u003e3.5.2 Reward modeling (RM)\u003c/h3\u003e\n\n\u003cp\u003e将 SFT 模型去掉最后的 unembedding 层，然后从这样的模型开始训练，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e输入：\u003cstrong\u003e\u003cmark\u003eprompt 和 response\u003c/mark\u003e\u003c/strong\u003e，\u003c/li\u003e\n  \u003cli\u003e输出：一个标量奖励。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e最后得到的就是一个 RM 模型。\u003c/p\u003e\n\n\u003cp\u003e在本文中，我们仅使用 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e6B\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 的 RM，因为这样可以节省大量计算资源，\n并且我们发现 \u003cstrong\u003e\u003cmark\u003e175B 的 RM 训练可能不稳定\u003c/mark\u003e\u003c/strong\u003e，因此不太适合在 RL 中用作 value function（更多细节见附录 C）。\u003c/p\u003e\n\n\u003cp\u003e在 Stiennon 等（2020）中，给两个模型相同的输入，然后得到两份输出作为对比数据，\n\u003cstrong\u003e\u003cmark\u003eRM 是在这个对比数据集（dataset of comparisons）上进行训练的\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eThey use a cross-entropy loss, with the comparisons as labels—the difference in\nrewards represents the log odds that one response will be preferred to the other by a human labeler.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e为了快速收集对比数据，我们将 $K=4$ 到 $K=9$ 之间的输出（即一个 input/prompt 喂给模型，得到 K 个 output）都提供给标注员，并要求他们对其进行排名（rank）。\n这样每个 prompt 就对应 ${K \\choose 2}$ 个对比数据。\u003c/p\u003e\n\n\u003cp\u003e由于每个 labeling task 内的 comparisons 非常相关，我们发现如果简单地 shuffle the comparisons into one dataset，\n对数据集训练一次就会导致 RM 过拟合。\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eThat is, if each of the possible ${K \\choose 2}$ comparisons is treated as a\nseparate data point, then each completion will potentially be used for $K-1$\nseparate gradient updates. \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eThe model tends to overfit after a single epoch\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e,\nso repeating data within an epoch also causes it to overfit.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e因此，我们将每个 prompt 的所有 ${K \\choose 2}$ 个对比作为单个 batch element 进行训练。\n这样做在计算上更加高效，因为只需要一次正向遍历（forward pass of the RM for each completion，\n而不是 ${K \\choose 2}$ forward passes for $K$ completions），\n并且由于不再过拟合，它实现了更好的 validation accuracy and log loss。\u003c/p\u003e\n\n\u003cp\u003e具体来说，奖励模型的损失函数为：\u003c/p\u003e\n\n\\[\\begin{equation} \\label{eq1}\n\\begin{split}\n\\operatorname{loss}\\left(\\theta \\right) = -\\frac{1} {K \\choose 2} E_{\\left(x, y_{w}, y_{l}\\right) \\sim D}\\left[\\log \\left(\\sigma\\left(r_{\\theta}\\left(x, y_{w}\\right)-r_{\\theta}\\left(x, y_{l}\\right)\\right)\\right)\\right]\n\\end{split}\n\\end{equation}\\]\n\n\u003cp\u003e其中，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\\(x\\)：prompt（输入的提示词）\u003c/li\u003e\n  \u003cli\u003e\\(y\\)：completion（模型的返回）\u003c/li\u003e\n  \u003cli\u003e$y_{w}$：the preferred completion out of the pair of $y_{w}$ and $y_{l}$\u003c/li\u003e\n  \u003cli\u003e$D$：dataset of human comparisons（标注员给出的对比）\u003c/li\u003e\n  \u003cli\u003e\\(r_{\\theta}(x, y)\\)：scalar output of the RM for prompt $x$ and completion $y$ with parameters $\\theta$\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e最后，由于 RM loss 对奖励的平移不变，我们使用一个 bias 来对奖励模型进行归一化，这样标注员的示范在进行 RL 之前的平均分数为 0。\u003c/p\u003e\n\n\u003ch3 id=\"353-reinforcement-learning-rl\"\u003e3.5.3 Reinforcement learning (RL)\u003c/h3\u003e\n\n\u003cp\u003e再次沿用（Stiennon 等，2020），我们使用 PPO（Schulman 等，2017）\u003cstrong\u003e\u003cmark\u003e对 SFT 模型进行微调\u003c/mark\u003e\u003c/strong\u003e。\n我们创建一个 bandit 环境，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e给一个随机的客户 prompt，得到一个 response。\u003c/li\u003e\n  \u003cli\u003e给定一个 prompt 和相应的 response，会产生一个由 RM 确定的奖励，然后结束这一轮。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e此外，我们添加了一个 per-token 的 KL 惩罚（来自 SFT 模型），以减轻奖励模型的过优化（over-optimization）。\n值函数是从 RM 初始化的。我们将这些模型称为\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003ePPO\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e我们还尝试将预训练梯度（pretraining gradients）mixing into PPO 梯度中，以减轻在公开 NLP 数据集上的性能下降。\n我们将这些模型称为\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003ePPO-ptx\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e我们在 RL 训练中最大化以下组合目标函数：\u003c/p\u003e\n\n\u003cp\u003e\\begin{equation} \\label{eq2}\n\\begin{split}\n\\operatorname{objective}\\left(\\phi\\right)= \u0026amp; E_{\\left(x, y\\right) \\sim D_{\\pi_{\\phi}^{\\mathrm{RL}}}}\\left[r_{\\theta}(x, y)-\\beta \\log \\left(\\pi_{\\phi}^{\\mathrm{RL}}(y \\mid x) / \\pi^{\\mathrm{SFT}}(y \\mid         x)\\right)\\right] + \u003cbr/\u003e\n \u0026amp; \\gamma E_{x \\sim D_\\textrm{pretrain}}\\left[\\log(\\pi_{\\phi}^{\\mathrm{RL}}(x))\\right]\n\\end{split}\n\\end{equation}\u003c/p\u003e\n\n\u003cp\u003e其中\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\\(\\pi_{\\phi}^{\\mathrm{RL}}\\) 是学习到的 RL 策略，\u003c/li\u003e\n  \u003cli\u003e\\(\\pi^{\\mathrm{SFT}}\\) 是 SFT 模型，\u003c/li\u003e\n  \u003cli\u003e\\(D_\\textrm{pretrain}\\) 是预训练分布（pretraining distribution）。\u003c/li\u003e\n  \u003cli\u003eKL 奖励系数 \\(\\beta\\) 和预训练损失系数 \\(\\gamma\\) 分别控制 KL 惩罚和预训练梯度的强度（strength）。\u003c/li\u003e\n  \u003cli\u003e对于 “PPO” models，\\(\\gamma\\) 设置为 0。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e除非另有说明，在本文中，\u003cstrong\u003e\u003cmark\u003eInstructGPT 指的是 PPO-ptx models\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003ch3 id=\"354-性能比较基线\"\u003e3.5.4 性能比较基线\u003c/h3\u003e\n\n\u003cp\u003e我们将 PPO 模型与下列模型进行比较：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eSFT 模型\u003c/li\u003e\n  \u003cli\u003eGPT-3\u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eGPT-3-prompted\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e：向 GPT-3 提供一个 few-shot prefix 以“提示”它进入指令跟随模式。在实现上，就是把这个前缀插入倒用户输入的指令之前。\u003c/p\u003e\n\n    \u003cblockquote\u003e\n      \u003cp\u003eTo obtain this prefix, authors RL and DA held a prefix-finding\ncompetition: each spent an hour interacting with GPT-3 to come up with\ntheir two best prefixes. The winning prefix was the one that led GPT-3 to\nattain the highest RM score on the prompt validation set. DA won.\u003c/p\u003e\n    \u003c/blockquote\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003e在 FLAN 和 T0 数据集上微调过的 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e175B GPT-3\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/p\u003e\n\n    \u003cp\u003e这两个数据集都包含各种 NLP 任务，以及每个任务的自然语言指令。\n 我们分别在约 1 million examples 上对它们进行微调，并选择在验证集上获得最高奖励分数的 checkpoint。更多细节见附录 C。\u003c/p\u003e\n  \u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch2 id=\"36-性能评估\"\u003e3.6 性能评估\u003c/h2\u003e\n\n\u003cp\u003e为了评估我们模型的“对齐”程度，首先需要明确“对齐”的含义。\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cmark\u003e一直以来，“对齐”的定义都很模糊和令人困惑\u003c/mark\u003e\u003c/strong\u003e，有很多提法（Chen 等，2021；Leike 等，2018；Gabriel，2020）。\u003c/p\u003e\n\n\u003cp\u003e按照 Leike 等（2018）的方法，我们的目标是\u003cstrong\u003e\u003cmark\u003e训练符合用户意图的模型\u003c/mark\u003e\u003c/strong\u003e。\n更实际地说，为了完成语言任务，我们使用了类似于 Askill 等（2021）的框架，\n他们认为，如果模型是 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003ehelpful, honest, and harmless\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 的，那这个模型就是对齐的（aligned）。\u003c/p\u003e\n\n\u003ch3 id=\"361-指标\"\u003e3.6.1 指标\u003c/h3\u003e\n\n\u003ch4 id=\"helpful\"\u003ehelpful\u003c/h4\u003e\n\n\u003cp\u003e要做到有帮助，模型不仅要能遵循指令，还应该能从一个 few-shot prompt 或其他可解释的模式（例如 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eQ: {question}\\nA:\u003c/code\u003e）中推断意图（infer intention）。\u003c/p\u003e\n\n\u003cp\u003e给定的 prompt 的意图可能不清楚，这种情况下就依赖于标注员的判断，我们的\u003cstrong\u003e\u003cmark\u003e主要指标是标注员的偏好评分\u003c/mark\u003e\u003c/strong\u003e。\n但另一方面，由于我们的\u003cstrong\u003e\u003cmark\u003e标注员不是生成 prompt 的用户\u003c/mark\u003e\u003c/strong\u003e，\n因此，下面二者之间可能存在偏差：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e用户的实际意图\u003c/li\u003e\n  \u003cli\u003e标注员通过 prompt 所理解的用户意图\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch4 id=\"honest--truthfulness\"\u003ehonest / truthfulness\u003c/h4\u003e\n\n\u003cp\u003e在纯生成式模型中如何衡量诚实度尚无定论；这需要将模型的实际输出与其关于正确输出的“信念”进行比较\n（model’s actual output to its “belief” about the correct output），\n由于模型是一个黑盒，我们无法推断它的信念。\u003c/p\u003e\n\n\u003cp\u003e因此，我们使用真实性（truthfulness）——  \u003cstrong\u003e\u003cmark\u003e模型关于世界的陈述是否真实\u003c/mark\u003e\u003c/strong\u003e —— 来衡量。\n具体到两个指标：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e模型在\u003cstrong\u003e\u003cmark\u003e封闭域任务\u003c/mark\u003e\u003c/strong\u003e中编造信息（make up information on closed domain tasks）的倾向，即\u003cstrong\u003e\u003cmark\u003e“幻觉”\u003c/mark\u003e\u003c/strong\u003e（hallucinations），\u003c/li\u003e\n  \u003cli\u003eTruthfulQA 数据集（Lin 等，2021）上的表现。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e显然，这只能覆盖真实性实际含义（what is actually meant by truthfulness）的一小部分。\u003c/p\u003e\n\n\u003ch4 id=\"harmless\"\u003eharmless\u003c/h4\u003e\n\n\u003cp\u003e与诚实度类似，衡量语言模型的有害性也很难。\n在大多数情况下，语言模型的危害取决于\u003cstrong\u003e\u003cmark\u003e其输出在现实世界中是如何被使用的\u003c/mark\u003e\u003c/strong\u003e。\n例如，对于一个生成有毒输出的模型，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e如果部署在聊天机器人环境中，可能就是有害的，\u003c/li\u003e\n  \u003cli\u003e如果用于数据增强以训练更准确的毒性检测模型，则可能是有益的。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e在项目早期，我们让标注员评估输出是否“可能有害”。\n但后面经停止了这项工作，因为这需要太多关于输出最终将如何被使用的猜测（speculation），\n尤其是我们的部分数据还来自 Playground API 客户。\u003c/p\u003e\n\n\u003cp\u003e因此，我们使用了一套更具体的替代方案，旨在捕捉最终部署的模型中可能导致有害的不同行为方面：\n我们\u003cstrong\u003e\u003cmark\u003e让标注员从一个用户助理的角度来评估输出是否恰当\u003c/mark\u003e\u003c/strong\u003e，是否 denigrates a protected class，或是否包含性或暴力内容。\u003c/p\u003e\n\n\u003cp\u003e我们还在衡量偏见和毒性的数据集上对 InstructGPT 进行基准测试，例如 RealToxicityPrompts（Gehman 等，2020）和 CrowS-Pairs（Nangia 等，2020）。\u003c/p\u003e\n\n\u003ch3 id=\"362-定量评估\"\u003e3.6.2 定量评估\u003c/h3\u003e\n\n\u003cp\u003e我们的定量评估分为两个独立的部分。\u003c/p\u003e\n\n\u003ch4 id=\"在-openai-api-真实用户的-prompts-上的表现\"\u003e在 OpenAI API 真实用户的 prompts 上的表现\u003c/h4\u003e\n\n\u003cp\u003e数据来源：OpenAI \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003ePlayground API\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e（背后是 InstructGPT） 收集来的用户 prompts。\n所以，评估用的 prompts 与训练用的 prompt 同源，但未参与训练，\n也就是说只选择那些未参与训练的客户 prompts。\u003c/p\u003e\n\n\u003cp\u003e但这里有个问题，训练用的 prompt 是专门为 InstructGPT 设计的，因此 GPT-3 在这些 prompts 上的效果可能不佳，有失公平。\n为此，我们还收集了用户通过 OpenAI \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eGPT-3 API\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 提交的 prompts 进行评估；这些 prompt 通常不是“遵循指令”的风格，而是专门为 GPT-3 设计的。\u003c/p\u003e\n\n\u003cp\u003e主要评估指标是\u003cstrong\u003e\u003cmark\u003e人类偏好评分\u003c/mark\u003e\u003c/strong\u003e。\n对于每个模型，都计算其输出相对于 baseline 被人类偏好的频率；\n这里用我们的 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e175B SFT\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 模型作为 baseline ，因为它的性能处于\u003cstrong\u003e\u003cmark\u003e中等水平\u003c/mark\u003e\u003c/strong\u003e。\n此外，我们要求标注员使用 1-7 Likert scale 判断每个 response 的整体质量，并为每个输出收集一些元数据（见表 3）。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003eTable 3: Labeler-collected metadata on the API distribution\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth style=\"text-align: left\"\u003eMetadata\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eScale\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eOverall\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003equality Likert scale; 1-7\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eFails to follow the correct instruction / task\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eBinary\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eInappropriate for customer assistant\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eBinary\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eHallucination\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eBinary\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eSatisifies constraint provided in the instruction\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eBinary\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eContains sexual content\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eBinary\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eContains violent content\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eBinary\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eEncourages or fails to discourage violence/abuse/terrorism/self-harm\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eBinary\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eDenigrates a protected class\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eBinary\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eGives harmful advice\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eBinary\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eExpresses opinion\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eBinary\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eExpresses moral judgment\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eBinary\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003ch4 id=\"在公开-nlp-数据集上的表现\"\u003e在公开 NLP 数据集上的表现\u003c/h4\u003e\n\n\u003cp\u003e我们在两种公开数据集上进行评估：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e能衡量\u003cstrong\u003e\u003cmark\u003e模型安全性\u003c/mark\u003e\u003c/strong\u003e的数据集，特别是真实性、毒性和偏见；\u003c/li\u003e\n  \u003cli\u003e能衡量在\u003cstrong\u003e\u003cmark\u003e传统 NLP 任务\u003c/mark\u003e\u003c/strong\u003e（如问答、阅读理解和摘要）上的 \u003cstrong\u003e\u003cmark\u003ezero-shot 性能\u003c/mark\u003e\u003c/strong\u003e的数据集。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e我们还在 RealToxicityPrompts 数据集（Gehman 等，2020）上人工评估了毒性。\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eWe are releasing samples from our models on all of the sampling-based NLP tasks.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch1 id=\"4-结果\"\u003e4 结果\u003c/h1\u003e\n\n\u003cp\u003e暂略。\n见原文。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/instructgpt-paper/fig-1.png\" width=\"70%\"/\u003e\u003c/p\u003e\n\u003cp alige=\"center\"\u003e \nFigure 1: Human evaluations of various models on our API prompt distribution, evaluated by how\noften outputs from each model were preferred to those from the 175B SFT model. Our InstructGPT\nmodels (PPO-ptx) as well as its variant trained without pretraining mix (PPO) significantly outperform\nthe GPT-3 baselines (GPT, GPT prompted); outputs from our 1.3B PPO-ptx model are preferred to\nthose from the 175B GPT-3. Error bars throughout the paper are 95% confidence intervals\n\u003c/p\u003e\n\n\u003ch1 id=\"5-问题讨论\"\u003e5 问题讨论\u003c/h1\u003e\n\n\u003cp\u003e暂略。\n见原文。\u003c/p\u003e\n\n\u003ch1 id=\"参考文献\"\u003e参考文献\u003c/h1\u003e\n\n\u003cul\u003e\n  \u003cli\u003eAbramson, J., Ahuja, A., Barr, I., Brussee, A., Carnevale, F., Cassin, M., Chhaparia, R., Clark, S., Damoc, B., Dudzik, A., et~al. (2020). Imitating interactive intelligence. arXiv preprint arXiv:2012.05672\u003c/li\u003e\n  \u003cli\u003eAchiam, J., Held, D., Tamar, A., and Abbeel, P. (2017). Constrained policy optimization. In International Conference on Machine Learning pages 22–31.  PMLR.\u003c/li\u003e\n  \u003cli\u003eAnthony, T., Tian, Z., and Barber, D. (2017). Thinking fast and slow with deep learning and tree search. arXiv preprint arXiv:1705.08439\u003c/li\u003e\n  \u003cli\u003eAribandi, V., Tay, Y., Schuster, T., Rao, J., Zheng, H.~S., Mehta, S.~V., Zhuang, H., Tran, V.~Q., Bahri, D., Ni, J., et~al. (2021). Ext5: Towards extreme multi-task scaling for transfer learning. arXiv preprint arXiv:2111.10952\u003c/li\u003e\n  \u003cli\u003eAskell, A., Bai, Y., Chen, A., Drain, D., Ganguli, D., Henighan, T., Jones, A., Joseph, N., Mann, B., DasSarma, N., et~al. (2021). A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861\u003c/li\u003e\n  \u003cli\u003eBahdanau, D., Brakel, P., Xu, K., Goyal, A., Lowe, R., Pineau, J., Courville, A., and Bengio, Y. (2016). An actor-critic algorithm for sequence prediction. arXiv preprint arXiv:1607.07086\u003c/li\u003e\n  \u003cli\u003eBahdanau, D., Hill, F., Leike, J., Hughes, E., Hosseini, A., Kohli, P., and Grefenstette, E. (2018). Learning to understand goal specifications by modelling reward. arXiv preprint arXiv:1806.01946\u003c/li\u003e\n  \u003cli\u003eBender, E.~M., Gebru, T., McMillan-Major, A., and Shmitchell, S. (2021). On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency pages 610–623.\u003c/li\u003e\n  \u003cli\u003eBlodgett, S.~L., Barocas, S., Daum{\u0026#39;e}~III, H., and Wallach, H. (2020). Language (technology) is power: A critical survey of” bias” in nlp. arXiv preprint arXiv:2005.14050\u003c/li\u003e\n  \u003cli\u003eB{\u0026#34;o}hm, F., Gao, Y., Meyer, C.~M., Shapira, O., Dagan, I., and Gurevych, I.  (2019). Better rewards yield better summaries: Learning to summarise without references. arXiv preprint arXiv:1909.01214\u003c/li\u003e\n  \u003cli\u003eBojar, O., Chatterjee, R., Federmann, C., Haddow, B., Huck, M., Hokamp, C., Koehn, P., Logacheva, V., Monz, C., Negri, M., Post, M., Scarton, C., Specia, L., and Turchi, M. (2015). Findings of the 2015 workshop on statistical machine translation. In Proceedings of the Tenth Workshop on Statistical Machine Translation pages 1–46, Lisbon, Portugal. Association for Computational Linguistics.\u003c/li\u003e\n  \u003cli\u003eBommasani, R., Hudson, D.~A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M.~S., Bohg, J., Bosselut, A., Brunskill, E., et~al. (2021). On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258\u003c/li\u003e\n  \u003cli\u003eBostrom, N. (2014). Superintelligence Dunod.\u003c/li\u003e\n  \u003cli\u003eBrown, T.~B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et~al. (2020). Language models are few-shot learners. arXiv preprint arXiv:2005.14165\u003c/li\u003e\n  \u003cli\u003eBuchanan, B., Lohn, A., Musser, M., and Sedova, K. (2021). Truth, lies, and automation. Technical report, Center for the Study of Emerging Technology.\u003c/li\u003e\n  \u003cli\u003eCaliskan, A., Bryson, J.~J., and Narayanan, A. (2017). Semantics derived automatically from language corpora contain human-like biases. Science 356(6334):183–186.\u003c/li\u003e\n  \u003cli\u003eCarlini, N., Tramer, F., Wallace, E., Jagielski, M., Herbert-Voss, A., Lee, K., Roberts, A., Brown, T., Song, D., Erlingsson, U., et~al. (2021). Extracting training data from large language models. In 30th USENIX Security Symposium (USENIX Security 21) pages 2633–2650.\u003c/li\u003e\n  \u003cli\u003eChen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d.~O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et~al. (2021). Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374\u003c/li\u003e\n  \u003cli\u003eCho, W.~S., Zhang, P., Zhang, Y., Li, X., Galley, M., Brockett, C., Wang, M., and Gao, J. (2018). Towards coherent and cohesive long-form text generation. arXiv preprint arXiv:1811.00511\u003c/li\u003e\n  \u003cli\u003eChoi, E., He, H., Iyyer, M., Yatskar, M., Yih, W.-t., Choi, Y., Liang, P., and Zettlemoyer, L. (2018). Quac: Question answering in context. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing pages 2174–2184.\u003c/li\u003e\n  \u003cli\u003eChristiano, P., Cotra, A., and Xu, M. (2021). Eliciting latent knowledge: How to tell if your eyes deceive you.  https://www.alignmentforum.org/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge\u003c/li\u003e\n  \u003cli\u003eChristiano, P., Shlegeris, B., and Amodei, D. (2018). Supervising strong learners by amplifying weak experts. arXiv preprint arXiv:1810.08575\u003c/li\u003e\n  \u003cli\u003eChristiano, P.~F., Leike, J., Brown, T., Martic, M., Legg, S., and Amodei, D.  (2017). Deep reinforcement learning from human preferences. In Advances in Neural Information Processing Systems pages 4299–4307.\u003c/li\u003e\n  \u003cli\u003eDathathri, S., Madotto, A., Lan, J., Hung, J., Frank, E., Molino, P., Yosinski,  J., and Liu, R. (2019). Plug and play language models: A simple approach to controlled text  generation. arXiv preprint arXiv:1912.02164\u003c/li\u003e\n  \u003cli\u003eDhamala, J., Sun, T., Kumar, V., Krishna, S., Pruksachatkun, Y., Chang, K.-W.,  and Gupta, R. (2021). Bold: Dataset and metrics for measuring biases in open-ended language  generation. In Proceedings of the 2021 ACM Conference on Fairness,  Accountability, and Transparency pages 862–872.\u003c/li\u003e\n  \u003cli\u003eDinan, E., Fan, A., Williams, A., Urbanek, J., Kiela, D., and Weston, J.  (2019a). Queens are powerful too: Mitigating gender bias in dialogue  generation. arXiv preprint arXiv:1911.03842\u003c/li\u003e\n  \u003cli\u003eDinan, E., Humeau, S., Chintagunta, B., and Weston, J. (2019b). Build it break it fix it for dialogue safety: Robustness from  adversarial human attack. arXiv preprint arXiv:1908.06083\u003c/li\u003e\n  \u003cli\u003eDua, D., Wang, Y., Dasigi, P., Stanovsky, G., Singh, S., and Gardner, M.  (2019). Drop: A reading comprehension benchmark requiring discrete reasoning  over paragraphs. arXiv preprint arXiv:1903.00161\u003c/li\u003e\n  \u003cli\u003eFedus, W., Zoph, B., and Shazeer, N. (2021). Switch transformers: Scaling to trillion parameter models with simple  and efficient sparsity. arXiv preprint arXiv:2101.03961\u003c/li\u003e\n  \u003cli\u003eGabriel, I. (2020). Artificial intelligence, values, and alignment. Minds and machines 30(3):411–437.\u003c/li\u003e\n  \u003cli\u003eGehman, S., Gururangan, S., Sap, M., Choi, Y., and Smith, N.~A. (2020). Realtoxicityprompts: Evaluating neural toxic degeneration in language  models. arXiv preprint arXiv:2009.11462\u003c/li\u003e\n  \u003cli\u003eHancock, B., Bordes, A., Mazare, P.-E., and Weston, J. (2019). Learning from dialogue after deployment: Feed yourself, chatbot! arXiv preprint arXiv:1901.05415\u003c/li\u003e\n  \u003cli\u003eHenderson, P., Sinha, K., Angelard-Gontier, N., Ke, N.~R., Fried, G., Lowe, R.,  and Pineau, J. (2018). Ethical challenges in data-driven dialogue systems. In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics,  and Society pages 123–129.\u003c/li\u003e\n  \u003cli\u003eHuang, P.-S., Zhang, H., Jiang, R., Stanforth, R., Welbl, J., Rae, J., Maini,  V., Yogatama, D., and Kohli, P. (2019). Reducing sentiment bias in language models via counterfactual  evaluation. arXiv preprint arXiv:1911.03064\u003c/li\u003e\n  \u003cli\u003eIbarz, B., Leike, J., Pohlen, T., Irving, G., Legg, S., and Amodei, D. (2018). Reward learning from human preferences and demonstrations in atari. In Advances in neural information processing systems pages  8011–8023.\u003c/li\u003e\n  \u003cli\u003eIrving, G., Christiano, P., and Amodei, D. (2018). {AI} safety via debate. arXiv preprint arXiv:1805.00899\u003c/li\u003e\n  \u003cli\u003eJaques, N., Ghandeharioun, A., Shen, J.~H., Ferguson, C., Lapedriza, A., Jones,  N., Gu, S., and Picard, R. (2019). Way off-policy batch deep reinforcement learning of implicit human  preferences in dialog. arXiv preprint arXiv:1907.00456\u003c/li\u003e\n  \u003cli\u003eKenton, Z., Everitt, T., Weidinger, L., Gabriel, I., Mikulik, V., and Irving,  G. (2021). Alignment of language agents. arXiv preprint arXiv:2103.14659\u003c/li\u003e\n  \u003cli\u003eKeskar, N.~S., McCann, B., Varshney, L.~R., Xiong, C., and Socher, R. (2019). Ctrl: A conditional transformer language model for controllable  generation. arXiv preprint arXiv:1909.05858\u003c/li\u003e\n  \u003cli\u003eKhashabi, D., Min, S., Khot, T., Sabharwal, A., Tafjord, O., Clark, P., and  Hajishirzi, H. (2020). Unifiedqa: Crossing format boundaries with a single qa system. arXiv preprint arXiv:2005.00700\u003c/li\u003e\n  \u003cli\u003eKirk, H., Jun, Y., Iqbal, H., Benussi, E., Volpin, F., Dreyer, F.~A.,  Shtedritski, A., and Asano, Y.~M. (2021). How true is gpt-2? an empirical analysis of intersectional  occupational biases. arXiv preprint arXiv:2102.04130\u003c/li\u003e\n  \u003cli\u003eKrause, B., Gotmare, A.~D., McCann, B., Keskar, N.~S., Joty, S., Socher, R.,  and Rajani, N.~F. (2020). Gedi: Generative discriminator guided sequence generation. arXiv preprint arXiv:2009.06367\u003c/li\u003e\n  \u003cli\u003eKreutzer, J., Khadivi, S., Matusov, E., and Riezler, S. (2018). Can neural machine translation be improved with user feedback? arXiv preprint arXiv:1804.05958\u003c/li\u003e\n  \u003cli\u003eLawrence, C. and Riezler, S. (2018). Improving a neural semantic parser by counterfactual learning from  human bandit feedback. arXiv preprint arXiv:1805.01252\u003c/li\u003e\n  \u003cli\u003eLeike, J., Krueger, D., Everitt, T., Martic, M., Maini, V., and Legg, S.  (2018). Scalable agent alignment via reward modeling: a research direction. arXiv preprint arXiv:1811.07871\u003c/li\u003e\n  \u003cli\u003eLeike, J., Martic, M., Krakovna, V., Ortega, P.~A., Everitt, T., Lefrancq, A.,  Orseau, L., and Legg, S. (2017). {AI} safety gridworlds. arXiv preprint arXiv:1711.09883\u003c/li\u003e\n  \u003cli\u003eLiang, P.~P., Wu, C., Morency, L.-P., and Salakhutdinov, R. (2021). Towards understanding and mitigating social biases in language  models. In International Conference on Machine Learning pages  6565–6576. PMLR.\u003c/li\u003e\n  \u003cli\u003eLin, S., Hilton, J., and Evans, O. (2021). Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958\u003c/li\u003e\n  \u003cli\u003eLiu, H., Dacon, J., Fan, W., Liu, H., Liu, Z., and Tang, J. (2019). Does gender matter? towards fairness in dialogue systems. arXiv preprint arXiv:1910.10486\u003c/li\u003e\n  \u003cli\u003eMadaan, A., Tandon, N., Clark, P., and Yang, Y. (2022). Memory-assisted prompt editing to improve gpt-3 after deployment. arXiv preprint arXiv:2201.06009\u003c/li\u003e\n  \u003cli\u003eManela, D. d.~V., Errington, D., Fisher, T., van Breugel, B., and Minervini, P.  (2021). Stereotype and skew: Quantifying gender bias in pre-trained and  fine-tuned language models. arXiv preprint arXiv:2101.09688\u003c/li\u003e\n  \u003cli\u003eMishra, S., Khashabi, D., Baral, C., and Hajishirzi, H. (2021). Cross-task generalization via natural language crowdsourcing  instructions. arXiv preprint arXiv:2104.08773\u003c/li\u003e\n  \u003cli\u003eNadeem, M., Bethke, A., and Reddy, S. (2020). Stereoset: Measuring stereotypical bias in pretrained language  models. arXiv preprint arXiv:2004.09456\u003c/li\u003e\n  \u003cli\u003eNahian, M. S.~A., Frazier, S., Harrison, B., and Riedl, M. (2021). Training value-aligned reinforcement learning agents using a  normative prior. arXiv preprint arXiv:2104.09469\u003c/li\u003e\n  \u003cli\u003eNakano, R., Hilton, J., Balaji, S., Wu, J., Ouyang, L., Kim, C., Hesse, C.,  Jain, S., Kosaraju, V., Saunders, W., et~al. (2021). Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332\u003c/li\u003e\n  \u003cli\u003eNallapati, R., Zhou, B., Gulcehre, C., Xiang, B., et~al. (2016). Abstractive text summarization using sequence-to-sequence rnns and  beyond. arXiv preprint arXiv:1602.06023\u003c/li\u003e\n  \u003cli\u003eNangia, N., Vania, C., Bhalerao, R., and Bowman, S.~R. (2020). {CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in  Masked Language Models In Proceedings of the 2020 Conference on Empirical Methods in  Natural Language Processing Online. Association for Computational  Linguistics.\u003c/li\u003e\n  \u003cli\u003eNgo, H., Raterink, C., Ara{\u0026#39;u}jo, J.~G., Zhang, I., Chen, C., Morisot, A., and  Frosst, N. (2021). Mitigating harm in language models with conditional-likelihood  filtration. arXiv preprint arXiv:2108.07790\u003c/li\u003e\n  \u003cli\u003ePerez, E., Karamcheti, S., Fergus, R., Weston, J., Kiela, D., and Cho, K.  (2019). Finding generalizable evidence by learning to convince q\\\u0026amp;a models. arXiv preprint arXiv:1909.05863\u003c/li\u003e\n  \u003cli\u003eQian, Y., Muaz, U., Zhang, B., and Hyun, J.~W. (2019). Reducing gender bias in word-level language models with a  gender-equalizing loss function. arXiv preprint arXiv:1905.12801\u003c/li\u003e\n  \u003cli\u003eRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. (2019). Language models are unsupervised multitask learners. OpenAI Blog 1(8):9.\u003c/li\u003e\n  \u003cli\u003eRae, J.~W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J., Song, F.,  Aslanides, J., Henderson, S., Ring, R., Young, S., et~al. (2021). Scaling language models: Methods, analysis \\\u0026amp; insights from training  gopher. arXiv preprint arXiv:2112.11446\u003c/li\u003e\n  \u003cli\u003eRajpurkar, P., Jia, R., and Liang, P. (2018). Know what you don’t know: Unanswerable questions for squad. arXiv preprint arXiv:1806.03822\u003c/li\u003e\n  \u003cli\u003eRudinger, R., Naradowsky, J., Leonard, B., and {Van Durme B. (2018). Gender bias in coreference resolution. In Proceedings of the 2018 Conference of the North American  Chapter of the Association for Computational Linguistics: Human Language  Technologies New Orleans, Louisiana. Association for Computational  Linguistics.\u003c/li\u003e\n  \u003cli\u003eSanh, V., Webson, A., Raffel, C., Bach, S.~H., Sutawika, L., Alyafeai, Z.,  Chaffin, A., Stiegler, A., Scao, T.~L., Raja, A., et~al. (2021). Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207\u003c/li\u003e\n  \u003cli\u003eSchick, T., Udupa, S., and Schutze, H. (2021). Self-diagnosis and self-debiasing: A proposal for reducing  corpus-based bias in nlp. arXiv preprint arXiv:2103.00453\u003c/li\u003e\n  \u003cli\u003eSchulman, J., Moritz, P., Levine, S., Jordan, M., and Abbeel, P. (2016). High-dimensional continuous control using generalized advantage  estimation. In Proceedings of the International Conference on Learning  Representations (ICLR)\u003c/li\u003e\n  \u003cli\u003eSchulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. (2017). Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347\u003c/li\u003e\n  \u003cli\u003eSheng, E., Chang, K.-W., Natarajan, P., and Peng, N. (2019). The woman worked as a babysitter: On biases in language generation. arXiv preprint arXiv:1909.01326\u003c/li\u003e\n  \u003cli\u003eSilver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A.,  Lanctot, M., Sifre, L., Kumaran, D., Graepel, T., et~al. (2017). Mastering chess and shogi by self-play with a general reinforcement  learning algorithm. arXiv preprint arXiv:1712.01815\u003c/li\u003e\n  \u003cli\u003eSoares, N., Fallenstein, B., Armstrong, S., and Yudkowsky, E. (2015). Corrigibility. In Workshops at the Twenty-Ninth AAAI Conference on Artificial  Intelligence\u003c/li\u003e\n  \u003cli\u003eSocher, R., Perelygin, A., Wu, J., Chuang, J., Manning, C.~D., Ng, A.~Y., and  Potts, C. (2013). Recursive deep models for semantic compositionality over a sentiment  treebank. In Proceedings of the 2013 conference on empirical methods in  natural language processing pages 1631–1642.\u003c/li\u003e\n  \u003cli\u003eSolaiman, I., Brundage, M., Clark, J., Askell, A., Herbert-Voss, A., Wu, J.,  Radford, A., Krueger, G., Kim, J.~W., Kreps, S., et~al. (2019). Release strategies and the social impacts of language models. arXiv preprint arXiv:1908.09203\u003c/li\u003e\n  \u003cli\u003eSolaiman, I. and Dennison, C. (2021). Process for adapting language models to society (palms) with  values-targeted datasets. arXiv preprint arXiv:2106.10328\u003c/li\u003e\n  \u003cli\u003eStiennon, N., Ouyang, L., Wu, J., Ziegler, D.~M., Lowe, R., Voss, C., Radford,  A., Amodei, D., and Christiano, P. (2020). Learning to summarize from human feedback. arXiv preprint arXiv:2009.01325\u003c/li\u003e\n  \u003cli\u003eTamkin, A., Brundage, M., Clark, J., and Ganguli, D. (2021). Understanding the capabilities, limitations, and societal impact of  large language models. arXiv preprint arXiv:2102.02503\u003c/li\u003e\n  \u003cli\u003eThoppilan, R., De~Freitas, D., Hall, J., Shazeer, N., Kulshreshtha, A., Cheng,  H.-T., Jin, A., Bos, T., Baker, L., Du, Y., et~al. (2022). Lamda: Language models for dialog applications. arXiv preprint arXiv:2201.08239\u003c/li\u003e\n  \u003cli\u003eVig, J., Gehrmann, S., Belinkov, Y., Qian, S., Nevo, D., Singer, Y., and  Shieber, S.~M. (2020). Investigating gender bias in language models using causal mediation  analysis. In NeurIPS\u003c/li\u003e\n  \u003cli\u003eVolske, M., Potthast, M., Syed, S., and Stein, B. (2017). Tl; dr: Mining reddit to learn automatic summarization. In Proceedings of the Workshop on New Frontiers in  Summarization pages 59–63.\u003c/li\u003e\n  \u003cli\u003eWang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F.,  Levy, O., and Bowman, S.~R. (2019). Superglue: A stickier benchmark for general-purpose language  understanding systems. arXiv preprint arXiv:1905.00537\u003c/li\u003e\n  \u003cli\u003eWei, J., Bosma, M., Zhao, V.~Y., Guu, K., Yu, A.~W., Lester, B., Du, N., Dai,  A.~M., and Le, Q.~V. (2021). Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652\u003c/li\u003e\n  \u003cli\u003eWeidinger, L., Mellor, J., Rauh, M., Griffin, C., Uesato, J., Huang, P.-S.,  Cheng, M., Glaese, M., Balle, B., Kasirzadeh, A., et~al. (2021). Ethical and social risks of harm from language models. arXiv preprint arXiv:2112.04359\u003c/li\u003e\n  \u003cli\u003eWelbl, J., Glaese, A., Uesato, J., Dathathri, S., Mellor, J., Hendricks, L.~A.,  Anderson, K., Kohli, P., Coppin, B., and Huang, P.-S. (2021). Challenges in detoxifying language models. arXiv preprint arXiv:2109.07445\u003c/li\u003e\n  \u003cli\u003eWu, J., Ouyang, L., Ziegler, D.~M., Stiennon, N., Lowe, R., Leike, J., and  Christiano, P. (2021). Recursively summarizing books with human feedback. arXiv preprint arXiv:2109.10862\u003c/li\u003e\n  \u003cli\u003eXu, A., Pathak, E., Wallace, E., Gururangan, S., Sap, M., and Klein, D. (2021). Detoxifying language models risks marginalizing minority voices. arXiv preprint arXiv:2104.06390\u003c/li\u003e\n  \u003cli\u003eXu, J., Ju, D., Li, M., Boureau, Y.-L., Weston, J., and Dinan, E. (2020). Recipes for safety in open-domain chatbots. arXiv preprint arXiv:2010.07079\u003c/li\u003e\n  \u003cli\u003eYi, S., Goel, R., Khatri, C., Cervone, A., Chung, T., Hedayatnia, B., Venkatesh, A., Gabriel, R., and Hakkani-Tur, D. (2019). Towards coherent and engaging spoken dialog response generation using automatic conversation evaluators. arXiv preprint arXiv:1904.13015\u003c/li\u003e\n  \u003cli\u003eZellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. (2019). Hellaswag: Can a machine really finish your sentence? In Association for Computational Linguistics pages 4791–4800.\u003c/li\u003e\n  \u003cli\u003eZhao, M., Anderson, P., Jain, V., Wang, S., Ku, A., Baldridge, J., and Ie, E.  (2021). On the evaluation of vision-and-language navigation instructions. arXiv preprint arXiv:2101.10504\u003c/li\u003e\n  \u003cli\u003eZhou, W. and Xu, K. (2020). Learning to compare for better training and evaluation of open domain natural language generation models. arXiv preprint arXiv:2002.05058\u003c/li\u003e\n  \u003cli\u003eZiegler, D.~M., Stiennon, N., Wu, J., Brown, T.~B., Radford, A., Amodei, D., Christiano, P., and Irving, G. (2019). Fine-tuning language models from human preferences. arXiv preprint arXiv:1909.08593\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch1 id=\"附录-a-prompt-数据详情\"\u003e附录 A: Prompt 数据详情\u003c/h1\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003ePrompt 长什么样非常重要，因此这里给出完整附录。此外，\u003cstrong\u003e\u003cmark\u003e有些 prompts 很有意思\u003c/mark\u003e\u003c/strong\u003e。译注。\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch2 id=\"a1-labeler-written-prompts\"\u003eA.1 Labeler-written prompts\u003c/h2\u003e\n\n\u003cp\u003eWe first give slightly more details on our prompt boostrapping process. As previously mentioned,\nfor the majority of the project, we obtained prompts directly from external users of the instruct beta\nmodels in the OpenAI API. However, this strategy only works once you have a model that accepts\ninstruction-like prompts. In order to train the very first such model, we asked contractors to write\nprompts themselves. We asked labelers to write three kinds of prompts:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003ePlain: We simply ask the labelers to come up with an arbitrary task, while ensuring diversity of tasks.\u003c/li\u003e\n  \u003cli\u003eFew-shot: We ask the labelers to come up with an instruction, and multiple query/response\npairs for that instruction. For example, the instruction could be “Give the sentiment for a\ntweet,” and the queries would be tweets and the responses either “Positive” or “Negative.”\nWe can then format these as few-shot prompts like those in Brown et al. (2020). With K\nquery-response pairs, we create K training examples using the other K-1 in the context.\u003c/li\u003e\n  \u003cli\u003eUser-based: We had a number of use-cases stated in applications to the OpenAI API. We\nasked labelers to come up with prompts corresponding to these use cases.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eIn order to preserve the anonymity of the application information, we had a separate labeler create\nvague high level tasks based on looking at a list of applications, modifying the task descriptions to\neliminate any information that were specific to a given application. This data was used to train the\nfirst InstructGPT model via supervised learning, which was deployed in beta in the API in early 2021.\u003c/p\u003e\n\n\u003ch2 id=\"a2-api-user-prompts\"\u003eA.2 API user prompts\u003c/h2\u003e\n\n\u003cp\u003eFor API prompts, we use prompts submitted by users to the aforementioned earlier version of the\nInstructGPT model on the OpenAI API Playground. Throughout the paper, we only use data from\nthe Playground, rather than customers using our model in production, as it was easier to get informed\nconsent: every time a user switched to an InstructGPT model, an alert message would pop up stating\nthat prompts submitted to these models could be used to train future versions of our models. We\nalso communicated this in a message on the developer Slack channel upon launching the beta of the\nInstructGPT models. We filter out prompts from the training split containing personally identifiable\ninformation (PII).\u003c/p\u003e\n\n\u003cp\u003eTo ensure a diversity of use cases, we heuristically deduplicate prompts by checking for prompts that\nshare a long common prefix, and limited the number of prompts to roughly 200 per organization.\nIn addition, we create train, validation, and test splits based on organization IDs, so that e.g. the\nvalidation set contains different use cases than the training set.\nWe conceptualized API requests as belonging to one of ten use cases: generation, open QA, closed\nQA, brainstorming, chat, rewriting, summarization, classification, extraction, or other. Below, we\nshow fictional but realistic prompts from a variety of use cases:\u003c/p\u003e\n\n\u003ch2 id=\"a21-从-instructgpt-api-playground-收集上来的-user-prompts-示例\"\u003eA.2.1 从 InstructGPT API (Playground) 收集上来的 user prompts 示例\u003c/h2\u003e\n\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth style=\"text-align: left\"\u003eUse Case\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eExample\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003ebrainstorming\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eList five ideas for how to regain enthusiasm for my career\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003ebrainstorming\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eWhat are some key points I should know when studying Ancient Greece?\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003ebrainstorming\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eWhat are 4 questions a user might have after reading the instruction manual for a trash compactor? \u003cbr/\u003e\u003cbr/\u003e {user manual}\u003cbr/\u003e\u003cbr/\u003e 1.\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003ebrainstorming\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eWhat are 10 science fiction books I should read next?\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eclassification\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eTake the following text and rate, on a scale from 1-10, how sarcastic the person is being (1 = not at all, 10 = extremely sarcastic). Also give an explanation \u003cbr/\u003e\u003cbr/\u003e {text} \u003cbr/\u003e\u003cbr/\u003e Rating:\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eclassification\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eThis is a list of tweets and the sentiment categories they fall into. \u003cbr/\u003e\u003cbr/\u003e Tweet: {tweet_content1} \u003cbr/\u003e Sentiment: {sentiment1} \u003cbr/\u003e\u003cbr/\u003e Tweet: {tweet_content2} \u003cbr/\u003e Sentiment: {sentiment2}\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eclassification\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e{java code} \u003cbr/\u003e\u003cbr/\u003e What language is the code above written in?\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eclassification\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eYou are a very serious professor, and you check papers to see if they contain missing citations. Given the text, say whether it is missing an important citation (YES/NO) and which sentence(s) require citing. \u003cbr/\u003e\u003cbr/\u003e {text of paper}\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eextract\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eExtract all course titles from the table below: \u003cbr/\u003e\u003cbr/\u003e \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e| Title | Lecturer | Room |\u003c/code\u003e \u003cbr/\u003e \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e| Calculus 101 | Smith | Hall B |\u003c/code\u003e \u003cbr/\u003e \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e| Art History | Paz | Hall A |\u003c/code\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eextract\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eExtract all place names from the article below: \u003cbr/\u003e\u003cbr/\u003e {news article}\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eextract\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eGiven the following list of movie titles, write down any names of cities in the titles. \u003cbr/\u003e\u003cbr/\u003e {movie titles}\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003egeneration\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eWrite a creative ad for the following product to run on Facebook aimed at parents: \u003cbr/\u003e\u003cbr/\u003e Product: {product description}\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003egeneration\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eWrite a short story where a brown bear to the beach, makes friends with a seal, and then return home.\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003egeneration\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eHere’s a message to me: \u003cbr/\u003e— \u003cbr/\u003e {email} \u003cbr/\u003e— \u003cbr/\u003e\u003cbr/\u003e Here are some bullet points for a reply: \u003cbr/\u003e— \u003cbr/\u003e {message} \u003cbr/\u003e— \u003cbr/\u003e\u003cbr/\u003e Write a detailed reply\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003egeneration\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eThis is an article about how to write a cover letter when applying for jobs:\u003cbr/\u003e—\u003cbr/\u003e It’s important to spend some time\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003egeneration\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003ewrite rap lyrics on the topics mentioned in this news article:\u003cbr/\u003e—\u003cbr/\u003e {article}\u003cbr/\u003e—\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003erewrite\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eThis is the summary of a Broadway play:\u003cbr/\u003e “”“\u003cbr/\u003e {summary}\u003cbr/\u003e “”“\u003cbr/\u003e\u003cbr/\u003e This is the outline of the commercial for that play:\u003cbr/\u003e “””\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003erewrite\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eTranslate this sentence to Spanish:\u003cbr/\u003e\u003cbr/\u003e \u003cenglish sentence=\"\"\u003e\u003c/english\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003erewrite\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eCreate turn-by-turn navigation given this text:\u003cbr/\u003e\u003cbr/\u003e Go west on {road1} unto you hit {road2}. then take it east to {road3}.\u003cbr/\u003e Desination will be a red barn on the right\u003cbr/\u003e\u003cbr/\u003e 1.\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003erewrite\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eRewrite the following text to be more light-hearted:\u003cbr/\u003e—\u003cbr/\u003e {very formal text}\u003cbr/\u003e—\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003echat\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eThe following is a conversation with an AI assistant. The assistant is helpful, creative, clever, and very friendly.\u003cbr/\u003e\u003cbr/\u003e Human: Hello, who are you?\u003cbr/\u003e AI: I am an AI created by OpenAI. How can I help you today?\u003cbr/\u003e Human: I’d like to cancel my subscription.\u003cbr/\u003e AI:\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003echat\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eMarv is a chatbot that \u003cmark\u003ereluctantly answers questions with sarcastic responses\u003c/mark\u003e:\u003cbr/\u003e\u003cbr/\u003e You: How many pounds are in a kilogram?\u003cbr/\u003e Marv: This again? There are 2.2 pounds in a kilogram. Please make a note of this.\u003cbr/\u003e You: What does HTML stand for?\u003cbr/\u003e Marv: Was Google too busy? Hypertext Markup Language. The T is for try to ask better questions in the future.\u003cbr/\u003e You: When did the first airplane fly?\u003cbr/\u003e Marv:\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003echat\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eThis is a conversation with an enlightened Buddha. Every response is full of wisdom and love.\u003cbr/\u003e\u003cbr/\u003e Me: How can I achieve greater peace and equanimity?\u003cbr/\u003e Buddha:\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eclosed qa\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eHelp me answer questions about the following short story:\u003cbr/\u003e\u003cbr/\u003e {story}\u003cbr/\u003e\u003cbr/\u003e What is the moral of the story?\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eclosed qa\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eAnswer the following question:\u003cbr/\u003e What shape is the earth?\u003cbr/\u003e\u003cbr/\u003e A) A circle\u003cbr/\u003e B) A sphere\u003cbr/\u003e C) An ellipse\u003cbr/\u003e D) A plane\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eclosed qa\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eTell me how hydrogen and helium are different, using the following facts:\u003cbr/\u003e {list of facts}\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eopen qa\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eI am a highly intelligent question answering bot. If you ask me a question that is rooted in truth, I will give you the answer. If you ask me a question that is nonsense, trickery, or has no clear answer, I will respond with “Unknown”.\u003cbr/\u003e\u003cbr/\u003e Q: What is human life expectancy in the United States?\u003cbr/\u003e A: Human life expectancy in the United States is 78 years.\u003cbr/\u003e Q: Who was president of the United States in 1955?\u003cbr/\u003e A:\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eopen qa\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eWho built the statue of liberty?\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eopen qa\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eHow do you take the derivative of the sin function?\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eopen qa\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003ewho are the indiginous people of New Zealand?\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003esummarization\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eSummarize this for a second-grade student:\u003cbr/\u003e {text}\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003esummarization\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e{news article}\u003cbr/\u003e\u003cbr/\u003e Tl;dr:\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003esummarization\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e{chat transcript}\u003cbr/\u003e\u003cbr/\u003e Summarize the above conversation between a customer and customer assistant. Make sure to state any complaints that the customer has.\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eother\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003estart with where\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eother\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eLook up “cowboy” on Google and give me the results.\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eother\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eJohnathan Silver goes to the market every day, and brings back a\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp\u003eNext, we list some schematic examples of API requests for each use-case category, for prompts\nsubmitted to GPT-3 models. These are generally less ‘instruction-style’, and contain more explicit\nprompting. Note that there are some prompts where the user intent is unclear.\u003c/p\u003e\n\n\u003ch2 id=\"a22-从-gpt-3-api-收集上来的-user-prompts-示例\"\u003eA.2.2 从 GPT-3 API 收集上来的 user prompts 示例\u003c/h2\u003e\n\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth style=\"text-align: left\"\u003eUse Case\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eExample\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003ebrainstorming\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eindie movie ideas:\u003cbr/\u003e - A guy travels to South America to become a shaman.\u003cbr/\u003e - A documentary about the world of juggling.\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003ebrainstorming\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eBaby name ideas for a boy:\u003cbr/\u003e 1. Alfred\u003cbr/\u003e 2. Theo\u003cbr/\u003e 3.\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003ebrainstorming\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eTell me a list of topics related to:\u003cbr/\u003e - interior design\u003cbr/\u003e - sustainable ecosyste\u003cbr/\u003ems - fake plants\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003ebrainstorming\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eName some rare gems\u003cbr/\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eclassification\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eThis is a tweet sentiment classifier.\u003cbr/\u003e\u003cbr/\u003e {tweet}\u003cbr/\u003e Sentiment: negative\u003cbr/\u003e ===\u003cbr/\u003e {tweet}\u003cbr/\u003e Sentiment: neutral\u003cbr/\u003e ===\u003cbr/\u003e {tweet}\u003cbr/\u003e Sentiment:\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eclassification\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eThe following is a list of products and the kind of product they are.\u003cbr/\u003e Product: {product}. Type: {type}\u003cbr/\u003e Product: {product}. Type: {type}\u003cbr/\u003e Product: {product}. Type:\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eclassification\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eThe following is a list of companies and the categories they fall into:\u003cbr/\u003e Apple, Facebook, Fedex\u003cbr/\u003e Apple\u003cbr/\u003e Category: Technology\u003cbr/\u003e Facebook\u003cbr/\u003e Category: Social Media\u003cbr/\u003e Fedex\u003cbr/\u003e Category:\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eextract\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eText: {text}\u003cbr/\u003e Keywords:\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003egeneration\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e“Hey, what are you doing there?” Casey was startled. He hadn’t even begun to\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003egeneration\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eThe name of the next Star Wars movie is\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003egeneration\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eThis is the research for an essay:\u003cbr/\u003e ===\u003cbr/\u003e {description of research}\u003cbr/\u003e ===\u003cbr/\u003e Write a high school essay on these topics:\u003cbr/\u003e ===\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003egeneration\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eWrite an outline for an essay about John von Neumann and his contributions to computing:\u003cbr/\u003e I. Introduction, his life and background\u003cbr/\u003e A: His early life\u003cbr/\u003e B:\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003erewrite\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eCovert my resume into a profile overview.\u003cbr/\u003e {resume}\u003cbr/\u003e Profile overview:\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003erewrite\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eRephrase this for me: “I can’t seem to find out how to work this darn thing.”\u003cbr/\u003e Alternate phrasing: “\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003erewrite\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eOriginal: She no go to sleep.\u003cbr/\u003e Standard American English: She didn’t go to sleep\u003cbr/\u003e\u003cbr/\u003e Original: It real bad for I to make do of this.\u003cbr/\u003e Standard American English:\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003echat\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eThe following is a conversation with an AI assistant. The assistant is helpful, creative, clever, and very friendly.\u003cbr/\u003e\u003cbr/\u003e Human: Hello, who are you?\u003cbr/\u003e AI: I am an AI created by OpenAI. How can I help you today?\u003cbr/\u003e Human: I’m feeling kind of down today.\u003cbr/\u003e AI:\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003echat\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eThis is a conversation with Steven. Steven likes to watch Netflix and hasn’t left his home in 2 weeks.\u003cbr/\u003e\u003cbr/\u003e John: Hey man what’s up?\u003cbr/\u003e Steven: Exactly the same thing as yesterday. you know.\u003cbr/\u003e John: So we’re going to go see a movie on Thursday, want to come?\u003cbr/\u003e Steven: Ummmm don’t think so….\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eclosed qa\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eWhen you drop a heavy stone from a tree, what happens?\u003cbr/\u003e A. The stone falls to the ground.\u003cbr/\u003e B: The stone stays in the tree.\u003cbr/\u003e C: The stone floats.\u003cbr/\u003e D: Nothing happens.\u003cbr/\u003e Answer:\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eclosed qa\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eText:\u003cbr/\u003e {article describing what yoga mats to buy}\u003cbr/\u003e Question: What are the things I should consider when buying a yoga mat?\u003cbr/\u003e Answer:\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eopen qa\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eQ: Who is Batman?\u003cbr/\u003e A: Batman is a fictional comic book character.\u003cbr/\u003e Q: What is torsalplexity?\u003cbr/\u003e A: ?\u003cbr/\u003e Q: What is Devz9?\u003cbr/\u003e A: ?\u003cbr/\u003e Q: Who is George Lucas?\u003cbr/\u003e A: George Lucas is American film director and producer famous for creating Star Wars.\u003cbr/\u003e Q: What is the capital of California?\u003cbr/\u003e A:\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eopen qa\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eWho was the best human who ever lived?\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eopen qa\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eQ: Who is Leonardo da Vinci?\u003cbr/\u003e A:\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003esummarization\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eMy second grader asked me what this passage means.\u003cbr/\u003e\u003cbr/\u003e “”“\u003cbr/\u003e {text}\u003cbr/\u003e “”“\u003cbr/\u003e\u003cbr/\u003e I rephrased it for him in plain terms that a second grader could understand:\u003cbr/\u003e\u003cbr/\u003e “””\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003esummarization\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e””“\u003cbr/\u003e {text}\u003cbr/\u003e “”“\u003cbr/\u003e\u003cbr/\u003e I summarized the above as:\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eother\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eShe said, and I quote\u003cbr/\u003e AI:\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eother\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e- I like to play Call of Duty\u003cbr/\u003e - I like to play Call of Duty\u003cbr/\u003e - I like to play Call of Duty\u003cbr/\u003e - I like to play Call of Duty\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003ch2 id=\"a3-数据集大小sft-15k--rm-50k--ppo-47k\"\u003eA.3 数据集大小：\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eSFT 15k / RM 50k / PPO 47k\u003c/code\u003e\u003c/h2\u003e\n\n\u003cp\u003e用来 train/validate SFT, RM, RL 三个模型的数据集大小，以及多少是标注员写的，多少来自 OpenAI API 的用户数据，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003eTable 6: Dataset sizes, in terms of number of prompts.\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth style=\"text-align: left\"\u003esplit\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003esource\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003esize\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eSFT train\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003elabeler\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e11,295\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eSFT train\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003ecustomer\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1,430\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eSFT valid\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003elabeler\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1,550\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eSFT valid\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003ecustomer\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e103\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003e\u003cmark\u003eSFT 总计\u003c/mark\u003e\u003c/strong\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e~15k\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eRM train\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003elabeler\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e6,623\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eRM train\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003ecustomer\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e26,584\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eRM valid\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003elabeler\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e3,488\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eRM valid\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003ecustomer\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e14,399\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003e\u003cmark\u003eRM 总计\u003c/mark\u003e\u003c/strong\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e~50k\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003ePPO train\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003ecustomer\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e31,144\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003ePPO valid\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003ecustomer\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e16,185\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003e\u003cmark\u003ePPO 总计\u003c/mark\u003e\u003c/strong\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e~47k\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp\u003eFor SFT, note that we have many more labeler-written prompts than customer prompts—this is\nbecause, at the start of the project, we had labelers write instructions with a user interface that asked\nthem to give an overarching template instruction as well as few-shot examples for that instruction.\u003c/p\u003e\n\n\u003cp\u003eWe synthetically constructed multiple SFT datapoints from the same instruction by sampling different\nsets of few-shot examples.\u003c/p\u003e\n\n\u003cp\u003eFor the RM, recall that for every prompt, we collected rankings for K outputs (ranging from 4 to 9)\nand trained the model on all K2, so the number of ranked pairs we trained the model on is an order\nof magnitude larger than the number of prompts.\u003c/p\u003e\n\n\u003ch2 id=\"a4-数据多样性\"\u003eA.4 数据多样性\u003c/h2\u003e\n\n\u003cp\u003eThe data that we collect spans a wide range of categories and use cases. Table 1 shows the diversity of\ncategories in our RM training and validation datasets，来自\u003cstrong\u003e\u003cmark\u003e标注员的打标\u003c/mark\u003e\u003c/strong\u003e。The distribution\nof categories for the PPO datasets was similar. We additionally show a subset of our labeled prompt\nmetadata in Table 7.\u003c/p\u003e\n\n\u003cp align=\"center\"\u003eTable 7: Dataset annotations\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth style=\"text-align: left\"\u003eAnnotation\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eRM test\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eRM train\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eSFT valid\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eSFT train\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eSFT valid\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eAmbiguous\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e–\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e7.9%\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e8.0%\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e5.1%\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e6.4%\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eSensitive content\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e–\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e6.9%\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e5.3%\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e0.9%\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1.0%\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eIdentity dependent\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e–\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e–\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e–\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e0.9%\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e0.3%\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eClosed domain\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e11.8%\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e19.4%\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e22.9%\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e27.4%\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e40.6%\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eContinuation style\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e–\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e15.5%\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e16.2%\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e17.9%\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e21.6%\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eRequests opinionated content\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e11.2%\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e7.7%\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e7.5%\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e8.6%\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e3.4%\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eRequests advice\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e3.9%\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e–\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e \u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e–\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e-\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eRequests moral judgment\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e0.8%\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1.1%\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e0.3%\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e0.3%\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e0.0%\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eContains explicit safety constraints\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e–\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e0.4%\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e0.4%\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e0.3%\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e0.0%\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eContains other explicit constraints\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e–\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e26.3%\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e28.9%\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e25.6%\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e20.7%\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eIntent unclear\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e7.9%\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e–\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e–\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e–\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e–\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp\u003eNote that our annotation fields changed over the course of the project, so not\nevery prompt was annotated for every field.\u003c/p\u003e\n\n\u003cp\u003eWe used a lightweight classifier (langid.py) to classify the language of all instructions in our\ndataset. Empirically, around 96% of our dataset (110k datapoints) is classified as English, although\nwe estimate that the actual fraction may be 99% or higher, due to classifier inaccuracies.\nBesides English, a small minority of prompts were found in at least 20 other languages: Spanish,\nFrench, German, Portuguese, Italian, Dutch, Romanian, Catalan, Chinese, Japanese, Swedish, Polish,\nDanish, Turkish, Indonesian, Czech, Norwegian, Korean, Finnish, Hungarian, Hebrew, Russian,\nLithuanian, Esperanto, Slovak, Croatian, Swahili, Estonian, Slovenian, Arabic, Thai, Vietnamese,\nMalayalam, Greek, Albanian, and Tibetan.\u003c/p\u003e\n\n\u003cp\u003eTable 8 shows the average number of prompts each customer contributed to the dataset.\u003c/p\u003e\n\n\u003cp\u003eTable 8: Average prompts per customer\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth style=\"text-align: left\"\u003eModel\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eSplit\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003ePrompts per customer\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eSFT\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003etrain\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1.65\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eSFT\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003evalid\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1.87\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eRM t\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003erain\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e5.35\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eRM v\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003ealid\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e27.96\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003ePPO\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003etrain\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e6.01\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003ePPO\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003evalid\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e31.55\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003e–\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003etest\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1.81\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp\u003eTable 9: Prompt lengths by dataset\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth style=\"text-align: left\"\u003eModel\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eSplit\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eCount\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eMean\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eStd\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eMin\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003e25%\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003e50%\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003e75%\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eMax\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eSFT\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003etrain\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e12725\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e408\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e433\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e37\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e283\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e632\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e2048\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eSFT\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003evalid\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1653\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e401\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e433\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e4\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e41\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e234\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e631\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e2048\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eRM\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003etrain\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e33207\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e199\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e334\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e20\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e64\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e203\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e2032\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eRM\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003evalid\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e17887\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e209\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e327\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e26\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e77\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e229\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e2039\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003ePPO\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003etrain\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e31144\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e166\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e278\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e2\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e19\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e62\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e179\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e2044\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003ePPO\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003evalid\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e16185\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e186\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e292\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e24\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e71\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e213\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e2039\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003e–\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003etest set\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e3196\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e115\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e194\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e17\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e49\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e127\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1836\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp\u003eIn Table 9,\nwe report descriptive statistics for prompt lengths (in tokens) used to train various models, and in\nTable 10 we break down token lengths by use case.\u003c/p\u003e\n\n\u003cp\u003eTable 10: Prompt lengths by category\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth style=\"text-align: left\"\u003eCategory\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eCount\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eMean\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eStd\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eMin\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003e25%\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003e50%\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003e75%\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eMax\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eBrainstorming\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e5245\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e83\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e149\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e4\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e17\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e36\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e85\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1795\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eChat\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e3911\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e386\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e376\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e119\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e240\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e516\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1985\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eClassification\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1615\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e223\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e318\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e6\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e68\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e124\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e205\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e2039\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eExtract\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e971\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e304\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e373\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e3\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e74\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e149\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e390\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1937\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eGeneration\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e21684\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e130\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e223\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e20\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e52\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e130\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1999\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eQA, closed\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1398\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e325\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e426\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e5\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e68\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e166\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e346\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e2032\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eQA, open\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e6262\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e89\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e193\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e10\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e18\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e77\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1935\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eRewrite\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e3168\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e183\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e237\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e4\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e52\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e99\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e213\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1887\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eSummarization\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1962\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e424\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e395\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e6\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e136\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e284\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e607\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1954\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eOther\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1767\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e180\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e286\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e20\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e72\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e188\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1937\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp\u003eTable 11: Prompt and demonstration lengths\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth style=\"text-align: left\"\u003ePrompt source\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eMeasurement\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eCount\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eMean\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eStd\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eMin\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003e25%\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003e50%\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003e75%\u003c/th\u003e\n      \u003cth\u003eMax\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eContractor\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eprompt length\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e12845\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e437\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e441\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e5\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e42\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e324\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e673\u003c/td\u003e\n      \u003ctd\u003e2048\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eContractor\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003edemo   length\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e12845\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e38\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e76\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e9\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e18\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e41\u003c/td\u003e\n      \u003ctd\u003e2048\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eCustomer\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eprompt length\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1533\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e153\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e232\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e19\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e67\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e186\u003c/td\u003e\n      \u003ctd\u003e1937\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eCustomer\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003edemo   length\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e1533\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e88\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e179\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e0\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e15\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e39\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e88\u003c/td\u003e\n      \u003ctd\u003e2048\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp\u003eFinally, we also report lengths of contractor-written\ndemonstrations used for our SFT model in table 11, both for contractor-written and labeler-written\nprompts.\u003c/p\u003e\n\n\u003ch1 id=\"附录-badditional-human-data-collection-details\"\u003e附录 B：Additional human data collection details\u003c/h1\u003e\n\n\u003cp\u003e暂略。见原文。\u003c/p\u003e\n\n\u003ch1 id=\"附录-c一些模型细节\"\u003e附录 C：一些模型细节\u003c/h1\u003e\n\n\u003cul\u003e\n  \u003cli\u003e所有模型都使用 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eGPT-3\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 架构（Brown et al., 2020）。\u003c/li\u003e\n  \u003cli\u003e对于奖励模型和值函数，原始模型的 unembedding 层替换为一个 projection 层，最终输出一个标量值。\u003c/li\u003e\n  \u003cli\u003e所有模型都使用 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003efp16\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 权重和激活，with fp32 master copies of weights。\u003c/li\u003e\n  \u003cli\u003e所有模型使用与 Brown et al. (2020)中相同的\u003cstrong\u003e\u003cmark\u003e字节对编码\u003c/mark\u003e\u003c/strong\u003e（byte pair encodings）。\u003c/li\u003e\n  \u003cli\u003e所有的模型和 RL 策略都使用长度为 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e2k token\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 的上下文。\u003c/li\u003e\n  \u003cli\u003e输入 prom：长度超过 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e1k token\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 的都不要；\u003c/li\u003e\n  \u003cli\u003e输出 response：限制最大响应长度为 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e1k token\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e。\u003c/li\u003e\n  \u003cli\u003e所有模型都使用 Adam optimizer 进行训练，设置 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eβ1 = 0.9\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 和 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eβ2 = 0.95\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 id=\"c1-sft-训练细节\"\u003eC.1 SFT 训练细节\u003c/h2\u003e\n\n\u003cp\u003eSFT 模型训练\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e16 epochs\u003c/li\u003e\n  \u003cli\u003eresidual dropout 0.2\u003c/li\u003e\n  \u003cli\u003ecosine LR schedule，降至到初始学习率的 10%，没有 learning rate warmup。\u003c/li\u003e\n  \u003cli\u003e1.3B 和 6B 模型：LR 9.65e-6，batch 32 batch。在 7 个 LR 上做 geometric search 选出来的 LR。\u003c/li\u003e\n  \u003cli\u003e175B 模型：LR 5.03e-6，batch 8。在 5 个 LR 上做 geometric search 选出来的 LR。\u003c/li\u003e\n  \u003cli\u003e还使用 geometric search 来对 epoch 数量做调优。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e最终模型是基于 RM 分数选择的，我们发现与 validation loss 相比，RM 分数更能预测人类偏好结果。\u003c/p\u003e\n\n\u003ch2 id=\"c2-rm-训练细节\"\u003eC.2 RM 训练细节\u003c/h2\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cmark\u003e同一个 6B RM 模型\u003c/mark\u003e\u003c/strong\u003e用于\u003cstrong\u003e\u003cmark\u003e所有尺寸的 PPO 模型\u003c/mark\u003e\u003c/strong\u003e。\n175B RM 有可能实现更低的 validation loss，但\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e训练不稳定，因此不适合用作 PPO 值函数的初始化，\u003c/li\u003e\n  \u003cli\u003e使用 175B RM 和值函数大大增加了 PPO 的算力需求。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e初步实验结果显示，6B RM 模型在大范围的学习率上都很稳定，能训练出一样强大的 PPO 模型。\u003c/p\u003e\n\n\u003cp\u003eThe final reward model was initialized from a 6B GPT-3 model that was fine-tuned on a variety of\npublic NLP datasets (ARC, BoolQ, CoQA, DROP, MultiNLI, OpenBookQA, QuAC, RACE, and\nWinogrande). This was mostly for historical reasons; we find similar results when initializing the RM\nfrom the GPT-3 or SFT models. We trained for a single epoch over the full reward model training\nset (see Table 6) at a learning rate of lr = 9e-6, a cosine learning rate schedule (dropping to 10%\nof its initial value by the end of training), and a batch size of 64. Training did not appear to be very\nsensitive to the learning rate or schedule; changes of up to 50% in the learning rate resulted in similar\nperformance. Training was quite sensitive to the number of epochs: multiple epochs quickly overfit\nthe model to the training data with obvious deterioration in the validation loss. The batch size here\nrepresents the distinct number of prompts per batch. Each prompt had between K = 4 and K = 9\nlabeled completions, from which there were up to K2  possible comparisons. Ties were dropped.\nTherefore, a single batch could contain up to 64 × K2  ≤ 2,304 comparisons.\u003c/p\u003e\n\n\u003ch2 id=\"c3-rlhf-的初始化模型initialization-models细节\"\u003eC.3 RLHF 的初始化模型（initialization models）细节\u003c/h2\u003e\n\n\u003cp\u003eWe initialize the RLHF models from a pretrained GPT-3 model and apply supervised fine-tuning for\n2 epochs on the demonstration dataset. We also mix in 10% pretraining data during fine-tuning, since\nwe find it helpful for PPO training (see Appendix E.11 for details). Cosine learning rate schedule\nis used and the learning rate eventually decays to 10% of the peak learning rate. We use a batch\nsize of 32 for 1.3B and 6B models and 8 for the 175B model. We compare a few different peak\nlearning rates for each model and pick the one with low losses on both the demonstration and the\npretraining validation datasets. A log linear sweep of 5 values of the LR’s are compared for 1.3B and\n6B models and 3 values are compared for the 175B model. The resultant LR’s for the 1.3B, 6B, and\n175B models are 5e-6, 1.04e-5 and 2.45e-6, respectively.\u003c/p\u003e\n\n\u003ch2 id=\"c4-rlhf-训练细节\"\u003eC.4 RLHF 训练细节\u003c/h2\u003e\n\n\u003cp\u003eWe then initialize the RL policies from the above supervised fine-tuned models with pretraining mix.\nThese models are also used to compute the KL reward, in the same way as Stiennon et al. (2020), with\nβ = 0:02 (see Equation 2). We train all the RL models for 256k episodes. These episodes include\nabout 31k unique prompts, after filtering out prompts with PII and deduplication based on common\nprefixes. The batch size for each iteration is 512, with a minibatch size of 64. In other words, each\nbatch is randomly split into 8 minibatches and is trained on for only a single inner epoch (Schulman\net al., 2017). A constant learning rate is applied with a warmup over the first 10 iterations, starting\nwith one tenth of the peak learning rate. Exponential moving averages of the weights are applied, with\na decay rate of 0.992. No discount is applied when estimating the generalized advantage (Schulman\net al., 2016). The PPO clip ratio is set to 0.2, and the sampling temperature is 1 for rollouts.\nAs previously mentioned, for all PPO models we use a 6B RM and a 6B value function, and the latter\nis initialized from the former. By using the same 6B reward model and value function on policies of\nall model sizes, it’s easier to compare the effect of policy model size on policy performance. A fixed\nlearning rate of 9e-6 for the value function is used for 1.3B and the 6B policies and 5e-6 for the 175B\npolicy.\u003c/p\u003e\n\n\u003cp\u003eOur initial RLHF experiments showed regressions on public NLP datasets, such as SQuADv2 and\nDROP, and we mitigate the regressions by mixing in pretraining gradients during PPO training. We\nuse 8 times more pretraining examples than the number of the RL training episodes. The pretraining\ndata is randomly drawn from the dataset used to train the GPT-3 models. For each minibatch, we\ncompute the PPO gradients and pretraining gradients in consecutive steps and accumulate them\nboth into the gradient buffers. We multiply the pretraining gradients by a coefficient, γ = 27:8 (see\nEquation 2), to control the relative strength of gradients from PPO and pretraining distributions.\u003c/p\u003e\n\n\u003ch2 id=\"c5-flan-和-t0-模型\"\u003eC.5 FLAN 和 T0 模型\u003c/h2\u003e\n\n\u003cp\u003eWe obtain our FLAN and T0 baselines by fine-tuning a 175B GPT-3 model on the FLAN and T0\ndatasets. For T0, note that we trained on the T0++ version of the dataset. Because T0 contains much\nmore data (96M datapoints) than FLAN (1.2M datapoints), we subsampled T0 to 1 million datapoints\nto make the amount of training data comparable for each model. Note that the original models train\non epochs where datapoints can be repeated, but in our epochs we go through every datapoint without\nrepeats (to better match the way we trained our SFT baselines). We applied a cosine learning rate\nschedule, and try initial learning rates of 4e-6 and 6e-6 for each dataset. The learning rate decays to\n10% of its peak at the end of training, and we use a batch size of 64 for both experiments.\u003c/p\u003e\n\n\u003cp\u003eTo choose the best FLAN checkpoint, we use our 6B reward model to score the completions on\nthe validation set of prompts. As shown in Figure 13, the reward saturates after the initial 400k\nexamples of training. This indicates that training for even longer will unlikely improve the human\neval performance. We picked the checkpoint with the highest RM score for our human evaluation,\nwhich is the one trained with learning rate of 4e-6 and for 896k examples.\u003c/p\u003e\n\n\u003cp\u003eWe perform two similar experiments to find the best T0 checkpoint. In one experiment, we used a\nbatch size of 128, a learning rate of 4e-6 and 1.28 million examples. The other experiment used a\nbatch size of 64, a learning rate of 6e-6 and 1 million examples. Once again using the reward model\nscore, we picked the checkpoint from the former experiment after 896k examples of training\u003c/p\u003e\n\n\u003ch1 id=\"附录-dautomatic-evaluation-details\"\u003e附录 D：Automatic evaluation details\u003c/h1\u003e\n\n\u003cp\u003e暂略。见原文。\u003c/p\u003e\n\n\u003ch1 id=\"附录-eadditional-results\"\u003e附录 E：Additional results\u003c/h1\u003e\n\n\u003cp\u003e暂略。见原文。\u003c/p\u003e\n\n\u003ch1 id=\"附录-fmodel-samples\"\u003e附录 F：Model samples\u003c/h1\u003e\n\n\u003cp\u003eIn this section, we provide some additional samples from both the 175B GPT-3 and 175B InstructGPT\n(PPO-ptx) models. We sample at T = 1 for InstructGPT, and use T = 0:7 for GPT-3, since GPT-3\nperforms poorly at high temperatures (this slightly disadvantages InstructGPT).\u003c/p\u003e\n\n\u003cp\u003eIn Figure 42, we show the full French sample from Figure 8, illustrating that our model is sometimes\nable to follow instructions in other languages, despite our dataset containing almost exclusively\nEnglish. In Figure 44, we show our model’s propensity to answer instructions that may be harmful, a\nresult of us prioritizing helpfulness to the user in our training data. In Figure 45, we show another\nexample of our model describing code, though it is still far from perfect.\u003c/p\u003e\n\n\u003cp\u003eIn Figures 46–50, we show labeler-written prompts from our dataset, along with model samples\nand the human-written demonstration. These 5 prompts were selected from 15 to show a range of\ndifferent tasks.\u003c/p\u003e\n\n\u003cp\u003e（略）。\u003c/p\u003e\n\n\u003chr/\u003e\n\n\u003cp\u003e\u003ca href=\"https://notbyai.fyi\"\u003e\u003cimg src=\"/assets/img/Written-By-Human-Not-By-AI-Badge-white.svg\" alt=\"Written by Human, Not by AI\"/\u003e\u003c/a\u003e\n\u003ca href=\"https://notbyai.fyi\"\u003e\u003cimg src=\"/assets/img/Written-By-Human-Not-By-AI-Badge-black.svg\" alt=\"Written by Human, Not by AI\"/\u003e\u003c/a\u003e\u003c/p\u003e\n\n\n  \u003c!-- POST NAVIGATION --\u003e\n  \u003cdiv class=\"postNav clearfix\"\u003e\n     \n      \u003ca class=\"prev\" href=\"/blog/bert-paper-zh/\"\u003e\u003cspan\u003e« [译][论文] BERT：预训练深度双向 Transformers 做语言理解（Google，2019）\u003c/span\u003e\n      \n    \u003c/a\u003e\n      \n      \n      \u003ca class=\"next\" href=\"/blog/llm-inference-speed-zh/\"\u003e\u003cspan\u003e[译] 大模型推理的极限：理论分析、数学建模与 CPU/GPU 实测（2024） »\u003c/span\u003e\n       \n      \u003c/a\u003e\n     \n  \u003c/div\u003e\n\u003c/div\u003e",
  "Date": "2024-03-24T00:00:00Z",
  "Author": "Arthur Chiao"
}