{
  "Source": "arthurchiao.art",
  "Title": "[译] 大模型推理的极限：理论分析、数学建模与 CPU/GPU 实测（2024）",
  "Link": "https://arthurchiao.art/blog/llm-inference-speed-zh/",
  "Content": "\u003cdiv class=\"post\"\u003e\n  \n  \u003ch1 class=\"postTitle\"\u003e[译] 大模型推理的极限：理论分析、数学建模与 CPU/GPU 实测（2024）\u003c/h1\u003e\n  \u003cp class=\"meta\"\u003ePublished at 2024-04-06 | Last Update 2024-04-25\u003c/p\u003e\n  \n  \u003ch3 id=\"译者序\"\u003e译者序\u003c/h3\u003e\n\n\u003cp\u003e本文翻译自 2024 年的一篇文章：\n\u003ca href=\"https://zeux.io/2024/03/15/llm-inference-sol/\"\u003eLLM inference speed of light\u003c/a\u003e，\n分析了大模型推理的速度瓶颈及量化评估方式，并给出了一些实测数据（我们在国产模型上的实测结果也大体吻合），\n对理解大模型推理内部工作机制和推理优化较有帮助。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/llm-inference-speed/a100-inference-latency.png\" width=\"80%\" height=\"80%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eA100-80GB PICe 推理延迟与吞吐。\u003ca href=\"https://github.com/zeux/calm/blob/main/tools/sol.ipynb\"\u003eImage Source\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e译者水平有限，不免存在遗漏或错误之处。如有疑问，敬请查阅原文。\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003e以下是译文。\u003c/p\u003e\n\n\u003chr/\u003e\n\n\u003cul id=\"markdown-toc\"\u003e\n  \u003cli\u003e\u003ca href=\"#译者序\" id=\"markdown-toc-译者序\"\u003e译者序\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#摘要\" id=\"markdown-toc-摘要\"\u003e摘要\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#1-推理机制\" id=\"markdown-toc-1-推理机制\"\u003e1 推理机制\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#11-transformer逐-token-生成无法并行\" id=\"markdown-toc-11-transformer逐-token-生成无法并行\"\u003e1.1 transformer：逐 token 生成，无法并行\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#12-生成过程建模矩阵乘法\" id=\"markdown-toc-12-生成过程建模矩阵乘法\"\u003e1.2 生成过程建模：矩阵乘法\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#13-瓶颈分析\" id=\"markdown-toc-13-瓶颈分析\"\u003e1.3 瓶颈分析\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#131-典型算力-带宽比\" id=\"markdown-toc-131-典型算力-带宽比\"\u003e1.3.1 典型“算力-带宽”比\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#132-瓶颈访存带宽\" id=\"markdown-toc-132-瓶颈访存带宽\"\u003e1.3.2 瓶颈：访存带宽\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#2-以-mistral-7b-为例极限推理延迟的计算\" id=\"markdown-toc-2-以-mistral-7b-为例极限推理延迟的计算\"\u003e2 以 Mistral-7B 为例，极限推理延迟的计算\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#21-参数权重数量的组成计算\" id=\"markdown-toc-21-参数权重数量的组成计算\"\u003e2.1 参数（权重）数量的组成/计算\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#22-计算一个-token-所需加载的数据量\" id=\"markdown-toc-22-计算一个-token-所需加载的数据量\"\u003e2.2 计算一个 token 所需加载的数据量\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#221-总数据量\" id=\"markdown-toc-221-总数据量\"\u003e2.2.1 总数据量\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#222-kv-cache-部分的数据量\" id=\"markdown-toc-222-kv-cache-部分的数据量\"\u003e2.2.2 KV-cache 部分的数据量\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#23-以-rtx-4090-为例极限延迟计算\" id=\"markdown-toc-23-以-rtx-4090-为例极限延迟计算\"\u003e2.3 以 RTX 4090 为例，极限延迟计算\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#24-chatglm3-6bqwen-7b-实测推理延迟译注\" id=\"markdown-toc-24-chatglm3-6bqwen-7b-实测推理延迟译注\"\u003e2.4 ChatGLM3-6B/Qwen-7B 实测推理延迟（译注）\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#3-数学模型和理论极限的用途\" id=\"markdown-toc-3-数学模型和理论极限的用途\"\u003e3 数学模型和理论极限的用途\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#31-评估推理系统好坏\" id=\"markdown-toc-31-评估推理系统好坏\"\u003e3.1 评估推理系统好坏\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#32-指导量化\" id=\"markdown-toc-32-指导量化\"\u003e3.2 指导量化\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#33-指导优化方向\" id=\"markdown-toc-33-指导优化方向\"\u003e3.3 指导优化方向\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#331-批处理-batch-1---n瓶颈-访存带宽---算力\" id=\"markdown-toc-331-批处理-batch-1---n瓶颈-访存带宽---算力\"\u003e3.3.1 批处理 batch \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e1 -\u0026gt; N\u003c/code\u003e：瓶颈 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e访存带宽 -\u0026gt; 算力\u003c/code\u003e\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#332-批处理无法改善所需加载的-kv-cache-数据量\" id=\"markdown-toc-332-批处理无法改善所需加载的-kv-cache-数据量\"\u003e3.3.2 批处理无法改善所需加载的 KV-cache 数据量\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#34-硬件相对推理速度评估\" id=\"markdown-toc-34-硬件相对推理速度评估\"\u003e3.4 硬件相对推理速度评估\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#4-gqa-group-query-attention-的影响\" id=\"markdown-toc-4-gqa-group-query-attention-的影响\"\u003e4 GQA (group query attention) 的影响\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#41-gqa-为什么能减少带宽\" id=\"markdown-toc-41-gqa-为什么能减少带宽\"\u003e4.1 GQA 为什么能减少带宽\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#42-有无-gqa-的数据量对比\" id=\"markdown-toc-42-有无-gqa-的数据量对比\"\u003e4.2 有无 GQA 的数据量对比\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#43-多用户场景下-kv-cache-占用的显存规模\" id=\"markdown-toc-43-多用户场景下-kv-cache-占用的显存规模\"\u003e4.3 多用户场景下 KV-cache 占用的显存规模\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#44-gqa减小从-kv-cache-加载的数据量\" id=\"markdown-toc-44-gqa减小从-kv-cache-加载的数据量\"\u003e4.4 GQA：减小从 KV-cache 加载的数据量\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#45-gqa-的问题\" id=\"markdown-toc-45-gqa-的问题\"\u003e4.5 GQA 的问题\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#5-总结\" id=\"markdown-toc-5-总结\"\u003e5 总结\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003chr/\u003e\n\n\u003ch1 id=\"摘要\"\u003e摘要\u003c/h1\u003e\n\n\u003cp\u003e在开发 \u003ca href=\"https://github.com/zeux/calm\"\u003ecalm\u003c/a\u003e 的过程中，我们考虑的一个核心问题是：\n\u003cstrong\u003e\u003cmark\u003e推理的极限在哪儿\u003c/mark\u003e\u003c/strong\u003e？因为我们需要以此为准绳，去衡量真实推理系统的速度。\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003ecalm 是一个基于 CUDA、完全从头开始编写的轻量级 transformer-based language models \u003cstrong\u003e\u003cmark\u003e推理实现\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e本文试图讨论这个极限及其影响。\n如果对推导细节感兴趣，可参考\u003ca href=\"https://github.com/zeux/calm/blob/main/tools/sol.ipynb\"\u003e这个 python notebook\u003c/a\u003e。\u003c/p\u003e\n\n\u003ch1 id=\"1-推理机制\"\u003e1 推理机制\u003c/h1\u003e\n\n\u003ch2 id=\"11-transformer逐-token-生成无法并行\"\u003e1.1 transformer：逐 token 生成，无法并行\u003c/h2\u003e\n\n\u003cp\u003e当语言模型\u003cstrong\u003e\u003cmark\u003e生成\u003c/mark\u003e\u003c/strong\u003e文本时，它是逐个 \u003ca href=\"https://tiktokenizer.vercel.app/\"\u003etoken\u003c/a\u003e 进行的。\n可以把\u003cstrong\u003e\u003cmark\u003e语言模型\u003c/mark\u003e\u003c/strong\u003e（特别是 decoder-only text transformer，本文统称为 LLM）\n\u003cstrong\u003e\u003cmark\u003e看做是一个函数\u003c/mark\u003e\u003c/strong\u003e，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e输入：一个 token\u003c/li\u003e\n  \u003cli\u003e输出：一组概率，每个概率对应词汇表中一个 token。\u003c/li\u003e\n  \u003cli\u003e推理程序使用概率来指导\u003cstrong\u003e\u003cmark\u003e抽样\u003c/mark\u003e\u003c/strong\u003e，产生（从词汇表中选择）下一个 token 作为最终输出。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e\u003cstrong\u003e\u003cmark\u003e词汇表\u003c/mark\u003e\u003c/strong\u003e（vocabulary）：通常由单词、单词片段、中文汉字等组成（这些都称为 token）。\nvocabulary 长什么样，可以可以看一下 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003ebert-base-chinese\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 的词典\n\u003ca href=\"https://huggingface.co/google-bert/bert-base-chinese/blob/main/vocab.txt\"\u003evocab.txt\u003c/a\u003e。\n更多基础：\u003c/p\u003e\n\n  \u003col\u003e\n    \u003cli\u003e\u003ca href=\"/blog/gpt-as-a-finite-state-markov-chain-zh/\"\u003eGPT 是如何工作的：200 行 Python 代码实现一个极简 GPT（2023）\u003c/a\u003e。\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"/blog/transformers-from-scratch-zh/\"\u003eTransformer 是如何工作的：600 行 Python 代码实现 self-attention 和两类 Transformer（2019）\u003c/a\u003e\u003c/li\u003e\n  \u003c/ol\u003e\n\n  \u003cp\u003e译注。\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e文本生成过程就是不断重复以上过程。可以看出，在生成一个文本序列时，\u003cstrong\u003e\u003cmark\u003e没有并行性的可能性\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003especulative execution\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 尝试通过一个 less accurate predictor 来实现某种程度的并行，本文不讨论。\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch2 id=\"12-生成过程建模矩阵乘法\"\u003e1.2 生成过程建模：矩阵乘法\u003c/h2\u003e\n\n\u003cp\u003e广义上，当处理一个 token 时，模型执行两种类型的操作：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e矩阵-向量乘法\u003c/mark\u003e\u003c/strong\u003e：一个大矩阵（例如 8192x8192）乘以一个向量，得到另一个向量，\u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003e\u003cstrong\u003e\u003cmark\u003eattention 计算\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n    \u003cp\u003e在生成过程中，模型不仅可以看到当前 token 的状态，还可以看到序列中所有之前 token 的内部状态 ——\n 这些状态被存储在一个称为 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eKV-cache\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 的结构中，\n 它本质上是文本中每个之前位置的 \u003cstrong\u003e\u003cmark\u003ekey 向量和 value 向量的集合\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n    \u003cp\u003eattention 为当前 token 生成一个 query 向量，计算它与所有之前位置的 key 向量之间的点积，\n 然后归一化得到的一组标量，并通过对所有之前的 value 向量进行加权求和来计算一个 value 向量，使用点积得到最终得分。\u003c/p\u003e\n  \u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eThis description omits multi-head attention and the details of “normalization”\n(softmax), but neither are critical for understanding the inference\nperformance.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch2 id=\"13-瓶颈分析\"\u003e1.3 瓶颈分析\u003c/h2\u003e\n\n\u003cp\u003e以上两步计算有一个重要的共同特征：从矩阵或 KV-cache 读取的每个元素，只需要进行\u003cstrong\u003e\u003cmark\u003e非常少量的浮点运算\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e矩阵-向量乘法对每个矩阵元素执行一次\u003cstrong\u003e\u003cmark\u003e乘加运算\u003c/mark\u003e\u003c/strong\u003e（2 FLOPs）；\u003c/li\u003e\n  \u003cli\u003eattention 对每个 key 执行一次\u003cstrong\u003e\u003cmark\u003e乘加\u003c/mark\u003e\u003c/strong\u003e，对每个 value 执行一次\u003cstrong\u003e\u003cmark\u003e乘加\u003c/mark\u003e\u003c/strong\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 id=\"131-典型算力-带宽比\"\u003e1.3.1 典型“算力-带宽”比\u003c/h3\u003e\n\n\u003cp\u003e现代 CPU/GPU 的 \u003cstrong\u003e\u003cmark\u003eALU 操作\u003c/mark\u003e\u003c/strong\u003e（乘法、加法）\u003cstrong\u003e\u003cmark\u003e内存 IO\u003c/mark\u003e\u003c/strong\u003e 速度要\u003cstrong\u003e\u003cmark\u003e快得多\u003c/mark\u003e\u003c/strong\u003e。例如：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eAMD Ryzen 7950X：\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e67 GB/s\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 内存带宽和 2735 GFLOPS，\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eFlop:byte = 40:1\u003c/code\u003e\u003c/li\u003e\n  \u003cli\u003eNVIDIA GeForce RTX 4090：\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e1008 GB/s\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 显存带宽和 83 TFLOPS，\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eFlop:byte = 82:1\u003c/code\u003e\u003c/li\u003e\n  \u003cli\u003eNVIDIA H100 SXM：\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e3350 GB/s\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 内存带宽和 67 TFLOPS，\n对于矩阵乘法，tensor core 提供 ~494 TFLOPS 稠密算力，\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eFlop:byte = 147:1\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e对于 FP16/FP8 等精度较低的浮点数，比率更夸张：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eH100 TensorCore 对于 dense FP8 矩阵的理论吞吐量为 1979 TFLOPS，\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eFLOP:byte = 590:1\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e在这些场景中，无论是否使用 TensorCore 或使用什么浮点格式，ALU 都非常充足。\u003c/p\u003e\n\n\u003ch3 id=\"132-瓶颈访存带宽\"\u003e1.3.2 瓶颈：访存带宽\u003c/h3\u003e\n\n\u003cp\u003e因此，transformer 这种\u003cstrong\u003e\u003cmark\u003e只需要对每个元素执行两次操作\u003c/mark\u003e\u003c/strong\u003e的场景，必定受到访存带宽的限制。\n所以，基于下面几个因素，\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e模型配置（参数多少）\u003c/li\u003e\n  \u003cli\u003eKV-cache 大小\u003c/li\u003e\n  \u003cli\u003e访存带宽\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e我们就能估计推理过程的最短耗时。\n下面以 \u003ca href=\"https://mistral.ai/news/announcing-mistral-7b/\"\u003eMistral 7B\u003c/a\u003e 为例来具体看看。\u003c/p\u003e\n\n\u003ch1 id=\"2-以-mistral-7b-为例极限推理延迟的计算\"\u003e2 以 Mistral-7B 为例，极限推理延迟的计算\u003c/h1\u003e\n\n\u003ch2 id=\"21-参数权重数量的组成计算\"\u003e2.1 参数（权重）数量的组成/计算\u003c/h2\u003e\n\n\u003cp\u003eMistral-7B 有 72 亿参数（所有矩阵元素的总数是 72 亿个）。\n参数的组成如下：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e4096 * 32000 = 131M\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 用于 embedding 矩阵；\n    \u003cul\u003e\n      \u003cli\u003e4096: hidden size (tokens per hidden-vector)\u003c/li\u003e\n      \u003cli\u003e32000: vocabulary size\u003c/li\u003e\n    \u003c/ul\u003e\n\n    \u003cp\u003e矩阵-向量乘法中不会使用这整个大矩阵，每个 token 只读取这个矩阵中的一行，因此数据量相对很小，后面的带宽计算中将忽略这个；\u003c/p\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e32 * (4096 * (128 * 32 + 128 * 8 * 2) + 4096 * 128 * 32) = 1342M\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 用于计算与 attention 相关的向量；\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e32 * (4096 * 14336 * 3) = 5637M\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 用于通过 feed-forward 转换 hidden states；\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e4096 * 32000 = 131M\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 用于将 hidden states 转换为 token 概率；这与 embedding 矩阵不同，会用于矩阵乘法。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e以上加起来，大约有 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e7111M\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e (\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e~7B\u003c/code\u003e) “活跃”参数用于矩阵乘法。\u003c/p\u003e\n\n\u003ch2 id=\"22-计算一个-token-所需加载的数据量\"\u003e2.2 计算一个 token 所需加载的数据量\u003c/h2\u003e\n\n\u003ch3 id=\"221-总数据量\"\u003e2.2.1 总数据量\u003c/h3\u003e\n\n\u003cp\u003e如果模型使用 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eFP16\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 作为矩阵元素的类型，\n\u003cstrong\u003e\u003cmark\u003e那每生成一个 token，需要加载到 ALU 上的数据量\u003c/mark\u003e\u003c/strong\u003e：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003ccode\u003e7111M params * 2Byte/param = \u003cmark\u003e~14.2 GB\u003c/mark\u003e\u003c/code\u003e\u003c/p\u003e\n\n\u003cp\u003e虽然计算下一个 token 时每个矩阵都可以复用，但硬件缓存的大小通常只有几十 MB，\n矩阵无法放入缓存中，因此我们可以断定，这个\u003cstrong\u003e\u003cmark\u003e生成（推理）过程的速度不会快于显存带宽\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003eattention 计算需要读取当前 token 及前面上下文中所有 tokens 对应的 KV-cache，\n所以\u003cstrong\u003e\u003cmark\u003e读取的数据量\u003c/mark\u003e\u003c/strong\u003e取决于\u003cstrong\u003e\u003cmark\u003e生成新 token 时模型看到多少前面的 token\u003c/mark\u003e\u003c/strong\u003e，\n这包括\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e系统提示词（通常对用户隐藏）\u003c/li\u003e\n  \u003cli\u003e用户提示词\u003c/li\u003e\n  \u003cli\u003e前面的模型输出\u003c/li\u003e\n  \u003cli\u003e可能还包括长聊天会话中多个用户的提示词。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch3 id=\"222-kv-cache-部分的数据量\"\u003e2.2.2 KV-cache 部分的数据量\u003c/h3\u003e\n\n\u003cp\u003e对于 Mistral，KV-cache\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e为每层的每个 key 存储 8 个 128 元素向量，\u003c/li\u003e\n  \u003cli\u003e为每个层的每个 value 存储 8 个 128 元素向量，\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e这加起来，每个 token 对应 32 * 128 * 8 * 2 = 65K 个元素；\n如果 KV-cache 使用 FP16，那么对于 token number P，我们需要读取 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eP * 130 KB\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 的数据。\n例如， token number 1000 将需要从 KV-cache 读取 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e130MB\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 的数据。\n跟 14.2GB 这个总数据量相比，这 130MB 可以忽略不计了。\u003c/p\u003e\n\n\u003ch2 id=\"23-以-rtx-4090-为例极限延迟计算\"\u003e2.3 以 RTX 4090 为例，极限延迟计算\u003c/h2\u003e\n\n\u003cp\u003e根据以上数字，现在可以很容易地计算出推理所需的最小时间。\u003c/p\u003e\n\n\u003cp\u003e例如，在 NVIDIA \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eRTX 4090\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e（1008 GB/s）上，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e14.2GB (fp16) 需要 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e~14.1ms\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 读取，因此可以预期对于位置靠前的 token，\n每个 token 大约需要 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e14.1ms\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e（KV-cache 影响可以忽略不计）。\u003c/li\u003e\n  \u003cli\u003e如果使用 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e8bit\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 权重，需要读取 7.1GB，这需要大约 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e7.0ms\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e这些都是理论下限，代表了生成每个 token 的最小可能时间。\u003c/p\u003e\n\n\u003ch2 id=\"24-chatglm3-6bqwen-7b-实测推理延迟译注\"\u003e2.4 ChatGLM3-6B/Qwen-7B 实测推理延迟（译注）\u003c/h2\u003e\n\n\u003cp\u003e简单的单卡推理测试，16bit 权重，平均延迟，仅供参考：\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth style=\"text-align: left\"\u003eLLM\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eRTX 4090 24GB\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e (2022)\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eA100 80GB\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e (2020)\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eV100 32GB\u003c/code\u003e (2017)\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eChatGLM3-6B\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e16ms/token\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e18ms/token\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e32ms/token\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eQwen-7B\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e19ms/token\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e29ms/token\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e41ms/token\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp\u003e可以看到，单就\u003cstrong\u003e\u003cmark\u003e推理速度\u003c/mark\u003e\u003c/strong\u003e来说，只要模型能塞进去（\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e\u0026lt; 24GB\u003c/code\u003e），4090 与 A100 相当甚至更快，比 V100 快一倍。\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e说明：以上测的是 4090，不带 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eD\u003c/code\u003e（\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e4090D\u003c/code\u003e）。\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch1 id=\"3-数学模型和理论极限的用途\"\u003e3 数学模型和理论极限的用途\u003c/h1\u003e\n\n\u003cp\u003e以上根据数学建模和计算得出了一些理论极限数字，接下来看看这些理论极限有什么用。\u003c/p\u003e\n\n\u003ch2 id=\"31-评估推理系统好坏\"\u003e3.1 评估推理系统好坏\u003c/h2\u003e\n\n\u003cp\u003e要接近理论极限，需要一个高质量的软件实现，以及能够达到峰值带宽的硬件。\n因此如果你的软件+硬件离理论最优很远，那肯定就有问题：可能在软件方面，也可能在硬件方面。\u003c/p\u003e\n\n\u003cp\u003e例如，在 RTX 4090 上 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ecalm\u003c/code\u003e 使用 16 位权重时达到 ~15.4 ms/tok，使用 8 位权重时达到 ~7.8 ms/tok，\n达到了理论极限的 90%。\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eClose, but not quite there - 100% bandwidth utilization is unfortunately very hard to get close to on NVidia GPUs for this workload. Larger GPUs like H100 are even more difficult to fully saturate; on Mixtral - this is a different architecture but it obeys the same tradeoffs for single sequence generation if you only count active parameters - calm achieves ~80% of theoretically possible performance, although large denser models like Llama 70B can get closer to the peak.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e在 Apple M2 Air 上使用 CPU 推理时，\u003ccode class=\"language-plaintext highlighter-rouge\"\u003ecalm\u003c/code\u003e 和 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ellama.cpp\u003c/code\u003e 只达到理论 100 GB/s 带宽的 ~65%，\n然后带宽就上不去了，这暗示需要尝试 Apple iGPU 了。\u003c/p\u003e\n\n\u003ch2 id=\"32-指导量化\"\u003e3.2 指导量化\u003c/h2\u003e\n\n\u003cp\u003e带宽与每个权重使用的 bit 数成正比；这意味着\u003cstrong\u003e\u003cmark\u003e更小的权重格式（量化）能实现更低的延迟\u003c/mark\u003e\u003c/strong\u003e。\n例如，在 RTX 4090 上 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ellama.cpp\u003c/code\u003e 使用 Mistral 7B\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e16  bit 权重：~17.1 ms/tok（82% 的峰值）\u003c/li\u003e\n  \u003cli\u003e8.5 bit 权重：~10.3ms/tok （71% 的峰值）\u003c/li\u003e\n  \u003cli\u003e4.5 bit 权重：~6.7ms/tok （58% 的峰值）\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e因此对于低延迟场景，可以考虑低精度量化。\u003c/p\u003e\n\n\u003ch2 id=\"33-指导优化方向\"\u003e3.3 指导优化方向\u003c/h2\u003e\n\n\u003cp\u003e除了为推理延迟提供下限外，上述建模还表明：\u003cstrong\u003e\u003cmark\u003e推理过程并未充分利用算力\u003c/mark\u003e\u003c/strong\u003e（ALU）。\n要解决这个问题，需要重新平衡 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eFLOP:byte\u003c/code\u003e 比例，\n\u003ca href=\"https://medium.com/@TitanML/in-the-fast-lane-speculative-decoding-10x-larger-model-no-extra-cost-f33ea39d065a\"\u003especulative decoding\u003c/a\u003e 等技术试图部分解决这个问题。\u003c/p\u003e\n\n\u003ch3 id=\"331-批处理-batch-1---n瓶颈-访存带宽---算力\"\u003e3.3.1 批处理 batch \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e1 -\u0026gt; N\u003c/code\u003e：瓶颈 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e访存带宽 -\u0026gt; 算力\u003c/code\u003e\u003c/h3\u003e\n\n\u003cp\u003e这里再另一种场景：\u003cstrong\u003e\u003cmark\u003e多用户场景\u003c/mark\u003e\u003c/strong\u003e。注意到，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e当多个用户请求同时处理时，我们用相同的矩阵同时执行多个矩阵-向量乘法，\n这里可以将\u003cstrong\u003e\u003cmark\u003e多个矩阵-向量乘法\u003c/mark\u003e\u003c/strong\u003e变成\u003cstrong\u003e\u003cmark\u003e一个矩阵-矩阵乘法\u003c/mark\u003e\u003c/strong\u003e。\u003c/li\u003e\n  \u003cli\u003e对于足够大的矩阵来说，只要矩阵-矩阵乘法实现得当，速度就比访存 IO 快，\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e因此这种场景下，瓶颈不再是访存 IO，而是算力（ALU）。这就是为什么这种 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eALU:byte\u003c/code\u003e\n不平衡\u003cstrong\u003e\u003cmark\u003e对于生产推理系统不是关键问题\u003c/mark\u003e\u003c/strong\u003e ——\n当使用 ChatGPT 时，你的请求与同一 GPU 上许多其他用户的请求并发评估，GPU 显存带宽利用更加高效。\u003c/p\u003e\n\n\u003ch3 id=\"332-批处理无法改善所需加载的-kv-cache-数据量\"\u003e3.3.2 批处理无法改善所需加载的 KV-cache 数据量\u003c/h3\u003e\n\n\u003cp\u003e批处理通常不会减轻 KV-cache 带宽（除非多个请求共享非常大的前缀），因为 KV-cache\n大小和带宽\u003cstrong\u003e\u003cmark\u003e随请求数量的增加而增加\u003c/mark\u003e\u003c/strong\u003e，而不像权重矩阵保持不变。\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e像 Mistral 这样的混合专家模型（MoE）scaling 特性稍有不同：batching initially\nonly increases the bandwidth required, but once the expert utilization\nbecomes significant the inference becomes increasingly ALU bound.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch2 id=\"34-硬件相对推理速度评估\"\u003e3.4 硬件相对推理速度评估\u003c/h2\u003e\n\n\u003cp\u003e带宽是评估推理性能的关键指标，对于模型变化/设备类型或架构来说是一个恒定的，\n因此即使无法使用 batch processing，也可以用它来评估你用的硬件。\u003c/p\u003e\n\n\u003cp\u003e例如，NVIDIA RTX 4080 有 716 GB/s 带宽，所以可以预期它的推理速度是 RTX 4090 的\n~70% —— 注意，游戏、光线追踪或推理其他类型的神经网络等方面，相对性能可能与此不同！\u003c/p\u003e\n\n\u003ch1 id=\"4-gqa-group-query-attention-的影响\"\u003e4 GQA (group query attention) 的影响\u003c/h1\u003e\n\n\u003cp\u003eMistral-7B 是一个非常平衡的模型；在上面的所有计算中，\u003cstrong\u003e\u003cmark\u003e几乎都能忽略 KV-cache 部分的 IO 开销\u003c/mark\u003e\u003c/strong\u003e。\n这背后的原因：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e较短的上下文（Mistral-7B 使用 windowed attention，限制 4096 token 的窗口），\u003c/li\u003e\n  \u003cli\u003e使用了 GQA，这个是更重要的原因。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e\u003ca href=\"/blog/llama2-paper-zh/\"\u003eLLaMA 2：开放基础和微调聊天模型（Meta/Facebook，2023）\u003c/a\u003e 也使用了 GQA。\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch2 id=\"41-gqa-为什么能减少带宽\"\u003e4.1 GQA 为什么能减少带宽\u003c/h2\u003e\n\n\u003cp\u003e在 GQA 中（with a 4x ratio），为了得到 attention 的 4 个点积，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e不是使用 4 个 query 向量并分别与 4 个相应的 key 向量计算点积，\u003c/li\u003e\n  \u003cli\u003e而是只取一个 key 向量，然后执行 4 个点积。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e这能够减少 KV-cache 的大小和所需带宽，也在某种程度上重新平衡了 ALU:bandwidth 比例。\u003c/p\u003e\n\n\u003ch2 id=\"42-有无-gqa-的数据量对比\"\u003e4.2 有无 GQA 的数据量对比\u003c/h2\u003e\n\n\u003cp\u003e这对于 KV-cache 内存大小也很关键，不过，这可能\u003cstrong\u003e\u003cmark\u003e对短上下文模型不太明显\u003c/mark\u003e\u003c/strong\u003e：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e4096 token 上下文的 Mistral 需要 0.5GiB，\u003c/li\u003e\n  \u003cli\u003e没有 GQA 的可比模型（如 Llama 7B）“只需要”2 GiB。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e让我们看看一个最近不使用 GQA 的模型，Cohere 的 \u003ca href=\"https://txt.cohere.com/command-r/\"\u003eCommand-R\u003c/a\u003e。\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eCommand-R has a large vocab (256K) and large hidden state (8192) so it spends\na whopping 2B parameters on embeddings, but it reuses the same matrix for\nembedding and classification so we don’t need to exclude this from the\ninference bandwidth calculation.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e模型本身有大约 35b 参数，所以以 16 位/权重计算，我们在推理期间需要为每个 token 读取 70 GB 的权重。\n对于每个 token ，它需要在 KV-cache 中存储 40 * 128 * 64 * 2 = 655K 元素，以 16 位/元素计算是每个 token  1.3 MB。\u003c/p\u003e\n\n\u003cp\u003e因此，一个 4096 token 的上下文将需要大约 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e5.3GB\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e；\n与 ~70 GB 的权重相比，这已经相当显著了。然而，如果考虑到 Cohere 的模型宣传有 200K token 上下文窗口 ——\n计算最后一个 token 需要读取 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e260 GB\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e（还需要 260GB 的显存来存储它）！\u003c/p\u003e\n\n\u003ch2 id=\"43-多用户场景下-kv-cache-占用的显存规模\"\u003e4.3 多用户场景下 KV-cache 占用的显存规模\u003c/h2\u003e\n\n\u003cp\u003e这么大的模型，典型的\u003cstrong\u003e\u003cmark\u003e生产环境配置\u003c/mark\u003e\u003c/strong\u003e（单用户），\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eweights\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 通常使用 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e4bit\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 量化（通常的实现占用 ~4.5bit/权重）\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eKV-cache\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 可能会使用 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e8bit\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e（FP8）值。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e如果我们“保守地”假设上下文为 100K，则\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e模型权重占 ~19.7GB\u003c/li\u003e\n  \u003cli\u003eKV-cache 占 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e~65GB\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e计算到最后一个 token 时，我们需要从内存中读取这么大的数据。\n可以看到，突然之间，\u003cstrong\u003e\u003cmark\u003eattention 计算部分\u003c/mark\u003e\u003c/strong\u003e的数据量（最终转变成耗时）从微不足道变成了占 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e~75%\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e！\u003c/p\u003e\n\n\u003cp\u003e虽然 100K 上下文可能看起来有点极端，但在短上下文+多用户场景中，情况也是类似的：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e批处理优化将多次矩阵-向量乘法变成了一次矩阵-矩阵乘法（为一批用户请求读取一次模型权重），瓶颈来到算力（ALU），\u003c/li\u003e\n  \u003cli\u003e但\u003cstrong\u003e\u003cmark\u003e每个用户请求通常都有自己的 KV-cache\u003c/mark\u003e\u003c/strong\u003e，\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e因此最终的 attention 仍然受访存带宽限制，并且需要大量内存/显存才能将所有用户请求放到单个节点！\u003c/p\u003e\n\n\u003ch2 id=\"44-gqa减小从-kv-cache-加载的数据量\"\u003e4.4 GQA：减小从 KV-cache 加载的数据量\u003c/h2\u003e\n\n\u003cp\u003e如果模型使用 4x GQA，KV-cache 的大小和所需带宽将会变成原来的 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e1/4\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e对于 100k+ token 的上下文场景，虽然 KV-cache 的开销仍然很大（\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e65GB -\u0026gt; 16GB+\u003c/code\u003e），但已经进入实用范围。\u003c/p\u003e\n\n\u003ch2 id=\"45-gqa-的问题\"\u003e4.5 GQA 的问题\u003c/h2\u003e\n\n\u003cp\u003e对于 Cohere 的目标使用场景，引入 GQA 可能会导致\u003cstrong\u003e\u003cmark\u003e模型质量有下降\u003c/mark\u003e\u003c/strong\u003e，具体得看他们的技术报告。\u003c/p\u003e\n\n\u003cp\u003e但是，纯粹从成本/性能角度来看，\u003cstrong\u003e\u003cmark\u003e每个基于 transformer 的 LLM 都需要评估是否能引入 GQA\u003c/mark\u003e\u003c/strong\u003e，因为收益太大了。\u003c/p\u003e\n\n\u003ch1 id=\"5-总结\"\u003e5 总结\u003c/h1\u003e\n\n\u003cp\u003e对于大模型推理场景，计算和访存的次数是已知的，因此可以进行数学建模，计算理论极限。\n这非常有用，不仅可以用来\u003cstrong\u003e\u003cmark\u003e验证推理系统的性能\u003c/mark\u003e\u003c/strong\u003e，\n而且能\u003cstrong\u003e\u003cmark\u003e预测架构变化带来的影响\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003chr/\u003e\n\n\u003cp\u003e\u003ca href=\"https://notbyai.fyi\"\u003e\u003cimg src=\"/assets/img/Written-By-Human-Not-By-AI-Badge-white.svg\" alt=\"Written by Human, Not by AI\"/\u003e\u003c/a\u003e\n\u003ca href=\"https://notbyai.fyi\"\u003e\u003cimg src=\"/assets/img/Written-By-Human-Not-By-AI-Badge-black.svg\" alt=\"Written by Human, Not by AI\"/\u003e\u003c/a\u003e\u003c/p\u003e\n\n\n  \u003c!-- POST NAVIGATION --\u003e\n  \u003cdiv class=\"postNav clearfix\"\u003e\n     \n      \u003ca class=\"prev\" href=\"/blog/instructgpt-paper-zh/\"\u003e\u003cspan\u003e« [译][论文] InstructGPT：基于人类反馈训练语言模型遵从指令的能力（OpenAI，2022）\u003c/span\u003e\n      \n    \u003c/a\u003e\n      \n      \n      \u003ca class=\"next\" href=\"/blog/meta-ai-infra-zh/\"\u003e\u003cspan\u003e[译] Meta/Facebook 超大规模 AI/GPU 基础设施设计（2024） »\u003c/span\u003e\n       \n      \u003c/a\u003e\n     \n  \u003c/div\u003e\n\u003c/div\u003e",
  "Date": "2024-04-06T00:00:00Z",
  "Author": "Arthur Chiao"
}