{
  "Source": "arthurchiao.art",
  "Title": "Trip.com: Stepping into Cloud Native Networking Era with Cilium+BGP",
  "Link": "https://arthurchiao.art/blog/trip-stepping-into-cloud-native-networking-era/",
  "Content": "\u003cdiv class=\"post\"\u003e\n  \n  \u003ch1 class=\"postTitle\"\u003eTrip.com: Stepping into Cloud Native Networking Era with Cilium+BGP\u003c/h1\u003e\n  \u003cp class=\"meta\"\u003ePublished at 2020-11-04 | Last Update 2020-11-04\u003c/p\u003e\n  \n  \u003cblockquote\u003e\n  \u003cp\u003eThis post also provides a \u003ca href=\"/blog/trip-stepping-into-cloud-native-networking-era-zh/\"\u003eChinese version\u003c/a\u003e.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThis post serves as a successor to our previous post\n\u003ca href=\"/blog/trip-first-step-towards-cloud-native-networking/\"\u003e\u003cstrong\u003e\u003cem\u003eTrip.com: First Step towards Cloud Native Networking\u003c/em\u003e\u003c/strong\u003e\u003c/a\u003e.\nWe will update some of our recent progresses on Cilium-based networking \u0026amp; security.\u003c/p\u003e\n\n\u003chr/\u003e\n\n\u003cul id=\"markdown-toc\"\u003e\n  \u003cli\u003e\u003ca href=\"#1-networking-tripcom-a-quick-revisit\" id=\"markdown-toc-1-networking-tripcom-a-quick-revisit\"\u003e1 Networking @Trip.com: a quick revisit\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#2-cloud-native-networking\" id=\"markdown-toc-2-cloud-native-networking\"\u003e2 Cloud native networking\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#21-bgp-peering-model\" id=\"markdown-toc-21-bgp-peering-model\"\u003e2.1 BGP peering model\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#22-typical-traffic-path-pod-to-service\" id=\"markdown-toc-22-typical-traffic-path-pod-to-service\"\u003e2.2 Typical traffic path: pod-to-service\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#23-l4l7-solutions-at-cluster-edge\" id=\"markdown-toc-23-l4l7-solutions-at-cluster-edge\"\u003e2.3 L4/L7 solutions at cluster edge\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#3-cloud-native-security\" id=\"markdown-toc-3-cloud-native-security\"\u003e3 Cloud native security\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#31-cilium-network-policy-cnp\" id=\"markdown-toc-31-cilium-network-policy-cnp\"\u003e3.1 Cilium Network Policy (CNP)\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#32-carry-out-challenges\" id=\"markdown-toc-32-carry-out-challenges\"\u003e3.2 Carry-out challenges\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#multi-cluster\" id=\"markdown-toc-multi-cluster\"\u003eMulti-cluster\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#hybrid-infrastructures\" id=\"markdown-toc-hybrid-infrastructures\"\u003eHybrid infrastructures\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#33-security-solution-design\" id=\"markdown-toc-33-security-solution-design\"\u003e3.3 Security solution design\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#331-group-cilium-clusters-into-clustermesh\" id=\"markdown-toc-331-group-cilium-clusters-into-clustermesh\"\u003e3.3.1 Group Cilium clusters into ClusterMesh\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#332-extend-cilium-to-perceive-non-cilium-endpoints\" id=\"markdown-toc-332-extend-cilium-to-perceive-non-cilium-endpoints\"\u003e3.3.2 Extend Cilium to perceive non-Cilium endpoints\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#4-conclusion\" id=\"markdown-toc-4-conclusion\"\u003e4 Conclusion\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#references\" id=\"markdown-toc-references\"\u003eReferences\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003chr/\u003e\n\n\u003ch1 id=\"1-networking-tripcom-a-quick-revisit\"\u003e1 Networking @Trip.com: a quick revisit\u003c/h1\u003e\n\n\u003cp\u003eFor historical reasons, Neutron+OVS has been our networking stack in the past\nyears - even for our Kubernetes clusters. As cloud native era approaches,\nthis solution gets increasingly cumbersome, especially its inherent\nhardware and software bottlenecks in the face of the sheer scale of\ncontainers today [1].\u003c/p\u003e\n\n\u003cp\u003eSo, to address the bottlenecks, as well as to meet the ever-increasing new\nnetworking requirements, we devoted lots of efforts to investigating and\nevaluating various kinds of new generation networking solutions, and in the end\nCilium won our favor.\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/trip-first-step-towards-cloud-native-networking/network-evolution-2.png\" width=\"60%\" height=\"60%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 1-1. Networking solutions over the past years [2]\u003c/p\u003e\n\n\u003cp\u003eIn combination with BGP [3], Cilium landed our production environment at the end\nof 2019. Since then, we have been migrating our existing Pods from legacy\nnetwork to Cilium.\u003c/p\u003e\n\n\u003ch1 id=\"2-cloud-native-networking\"\u003e2 Cloud native networking\u003c/h1\u003e\n\n\u003cp\u003eAs one of the early practitioners, we’ve made certain customizations to smoothly\nrollout Cilium into our existing infrastructure. Below lists some of them [2]:\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eDeploy with \u003ccode class=\"language-plaintext highlighter-rouge\"\u003edocker-compsoe + salt\u003c/code\u003e instead of \u003ccode class=\"language-plaintext highlighter-rouge\"\u003edaemonset+configmap\u003c/code\u003e.\u003c/li\u003e\n  \u003cli\u003eRun BGP with \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eBIRD\u003c/code\u003e instead of \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ekube-router\u003c/code\u003e.\u003c/li\u003e\n  \u003cli\u003eFixed IP address patch for StatefulSet (working together with sticky scheduling).\u003c/li\u003e\n  \u003cli\u003eCustom monitoring and alerting.\u003c/li\u003e\n  \u003cli\u003eMany configuration customizations.\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eWe have detailed most of these changes and customizations in [2], refer to the\npost if you are interested.\u003c/p\u003e\n\n\u003cp\u003eIn the next, we’d like to elaborate on some topics that are not\ncovered much before.\u003c/p\u003e\n\n\u003ch2 id=\"21-bgp-peering-model\"\u003e2.1 BGP peering model\u003c/h2\u003e\n\n\u003cp\u003eWith Cilium+BIRD, networking is split into two complementary parts,\nwith the host as boundary, as shown in Fig 2-1:\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/trip-first-step-towards-cloud-native-networking/new-solution-topo.png\" width=\"70%\" height=\"70%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 2-1. High level topology of the Cilium+BGP solution [2]\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\u003cstrong\u003eInner-host networking\u003c/strong\u003e: handled by \u003cstrong\u003eCilium\u003c/strong\u003e (and the kernel stack),\nwhich is responsible for\n    \u003cul\u003e\n      \u003cli\u003eSetting up and tearing down virtual networks for Pods,\u003c/li\u003e\n      \u003cli\u003eGenerating, compiling and loading eBPF for Pods,\u003c/li\u003e\n      \u003cli\u003eHandling inter-Pod communications where the two sides are on the same host.\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eCross-host networking\u003c/strong\u003e: accomplished via \u003cstrong\u003eBGP\u003c/strong\u003e (and kernel routing):\n    \u003cul\u003e\n      \u003cli\u003eExchanging routes (PodCIDRs) with underlying data center network with BIRD.\u003c/li\u003e\n      \u003cli\u003eRouting Pod traffic that destinated for endpoints on other hosts.\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eAnd regarding to the second part - \u003cstrong\u003ecross-host networking with BGP\u003c/strong\u003e - a BGP\npeering model is needed, which solves questions such as,\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eWhat’s responsibilities the BGP agents take? As full functionality agents, or\njust simple BGP speakers?\u003c/li\u003e\n  \u003cli\u003eWhich part of the data center network will establish connections with BGP agents?\u003c/li\u003e\n  \u003cli\u003eWhich protocol is used for exchanging routes, iBGP or eBGP?\u003c/li\u003e\n  \u003cli\u003eWhat’s your ASN scheme?\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eRegarding to specific requirements, it may end up with a really complex model.\nBut we made a simple one that fitted well into our capabilities and business\nneeds, described as below:\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eEach node runs a Cilium agent and a BIRD daemon,\n    \u003col\u003e\n      \u003cli\u003eAllocate a \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e/25\u003c/code\u003e or \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e/24\u003c/code\u003e PodCIDR when the node turns up.\u003c/li\u003e\n      \u003cli\u003eBIRD initiates 2 BGP connections with neighbors in the data center network.\u003c/li\u003e\n      \u003cli\u003e\u003cstrong\u003eBIRD anounces the PodCIDR to neighbors\u003c/strong\u003e, but \u003cstrong\u003eaccepts nothing\u003c/strong\u003e from the latter.\u003c/li\u003e\n    \u003c/ol\u003e\n  \u003c/li\u003e\n  \u003cli\u003eData center network accepts only \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e/25\u003c/code\u003e or \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e/24\u003c/code\u003e BGP announcements from nodes, but\ndoes not announce any routes to them.\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eThis scheme is simple in that,\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eData center network learns all nodes’ PodCIDRs via BGP, which made the \u003cstrong\u003ePod\ntraffic routable within the entire data center\u003c/strong\u003e.\u003c/li\u003e\n  \u003cli\u003eNodes learn nothing from data center network (and other nodes), which \u003cstrong\u003ekeeps\nthe kernel routing table at a constant size\u003c/strong\u003e, suffering no performance issues.\nAll traffic destinated for Pods on other hosts just goes through node’s default route.\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/trip-stepping-into-cloud-native-networking-era/bgp-peering.png\" width=\"40%\" height=\"40%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 2-2. BGP peering model in 3-tier network topology\u003c/p\u003e\n\n\u003cp\u003eOn choosing BGP protocols as well as establishing BGP connections, it depends on\ndifferent hardware network topologies,\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eFor access-aggregation-core topology, as shown in Fig 2-2,\n    \u003cul\u003e\n      \u003cli\u003eConnections established between nodes and \u003cstrong\u003eCore switches\u003c/strong\u003e.\u003c/li\u003e\n      \u003cli\u003eUse iBGP between them.\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003eFor Spine-Leaf topology\n    \u003cul\u003e\n      \u003cli\u003eConnections established between nodes and their \u003cstrong\u003eadjacent leaf swithes\u003c/strong\u003e.\u003c/li\u003e\n      \u003cli\u003eUse eBGP between them.\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eWe have summarized our practices as a getting started guide, see\n\u003ca href=\"https://docs.cilium.io/en/v1.8/gettingstarted/bird/\"\u003eUsing BIRD to run BGP\u003c/a\u003e [3].\u003c/p\u003e\n\n\u003ch2 id=\"22-typical-traffic-path-pod-to-service\"\u003e2.2 Typical traffic path: pod-to-service\u003c/h2\u003e\n\n\u003cp\u003eAs an example, let’s see a typical \u003cstrong\u003etraffic path\u003c/strong\u003e within this networking\nsolution: \u003cstrong\u003eaccesing Service from a Pod\u003c/strong\u003e, with the backend located on another\nnode, as shown below,\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/trip-stepping-into-cloud-native-networking-era/pod-to-pod-path.png\" width=\"100%\" height=\"100%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 2-3. Traffic path: accessing Service from a Pod [4] \u003c/p\u003e\n\n\u003cp\u003eMajor steps as numbered in the picture:\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eAccess a Service (\u003ccode class=\"language-plaintext highlighter-rouge\"\u003ecurl \u0026lt;ServiceIP\u0026gt;:\u0026lt;port\u0026gt;\u003c/code\u003e) from Pod1 at Node1.\u003c/li\u003e\n  \u003cli\u003eClient side \u003cstrong\u003eservice handling\u003c/strong\u003e via eBPF: select a Service\nbackend, then \u003cstrong\u003eperform DNAT\u003c/strong\u003e, replacing the ServiceIP with Pod4IP in the\npacket’s \u003ccode class=\"language-plaintext highlighter-rouge\"\u003edst_ip\u003c/code\u003e field.\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eKernel routing\u003c/strong\u003e: look up kernel routing table with packet’s destination\nIP; match default route, decide that this packet should be sent out via host bond/NIC.\u003c/li\u003e\n  \u003cli\u003ePacket arrived bond/NIC: send out to Node1’s gateway, which locates at data\ncenter network.\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eRouting within data center network\u003c/strong\u003e. As data center network has already\nlearned PodCIDRs from Node1 and Node2 via BGP before this, it could now\ndetermine by destination IP that this packet should be sent to Node2.\u003c/li\u003e\n  \u003cli\u003eTraffic arrived Node2’s NIC/bond: handled by another piece of eBPF code,\nwhich \u003cstrong\u003eextracts packet header\u003c/strong\u003e, looks for a Pod-specific eBPF code, and tail\ncall to it.\u003c/li\u003e\n  \u003cli\u003ePerform \u003cstrong\u003eingress policy enforcement\u003c/strong\u003e for this packet. If allowed, deliver\nit to Pod4.\u003c/li\u003e\n  \u003cli\u003ePacket arrived Pod4.\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eWe have a dedicated post for illustrating this process and the code\nimplementation at each stage, see [4] if you’re interested.\u003c/p\u003e\n\n\u003ch2 id=\"23-l4l7-solutions-at-cluster-edge\"\u003e2.3 L4/L7 solutions at cluster edge\u003c/h2\u003e\n\n\u003cp\u003eBy its design, \u003cstrong\u003eServiceIP is meant to be accessed only within each Kubernetes\ncluster\u003c/strong\u003e, what if I’d like to \u003cstrong\u003eaccess a Service from outside of the cluster\u003c/strong\u003e?\nFor example, from a bare metal cluster, an OpenStack cluster, or another\nKubernetes cluster?\u003c/p\u003e\n\n\u003cp\u003eThe good news is that, Kubernetes already ships several models for these\naccessing patterns, for example,\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\u003cstrong\u003eL7 model\u003c/strong\u003e: named Ingress, supports accesing Services via layer 7, e.g. via HTTP API.\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eL4 model\u003c/strong\u003e: including externalIPs Service, LoadBalancer Service, supports\naccessing Services via L4, e.g. VIP+Port.\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eThe bad news is: \u003cstrong\u003eKubernetes only provides these models, but\nthe implementations are left to each vendor\u003c/strong\u003e. For example, if you are\nusing AWS, its ALB and ELB just corresponds to the L7 and L4 implementation,\nrespectively.\u003c/p\u003e\n\n\u003cp\u003eFor our on-premises clusters, we proposed a L4 solution with Cilium+BGP+ECMP.\nIt’s essentially a L4LB, which provides VIPs that could be used\nby those externalIPs and LoadBalancer type Services in Kubernetes cluster:\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/k8s-l4lb/l4lb-topo.png\" width=\"85%\" height=\"85%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 2-4. L4LB solution with Cilium+BGP+ECMP [5]\u003c/p\u003e\n\n\u003cp\u003eBased on this L4 solution, we deployed istio ingress-gateway, which implements\nthe L7 model. A typical traffic path:\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/k8s-l4lb/l4lb-traffic-path.png\" width=\"85%\" height=\"85%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 2-5. Traffic path when accesing Service from outside the Kubernetes cluster [5]\u003c/p\u003e\n\n\u003cp\u003eWe have a dedicated post for illustrating this, see [5].\u003c/p\u003e\n\n\u003ch1 id=\"3-cloud-native-security\"\u003e3 Cloud native security\u003c/h1\u003e\n\n\u003cp\u003eCilium features two cutting edge functionalities:\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eNetworking: eBPF-based, flexible, dynamic, high performance.\u003c/li\u003e\n  \u003cli\u003eSecurity: CiliumNetworkPolicy as a superset of Kubernetes’s NetworkPolicy\nmodel, provides L3-L7 network policy enforcement.\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eAs a big step, we are trying to carry out the security capabilities into\nour infrastructure.\u003c/p\u003e\n\n\u003ch2 id=\"31-cilium-network-policy-cnp\"\u003e3.1 Cilium Network Policy (CNP)\u003c/h2\u003e\n\n\u003cp\u003eLet’s first take a simple example, have a glance at what a CiliumNetworkPolicy\n(CNP) looks like [6]:\u003c/p\u003e\n\n\u003cdiv class=\"language-yaml highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"na\"\u003eapiVersion\u003c/span\u003e\u003cspan class=\"pi\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"s\"\u003ecilium.io/v2\u0026#34;\u003c/span\u003e\n\u003cspan class=\"na\"\u003ekind\u003c/span\u003e\u003cspan class=\"pi\"\u003e:\u003c/span\u003e \u003cspan class=\"s\"\u003eCiliumNetworkPolicy\u003c/span\u003e\n\u003cspan class=\"na\"\u003emetadata\u003c/span\u003e\u003cspan class=\"pi\"\u003e:\u003c/span\u003e\n  \u003cspan class=\"na\"\u003ename\u003c/span\u003e\u003cspan class=\"pi\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"s\"\u003eclustermesh-ingress-l4-policy\u0026#34;\u003c/span\u003e\n  \u003cspan class=\"na\"\u003edescription\u003c/span\u003e\u003cspan class=\"pi\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"s\"\u003edemo:\u003c/span\u003e\u003cspan class=\"nv\"\u003e \u003c/span\u003e\u003cspan class=\"s\"\u003eallow\u003c/span\u003e\u003cspan class=\"nv\"\u003e \u003c/span\u003e\u003cspan class=\"s\"\u003eonly\u003c/span\u003e\u003cspan class=\"nv\"\u003e \u003c/span\u003e\u003cspan class=\"s\"\u003eemployee\u003c/span\u003e\u003cspan class=\"nv\"\u003e \u003c/span\u003e\u003cspan class=\"s\"\u003eto\u003c/span\u003e\u003cspan class=\"nv\"\u003e \u003c/span\u003e\u003cspan class=\"s\"\u003eaccess\u003c/span\u003e\u003cspan class=\"nv\"\u003e \u003c/span\u003e\u003cspan class=\"s\"\u003eprotected-db\u0026#34;\u003c/span\u003e\n\u003cspan class=\"na\"\u003espec\u003c/span\u003e\u003cspan class=\"pi\"\u003e:\u003c/span\u003e\n  \u003cspan class=\"na\"\u003eendpointSelector\u003c/span\u003e\u003cspan class=\"pi\"\u003e:\u003c/span\u003e\n    \u003cspan class=\"na\"\u003ematchLabels\u003c/span\u003e\u003cspan class=\"pi\"\u003e:\u003c/span\u003e\n      \u003cspan class=\"na\"\u003eapp\u003c/span\u003e\u003cspan class=\"pi\"\u003e:\u003c/span\u003e \u003cspan class=\"s\"\u003eprotected-db\u003c/span\u003e\n  \u003cspan class=\"na\"\u003eingress\u003c/span\u003e\u003cspan class=\"pi\"\u003e:\u003c/span\u003e\n  \u003cspan class=\"pi\"\u003e-\u003c/span\u003e \u003cspan class=\"na\"\u003etoPorts\u003c/span\u003e\u003cspan class=\"pi\"\u003e:\u003c/span\u003e\n    \u003cspan class=\"pi\"\u003e-\u003c/span\u003e \u003cspan class=\"na\"\u003eports\u003c/span\u003e\u003cspan class=\"pi\"\u003e:\u003c/span\u003e\n      \u003cspan class=\"pi\"\u003e-\u003c/span\u003e \u003cspan class=\"na\"\u003eport\u003c/span\u003e\u003cspan class=\"pi\"\u003e:\u003c/span\u003e \u003cspan class=\"s2\"\u003e\u0026#34;\u003c/span\u003e\u003cspan class=\"s\"\u003e6379\u0026#34;\u003c/span\u003e\n        \u003cspan class=\"na\"\u003eprotocol\u003c/span\u003e\u003cspan class=\"pi\"\u003e:\u003c/span\u003e \u003cspan class=\"s\"\u003eTCP\u003c/span\u003e\n    \u003cspan class=\"na\"\u003efromEndpoints\u003c/span\u003e\u003cspan class=\"pi\"\u003e:\u003c/span\u003e\n      \u003cspan class=\"pi\"\u003e-\u003c/span\u003e \u003cspan class=\"na\"\u003ematchLabels\u003c/span\u003e\u003cspan class=\"pi\"\u003e:\u003c/span\u003e\n          \u003cspan class=\"na\"\u003eapp\u003c/span\u003e\u003cspan class=\"pi\"\u003e:\u003c/span\u003e \u003cspan class=\"s\"\u003eemployee\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eThe above yaml says:\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eCreate a CNP, with the provided \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ename\u003c/code\u003e and \u003ccode class=\"language-plaintext highlighter-rouge\"\u003edescription\u003c/code\u003e.\u003c/li\u003e\n  \u003cli\u003eEnforce this CNP to endpoints (pods) that match label \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eapp=protected-db\u003c/code\u003e.\u003c/li\u003e\n  \u003cli\u003eFilter over the \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eingress\u003c/code\u003e (inbound) traffic of the matched endpoints (server side), allow only if\n    \u003cul\u003e\n      \u003cli\u003eProtocols and ports each match \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eTCP\u003c/code\u003e and \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e6379\u003c/code\u003e.\u003c/li\u003e\n      \u003cli\u003eTraffic is from client endpoints which are labeled \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eapp:employee\u003c/code\u003e.\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eAs can be seen, CNP is really flexible and easy to use. But to roll it out\ninto real environments in enterprises, there may be considerable challenges.\u003c/p\u003e\n\n\u003ch2 id=\"32-carry-out-challenges\"\u003e3.2 Carry-out challenges\u003c/h2\u003e\n\n\u003cp\u003eAs an example, we think below challenges are not specific to us alone.\u003c/p\u003e\n\n\u003ch3 id=\"multi-cluster\"\u003eMulti-cluster\u003c/h3\u003e\n\n\u003cp\u003eIf all your applications run in Cilium, and all your to-be-secured applications\nconverged to a single cluster (most public cloud vendors suggest one big Kubernetes\ncluster inside each region), then it’ll be fairly easy to rollout things.\u003c/p\u003e\n\n\u003cp\u003eBut this assumption almost always proves to be false in the reality, especially\nin companies whose infrastructures have evolved from many many years ago. In\nother words, “neat” and well-orginazed infrastructures are ideal rather than\nreal.\u003c/p\u003e\n\n\u003ch3 id=\"hybrid-infrastructures\"\u003eHybrid infrastructures\u003c/h3\u003e\n\n\u003cp\u003eThe more bigger challenge for us is that, we still have so many non-Cilium or even\nnon-Kubernetes clusters.\u003c/p\u003e\n\n\u003cp\u003eThe reality we are facing is: applications scattered\namong Cilium-powered Kubernetes clusters, Neutron-powered Kubernetes clusters,\nOpenStack clusters, and bare metal clusters.\u003c/p\u003e\n\n\u003cp\u003eAlthough in the long run, Neutron-powered Kubernetes clusters will fade out, but\nOpenStack clusters as well as bare metal clusters will continue to live\n(although may gradually scale down), so we must consider them when planning.\u003c/p\u003e\n\n\u003ch2 id=\"33-security-solution-design\"\u003e3.3 Security solution design\u003c/h2\u003e\n\n\u003cp\u003eThe security solution we came up:\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\n    \u003cp\u003e\u003cstrong\u003eEnforce CNP at only server side\u003c/strong\u003e, clients could come from any cluster, any platform.\u003c/p\u003e\n\n    \u003cp\u003eThis limits the scope and simplifies the overall design.\u003c/p\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eOnly consider \u003cstrong\u003e(the server side) Cilium Pods\u003c/strong\u003e at the first stage rollout of this solution.\u003c/p\u003e\n\n    \u003cp\u003eThis is a good starting point, and we expect major part of our server side\n applications will be running in Cilium clusters.\u003c/p\u003e\n  \u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eThe, the remaining question is: \u003cstrong\u003ehow to enforce network policy over clients\nthat coming from outside of a cluster, or even outside of Cilium’s awareness\u003c/strong\u003e?\nOur answer is:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eFirst, group Cilium clusters into a ClusterMesh;\u003c/li\u003e\n  \u003cli\u003eThen, “extend” ClusterMesh to make it be aware of external endpoints.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eEach explained below.\u003c/p\u003e\n\n\u003ch3 id=\"331-group-cilium-clusters-into-clustermesh\"\u003e3.3.1 Group Cilium clusters into ClusterMesh\u003c/h3\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/trip-stepping-into-cloud-native-networking-era/clustermesh.png\" width=\"100%\" height=\"100%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 3-1. Vanilla Cilium ClusterMesh [6]\u003c/p\u003e\n\n\u003cp\u003eClusterMesh [7] is a multi-cluster solution provided by Cilium. This solves the\nmulti-cluster problem if all applications are deployed as native Cilium endpoints (Pods).\u003c/p\u003e\n\n\u003cp\u003eUsing ClusterMesh sounds to be straight forward, but actually it was not\nour first choice then. Several reasons:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eFirst, regarding to some internal scenarios, we’ve considered developing an\nin-house component to synchronize metadata between Kubernetes clusters.\u003c/li\u003e\n  \u003cli\u003eBesides, we haven’t seen any sharings which claimed that ClusterMesh got used\nat large scale.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 id=\"332-extend-cilium-to-perceive-non-cilium-endpoints\"\u003e3.3.2 Extend Cilium to perceive non-Cilium endpoints\u003c/h3\u003e\n\n\u003cp\u003eHere, non-Cilium endpoints include Neutron-powered Pods, VMs, BMs.\u003c/p\u003e\n\n\u003cp\u003eLooking at the code, Cilium already has an abstraction for these endpoints,\nnamed \u003cstrong\u003eexternal endpoints\u003c/strong\u003e, but, this feature is currently lessly implemented\nand publicized.\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/trip-stepping-into-cloud-native-networking-era/trip-security-solution.png\" width=\"100%\" height=\"100%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 3-2. Proposed security solution over hybrid infrastructures\u003c/p\u003e\n\n\u003cp\u003eAs shown above, as an (community compatible) extension to Cilium, we developed a\ncustom API suite, which allows specific owners to \u003cstrong\u003efed their instances’\nmetadata into Cilium cluster\u003c/strong\u003e, the \u003cstrong\u003eCilium will perceive them as external\nendpoints\u003c/strong\u003e. And more, we ensure that external points’ \u003cstrong\u003ecreate/update/delete\nevents will be timely notified to Cilium\u003c/strong\u003e.\u003c/p\u003e\n\n\u003cp\u003eCombining 3.3.1 \u0026amp; 3.3.2, our \u003cstrong\u003eextended ClusterMesh\u003c/strong\u003e now possesses an entire view over\nour hybrid infrastructures, which is sufficient for enforcing network\npolicy over all types of clients.\u003c/p\u003e\n\n\u003ch1 id=\"4-conclusion\"\u003e4 Conclusion\u003c/h1\u003e\n\n\u003cp\u003eThis post shares some of our recent processes on Cilium-based networking \u0026amp;\nsecurity.\u003c/p\u003e\n\n\u003ch1 id=\"references\"\u003eReferences\u003c/h1\u003e\n\n\u003col\u003e\n  \u003cli\u003e\u003ca href=\"/blog/ctrip-network-arch-evolution/\"\u003eCtrip Network Architecture Evolution in the Cloud Computing Era\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"/blog/trip-first-step-towards-cloud-native-networking/\"\u003eTrip.com: First Step towards Cloud Native Networking\u003c/a\u003e.\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://docs.cilium.io/en/v1.8/gettingstarted/bird/\"\u003eCilium Doc: Using BIRD to run BGP\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"/blog/cilium-life-of-a-packet-pod-to-service/\"\u003eLife of a Packet in Cilium: Discovering the Pod-to-Service Traffic Path and BPF Processing Logics\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"/blog/k8s-l4lb/\"\u003eL4LB for Kubernetes: Theory and Practice with Cilium+BGP+ECMP\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"/blog/cilium-clustermesh/\"\u003eCilium ClusterMesh: A Hands-on Guide\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://docs.cilium.io/en/stable/gettingstarted/clustermesh/\"\u003eCilium Doc: clustermesh\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\n\n  \u003c!-- POST NAVIGATION --\u003e\n  \u003cdiv class=\"postNav clearfix\"\u003e\n     \n      \u003ca class=\"prev\" href=\"/blog/trip-stepping-into-cloud-native-networking-era-zh/\"\u003e\u003cspan\u003e« 迈入 Cilium+BGP 的云原生网络时代\u003c/span\u003e\n      \n    \u003c/a\u003e\n      \n      \n      \u003ca class=\"next\" href=\"/blog/network-evolves-zh/\"\u003e\u003cspan\u003e计算规模驱动下的网络方案演进 »\u003c/span\u003e\n       \n      \u003c/a\u003e\n     \n  \u003c/div\u003e\n\u003c/div\u003e",
  "Date": "2020-11-04T00:00:00Z",
  "Author": "Arthur Chiao"
}