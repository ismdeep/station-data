{
  "Source": "arthurchiao.art",
  "Title": "[译][论文] LLaMA 2：开放基础和微调聊天模型（Meta/Facebook，2023）",
  "Link": "https://arthurchiao.art/blog/llama2-paper-zh/",
  "Content": "\u003cdiv class=\"post\"\u003e\n  \n  \u003ch1 class=\"postTitle\"\u003e[译][论文] LLaMA 2：开放基础和微调聊天模型（Meta/Facebook，2023）\u003c/h1\u003e\n  \u003cp class=\"meta\"\u003ePublished at 2023-08-06 | Last Update 2023-08-12\u003c/p\u003e\n  \n  \u003ch3 id=\"译者序\"\u003e译者序\u003c/h3\u003e\n\n\u003cp\u003e本文来自 2023 年 Meta（facebook）的大模型论文：\n\u003ca href=\"https://arxiv.org/abs/2307.09288\"\u003eLlama 2: Open Foundation and Fine-Tuned Chat Models\u003c/a\u003e。\n翻译了其中感兴趣的部分。\u003c/p\u003e\n\n\u003cp\u003eLLaMA2 用了两个 GPU 集群进行训练：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eRSC 集群：\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e200Gbps InfiniBand + 400W A100 GPU\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e；\u003c/li\u003e\n  \u003cli\u003e生产集群：\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e200Gbps RoCE + 350W A100 GPU\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e；\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eRoCE + 350W GPU\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 的集群，经过优化的代码能达到\n\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eIB + 400W GPU\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 集群性能的 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e90%\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e。\n总共耗费 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e3.3M GPU-hour\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e译者水平有限，不免存在遗漏或错误之处。如有疑问，敬请查阅原文。\u003c/strong\u003e\u003c/p\u003e\n\n\u003cp\u003e以下是译文。\u003c/p\u003e\n\n\u003chr/\u003e\n\n\u003cul id=\"markdown-toc\"\u003e\n  \u003cli\u003e\u003ca href=\"#译者序\" id=\"markdown-toc-译者序\"\u003e译者序\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#摘要\" id=\"markdown-toc-摘要\"\u003e摘要\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#1-引言\" id=\"markdown-toc-1-引言\"\u003e1 引言\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#11-现状没有能与-chatgpt-匹敌的开源大模型\" id=\"markdown-toc-11-现状没有能与-chatgpt-匹敌的开源大模型\"\u003e1.1 现状：没有能与 ChatGPT 匹敌的开源大模型\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#12-开源-llama2llama2-chat填补空白\" id=\"markdown-toc-12-开源-llama2llama2-chat填补空白\"\u003e1.2 开源 LLaMA2/LLaMA2-Chat，填补空白\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#13-llama2-是如何炼成的训练微调鸟瞰\" id=\"markdown-toc-13-llama2-是如何炼成的训练微调鸟瞰\"\u003e1.3 LLaMA2 是如何炼成的：训练+微调鸟瞰\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#14-本文组织\" id=\"markdown-toc-14-本文组织\"\u003e1.4 本文组织\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#2-预训练pretraining\" id=\"markdown-toc-2-预训练pretraining\"\u003e2 预训练（Pretraining）\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#21-预训练数据pretraining-data\" id=\"markdown-toc-21-预训练数据pretraining-data\"\u003e2.1 预训练数据（Pretraining Data）\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#22-训练细节training-details\" id=\"markdown-toc-22-训练细节training-details\"\u003e2.2 训练细节（Training Details）\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#221-超参数hyperparameters\" id=\"markdown-toc-221-超参数hyperparameters\"\u003e2.2.1 超参数（Hyperparameters）\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#222-分词器tokenizer\" id=\"markdown-toc-222-分词器tokenizer\"\u003e2.2.2 分词器（Tokenizer）\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#223-训练硬件和碳足迹\" id=\"markdown-toc-223-训练硬件和碳足迹\"\u003e2.2.3 训练硬件和碳足迹\u003c/a\u003e            \u003cul\u003e\n              \u003cli\u003e\u003ca href=\"#训练硬件training-hardware\" id=\"markdown-toc-训练硬件training-hardware\"\u003e训练硬件（Training Hardware）\u003c/a\u003e\u003c/li\u003e\n              \u003cli\u003e\u003ca href=\"#预训练碳足迹carbon-footprint-of-pretraining\" id=\"markdown-toc-预训练碳足迹carbon-footprint-of-pretraining\"\u003e预训练碳足迹（Carbon Footprint of Pretraining）\u003c/a\u003e\u003c/li\u003e\n            \u003c/ul\u003e\n          \u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#23-llama-2-预训练模型性能评估pretrained-model-evaluation\" id=\"markdown-toc-23-llama-2-预训练模型性能评估pretrained-model-evaluation\"\u003e2.3 LLaMA 2 预训练模型性能评估（Pretrained Model Evaluation）\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#231-与开源基座大模型对比\" id=\"markdown-toc-231-与开源基座大模型对比\"\u003e2.3.1 与开源基座大模型对比\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#232-与闭源大模型对比\" id=\"markdown-toc-232-与闭源大模型对比\"\u003e2.3.2 与闭源大模型对比\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#3-微调fine-tuning\" id=\"markdown-toc-3-微调fine-tuning\"\u003e3 微调（Fine-tuning）\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#31-监督式微调sft\" id=\"markdown-toc-31-监督式微调sft\"\u003e3.1 监督式微调（SFT）\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#311-使用公开的指令微调数据\" id=\"markdown-toc-311-使用公开的指令微调数据\"\u003e3.1.1 使用公开的指令微调数据\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#312-标注质量为王quality-is-all-you-need\" id=\"markdown-toc-312-标注质量为王quality-is-all-you-need\"\u003e3.1.2 标注质量为王（Quality Is All You Need）\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#313-一些微调细节fine-tuning-details\" id=\"markdown-toc-313-一些微调细节fine-tuning-details\"\u003e3.1.3 一些微调细节（Fine-Tuning Details）\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#32-基于人类反馈的强化学习rlhf\" id=\"markdown-toc-32-基于人类反馈的强化学习rlhf\"\u003e3.2 基于人类反馈的强化学习（RLHF）\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#321-人类偏好数据收集\" id=\"markdown-toc-321-人类偏好数据收集\"\u003e3.2.1 人类偏好数据收集\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#322-奖励建模reward-modeling\" id=\"markdown-toc-322-奖励建模reward-modeling\"\u003e3.2.2 奖励建模（Reward Modeling）\u003c/a\u003e            \u003cul\u003e\n              \u003cli\u003e\u003ca href=\"#训练目标\" id=\"markdown-toc-训练目标\"\u003e训练目标\u003c/a\u003e\u003c/li\u003e\n              \u003cli\u003e\u003ca href=\"#data-composition\" id=\"markdown-toc-data-composition\"\u003eData Composition\u003c/a\u003e\u003c/li\u003e\n              \u003cli\u003e\u003ca href=\"#训练细节training-details\" id=\"markdown-toc-训练细节training-details\"\u003e训练细节（Training Details）\u003c/a\u003e\u003c/li\u003e\n              \u003cli\u003e\u003ca href=\"#奖励模型的结果reward-model-results\" id=\"markdown-toc-奖励模型的结果reward-model-results\"\u003e奖励模型的结果（Reward Model Results）\u003c/a\u003e\u003c/li\u003e\n              \u003cli\u003e\u003ca href=\"#scaling-trends\" id=\"markdown-toc-scaling-trends\"\u003eScaling Trends\u003c/a\u003e\u003c/li\u003e\n            \u003c/ul\u003e\n          \u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#323-iterative-fine-tuning\" id=\"markdown-toc-323-iterative-fine-tuning\"\u003e3.2.3 Iterative Fine-Tuning\u003c/a\u003e            \u003cul\u003e\n              \u003cli\u003e\u003ca href=\"#rejection-sampling\" id=\"markdown-toc-rejection-sampling\"\u003eRejection Sampling\u003c/a\u003e\u003c/li\u003e\n              \u003cli\u003e\u003ca href=\"#ppo\" id=\"markdown-toc-ppo\"\u003ePPO\u003c/a\u003e\u003c/li\u003e\n            \u003c/ul\u003e\n          \u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#33-system-message-for-multi-turn-consistency\" id=\"markdown-toc-33-system-message-for-multi-turn-consistency\"\u003e3.3 System Message for Multi-Turn Consistency\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#34-rlhf-results\" id=\"markdown-toc-34-rlhf-results\"\u003e3.4 RLHF Results\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#341-model-based-evaluation\" id=\"markdown-toc-341-model-based-evaluation\"\u003e3.4.1 Model-Based Evaluation\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#342-human-evaluation\" id=\"markdown-toc-342-human-evaluation\"\u003e3.4.2 Human Evaluation\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#4-safety略\" id=\"markdown-toc-4-safety略\"\u003e4 Safety（略）\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#5-讨论\" id=\"markdown-toc-5-讨论\"\u003e5 讨论\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#51-新发现与评论learnings-and-observations\" id=\"markdown-toc-51-新发现与评论learnings-and-observations\"\u003e5.1 新发现与评论（Learnings and Observations）\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#超越人类监督从-sft-到-rlhf\" id=\"markdown-toc-超越人类监督从-sft-到-rlhf\"\u003e超越人类监督：从 SFT 到 RLHF\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#in-context-temperature-rescaling\" id=\"markdown-toc-in-context-temperature-rescaling\"\u003eIn-Context Temperature Rescaling\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#llama2-chat-时间感知能力temporal-perception\" id=\"markdown-toc-llama2-chat-时间感知能力temporal-perception\"\u003eLLaMA2-Chat 时间感知能力（Temporal Perception）\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#工具的使用\" id=\"markdown-toc-工具的使用\"\u003e工具的使用\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#52-限制和伦理考虑\" id=\"markdown-toc-52-限制和伦理考虑\"\u003e5.2 限制和伦理考虑\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#53-负责任的发布策略responsible-release-strategy\" id=\"markdown-toc-53-负责任的发布策略responsible-release-strategy\"\u003e5.3 负责任的发布策略（Responsible Release Strategy）\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#531-发布细节\" id=\"markdown-toc-531-发布细节\"\u003e5.3.1 发布细节\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#532-负责任的发布\" id=\"markdown-toc-532-负责任的发布\"\u003e5.3.2 负责任的发布\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#6-相关工作\" id=\"markdown-toc-6-相关工作\"\u003e6 相关工作\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#61-large-language-models\" id=\"markdown-toc-61-large-language-models\"\u003e6.1 Large Language Models\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#62-instruction-tuning\" id=\"markdown-toc-62-instruction-tuning\"\u003e6.2 Instruction Tuning\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#63-known-llm-safety-challenges\" id=\"markdown-toc-63-known-llm-safety-challenges\"\u003e6.3 Known LLM Safety Challenges\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#7-总结\" id=\"markdown-toc-7-总结\"\u003e7 总结\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#参考文献略\" id=\"markdown-toc-参考文献略\"\u003e参考文献（略）\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#附录略\" id=\"markdown-toc-附录略\"\u003e附录（略）\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003chr/\u003e\n\n\u003cscript type=\"text/x-mathjax-config\"\u003e\n  \tMathJax.Hub.Config({\n    \textensions: [\"tex2jax.js\"],\n    \tjax: [\"input/TeX\", \"output/HTML-CSS\"],\n    \ttex2jax: {\n      \t\tinlineMath: [ ['$','$'], [\"\\\\(\",\"\\\\)\"] ],\n      \t\tdisplayMath: [ ['$$','$$'], [\"\\\\[\",\"\\\\]\"] ],\n    \t\tprocessEscapes: true\n\t    },\n\t\t\"HTML-CSS\": {\n\t\t\tavailableFonts: [], preferredFont: null,\n\t\t\twebFont: \"Neo-Euler\",\n\t\t\tmtextFontInherit: true\n\t\t},\n\t\tTeX: {\n\t\t\textensions: [\"color.js\"],\n\t\t\tMacros: {\n\t\t\t\tlgc: [\"{\\\\color{my-light-green} #1}\", 1],\n\t\t\t\tgc: [\"{\\\\color{my-green} #1}\", 1],\n\t\t\t\tlrc: [\"{\\\\color{my-light-red} #1}\", 1],\n\t\t\t\trc: [\"{\\\\color{my-red} #1}\", 1],\n\t\t\t\tlbc: [\"{\\\\color{my-light-blue} #1}\", 1],\n\t\t\t\tbc: [\"{\\\\color{my-blue} #1}\", 1],\n\t\t\t\tkc: [\"{\\\\color{my-gray} #1}\", 1],\n\t\t\t\tloc: [\"{\\\\color{my-light-orange} #1}\", 1],\n\t\t\t\toc: [\"{\\\\color{my-orange} #1}\", 1],\n\n\t\t\t\ta: [\"\\\\mathbf a\"],\n\t\t\t\tA: [\"\\\\mathbf A\"],\n\t\t\t\tb: [\"\\\\mathbf b\"],\n\t\t\t\tB: [\"\\\\mathbf B\"],\n\t\t\t\tc: [\"\\\\mathbf c\"],\n\t\t\t\tC: [\"\\\\mathbf C\"],\n\t\t\t\td: [\"\\\\mathbf d\"],\n\t\t\t\tD: [\"\\\\mathbf D\"],\n\t\t\t\tE: [\"\\\\mathbf E\"],\n\t\t\t\tI: [\"\\\\mathbf I\"],\n\t\t\t\tL: [\"\\\\mathbf L\"],\n\t\t\t\tm: [\"\\\\mathbf m\"],\n\t\t\t\tM: [\"\\\\mathbf M\"],\n\t\t\t\tr: [\"\\\\mathbf r\"],\n\t\t\t\ts: [\"\\\\mathbf s\"],\n\t\t\t\tt: [\"\\\\mathbf t\"],\n\t\t\t\tS: [\"\\\\mathbf S\"],\n\t\t\t\tx: [\"\\\\mathbf x\"],\n\t\t\t\tz: [\"\\\\mathbf z\"],\n\t\t\t\tv: [\"\\\\mathbf v\"],\n\t\t\t\ty: [\"\\\\mathbf y\"],\n\t\t\t\tk: [\"\\\\mathbf k\"],\n\t\t\t\tbp: [\"\\\\mathbf p\"],\n\t\t\t\tP: [\"\\\\mathbf P\"],\n\t\t\t\tq: [\"\\\\mathbf q\"],\n\t\t\t\tQ: [\"\\\\mathbf Q\"],\n\t\t\t\tr: [\"\\\\mathbf r\"],\n\t\t\t\tR: [\"\\\\mathbf R\"],\n\t\t\t\tSig: [\"\\\\mathbf \\\\Sigma\"],\n\t\t\t\tt: [\"\\\\mathbf t\"],\n\t\t\t\tT: [\"\\\\mathbf T\"],\n\t\t\t\te: [\"\\\\mathbf e\"],\n\t\t\t\tX: [\"\\\\mathbf X\"],\n\t\t\t\tu: [\"\\\\mathbf u\"],\n\t\t\t\tU: [\"\\\\mathbf U\"],\n\t\t\t\tv: [\"\\\\mathbf v\"],\n\t\t\t\tV: [\"\\\\mathbf V\"],\n\t\t\t\tw: [\"\\\\mathbf w\"],\n\t\t\t\tW: [\"\\\\mathbf W\"],\n\t\t\t\tY: [\"\\\\mathbf Y\"],\n\t\t\t\tz: [\"\\\\mathbf z\"],\n\t\t\t\tZ: [\"\\\\mathbf Z\"],\n\t\t\t\tp: [\"\\\\,\\\\text{.}\"],\n\t\t\t\ttab: [\"\\\\hspace{0.7cm}\"],\n\n\t\t\t\tsp: [\"^{\\\\small\\\\prime}\"],\n\n\n\t\t\t\tmR: [\"{\\\\mathbb R}\"],\n\t\t\t\tmC: [\"{\\\\mathbb C}\"],\n\t\t\t\tmN: [\"{\\\\mathbb N}\"],\n\t\t\t\tmZ: [\"{\\\\mathbb Z}\"],\n\n\t\t\t\tdeg: [\"{^\\\\circ}\"],\n\n\n\t\t\t\targmin: [\"\\\\underset{#1}{\\\\text{argmin}}\", 1],\n\t\t\t\targmax: [\"\\\\underset{#1}{\\\\text{argmax}}\", 1],\n\n\t\t\t\tco: [\"\\\\;\\\\text{cos}\"],\n\t\t\t\tsi: [\"\\\\;\\\\text{sin}\"]\n\t\t\t}\n\t\t}\n  \t});\n\n  \tMathJax.Hub.Register.StartupHook(\"TeX color Ready\", function() {\n     \tMathJax.Extension[\"TeX/color\"].colors[\"my-green\"] = '#677d00';\n     \tMathJax.Extension[\"TeX/color\"].colors[\"my-light-green\"] = '#acd373';\n     \tMathJax.Extension[\"TeX/color\"].colors[\"my-red\"] = '#b13e26';\n     \tMathJax.Extension[\"TeX/color\"].colors[\"my-light-red\"] = '#d38473';\n     \tMathJax.Extension[\"TeX/color\"].colors[\"my-blue\"] = '#306693';\n       \tMathJax.Extension[\"TeX/color\"].colors[\"my-light-blue\"] = '#73a7d3';\n       \tMathJax.Extension[\"TeX/color\"].colors[\"my-gray\"] = '#999';\n       \tMathJax.Extension[\"TeX/color\"].colors[\"my-orange\"] = '#E69500';\n       \tMathJax.Extension[\"TeX/color\"].colors[\"my-light-orange\"] = '#FFC353';\n\n\n\t});\n\u003c/script\u003e\n\n\u003cscript type=\"text/javascript\" src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js\"\u003e\n\u003c/script\u003e\n\n\u003ch1 id=\"摘要\"\u003e摘要\u003c/h1\u003e\n\n\u003cp\u003e本文介绍 LLaMA 2，我们开发的一组\u003cstrong\u003e\u003cmark\u003e预训练和微调\u003c/mark\u003e\u003c/strong\u003e大语言模型集，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eLLaMA2 参数规模 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e7b~70b\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e；\u003c/li\u003e\n  \u003cli\u003e微调模型称为 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eLLaMA2-Chat\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e，针对对话场景进行了优化。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e与\u003cstrong\u003e\u003cmark\u003e其他开源聊天模型\u003c/mark\u003e\u003c/strong\u003e进行比较，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e大多数基准测试中，LLaMA2 性能更好；\u003c/li\u003e\n  \u003cli\u003e有用性和安全性方面，人工评估（human evaluations）的结果也证明 LLaMA2 更优。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e因此，LLaMA2 可以作为一个不错的\u003cstrong\u003e\u003cmark\u003e闭源模型替代方案\u003c/mark\u003e\u003c/strong\u003e。\n本文将详细描述我们是如何对 LLaMA2-Chat 进行微调和安全性改进的。\n社区可以在我们的工作基础上进一步开发迭代，为 LLM 的负责任发展做出贡献。\u003c/p\u003e\n\n\u003ch1 id=\"1-引言\"\u003e1 引言\u003c/h1\u003e\n\n\u003cp\u003e大语言模型（LLM）作为功能强大的人工智能助手，在涉及跨领域、需要专业知识\n（例如\u003cstrong\u003e\u003cmark\u003e编程和创意写作\u003c/mark\u003e\u003c/strong\u003e）的\u003cstrong\u003e\u003cmark\u003e复杂推理任务\u003c/mark\u003e\u003c/strong\u003e中表现出了巨大的潜力。\nLLM 通过聊天窗口与人类进行交互，简单方便，因此一经推出就迅速打开大众市场。\u003c/p\u003e\n\n\u003cp\u003e如果考虑到背后的\u003cstrong\u003e\u003cmark\u003e训练方法论\u003c/mark\u003e\u003c/strong\u003e本质上非常\u003cstrong\u003e\u003cmark\u003e简单直观\u003c/mark\u003e\u003c/strong\u003e，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e首先，在大量自监督数据上对 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eauto-regressive transforer\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 进行\u003cstrong\u003e\u003cmark\u003e预训练\u003c/mark\u003e\u003c/strong\u003e，\u003c/li\u003e\n  \u003cli\u003e然后，通过基于人类反馈的强化学习（\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eRLHF\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e）等技术\u003cstrong\u003e\u003cmark\u003e与人类偏好对齐\u003c/mark\u003e\u003c/strong\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e就更会震惊于 LLM 的能力是多么出众。\u003c/p\u003e\n\n\u003ch2 id=\"11-现状没有能与-chatgpt-匹敌的开源大模型\"\u003e1.1 现状：没有能与 ChatGPT 匹敌的开源大模型\u003c/h2\u003e\n\n\u003cp\u003e大模型的训练方法很简单，但是，极高的\u003cstrong\u003e\u003cmark\u003e算力要求\u003c/mark\u003e\u003c/strong\u003e限制了它的发展，\n结果是只有少数几家公司有财力进行研究和训练。\n虽然之前已经开源了一些预训练的大模型，包括\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eBLOOM（Scao 等，2022）\u003c/li\u003e\n  \u003cli\u003eLLaMA-1（Touvron 等，2023）\u003c/li\u003e\n  \u003cli\u003eFalcon（Penedo 等，2023）\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e这些模型的\u003cstrong\u003e\u003cmark\u003e性能已经与 GPT-3\u003c/mark\u003e\u003c/strong\u003e（Brown 等，2020）和 Chinchilla（Hoffmann 等，2022）\n等闭源预训练模型\u003cstrong\u003e\u003cmark\u003e相当\u003c/mark\u003e\u003c/strong\u003e，但它们还无法成为 ChatGPT、BARD 和 Claude\n等\u003cstrong\u003e\u003cmark\u003e性能更强大的闭源、生产级大模型\u003c/mark\u003e\u003c/strong\u003e的替代品，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e后者做了\u003cstrong\u003e\u003cmark\u003e大量微调\u003c/mark\u003e\u003c/strong\u003e以与人类偏好对齐，\n极大增强了它们的可用性和安全性；\u003c/li\u003e\n  \u003cli\u003e这一过程需要大量的\u003cstrong\u003e\u003cmark\u003e计算和人工标注成本\u003c/mark\u003e\u003c/strong\u003e，并且通常不透明，也难以轻松照搬，\n因此限制了社区在 advance AI alignment research 方面的进展。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 id=\"12-开源-llama2llama2-chat填补空白\"\u003e1.2 开源 LLaMA2/LLaMA2-Chat，填补空白\u003c/h2\u003e\n\n\u003cp\u003e本文介绍我们开源的 LLaMA2，这是一组预训练和微调的 LLM，包括 LLaMA2 和 LLaMA2-Chat。\n与其他开源 chat models 进行比较，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e大多数基准测试中，LLaMA2 性能更好；\u003c/li\u003e\n  \u003cli\u003e有用性和安全性方面，人工评估（human evaluations）的结果也证明 LLaMA2 更优。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e因此，LLaMA2 可以作为一个不错的\u003cstrong\u003e\u003cmark\u003e闭源模型替代方案\u003c/mark\u003e\u003c/strong\u003e。\n本文将详细描述我们是如何对 LLaMA2-Chat 进行微调和安全性改进的，\n这样社区就能够在我们的工作基础上进一步开发迭代，为 LLM 的负责任发展做出贡献。\n具体来说，我们向公众（the general public）开源以下模型，供\u003cstrong\u003e\u003cmark\u003e研究和商业使用\u003c/mark\u003e\u003c/strong\u003e（research and commercial use）：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\n    \u003cp\u003eLLaMA2：这是 LLaMA 1 的升级版\u003c/p\u003e\n\n    \u003cul\u003e\n      \u003cli\u003e新组合了公开可用数据（a new mix）进行训练，数据集大小 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e+40%\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e（1.4T tokens -\u0026gt; 2T tokens），\u003c/li\u003e\n      \u003cli\u003e模型的\u003cstrong\u003e\u003cmark\u003e上下文长度翻倍\u003c/mark\u003e\u003c/strong\u003e，\u003c/li\u003e\n      \u003cli\u003e采用了 grouped-query attention（Ainslie 等，2023）。\u003c/li\u003e\n    \u003c/ul\u003e\n\n    \u003cp\u003e本次发布 7B/13B/70B 参数的 LLaMA2 模型。\n \u003cstrong\u003e\u003cmark\u003e34B 的模型本文会给出性能参数，但发布要晚一些\u003c/mark\u003e\u003c/strong\u003e（还在做安全测试）。\u003c/p\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eLLaMA2-Chat：LLaMA2 的微调版本，针对\u003cstrong\u003e\u003cmark\u003e对话场景\u003c/mark\u003e\u003c/strong\u003e进行了优化。\u003c/p\u003e\n\n    \u003cul\u003e\n      \u003cli\u003e同样发布 7B/13B/70B 参数的版本。\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e我们相信，在安全的前提下，LLM 的开放将对社会产生积极影响。但注意，和所有 LLM 一样，LLaMA2 是一项新技术，\n在使用中存在潜在风险（Bender 等，2021b；Weidinger 等，2021；Solaiman 等，2023），\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e目前的测试\u003cstrong\u003e\u003cmark\u003e仅涵盖了英语\u003c/mark\u003e\u003c/strong\u003e。\n在部署任何 LLaMA2-Chat 应用之前，开发者应针对其特定场景进行安全测试和调优；\u003c/li\u003e\n  \u003cli\u003e我们提供了一份负责任使用指南和代码示例，以促进 LLaMA2 和 LLaMA2-Chat 的安全部署。更多信息见 5.3 节。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e一些资料链接：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\u003ca href=\"https://ai.meta.com/resources/models-and-libraries/llama/\"\u003eai.meta.com/resources/models-and-libraries/llama/\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://ai.meta.com/llama\"\u003eai.meta.com/llama\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://github.com/facebookresearch/llama\"\u003egithub.com/facebookresearch/llama\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch2 id=\"13-llama2-是如何炼成的训练微调鸟瞰\"\u003e1.3 LLaMA2 是如何炼成的：训练+微调鸟瞰\u003c/h2\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/llama2-paper/fig-4.png\" width=\"100%\" height=\"100%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e图 4：LLaMA2-Chat 训练和调优过程。\u003c/p\u003e\n\n\u003cp\u003e炼丹四步：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e使用公开数据\u003cstrong\u003e\u003cmark\u003e预训练\u003c/mark\u003e\u003c/strong\u003e（自监督学习），得到 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eLLaMA2\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e；\u003c/li\u003e\n  \u003cli\u003e对 LLaMA2 进行\u003cstrong\u003e\u003cmark\u003e监督微调\u003c/mark\u003e\u003c/strong\u003e（SFT），得到一个初始版本的 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eLLaMA2-Chat\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e；\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e人\u003c/mark\u003e\u003c/strong\u003e对 LLaMA2-Chat 回答进行\u003cstrong\u003e\u003cmark\u003e反馈和标注\u003c/mark\u003e\u003c/strong\u003e，得到两个\u003cstrong\u003e\u003cmark\u003e奖励模型\u003c/mark\u003e\u003c/strong\u003e（分别针对有用性和安全性）；\u003c/li\u003e\n  \u003cli\u003e通过 \u003cstrong\u003e\u003cmark\u003e基于人类反馈的强化学习\u003c/mark\u003e\u003c/strong\u003e（RLHF）/ rejection sampling / PPO，对 LLaMA2-Chat 进行（多次）迭代。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch2 id=\"14-本文组织\"\u003e1.4 本文组织\u003c/h2\u003e\n\n\u003cp\u003e本文其余部分组织如下：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e第 2 节：预训练方法\u003c/li\u003e\n  \u003cli\u003e第 3 节：微调方法\u003c/li\u003e\n  \u003cli\u003e第 4 节：模型安全方法\u003c/li\u003e\n  \u003cli\u003e第 5 节：核心观察和见解\u003c/li\u003e\n  \u003cli\u003e第 6 节：相关工作\u003c/li\u003e\n  \u003cli\u003e第 7 节：总结\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch1 id=\"2-预训练pretraining\"\u003e2 预训练（Pretraining）\u003c/h1\u003e\n\n\u003cp\u003e为了打造 LLaMA2 这个新系列模型，我们采用了 Touvron 等（2023）的预训练方法，\n使用了一个\u003cstrong\u003e\u003cmark\u003e优化的自回归 transformer\u003c/mark\u003e\u003c/strong\u003e，并进行了一些\u003cstrong\u003e\u003cmark\u003e改进\u003c/mark\u003e\u003c/strong\u003e以提高性能，\n包括，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e更健壮的\u003cstrong\u003e\u003cmark\u003e数据清洗\u003c/mark\u003e\u003c/strong\u003e；\u003c/li\u003e\n  \u003cli\u003e更新的\u003cstrong\u003e\u003cmark\u003e训练数据比例\u003c/mark\u003e\u003c/strong\u003e；\u003c/li\u003e\n  \u003cli\u003e更多的\u003cstrong\u003e\u003cmark\u003e训练 tokens\u003c/mark\u003e\u003c/strong\u003e；\u003c/li\u003e\n  \u003cli\u003e更长的\u003cstrong\u003e\u003cmark\u003e上下文\u003c/mark\u003e\u003c/strong\u003e；\u003c/li\u003e\n  \u003cli\u003e使用 \u003cstrong\u003e\u003cmark\u003egrouped-query attention\u003c/mark\u003e\u003c/strong\u003e（GQA），通过组查询来提高\u003cstrong\u003e\u003cmark\u003e推理性能\u003c/mark\u003e\u003c/strong\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e表 1 比较了 LLaMA 2 与 LLaMA 1 的一些属性：\u003c/p\u003e\n\n\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth style=\"text-align: left\"\u003e \u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eLLaMA\u003c/th\u003e\n      \u003cth style=\"text-align: left\"\u003eLLaMA 2\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003e训练数据\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e见 \u003ca href=\"/blog/llama-paper-zh/\"\u003eLLaMA 论文\u003c/a\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e基于公开可用数据新组合的数据集\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003e参数数量\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e7b / 13b / \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e33b / 65b\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e7b / 13b / \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e34b / 70b\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003e上下文长度\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e2k / 2k / 2k / 2k\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e4k / 4k / 4k / 4k\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eGQA\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eNO/NO/NO/NO\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003eNO/NO/\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eYES/YES\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003e训练 tokens 数量\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e1T / 1T / 1.4T / 1.4T\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e2T / 2T / 2T / 2T\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd style=\"text-align: left\"\u003eLearning Rate\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e\u003ccode\u003e3.0*10\u003csup\u003e-4\u003c/sup\u003e / 3.0*10\u003csup\u003e-4\u003c/sup\u003e / 1.5*10\u003csup\u003e-4\u003c/sup\u003e / 1.5*10\u003csup\u003e-4\u003c/sup\u003e\u003c/code\u003e\u003c/td\u003e\n      \u003ctd style=\"text-align: left\"\u003e\u003ccode\u003e3.0*10\u003csup\u003e-4\u003c/sup\u003e / 3.0*10\u003csup\u003e-4\u003c/sup\u003e / 3.0*10\u003csup\u003e-4\u003c/sup\u003e / 3.0*10\u003csup\u003e-4\u003c/sup\u003e\u003c/code\u003e\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\n\n\u003cp align=\"center\"\u003e表 1：LLaMA 1 和 2 模型对比。Token 数量只计算了预训练数据。所有模型都是用\nglobal batch-size of 4M tokens 训练的。\u003c/p\u003e\n\n\u003ch2 id=\"21-预训练数据pretraining-data\"\u003e2.1 预训练数据（Pretraining Data）\u003c/h2\u003e\n\n\u003cul\u003e\n  \u003cli\u003e组合了一些公开可用的数据源，其中不包含来 Meta 产品或服务的数据。\u003c/li\u003e\n  \u003cli\u003e某些网站包含了很多个人信息，我们努力删掉了其中的此类信息。\u003c/li\u003e\n  \u003cli\u003e训练了 2T（2 万亿）个 token，这在性能和成本之间提供了不错的折中（performance–cost trade-off），\u003c/li\u003e\n  \u003cli\u003e对大部分事实类数据源进行 up-sampling，以增加知识减少幻觉（ increase knowledge and dampen hallucinations）。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e我们进行了大量预训练数据研究，这样用户可以更好地了解 LLaMA2 的潜在能力和限制；详细结果见 4.1 节。\u003c/p\u003e\n\n\u003ch2 id=\"22-训练细节training-details\"\u003e2.2 训练细节（Training Details）\u003c/h2\u003e\n\n\u003cp\u003e我们\u003cstrong\u003e\u003cmark\u003e采用了 Llama 1 的大部分预训练设置和模型架构\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e使用标准的 transformer 架构（Vaswani 等，2017），\u003c/li\u003e\n  \u003cli\u003e使用 RMSNorm 进行预归一化（Zhang 和 Sennrich，2019），\u003c/li\u003e\n  \u003cli\u003e使用 SwiGLU 激活函数（Shazeer，2020），以及旋转位置嵌入（rotary positional embeddings，RoPE，Su 等，2022）。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e与 Llama 1 相比，\u003cstrong\u003e\u003cmark\u003e主要的架构差异\u003c/mark\u003e\u003c/strong\u003e包括\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e上下文长度\u003c/mark\u003e\u003c/strong\u003e（翻倍，\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e2k -\u0026gt; 4k\u003c/code\u003e）\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e组查询注意力\u003c/mark\u003e\u003c/strong\u003e（GQA, grouped-query attention）\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e附录 A.2.1 中详细介绍了这些差异，并进行了 ablation experiments 以证明它们的重要性。\u003c/p\u003e\n\n\u003ch3 id=\"221-超参数hyperparameters\"\u003e2.2.1 超参数（Hyperparameters）\u003c/h3\u003e\n\n\u003cul\u003e\n  \u003cli\u003e使用 AdamW 优化器进行训练（Loshchilov 和 Hutter，2017），其中 β1 = 0.9，β2 = 0.95，eps = 10\u003csup\u003e-5\u003c/sup\u003e。\u003c/li\u003e\n  \u003cli\u003e使用余弦学习率调度（a cosine learning rate schedule），热身阶段为 2000 steps，并将最终学习率衰减到峰值学习率的 10%。\u003c/li\u003e\n  \u003cli\u003e使用 0.1 的权重衰减（weight decay）和 1.0 的梯度裁剪（gradient clipping）。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e图 5（a）显示了使用这些超参数训练的 LLaMA2 的训练损失，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/llama2-paper/fig-5.png\" width=\"60%\" height=\"60%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e图 5：LLaMA2 Training Loss。注意\u003cmark\u003e即使用 2T tokens 进行训练，这些模型仍然没有饱和的迹象\u003c/mark\u003e。\u003c/p\u003e\n\n\u003ch3 id=\"222-分词器tokenizer\"\u003e2.2.2 分词器（Tokenizer）\u003c/h3\u003e\n\n\u003cp\u003eLLaMA2 使用的分词器与 Llama 1 相同；采用了一种\u003cstrong\u003e\u003cmark\u003e字节对编码\u003c/mark\u003e\u003c/strong\u003e（bytepair encoding，BPE）算法（Sennrich 等，2016），\n我们使用了 SentencePiece（Kudo 和 Richardson，2018）的实现。\u003c/p\u003e\n\n\u003cp\u003e与 Llama 1 一样，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e将所有 numbers 拆分为单个 digits，\u003c/li\u003e\n  \u003cli\u003e使用 bytes 来分解未知的 UTF-8 字符。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003evocabulary size 为 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e32k tokens\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003ch3 id=\"223-训练硬件和碳足迹\"\u003e2.2.3 训练硬件和碳足迹\u003c/h3\u003e\n\n\u003ch4 id=\"训练硬件training-hardware\"\u003e训练硬件（Training Hardware）\u003c/h4\u003e\n\n\u003cp\u003e我们在 Meta 的\u003cstrong\u003e\u003cmark\u003e超级集群\u003c/mark\u003e\u003c/strong\u003e（Research Super Cluster，RSC，Lee 和 Sengupta，2022）\n以及\u003cstrong\u003e\u003cmark\u003e内部生产集群\u003c/mark\u003e\u003c/strong\u003e上预训练 LLaMA2。\n两个集群 GPU 都是 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eNVIDIA A100\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e，网络也都是 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e200Gbps\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 互联，\n但互联方案和 GPU 最大功耗不同：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eRSC 集群：\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e200Gbps InfiniBand + 400W GPU\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e；\u003c/li\u003e\n  \u003cli\u003e生产集群：\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e200Gbps RoCE + 350W GPU\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e；RoCE 成本更低。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e结论：\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eRoCE + 350W GPU\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 的集群，经过优化的代码能达到\n\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eIB + 400W GPU\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 集群性能的 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e90%\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003ch4 id=\"预训练碳足迹carbon-footprint-of-pretraining\"\u003e预训练碳足迹（Carbon Footprint of Pretraining）\u003c/h4\u003e\n\n\u003cp\u003e根据之前的研究（Bender 等，2021a；Patterson 等，2021；Wu 等，2022；Dodge 等，2022），\n结合 GPU 设备的功耗估计以及碳效率，我们来计算 LLaMA2 预训练所产生的碳排放量。注意，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eGPU 的实际功耗取决于其利用率（util），我们估计 GPU 功耗使用的是热设计功率（\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eTDP\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e），二者可能会有所差异；\u003c/li\u003e\n  \u003cli\u003e我们的计算不考虑其他电力需求，例如互连、非 GPU 服务器功耗、数据中心制冷功耗等；\u003c/li\u003e\n  \u003cli\u003e与 AI 硬件（如 GPU）生产相关的碳排放量可能会增加整体碳足迹（Gupta 等，2022）。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e计算结果见表 2，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/llama2-paper/table-2.png\" width=\"60%\" height=\"60%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e表 2：预训练期间的 CO\u003csub\u003e2\u003c/sub\u003e 排放。Time: total GPU time required for training each model. Power\nConsumption: peak power capacity per GPU device for the GPUs used adjusted for power usage efficiency.\n100% of the emissions are directly offset by Meta’s sustainability program, and becausewe are openly releasing\nthese models, the pretraining costs do not need to be incurred by others.\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eA100-80GB\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e（400W/350W TDP）机器，总共耗费了 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e3.3M GPU-hour\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e；\u003c/li\u003e\n  \u003cli\u003e估算的总排放量为 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e539 tCO2eq\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e，可 100％ 由 \u003ca href=\"https://sustainability.fb.com/2021-sustainability-report/\"\u003eMeta 的可持续计划\u003c/a\u003e抵消；\u003c/li\u003e\n  \u003cli\u003eLLaMA2 的开源还意味着其他公司不需要承担这些预训练成本，节省了更多的全球资源。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 id=\"23-llama-2-预训练模型性能评估pretrained-model-evaluation\"\u003e2.3 LLaMA 2 预训练模型性能评估（Pretrained Model Evaluation）\u003c/h2\u003e\n\n\u003cp\u003e本节介绍在标准学术基准测试中，LLaMA 1/2 基础模型、MosaicML 预训练 transforer \n（\u003ca href=\"https://www.mosaicml.com/blog/mpt-7b\"\u003eMPT\u003c/a\u003e）及 Falcon（Almazrouei 等，2023）的结果。\n所有评估都使用我们的内部评估库。我们在内部重复了 MPT 和 Falcon 模型的结果。\n对于这些模型，始终选择我们评估框架和所有公开报告的结果中的最高分（the best score between our evaluation framework and\nany publicly reported results）。\u003c/p\u003e\n\n\u003cp\u003e基准测试分为以下几类（单个基准测试的结果见 A.2.2）：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e代码\u003c/mark\u003e\u003c/strong\u003e。LLaMA 在 HumanEval（Chen 等，2021）和 MBPP（Austin 等，2021）上的平均 pass@1 分数。\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e常识推理\u003c/mark\u003e\u003c/strong\u003e。PIQA（Bisk 等，2020）、SIQA（Sap 等，2019）、HellaSwag（Zellers 等，2019a）、WinoGrande（Sakaguchi 等，2021）、\nARC easy 和 challenge（Clark 等，2018）、OpenBookQA（Mihaylov 等，2018）和 CommonsenseQA（Talmor 等，2018）\n的平均分数。CommonSenseQA 的 7-shot 结果和其他所有基准测试的 0-shot 结果。\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e世界知识\u003c/mark\u003e\u003c/strong\u003e。评估了 NaturalQuestions（Kwiatkowski 等，2019）和 TriviaQA（Joshi 等，2017）的 5-shot 性能，并给出了平均分数。\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e阅读理解\u003c/mark\u003e\u003c/strong\u003e。在 SQuAD（Rajpurkar 等，2018）、QuAC（Choi 等，2018）和 BoolQ（Clark 等，2019）上的 0-shot 平均分数。\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e数学\u003c/mark\u003e\u003c/strong\u003e。GSM8K（8 shot）（Cobbe 等，2021）和 MATH（4 shot）（Hendrycks 等，2021）基准测试在 top 1 的平均分数。\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e聚合基准测试\u003c/mark\u003e\u003c/strong\u003e。MMLU（5 shot）（Hendrycks 等，2020）、Big Bench Hard（BBH）（3 shot）（Suzgun 等，2022）和 AGI Eval（3-5 shot）（Zhong 等，2023）的整体结果。\n对于 AGI Eval，只评估英文任务并给出平均分数。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 id=\"231-与开源基座大模型对比\"\u003e2.3.1 与开源基座大模型对比\u003c/h3\u003e\n\n\u003cp\u003e表 3 总结了一系列常见基准测试的整体性能。安全基准测试见 4.1 节中。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/llama2-paper/table-3.png\" width=\"80%\" height=\"80%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e表 3：\u003cmark\u003e与其他开源的基座大模型对比性能\u003c/mark\u003e，基于一些学术基准测试\u003c/p\u003e\n\n\u003cp\u003e可以看出，\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eLLaMA2 优于 LLaMA1；\u003c/li\u003e\n  \u003cli\u003e与 Llama 1 65B 相比，\u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eLLaMA2 70B\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 在 MMLU 和 BBH 上的结果分别提高了约 5 和 8 个百分点；\u003c/li\u003e\n  \u003cli\u003e除了 Code 基准测试，LLaMA2 7B 和 30B 模型在其他类别上都优于相应大小的 MPT 模型；\u003c/li\u003e\n  \u003cli\u003eLLaMA2 7B 和 34B 在所有基准测试类别上优于 Falcon 7B 和 40B 模型。\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003eLLaMA2 70B 模型优于所有开源模型\u003c/mark\u003e\u003c/strong\u003e。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch3 id=\"232-与闭源大模型对比\"\u003e2.3.2 与闭源大模型对比\u003c/h3\u003e\n\n\u003cp\u003e除了开源模型，我们还将 LLaMA2 70B 的结果与闭源模型进行了比较。如表 4 所示，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/llama2-paper/table-4.png\" width=\"80%\" height=\"80%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e表 4：基于学术基准测试，\u003cmark\u003e对比 LLaMA2 和闭源模型\u003c/mark\u003e。\nGPT-3.5/GPT-4 的结果来自 OpenAI (2023)；PaLM 的结果来自 Chowdhery et al. (2022)；\nPaLM-2-L 的结果来自 Anil et al. (2023). \u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eLLaMA2 70B 在 MMLU 和 GSM8K 上\u003cstrong\u003e\u003cmark\u003e与 GPT-3.5（OpenAI，2023）接近\u003c/mark\u003e\u003c/strong\u003e，但在编码基准测试上存在显著差距；\u003c/li\u003e\n  \u003cli\u003eLLaMA2 70B 与 PaLM（540B）（Chowdhery 等，2022）相当，甚至比后者更好；\u003c/li\u003e\n  \u003cli\u003eLLaMA2 70B \u003cstrong\u003e\u003cmark\u003e与 GPT-4/PaLM-2-L 仍存在较大差距\u003c/mark\u003e\u003c/strong\u003e。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e我们还分析了潜在的数据污染，详细信息见 A.6 节。\u003c/p\u003e\n\n\u003ch1 id=\"3-微调fine-tuning\"\u003e3 微调（Fine-tuning）\u003c/h1\u003e\n\n\u003cp\u003eLLaMA2-Chat 经过了几个月的对齐（alignment）迭代，\n包括指令微调（instruction tuning）和 RLHF，这些都\u003cstrong\u003e\u003cmark\u003e需要大量的计算和标注资源\u003c/mark\u003e\u003c/strong\u003e。\n本节介绍我们的一些实验及发现。\u003c/p\u003e\n\n\u003ch2 id=\"31-监督式微调sft\"\u003e3.1 监督式微调（SFT）\u003c/h2\u003e\n\n\u003ch3 id=\"311-使用公开的指令微调数据\"\u003e3.1.1 使用公开的指令微调数据\u003c/h3\u003e\n\n\u003cp\u003e与 Touvron 等人（2023）类似，我们使用了公开可用 instruction tuning 数据（Chung 等，2022）开始 SFT 阶段。\u003c/p\u003e\n\n\u003ch3 id=\"312-标注质量为王quality-is-all-you-need\"\u003e3.1.2 标注质量为王（Quality Is All You Need）\u003c/h3\u003e\n\n\u003cp\u003e还有一些不同来源的第三方 SFT 数据，但我们发现其中一些的\u003cstrong\u003e\u003cmark\u003e多样性和质量欠佳\u003c/mark\u003e\u003c/strong\u003e ——\n尤其是对于将 LLM 对齐到对话式（dialogue-style）指令时。\n因此，我们首先收集了数千个高质量的 SFT 数据示例，如表 5 所示，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/llama2-paper/table-5.png\" width=\"90%\" height=\"90%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e表 5：\u003cmark\u003eSFT annotation 示例\u003c/mark\u003e。分别展示了一个 helpfulness 和一个 safety annotation，其中的 \u003cmark\u003eprompt 和 answer 都是人（标注员）写的\u003c/mark\u003e。\u003c/p\u003e\n\n\u003cp\u003e这些标注数据是从我们的供应商获取的，我们发现\u003cstrong\u003e\u003cmark\u003e只需少量高质量 SFT 标注数据就能显著提升结果质量\u003c/mark\u003e\u003c/strong\u003e，\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e这与 Zhou 等人（2023）的发现类似，后者也发现只需要一小组干净的 instruction-tuning data 就足以获得高质量；\u003c/li\u003e\n  \u003cli\u003e根据我们的实际经验，\u003cstrong\u003e\u003cmark\u003e几万个 SFT 标注\u003c/mark\u003e\u003c/strong\u003e就足以实现高质量的结果；\n  因此，我们总共收集了 27,540 个 SFT annotation，没有再收集更多；请注意，我们 SFT annotations 没使用任何 Meta 用户数据；\u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003e我们还观察到，不同标注平台和供应商（annotation platforms and vendors）\n  可能导致明显不同的下游模型性能，这凸显了在使用供应商获取标注时数据检查的重要性。\u003c/p\u003e\n\n    \u003cp\u003e为了验证数据质量，我们仔细检查了一组 180 个示例，将人工提供的标注与模\n 型生成的进行了人工对比。令人惊讶的是，我们发现 SFT 之后模型的抽样输出（\n outputs sampled from the resulting SFT model）与人工标注员提供的 SFT 数据\n 质量相当，这表明我们可以调整优先级，将更多的标准精力投入到 preference-based\n annotation for RLHF。\u003c/p\u003e\n  \u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch3 id=\"313-一些微调细节fine-tuning-details\"\u003e3.1.3 一些微调细节（Fine-Tuning Details）\u003c/h3\u003e\n\n\u003cp\u003e对于监督微调，我们使用一个 cosine learning rate schedule，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e初始学习率为 2×10-5，\u003c/li\u003e\n  \u003cli\u003e权重衰减为 0.1，\u003c/li\u003e\n  \u003cli\u003ebatch size 64，\u003c/li\u003e\n  \u003cli\u003e序列长度为 4096 token。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e对于微调过程，每个样本由一个\u003cstrong\u003e\u003cmark\u003e提示（prompt）和一个回答（answer）\u003c/mark\u003e\u003c/strong\u003e组成。\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e为了确保模型序列长度正确填充（properly filled），我们将训练集中的所有提示和回答连接起来，\n然后使用一个特殊的 token 来分隔提示和回答段落。\u003c/li\u003e\n  \u003cli\u003e使用自回归目标，并 zero-out the loss on tokens from the user prompt，\n因此只在 answer token 上进行反向传播。\u003c/li\u003e\n  \u003cli\u003e最后，我们对模型进行 \u003cstrong\u003e\u003cmark\u003e2 个 epoch\u003c/mark\u003e\u003c/strong\u003e 的微调。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 id=\"32-基于人类反馈的强化学习rlhf\"\u003e3.2 基于人类反馈的强化学习（RLHF）\u003c/h2\u003e\n\n\u003cp\u003eRLHF 是一种模型训练过程（model training procedure），\u003cstrong\u003e\u003cmark\u003e应用在微调模型之上\u003c/mark\u003e\u003c/strong\u003e，\n使\u003cstrong\u003e\u003cmark\u003e模型行为与人类偏好和指令进一步对齐\u003c/mark\u003e\u003c/strong\u003e。\n给定两个模型的输出，\u003cstrong\u003e\u003cmark\u003e人类标注员\u003c/mark\u003e\u003c/strong\u003e选出他们更喜欢的那一个（打标），\n我们认为这样得到的结果代表了普遍的人类偏好。\n然后，拿这些人类反馈来训练一个\u003cstrong\u003e\u003cmark\u003e奖励模型\u003c/mark\u003e\u003c/strong\u003e，\n这个模型在学习完人类标注员的偏好模式之后，就可以自动做偏好决策了。\u003c/p\u003e\n\n\u003ch3 id=\"321-人类偏好数据收集\"\u003e3.2.1 人类偏好数据收集\u003c/h3\u003e\n\n\u003cp\u003e奖励建模需要收集人类偏好数据。\n我们选择了一种“二选一比较协议”（binary comparison protocol），主要是因为它能够最大化我们收集的 prompts 的多样性。\n其他策略也值得考虑，我们将留待未来的工作。\u003c/p\u003e\n\n\u003cp\u003e我们的\u003cstrong\u003e\u003cmark\u003e标注过程\u003c/mark\u003e\u003c/strong\u003e如下：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e标注员首先写一个提示，然后基于提供的判断标准，在两个 sampled model response 中选择一个好的；\u003c/li\u003e\n  \u003cli\u003e为了最大化多样性，这两个回答是从两个不同的 model variants 中抽样得到的，并且会改变温度超参数；\u003c/li\u003e\n  \u003cli\u003e除了二选一，我们还要求标注员标记他们的\u003cstrong\u003e\u003cmark\u003e喜欢程度\u003c/mark\u003e\u003c/strong\u003e：明显更好/更好/略微好/几乎无差别/不确定。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e对于偏好标注，我们关注的是\u003cstrong\u003e\u003cmark\u003e有用性和安全性\u003c/mark\u003e\u003c/strong\u003e（helpfulness and safety），\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e有用性指的是 LLaMA2-Chat 的回答\u003cstrong\u003e\u003cmark\u003e满足用户请求和提供所需信息\u003c/mark\u003e\u003c/strong\u003e的程度；\u003c/li\u003e\n  \u003cli\u003e安全性指的是 LLaMA2-Chat 的回答是否不安全，例如，\u003cstrong\u003e\u003cmark\u003e“列出制做炸弹的详细步骤”\u003c/mark\u003e\u003c/strong\u003e\n可能符合“有用性”标准，但根据我们的准则它不满足“安全性”。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e将这两者区分开，使我们能对二者分别应用具体的准则并更好地指导标注员。例如，\n常规指导原则之外，我们的安全标注（safety annotations）还提供了对 adversarial prompts 的指导。\u003c/p\u003e\n\n\u003cp\u003e除了标注指导原则的差异，我们在安全阶段还额外收集了一个安全标签。\n这个额外的信息将模型的回答分为三个类别：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e选中的回答是安全的，另一个回答不安全；\u003c/li\u003e\n  \u003cli\u003e两个回答都是安全的；\u003c/li\u003e\n  \u003cli\u003e两个回答都不安全。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e安全数据集中分别有 18%、47% 和 35% 的样本分布在这三个类别中。\n我们认为不存在“选中的回答不安全，而另一个回答安全”的场景，\n因为我们相信更安全的回答也会被人类认为更好/更受欢迎。\n关安全准则和更详细的安全标注信息，见 4.2.1。\u003c/p\u003e\n\n\u003cp\u003e人类标注是按每周级别（weekly）批次收集的。随着偏好数据的增多，奖励模型得到了改进，\n能够训练出质量越来越好的 LLaMA2-Chat 版本（见第 5 节，图 20 中的结果）。\nLLaMA2-Chat 的改进也使模型的\u003cstrong\u003e\u003cmark\u003e数据分布产生了漂移\u003c/mark\u003e\u003c/strong\u003e（shift）。\n如果不将这个新的数据分布输入奖励模型，它的准确性会迅速下降 ——\n例如，hyper-specialization (Scialom et al., 2020b) ，—— 因此在新一轮 LLaMA2-Chat 调优迭代之前，\n\u003cstrong\u003e\u003cmark\u003e使用最新的偏好数据迭代一次\u003c/mark\u003e\u003c/strong\u003e非常重要。\n这一步有助于\u003cstrong\u003e\u003cmark\u003e保持奖励模型的数据分布准确性\u003c/mark\u003e\u003c/strong\u003e，为最新模型提供准确的奖励。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/llama2-paper/table-6.png\" width=\"90%\" height=\"90%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e表 6：用于奖励模型的\u003cmark\u003e人类偏好数据统计\u003c/mark\u003e. We list both the open-source and\ninternally collected human preference data used for reward modeling. Note that a binary human preference\ncomparison contains 2 responses (chosen and rejected) sharing the same prompt (and previous dialogue).\nEach example consists of a prompt (including previous dialogue if available) and a response, which is the\ninput of the reward model. We report the number of comparisons, the average number of turns per dialogue,\nthe average number of tokens per example, per prompt and per response. More details on Meta helpfulness\nand safety data per batch can be found in Appendix A.3.1.\n\u003c/p\u003e\n\n\u003cp\u003e表 6 总结了我们的奖励模型数据信息，并与多个开源偏好数据集进行了对比，\n包括 Anthropic Helpful and Harmless（Bai 等，2022a），OpenAI Summarize（Stiennon 等，2020），\nOpenAI WebGPT（Nakano 等，2021），StackExchange（Lambert 等，2023），\nStanford Human Preferences（Ethayarajh 等，2022）和 Synthetic GPT-J（Havrilla）。\u003c/p\u003e\n\n\u003cp\u003e基于前面介绍的指导原则，我们收集的超过 100 万个 binary comparison，\n得到一个大型数据集，我们称之为 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eMeta reward modeling data\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e。\n请注意，根据 text domain 的不同，提示和回答中的 token 数量会不一样。\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e总结性文档（summarization）和在线论坛数据通常 prompt 更长，\u003c/li\u003e\n  \u003cli\u003e对话式 prompt 通常较短。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e与现有的开源数据集相比，我们的偏好数据具有更多的对话轮次，并且长度更长。\u003c/p\u003e\n\n\u003ch3 id=\"322-奖励建模reward-modeling\"\u003e3.2.2 奖励建模（Reward Modeling）\u003c/h3\u003e\n\n\u003cp\u003e奖励模型的工作机制：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e输入：模型的 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eresponse\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 及其相应的 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eprompt\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e（包括前几轮的上下文）；\u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003e输出：一个\u003cstrong\u003e\u003cmark\u003e标量分数\u003c/mark\u003e\u003c/strong\u003e，表示模型的\u003cstrong\u003e\u003cmark\u003e生成质量\u003c/mark\u003e\u003c/strong\u003e（例如有用性和安全性）。\u003c/p\u003e\n\n    \u003cp\u003e利用这些分数作为奖励，可以在 RLHF 期间优化 LLaMA2-Chat，实现更好的人类偏好对齐，改进有用性和安全性。\u003c/p\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e有人已经发现，有用性和安全性有时需要折中（Bai 等，2022a），这可能会使单个奖励模型在优化这两个方面时具有挑战性。\n为了解决这个问题，我们\u003cstrong\u003e\u003cmark\u003e训练了两个单独的奖励模型\u003c/mark\u003e\u003c/strong\u003e，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e一个针对有用性进行优化（称为 Helpfulness RM），\u003c/li\u003e\n  \u003cli\u003e一个针对安全性进行优化（Safety RM）。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e我们用预训练的 \u003cstrong\u003e\u003cmark\u003eLLaMA2-Chat checkpoint 初始化奖励模型\u003c/mark\u003e\u003c/strong\u003e，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e这使得两个模型都受益于预训练模型已学到的知识。简而言之，奖励模型“知道”聊天模型知道的所有内容。\u003c/li\u003e\n  \u003cli\u003e这可以防止两个模型信息不匹配，例如，这可能导致产生幻觉（hallucinations）。\u003c/li\u003e\n  \u003cli\u003e模型架构和超参数与预训练模型相同，只是用于预测下一个 token 的 classification head\n替换为一个 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003eregression head\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e，用于输出标量奖励。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch4 id=\"训练目标\"\u003e训练目标\u003c/h4\u003e\n\n\u003cp\u003e为了训练奖励模型，我们将收集的人类偏好数据转换为 binary ranking label 格式（即 chosen \u0026amp; rejected），\n并强制选中的响应有更高的分数。我们使用了与 Ouyang 等人（2022）一致的 binary ranking loss：\u003c/p\u003e\n\n\\[\\begin{equation}\n\\mathcal{L}_{\\text{ranking}} = -\\text{log}(\\sigma(r_\\theta(x,y_{c}) - r_\\theta(x,y_{r})))\n\\label{eq:rating_loss_default}\n\\end{equation}\\]\n\n\u003cp\u003ewhere $r_\\theta(x, y)$ is the scalar score output for prompt x and completion y with model weights θ. yc is the\npreferred response that annotators choose and yr is the rejected counterpart.\nBuilt on top of this binary ranking loss, we further modify it separately for better helpfulness and safety\nreward models as follows. Given that our preference ratings is decomposed as a scale of four points (e.g.,\nsignificantly better), as presented in Section 3.2.1, it can be useful to leverage this information to explicitly\nteach the reward model to assign more discrepant scores to the generations that have more differences. To\ndo so, we further add a margin component in the loss:\u003c/p\u003e\n\n\\[\\begin{equation}\n\\mathcal{L}_{\\text{ranking}} = -\\text{log}(\\sigma(r_\\theta(x,y_{c}) - r_\\theta(x,y_{r}) - m(r)))\n\\label{eq:rating_loss}\n\\end{equation}\\]\n\n\u003cp\u003ewhere the margin m(r) is a discrete function of the preference rating. Naturally, we use a large margin\nfor pairs with distinct responses, and a smaller one for those with similar responses (shown in Table 27).\nWe found this margin component can improve Helpfulness reward model accuracy especially on samples\nwhere two responses are more separable. More detailed ablation and analysis can be found in Table 28 in\nAppendix A.3.3.\u003c/p\u003e\n\n\u003ch4 id=\"data-composition\"\u003eData Composition\u003c/h4\u003e\n\n\u003cp\u003eWe combine our newly collected data with existing open-source preference datasets\nto form a larger training dataset. Initially, open-source datasets were used to bootstrap our reward models\nwhile we were in the process of collecting preference annotation data. We note that in the context of RLHF in\nthis study, the role of reward signals is to learn human preference for LLaMA2-Chat outputs rather than\nany model outputs. However, in our experiments, we do not observe negative transfer from the open-source\npreference datasets. Thus, we have decided to keep them in our data mixture, as they could enable better\ngeneralization for the reward model and prevent reward hacking, i.e. LLaMA2-Chat taking advantage of\nsome weaknesses of our reward, and so artificially inflating the score despite performing less well.\nWith training data available from different sources, we experimented with different mixing recipes for both\nHelpfulness and Safety reward models to ascertain the best settings. After extensive experimentation, the\nHelpfulness reward model is eventually trained on all Meta Helpfulness data, combined with an equal\nparts of the remaining data uniformly sampled from Meta Safety and from the open-source datasets. The\nMeta Safety reward model is trained on all Meta Safety and Anthropic Harmless data, mixed with Meta\nHelpfulness and open-source helpfulness data in a 90/10 proportion. We found that the setting with 10%\nhelpfulness data is especially beneficial for the accuracy on samples where both the chosen and rejected\nresponses were deemed safe.\u003c/p\u003e\n\n\u003ch4 id=\"训练细节training-details\"\u003e训练细节（Training Details）\u003c/h4\u003e\n\n\u003cp\u003eWe train for one epoch over the training data. In earlier experiments, we found that\ntraining longer can lead to over-fitting. We use the same optimizer parameters as for the base model. The\nmaximum learning rate is 5 × 10−6 for the 70B parameter LLaMA2-Chat and 1 × 10−5 for the rest. The\nlearning rate is decreased on a cosine learning rate schedule, down to 10% of the maximum learning rate.\nWe use a warm-up of 3% of the total number of steps, with a minimum of 5. The effective batch size is kept\nfixed at 512 pairs, or 1024 rows per batch.\u003c/p\u003e\n\n\u003ch4 id=\"奖励模型的结果reward-model-results\"\u003e奖励模型的结果（Reward Model Results）\u003c/h4\u003e\n\n\u003cp\u003eOn each batch of human preference annotation for reward modeling, we held out\n1000 examples as a test set to evaluate our models. We refer to the union of all prompts for the corresponding\ntest sets as “Meta Helpfulness” and “Meta Safety,” respectively.\u003c/p\u003e\n\n\u003cp\u003eAs reference points, we also evaluated other publicly available alternatives as baselines: SteamSHP-XL\n(Ethayarajh et al., 2022) based on FLAN-T5-xl, the Open Assistant reward model based on DeBERTa V3 Large\n(He et al., 2020), and GPT4 accessible through the OpenAI’s API. Note that at inference time, as opposed to\ntraining, all the reward models can predict a scalar for a single output, without requiring to access its paired\noutput. For GPT-4, we prompt with a zero-shot question “Choose the best answer between A and B,” where A\nand B are the two responses for comparison.\u003c/p\u003e\n\n\u003cp\u003eWe report the results in terms of accuracy in Table 7. As expected, our own reward models perform the best\non our internal test sets collected based on LLaMA2-Chat, with the Helpfulness reward model performing\nbest on the Meta Helpfulness test set, and similarly the Safety reward model performing best on the Meta\nSafety test set. Overall, our reward models outperform all of the baselines, including GPT-4. Interestingly,\nGPT-4 performs better than other non-Meta reward models, despite not being trained directly nor targeting\nspecifically this reward modeling task.\u003c/p\u003e\n\n\u003cp\u003eThe fact that helpfulness and safety performed the best on their own domain is potentially due to the tension\nbetween the two objectives (i.e., being as helpful as possible versus refusing unsafe prompts when necessary),\nwhich may confuse the reward model during training. In order for a single model to perform well on both\ndimensions, it needs to not only learn to select the better response given a prompt but also to distinguish\nadversarial prompts from safe ones. As a result, optimizing two separate models eases the reward modeling\ntask. More detailed analysis on this tension between safety and helpfulness can be found in Appendix A.4.1.\nWhen we group the scores by preference rating in Table 8, we can see that the accuracy is superior for the\n“significantly better” test set and degrades gradually as comparison pairs become more similar (e.g., “slightly\nbetter”). It is expected that learning to model human preferences becomes challenging when deciding\nbetween two similar model responses, due to annotator subjectivity and their reliance on nuanced details\nthat may differentiate responses. We emphasize that the accuracy on more distinct responses matters the\nmost to improve LLaMA2-Chat performance. The human preference annotation agreement rate is also higher\non more distinct responses than similar pairs.\u003c/p\u003e\n\n\u003ch4 id=\"scaling-trends\"\u003eScaling Trends\u003c/h4\u003e\n\n\u003cp\u003eWe study the scaling trends in terms of data and model size for the reward model, finetuning\ndifferent model sizes on an increasing amount of the reward model data collected each week (see the\ndetails on volume per batch in Table 26). Figure 6 reports these trends, showing the expected result that larger\nmodels obtain higher performance for a similar volume of data. More importantly, the scaling performance\nhas not yet plateaued given the existing volume of data annotation used for training, a signal that there is\nroom for more improvement with more annotations. We note that reward model accuracy is one of the most\nimportant proxies for the final performance of LLaMA2-Chat. While best practices for comprehensively\nevaluating a generative model is an open research question, the ranking task of the reward has no ambiguity.\nTherefore, everything else being equal, an improvement of the reward model can be directly translated into\nan improvement for LLaMA2-Chat.\u003c/p\u003e\n\n\u003ch3 id=\"323-iterative-fine-tuning\"\u003e3.2.3 Iterative Fine-Tuning\u003c/h3\u003e\n\n\u003cp\u003eAs we received more batches of human preference data annotation, we were able to train better reward\nmodels and collect more prompts. We therefore trained successive versions for RLHF models, referred to\nhere as RLHF-V1, . . . , RLHF-V5.\nWe explored RLHF fine-tuning with two main algorithms:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eProximal Policy Optimization (PPO) (Schulman et al., 2017), the standard in RLHF literature.\u003c/li\u003e\n  \u003cli\u003eRejection Sampling fine-tuning. We sample K outputs from the model and select the best candidate\nwith our reward, consistent with Bai et al. (2022b). The same re-ranking strategy for LLMs was also\nproposed in Deng et al. (2019), where the reward is seen as an energy function. Here, we go one step\nfurther, and use the selected outputs for a gradient update. For each prompt, the sample obtaining\nthe highest reward score is considered the new gold standard. Similar to Scialom et al. (2020a), we\nthen fine-tune our model on the new set of ranked samples, reinforcing the reward.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eThe two RL algorithms mainly differ in:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eBreadth — in Rejection Sampling, the model explores K samples for a given prompt, while only one generation is done for PPO.\u003c/li\u003e\n  \u003cli\u003eDepth — in PPO, during training at step t the sample is a function of the updated model policy from\nt − 1 after the gradient update of the previous step. In Rejection Sampling fine-tuning, we sample\nall the outputs given the initial policy of our model to collect a new dataset, before applying the\nfine-tuning similar to SFT. However, since we applied iterative model updates, the fundamental\ndifferences between the two RL algorithms are less pronounced.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eUntil RLHF (V4), we used only Rejection Sampling fine-tuning, and after that, we combined the two\nsequentially, applying PPO on top of the resulted Rejection Sampling checkpoint before sampling again.\u003c/p\u003e\n\n\u003ch4 id=\"rejection-sampling\"\u003eRejection Sampling\u003c/h4\u003e\n\n\u003cp\u003eWe perform rejection sampling only with our largest 70B LLaMA2-Chat. All smaller\nmodels are fine-tuned on rejection sampled data from the larger model, thus distilling the large-model\ncapabilities into the smaller ones. We leave further analysis of the effect of this distillation for future work.\u003c/p\u003e\n\n\u003cp\u003eAt each iterative stage, we sample K answers for each prompt from the most recent model. We score each\nsample given the best reward model accessible at the time of the experiment, and then select the best answer\nfor a given prompt. In earlier versions of our model, up to RLHF V3, our approach was to confine answer\nselection solely to the “bag” of samples gathered from the preceding iteration. For example, RLHF V3 was\ntrained using only samples from RLHF V2. However, despite continuous improvement, this method led to a\nregression in some capabilities. For example, RLHF V3 struggled more than previous versions to compose\nrhyming lines in poems, as discerned through qualitative analysis, suggesting that further investigation into\nthe causes of and mitigations for forgetting (Kirkpatrick et al., 2017; Nguyen et al., 2019; Ramasesh et al.,\n2021) could be a fruitful area for additional future research.\u003c/p\u003e\n\n\u003cp\u003eIn response, on subsequent iterations, we modified our strategy, incorporating top-performing samples from\nall prior iterations, such as those used in RLHF-V1 and RLHF-V2. Although we do not present specific\nfigures, this adjustment demonstrated considerable enhancements in performance and effectively addressed\nthe previously noted issues. This mitigation can be seen as analogous to Synnaeve et al. (2019) and Vinyals\net al. (2019) in the RL literature.\u003c/p\u003e\n\n\u003cp\u003eWe illustrate the benefit of Rejection Sampling in Figure 7. The delta between the maximum and median\ncurves can be interpreted as the potential gain of fine-tuning on the best output. As expected, this delta\nincreases with more samples, since the maximum increases (i.e., more samples, more opportunities to\ngenerate a good trajectory), while the median remains stationary. There is a direct connection between the\nexploration and the maximum reward we can obtain among the samples. The temperature parameter also\nplays an important role for exploration, as a higher temperature enables us to sample more diverse outputs.\nIn Figure 8, we report for a LLaMA2-Chat-SFT (left) and a LLaMA2-Chat-RLHF (right), the maximum\nreward curves among N samples (with N ∈ [1, . . . , 100]), for different temperatures. We can observe that\nthe optimal temperature is not constant during the iterative model updates: RLHF has a direct impact on\nrescaling the temperature. For LLaMA2-Chat-RLHF, the optimal temperature when sampling between 10\nand 100 outputs is T ∈ [1.2, 1.3]. Given a finite compute budget, it is therefore necessary to re-adjust the\ntemperature progressively. Note that this temperature rescaling happens for a constant number of steps for\neach model, and always starting from the base model on each new RLHF version.\u003c/p\u003e\n\n\u003ch4 id=\"ppo\"\u003ePPO\u003c/h4\u003e\n\n\u003cp\u003eWe further train our language model following the RL scheme of Stiennon et al. (2020), which uses the\nreward model as an estimate for the true reward function (human preference) and the pretrained language\nmodel as the policy to optimize. During this phase, we seek to optimize the following objective:\narg max\u003c/p\u003e\n\n\\[\\begin{equation}\n   \\arg \\max _\\pi \\mathbb{E}_{p \\sim \\mathcal{D}, g \\sim \\pi}[R(g \\mid p)]\n\\end{equation}\\]\n\n\u003cp\u003eWe iteratively improve the policy by sampling prompts p from our dataset D and generations g from the\npolicy π and use the PPO algorithm and loss function to achieve this objective.\nThe final reward function we use during optimization,\u003c/p\u003e\n\n\\[\\begin{equation}\n   R(g \\mid p) = \\tilde{R}_{c}(g \\mid p) - \\beta D_{KL}(\\pi_{\\theta}(g \\mid p) \\parallel \\pi_{0}(g \\mid p))\n\\end{equation}\\]\n\n\u003cp\u003econtains a penalty term for diverging from the original policy π0. As was observed in other works (Stiennon\net al., 2020; Ouyang et al., 2022), we find this constraint is useful for training stability, and to reduce reward\nhacking wherebywewould achieve high scores from the reward model but lowscores from human evaluation.\nWe define Rc to be a piecewise combination of the safety (Rs) and helpfulness (Rh) reward models. We\nhave tagged prompts in our dataset that might elicit potentially unsafe responses and prioritize the scores\nfrom the safety model. The threshold of 0.15 is chosen for filtering unsafe responses, corresponding to a\nprecision of 0.89 and a recall of 0.55 evaluated on the Meta Safety test set. We also find it important to whiten\nthe final linear scores (shown here by reversing the sigmoid with the logit function) in order to increase\nstability and balance properly with the KL penalty term (β) above.\u003c/p\u003e\n\n\u003cp\u003eFor all models, we use the AdamW optimizer (Loshchilov and Hutter, 2017), with β1 = 0.9, β2 = 0.95, eps =\n10−5. We use a weight decay of 0.1, gradient clipping of 1.0, and a constant learning rate of 10−6. For each\nPPO iteration we use a batch size of 512, a PPO clip threshold of 0.2, a mini-batch size of 64, and take one\ngradient step per mini-batch. For the 7B and 13B models, we set β = 0.01 (KL penalty), and for the 34B and\n70B models, we set β = 0.005.\u003c/p\u003e\n\n\u003cp\u003eWe train for between 200 and 400 iterations for all our models, and use evaluations on held-out prompts for\nearly stopping. Each iteration of PPO on the 70B model takes on average ≈ 330 seconds. To train quickly with\nlarge batch sizes, we use FSDP (Zhao et al., 2023). This was effective when using O(1) forward or backward\npasses, but caused a large slow down (≈ 20×) during generation, even when using a large batch size and KV\ncache. We were able to mitigate this by consolidating the model weights to each node once before generation\nand then freeing the memory after generation, resuming the rest of the training loop.\u003c/p\u003e\n\n\u003ch2 id=\"33-system-message-for-multi-turn-consistency\"\u003e3.3 System Message for Multi-Turn Consistency\u003c/h2\u003e\n\n\u003cp\u003eIn a dialogue setup, some instructions should apply for all the conversation turns, e.g., to respond succinctly,\nor to “act as” some public figure. When we provided such instructions to LLaMA2-Chat, the subsequent\nresponse should always respect the constraint. However, our initial RLHF models tended to forget the initial\ninstruction after a few turns of dialogue, as illustrated in Figure 9 (left).\u003c/p\u003e\n\n\u003cp\u003eTo address these limitations, we propose Ghost Attention (GAtt), a very simple method inspired by Context\nDistillation (Bai et al., 2022b) that hacks the fine-tuning data to help the attention focus in a multi-stage\nprocess. GAtt enables dialogue control over multiple turns, as illustrated in Figure 9 (right).\u003c/p\u003e\n\n\u003cp\u003eGAtt Method. Assume we have access to a multi-turn dialogue dataset between two persons (e.g., a user\nand an assistant), with a list of messages [u1, a1, . . . , un, an], where un and an correspond to the user and\nassistant messages for turn n, respectively. Then, we define an instruction, inst, that should be respected\nthroughout the dialogue. For example, inst could be “act as.” We can then synthetically concatenate this\ninstruction to all the user messages of the conversation.\u003c/p\u003e\n\n\u003cp\u003eNext, we can sample from this synthetic data using the latest RLHF model. We now have a context-dialogue\nand the sample with which to fine-tune a model, in a process analogous to Rejection Sampling. Instead of\naugmenting all context-dialogue turns with the instruction, we can drop it in all but the first turn, but this\nwould lead to a mismatch at training time between the system message, i.e., all the intermediate assistant\nmessages that come before the last turn, and our sample. To fix this issue, which could hurt the training, we\nsimply set the loss to 0 for all the tokens from the previous turns, including assistant messages.\u003c/p\u003e\n\n\u003cp\u003eFor the training instructions, we created a few synthetic constraints to sample from: Hobbies (“You enjoy\ne.g. Tennis”), Language (“Speak in e.g. French”), or Public Figure (“Act as e.g. Napoleon”). To obtain the lists\nof hobbies and public figures, we asked LLaMA2-Chat to generate it, avoiding a mismatch between the\ninstruction and model knowledge (e.g., asking the model to act as someone it had not encountered during\ntraining). To make the instructions more complex and diverse, we construct the final instruction by randomly\ncombining the above constraints. When constructing the final system message for the training data, we also\nmodify the original instruction half of the time to be less verbose, e.g., “Always act as Napoleon from now”-\u0026gt;\n”Figure: Napoleon.” These steps produce an SFT dataset, on which we can fine-tune LLaMA2-Chat.\u003c/p\u003e\n\n\u003cp\u003eGAtt Evaluation. We applied GAtt after RLHF V3. We report a quantitative analysis indicating that GAtt is\nconsistent up to 20+ turns, until the maximum context length is reached (see Appendix A.3.5). We tried to\nset constraints not present in the training of GAtt at inference time, for instance “Always answer with Haiku,”\nfor which the model remained consistent as illustrated in Appendix Figure 28.\u003c/p\u003e\n\n\u003cp\u003eTo illustrate how GAtt helped reshape attention during fine-tuning, we display the maximum attention\nactivations of the model in Figure 10. The left-hand side of each figure corresponds to the system message\n(“Act as OscarWilde”). We can see that the GAtt-equipped model (right) maintains large attention activations\nwith respect to the system message for a larger portion of the dialogue, as compared to the model without\nGAtt (left).\nDespite its utility, the current implementation of GAtt is vanilla, and more development and iteration on\nthis technique could likely further benefit the model. For instance, we could teach the model to change the\nsystem message during the conversation by integrating such data during fine-tuning.\u003c/p\u003e\n\n\u003ch2 id=\"34-rlhf-results\"\u003e3.4 RLHF Results\u003c/h2\u003e\n\n\u003ch3 id=\"341-model-based-evaluation\"\u003e3.4.1 Model-Based Evaluation\u003c/h3\u003e\n\n\u003cp\u003eEvaluating LLMs is a challenging open-research problem. Human evaluation, while a gold standard, can\nbe complicated by various HCI considerations (Clark et al., 2021; Gehrmann et al., 2023), and is not always\nscalable. Thus, to select the best-performing models among several ablations at each iteration from RLHF-V1\nto V5, we first observed the improvement of the rewards from the latest reward models, to save costs and\nincrease iteration speed. We later validated major model versions with human evaluations.\u003c/p\u003e\n\n\u003cp\u003eHow Far Can Model-Based Evaluation Go? To measure the robustness of our reward model, we collected\na test set of prompts for both helpfulness and safety, and asked three annotators to judge the quality of the\nanswers based on a 7-point Likert scale (the higher the better). We observe that our reward models overall\nare well calibrated with our human preference annotations, as illustrated in Figure 29 in the appendix. This\nconfirms the relevance of using our reward as a point-wise metric, despite being trained with a Pairwise\nRanking Loss.\u003c/p\u003e\n\n\u003cp\u003eStill, as Goodhart’s Law states, when a measure becomes a target, it ceases to be a good measure. To ensure\nour measure won’t diverge from the human preferences, we additionally used a more general reward, trained\non diverse open-source Reward Modeling datasets. We have not yet observed any such divergence, and\nhypothesize that iterative model updates may be helping to prevent this.\nAs a last verification step to ensure no regression between our new model and the previous one, we use both\nto sample during the next annotation iteration. This enables a model comparison “for free” on new prompts\nand can help to increase diversity when sampling.\u003c/p\u003e\n\n\u003cp\u003eProgression of Models. Figure 11 reports the progress of our different SFT and then RLHF versions for\nboth Safety and Helpfulness axes, measured by our in-house Safety and Helpfulness reward models. On\nthis set of evaluations, we outperform ChatGPT on both axes after RLHF-V3 (harmlessness and helpfulness\u003c/p\u003e\n\u003cblockquote\u003e\n  \u003cp\u003e50%). Despite the aforementioned relevance of using our reward as a point-wise metric, it can arguably be\nbiased in favor of LLaMA2-Chat. Therefore, for a fair comparison, we additionally compute the final results\nusing GPT-4 to assess which generation is preferred. The order in which ChatGPT and LLaMA2-Chat outputs\nappeared in GPT-4 prompt are randomly swapped to avoid any bias. As expected, the win-rate in favor of\nLLaMA2-Chat is less pronounced, although obtaining more than a 60% win-rate for our latest LLaMA2-Chat.\nThe prompts correspond to a validation set of 1, 586 and 584 prompts for safety and helpfulness, respectively.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch3 id=\"342-human-evaluation\"\u003e3.4.2 Human Evaluation\u003c/h3\u003e\n\n\u003cp\u003eHuman evaluation is often considered the gold standard for judging models for natural language generation,\nincluding dialogue models. To evaluate the quality of major model versions, we asked human evaluators to\nrate them on helpfulness and safety. We compare the LLaMA2-Chat models to open-source models (Falcon,\nMPT MosaicML NLP Team et al. (2023), Vicuna Chiang et al. (2023), as well as closed-source models (Chat-\nGPT (OpenAI, 2023) and PaLM Anil et al. (2023)) on over 4, 000 single and multi-turn prompts. For ChatGPT,\nwe use gpt-3.5-turbo-0301 model in all generations. For PaLM, we use the chat-bison-001 model in all\ngenerations. The final prompt count for human evaluations for each model is shown in Table 32. See more\nmethodology details in Appendix, Section A.3.7. The following section shows helpfulness results; safety\nresults are presented in Section 4.4.\u003c/p\u003e\n\n\u003cp\u003eResults. As shown in Figure 12, LLaMA2-Chat models outperform open-source models by a significant\nmargin on both single turn and multi-turn prompts. Particularly, LLaMA2-Chat 7B model outperforms\nMPT-7B-chat on 60% of the prompts. LLaMA2-Chat 34B has an overall win rate of more than 75% against\nequivalently sized Vicuna-33B and Falcon 40B models.\u003c/p\u003e\n\n\u003cp\u003eThe largest LLaMA2-Chat model is competitive with ChatGPT. LLaMA2-Chat 70B model has a win rate of\n36% and a tie rate of 31.5% relative to ChatGPT. LLaMA2-Chat 70B model outperforms PaLM-bison chat\nmodel by a large percentage on our prompt set. More results and analysis is available in Section A.3.7.\nInter-Rater Reliability (IRR). In our human evaluations, three different annotators provided independent\nassessments for each model generation comparison. High IRR scores (closer to 1.0) are typically seen as\nbetter from a data quality perspective, however, context is important. Highly subjective tasks like evaluating\nthe overall helpfulness of LLM generations will usually have lower IRR scores than more objective labelling\ntasks. There are relatively few public benchmarks for these contexts, so we feel sharing our analysis here will\nbenefit the research community.\u003c/p\u003e\n\n\u003cp\u003eWe used Gwet’s AC1/2 statistic (Gwet, 2008, 2014) to measure inter-rater reliability (IRR), as we found it to\nbe the most stable metric across different measurement scenarios. On the 7-point Likert scale helpfulness\ntask that is used in our analysis, Gwet’s AC2 score varies between 0.37 and 0.55 depending on the specific\nmodel comparison. We see scores on the lower end of that range for ratings from model comparisons with\nsimilar win rates to each other (like the LLaMA2-Chat-70B-chat vs. ChatGPT comparison). We see scores on\nthe higher end of that range for ratings from model comparisons with a more clear winner (like the Llama\n2-Chat-34b-chat vs. Falcon-40b-instruct).\u003c/p\u003e\n\n\u003cp\u003eLimitations of human evaluations. While our results indicate that LLaMA2-Chat is on par with ChatGPT\non human evaluations, it is important to note that human evaluations have several limitations.\u003c/p\u003e\n\u003cul\u003e\n  \u003cli\u003eBy academic and research standards, we have a large prompt set of 4k prompts. However, it does not cover real-world usage of these models, which will likely cover a significantly larger number of use cases.\u003c/li\u003e\n  \u003cli\u003eDiversity of the prompts could be another factor in our results. For example, our prompt set does not include any coding- or reasoning-related prompts.\u003c/li\u003e\n  \u003cli\u003eWe only evaluate the final generation of a multi-turn conversation. A more interesting evaluation could be to ask the models to complete a task and rate the overall experience with the model over multiple turns.\u003c/li\u003e\n  \u003cli\u003eHuman evaluation for generative models is inherently subjective and noisy. As a result, evaluation on a different set of prompts or with different instructions could result in different results.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch1 id=\"4-safety略\"\u003e4 Safety（略）\u003c/h1\u003e\n\n\u003ch1 id=\"5-讨论\"\u003e5 讨论\u003c/h1\u003e\n\n\u003ch2 id=\"51-新发现与评论learnings-and-observations\"\u003e5.1 新发现与评论（Learnings and Observations）\u003c/h2\u003e\n\n\u003cp\u003e我们的调优过程揭示了一些有趣的结果，比如 LLaMA2-Chat 在时间维度上组织知识的能力，以及调用外部工具 API 的能力。\u003c/p\u003e\n\n\u003ch3 id=\"超越人类监督从-sft-到-rlhf\"\u003e超越人类监督：从 SFT 到 RLHF\u003c/h3\u003e\n\n\u003cp\u003e在项目开始时，我们中的许多人都倾向于使用\u003cstrong\u003e\u003cmark\u003e有监督标注\u003c/mark\u003e\u003c/strong\u003e（supervised annotation），\nattracted by its denser signal。\n同时，\u003cstrong\u003e\u003cmark\u003e强化学习\u003c/mark\u003e\u003c/strong\u003e（reinforcement learning）的不稳定性已经众所周知，\n因此自然语言处理领域对其还是抱有一种怀疑态度。\n但事实证明强化学习非常有效，尤其是考虑到其\u003cstrong\u003e\u003cmark\u003e成本和时间效率\u003c/mark\u003e\u003c/strong\u003e。\n我们的研究结果认为，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003eRLHF 成功的关键\u003c/mark\u003e\u003c/strong\u003e是它在\n\u003cstrong\u003e\u003cmark\u003e标注过程中促进了人和 LLM 之间的协同\u003c/mark\u003e\u003c/strong\u003e（the synergy it fosters between humans and LLMs）。\u003c/li\u003e\n  \u003cli\u003e即使是熟练的标注员，每个人的\u003cstrong\u003e\u003cmark\u003e标注（书写）风格也存在显著差异\u003c/mark\u003e\u003c/strong\u003e。经过 SFT 标注微调出来的模型\n学习到了这种多样性 —— 不幸的是，这包括那些标注质量很差的长尾部分。\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e模型性能\u003c/mark\u003e\u003c/strong\u003e受限于\u003cstrong\u003e\u003cmark\u003e最熟练的标注员的能力\u003c/mark\u003e\u003c/strong\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e当比较两个回答哪个更好时，人类标注员的判断基本上都是一致的。\n因此，奖励机制迅速会将低分分配给质量差的尾部，并朝着人类偏好对齐。\n这一现象在图 20 中有所体现，可以看到经过几轮迭代，\u003cstrong\u003e\u003cmark\u003e最差的答案逐渐被移除，使分布向右移动\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/llama2-paper/fig-20.png\" width=\"90%\" height=\"90%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e图 20：随着 LLaMA2-Chat 的不断迭代（从 SFT 到 RLHF），\u003cmark\u003e奖励分布逐渐朝着高分漂移\u003c/mark\u003e（distribution shift）。\u003c/p\u003e\n\n\u003cp\u003e此外，在标注过程中，模型甚至有潜力探索超过最优秀的标注员的写作轨迹（venture into writing trajectories）。\n但当比较两个回答时，人类仍然可以提供有价值的反馈，超越他们自己的写作能力。\n类比一下就是：\u003cstrong\u003e\u003cmark\u003e虽然我们并不都是出色的艺术家，但欣赏和批评艺术的能力仍然是有的\u003c/mark\u003e\u003c/strong\u003e。\n我们认为，LLM 在某些任务中\u003cstrong\u003e\u003cmark\u003e超越人类标注员的卓越写作能力\u003c/mark\u003e\u003c/strong\u003e，\n\u003cstrong\u003e\u003cmark\u003e基本上是由 RLHF 驱动的\u003c/mark\u003e\u003c/strong\u003e，Gilardi 等人（2023）和 Huang 等人（2023）之前也已经提到这一点。\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cmark\u003eSupervised data 可能不再是黄金标准\u003c/mark\u003e\u003c/strong\u003e，这种演变迫使我们\u003cstrong\u003e\u003cmark\u003e重新评估“监督”（supervision）这一概念\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n\n\u003ch3 id=\"in-context-temperature-rescaling\"\u003eIn-Context Temperature Rescaling\u003c/h3\u003e\n\n\u003cp\u003e我们观察到一个 RLHF 相关的有趣现象，就目前所知，之前还没有文章提到这一点：\ndynamic re-scaling of temperature contingent upon the context。\n如图 8 所暗示，温度似乎受到 RLHF 的影响，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/llama2-paper/fig-8.png\" width=\"80%\" height=\"80%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e图 8：对输出抽样并用奖励模型对它们打分时，RLHF 对温度的影响。\u003c/p\u003e\n\n\u003cp\u003e有趣的是，我们的研究结果还揭示了这些分布漂移（shifts）对 prompts 的分布并非均匀的，\n如图 21 所示，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/llama2-paper/fig-21.png\" width=\"70%\" height=\"70%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e图 21：\u003cmark\u003eRLHF 学会了根据 prompt 类型适应温度\u003c/mark\u003e。\nLower Self-BLEU corresponds to more diversity: RLHF eliminates diversity in responses to factual prompts but retains more\ndiversity when generating responses to creative prompts. We prompt each model with a diverse set of\n10 creative and 10 factual instructions and sample 25 responses. This is repeated for the temperatures\nT ∈ {k/10 | k ∈ N : 1 ≤ k ≤ 15}. For each of the 25 responses we compute the Self-BLEU metric and report\nthe mean and standard deviation against the temperature.\n\u003c/p\u003e\n\n\u003cp\u003e例如，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\n    \u003cp\u003e左边是\u003cstrong\u003e\u003cmark\u003e基于事实信息的提示\u003c/mark\u003e\u003c/strong\u003e，如 “What is the capital of ?”，\u003c/p\u003e\n\n    \u003cp\u003eSelf-BLEU 斜率随时间减小。这表明虽然温度在上升，但模型学会了\u003cstrong\u003e\u003cmark\u003e对事实类 prompts 提供一致和相同的回答\u003c/mark\u003e\u003c/strong\u003e。\u003c/p\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003e右边是\u003cstrong\u003e\u003cmark\u003e创造力相关的提示\u003c/mark\u003e\u003c/strong\u003e，如 “Write a poem,”。\u003c/p\u003e\n\n    \u003cp\u003e多次 RLHF 迭代后，\u003cstrong\u003e\u003cmark\u003e温度的增加仍然会使产生的回答发生变化\u003c/mark\u003e\u003c/strong\u003e。\n  这可以在 Self-BLEU 斜率中观察到，它呈现出与 SFT 模型相似的模式。\u003c/p\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 id=\"llama2-chat-时间感知能力temporal-perception\"\u003eLLaMA2-Chat 时间感知能力（Temporal Perception）\u003c/h3\u003e\n\n\u003cp\u003eLLaMA2-Chat 展示了令人印象深刻的泛化或\u003cstrong\u003e\u003cmark\u003e推广能力\u003c/mark\u003e\u003c/strong\u003e（generalization ability），\n如下图所示（\u003cstrong\u003e\u003cmark\u003e注意每个对话的知识截止时间\u003c/mark\u003e\u003c/strong\u003e）：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/llama2-paper/fig-22.png\" width=\"95%\" height=\"95%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e图 22：\u003cmark\u003e时间感知能力\u003c/mark\u003e —— 使用了 1,000 SFT time-focused data，LLaMA2-Chat 学到了\u003cmark\u003e“时间”概念\u003c/mark\u003e。\u003c/p\u003e\n\n\u003cp\u003e为了在 LLaMA2-Chat 中灌输时间的概念，我们收集了与特定日期相关的 1,000 个 SFT 示例。\n这些示例包括诸如 “How long ago did Barack Obama become president?” 的问题。\n每个问题都与两个关键的元数据相关联：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e提问时的日期\u003c/mark\u003e\u003c/strong\u003e：这会影响回答；\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e事件日期\u003c/mark\u003e\u003c/strong\u003e：在此日期之前，该问题将毫无意义。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e我们手动测试了几十个示例，观察到即使只提供一份最小数据，\n我们的模型也表现出了\u003cstrong\u003e\u003cmark\u003e在时间上组织知识\u003c/mark\u003e\u003c/strong\u003e（organize its knowledge in a temporal manner）的强大能力。\n以上观察结果表明，\u003cstrong\u003e\u003cmark\u003e尽管 LLM 的训练仅涉及两方面\u003c/mark\u003e\u003c/strong\u003e：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e训练方式\u003c/mark\u003e\u003c/strong\u003e：预测下一个 token\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e\u003cmark\u003e训练数据\u003c/mark\u003e\u003c/strong\u003e：时间上随机和无序的数据\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e但它们\u003cstrong\u003e\u003cmark\u003e对时间的概念内化程度\u003c/mark\u003e\u003c/strong\u003e（internalized the concept of time）\n比先前预计的更高很多。\u003c/p\u003e\n\n\u003ch3 id=\"工具的使用\"\u003e工具的使用\u003c/h3\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cmark\u003eLLM 与工具的整合\u003c/mark\u003e\u003c/strong\u003e是一个正在发展壮大的研究领域（Mialon 等，2023）。\nToolformer（Schick 等，2023）设计的方法包括对数百万条轨迹进行采样，\n同时为每个工具制定一些 few-shot examples。\n然而，该技术仅适用于每个示例使用单个工具（using a single tool per example）的情况，\n无法扩展到连续使用多个工具的场景。\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cmark\u003eOpenAI\u003c/mark\u003e\u003c/strong\u003e 发布的\u003ca href=\"https://openai.com/blog/chatgpt-plugins\"\u003e插件\u003c/a\u003e引发了学术界的广泛讨论，引出了一些问题，例如：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e如何有效地教模型使用工具？\u003c/li\u003e\n  \u003cli\u003e这个过程是否需要大量的数据集？\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e我们的实验表明，在对齐过程中，大模型会自发地出现零样本方式。\n图 23 展示了一个例子，尽管我们从未明确标注过工具使用，但模型展示了在零样本环境中利用一系列工具的能力，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/llama2-paper/fig-23.png\" width=\"80%\" height=\"80%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e图 23：\u003cmark\u003eLLaMA2-Chat 涌现出的工具使用能力\u003c/mark\u003e。\n无需专门训练，\u003cmark\u003e仅通过语义（senmantics），它就能理解工具的用途和用法\u003c/mark\u003e。\n\u003c/p\u003e\n\n\u003cp\u003e此外，我们的研究还扩展到了 LLaMA2-Chat 能使用计算器之后的性能。结果见表 15，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/llama2-paper/table-15.png\" width=\"60%\" height=\"60%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003e图 15： Performance with tool use. Evaluation on the math datasets used in Toolformer. For different\nbaselines, we report the scores from Schick et al. (2023).\n\u003c/p\u003e\n\n\u003cp\u003eLLM 的工具使用虽然令人兴奋，但也\u003cstrong\u003e\u003cmark\u003e可能引发一些安全问题\u003c/mark\u003e\u003c/strong\u003e。\n我们鼓励在这个领域进行更多的社区研究和测试。\u003c/p\u003e\n\n\u003ch2 id=\"52-限制和伦理考虑\"\u003e5.2 限制和伦理考虑\u003c/h2\u003e\n\n\u003cp\u003eLLaMA2-Chat 与其他 LLM 类似，都有一些 well-recognized 的限制，\n包括\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e预训练后\u003cstrong\u003e\u003cmark\u003e知识停止更新\u003c/mark\u003e\u003c/strong\u003e；\u003c/li\u003e\n  \u003cli\u003e可能生成\u003cstrong\u003e\u003cmark\u003e非事实内容\u003c/mark\u003e\u003c/strong\u003e（non-factual generation），如不合格的建议；\u003c/li\u003e\n  \u003cli\u003e易于产生幻觉的倾向（propensity towards hallucinations）。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e此外，最初的 LLaMA2-Chat 版本主要集中在英语数据上。\n虽然我们的实验表明在其他语言上也已经获得了一定的熟练度，但这种熟练度仍然受限，\n主要是由于非英语的预训练数据有限（如表 10 所示）。\n因此，该模型\u003cstrong\u003e\u003cmark\u003e在英语以外的语言中性能仍然较弱\u003c/mark\u003e\u003c/strong\u003e，应谨慎使用。\u003c/p\u003e\n\n\u003cp\u003e与其他 LLM 一样，LLaMA2 可能会生成有害、冒犯或带有偏见的内容，\n因为它是基于公开可用的在线数据集训练的。\n我们尝试通过微调来减轻这个问题，但一些问题可能仍然存在，特别是在英语以外的语言中，\n因为这些语言的公开可用数据较少。\n随着在解决这些问题上的进展，我们将继续进行微调并发布更新版本。\u003c/p\u003e\n\n\u003cp\u003e坏人也可能使用 AI 模型，因此聊天式 AI agent 可能被用于恶意目的，\n如生成错误信息或获取关于生物恐怖主义或网络犯罪等主题的信息。\n我们已经努力调整模型，避免涉及这些主题，并减少其在这些用例中可能提供的能力。\u003c/p\u003e\n\n\u003cp\u003e虽然我们试图在安全性和有用性之间取得合理的平衡，但在某些情况下，我们的安全调整可能过于谨慎。\nLLaMA2-Chat 的用户可能会观察到\u003cstrong\u003e\u003cmark\u003e过于谨慎的处理方式\u003c/mark\u003e\u003c/strong\u003e，\n模型可能会拒绝某些请求，或拒绝回答某些安全细节问题。\n预训练模型的用户要格外谨慎，请按照我们的“负责任使用指南”中所述，采取额外的调优和部署。\u003c/p\u003e\n\n\u003ch2 id=\"53-负责任的发布策略responsible-release-strategy\"\u003e5.3 负责任的发布策略（Responsible Release Strategy）\u003c/h2\u003e\n\n\u003ch3 id=\"531-发布细节\"\u003e5.3.1 发布细节\u003c/h3\u003e\n\n\u003cp\u003eLLaMA2 允许用于\u003ca href=\"https://ai.meta.com/resources/models-and-libraries/llama/\"\u003e研究和商业用途\u003c/a\u003e。\n使用 LLaMA2 的人必须遵守其许可证和我们的 Acceptable Use Policy，禁止任何违反政策、法律、规则和法规的用途。\u003c/p\u003e\n\n\u003cp\u003e我们还提供了\u003ca href=\"https://github.com/facebookresearch/llama\"\u003e代码示例\u003c/a\u003e，以帮助开发者重复我们在 LLaMA2-Chat 中的 safe generations，\n以及在用户输入和模型输出层应用（apply）一些基础安全技术。\u003c/p\u003e\n\n\u003cp\u003e最后，我们提供了一份“负责任使用指南”（Responsible Use Guide），\n里面有关于安全开发和部署（safe development and deployment）的指导原则。\u003c/p\u003e\n\n\u003ch3 id=\"532-负责任的发布\"\u003e5.3.2 负责任的发布\u003c/h3\u003e\n\n\u003cp\u003e许多公司选择关起门来自己造 AI，但我们决定公开发布 LLaMA2，以鼓励负责任的人工智能创新。\n根据我们的经验，开放的方式能借助 AI 社区的集体智慧、多样性和创造力，对这项技术的普更有意义。\u003c/p\u003e\n\n\u003cp\u003e合作将使这些模型变得更好、更安全。整个人工智能社区 —— 学术研究人员、公民社会、决策者和产业界 ——\n必须共同努力，对当前人工智能系统的风险进行严格分析和曝光，并构建解决潜在滥用问题的解决方案。\n这种方法不仅促进了与大型科技公司之外的各方进行真正的合作，也是基础模型的获取更加民主化的基石。\n正如 Zellers 等人（2019b）提出的，开放发布促进了透明度，使更多人能够使用人工智能工具，\n实现了技术的民主化和人工智能专业知识的去中心化（democratizing the technology and decentralizing AI expertise）。\n我们相信，人工智能专业知识的去中心化不仅仅是知识的分发，它还能刺激创新，加速行业进步。\u003c/p\u003e\n\n\u003cp\u003e最后，公开发布这些模型可以整合成本，消除准入壁垒，使小型企业能够利用 LLM 的创新来探索和构建文本生成使用场景。\u003c/p\u003e\n\n\u003cp\u003e最终，我们相信这将为全球各种规模的组织创造一个更加公平的竞争环境，使大家都能从人工智能的进步带来的经济增长中受益。\u003c/p\u003e\n\n\u003cp\u003e我们知道，不是每个使用人工智能模型的人都有良好的意图；关于人工智能将如何影响我们的世界，人们也存在合理的担忧。\n有害内容生成（Toxic content generation）和有问题的关联（problematic associations）是人工智能社区尚未完全解决的重要风险。\n正如本文所指出，我们在限制这类响应的普遍性方面已经取得了进展。\n虽然我们认识到还有更多的工作要做，但这种认识只会加深我们对开放科学和人工智能社区合作的承诺。\u003c/p\u003e\n\n\u003ch1 id=\"6-相关工作\"\u003e6 相关工作\u003c/h1\u003e\n\n\u003ch2 id=\"61-large-language-models\"\u003e6.1 Large Language Models\u003c/h2\u003e\n\n\u003cp\u003eThe recent years have witnessed a substantial evolution in the field of LLMs.\nFollowing the scaling laws of Kaplan et al. (2020), several Large Language Models with more than 100B\nparameters have been proposed, from GPT-3 (Brown et al., 2020) to Gopher (Rae et al., 2022) or specialized\nmodels, e.g. Galactica, for science(Taylor et al., 2022). With 70B parameters, Chinchilla (Hoffmann et al.,\n2022) redefined those scaling laws towards the number of tokens rather than model weights. Notable in\nthis progression is the rise of Llama, recognized for its focus on computational efficiency during inference\n(Touvron et al., 2023). A parallel discourse has unfolded around the dynamics of open-source versus closedsource\nmodels. Open-source releases like BLOOM (Scao et al., 2022) and Falcon (Penedo et al., 2023) have\nrisen to challenge their closed-source counterparts like GPT-3 and Chinchilla. Yet, when it comes to the\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003ehttps://ai.meta.com/llama\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e“production-ready” LLMs such as ChatGPT, Bard, and Claude, there’s a marked distinction in performance\nand usability. These models rely on intricate tuning techniques to align with human preferences (Gudibande\net al., 2023), a process that is still being explored and refined within the open-source community.\nAttempts to close this gap have emerged, with distillation-based models such as Vicuna (Chiang et al., 2023)\nand Alpaca (Taori et al., 2023) adopting a unique approach to training with synthetic instructions (Honovich\net al., 2022; Wang et al., 2022). However, while these models show promise, they still fall short of the bar set\nby their closed-source counterparts.\u003c/p\u003e\n\n\u003ch2 id=\"62-instruction-tuning\"\u003e6.2 Instruction Tuning\u003c/h2\u003e\n\n\u003cp\u003eWei et al. (2021) obtained zero-shot performance on unseen tasks by fine-tuning LLMs\non numerous datasets. Chung et al. (2022) and Longpre et al. (2023) investigate the impact of instruction\ntuning as a function of number of tasks, model size, prompt settings, etc. Prompts used for instruction tuning\ncan be created by humans or by LLMs themselves (Zhou et al., 2022), and follow-up instructions can be used\nto refine initial generations to make them more useful, engaging, and unbiased (Ganguli et al., 2023; Madaan\net al., 2023). An approach related to instruction tuning is chain-of-thought prompting (Wei et al., 2022b), in\nwhich models are prompted to explain their reasoning when given a complex problem, in order to increase\nthe likelihood that their final answer is correct.\u003c/p\u003e\n\n\u003cp\u003eRLHF has emerged as a powerful strategy for fine-tuning Large Language Models, enabling significant\nimprovements in their performance (Christiano et al., 2017). The method, first showcased by Stiennon et al.\n(2020) in the context of text-summarization tasks, has since been extended to a range of other applications.\nIn this paradigm, models are fine-tuned based on feedback from human users, thus iteratively aligning the\nmodels’ responses more closely with human expectations and preferences.\u003c/p\u003e\n\n\u003cp\u003eOuyang et al. (2022) demonstrates that a combination of instruction fine-tuning and RLHF can help fix\nissues with factuality, toxicity, and helpfulness that cannot be remedied by simply scaling up LLMs. Bai\net al. (2022b) partially automates this fine-tuning-plus-RLHF approach by replacing the human-labeled\nfine-tuning data with the model’s own self-critiques and revisions, and by replacing human raters with a\nmodel when ranking model outputs in RLHF, a process known as “RL from AI Feedback” (RLAIF).\u003c/p\u003e\n\n\u003ch2 id=\"63-known-llm-safety-challenges\"\u003e6.3 Known LLM Safety Challenges\u003c/h2\u003e\n\n\u003cp\u003eRecent literature has extensively explored the risks and challenges linked\nwith Large Language Models. Bender et al. (2021b) and Weidinger et al. (2021) underscore various hazards\nlike bias, toxicity, private data leakage, and the potential for malicious uses. Solaiman et al. (2023) categorizes\nthese impacts into two groups—those that can be assessed within the base system and those requiring a\nsocietal context evaluation, while Kumar et al. (2022) offers potential mitigation strategies to curb harm.\nWork from Roller et al. (2020) and Dinan et al. (2021) also illuminates the difficulties tied to chatbot-oriented\nLLMs, with concerns ranging from privacy to misleading expertise claims. Deng et al. (2023) proposes\na taxonomic framework to tackle these issues, and Bergman et al. (2022) delves into the balance between\npotential positive and negative impacts from releasing dialogue models.\u003c/p\u003e\n\n\u003cp\u003eInvestigations into red teaming reveal specific challenges in tuned LLMs, with studies by Ganguli et al. (2022)\nand Zhuo et al. (2023) showcasing a variety of successful attack types and their effects on the generation of\nharmful content. National security agencies and various researchers, such as (Mialon et al., 2023), have also\nraised red flags around advanced emergent model behaviors, cyber threats, and potential misuse in areas like\nbiological warfare. Lastly, broader societal issues like job displacement due to accelerated AI research and an\nover-reliance on LLMs leading to training data degradation are also pertinent considerations (Acemoglu\nand Restrepo, 2018; Autor and Salomons, 2018;Webb, 2019; Shumailov et al., 2023). We are committed to\ncontinuing our work engaging with the broader policy, academic, and industry community on these issues.\u003c/p\u003e\n\n\u003ch1 id=\"7-总结\"\u003e7 总结\u003c/h1\u003e\n\n\u003cp\u003e本文介绍了 LLaMA2 —— 一组新的预训练和微调模型，参数规模从 7b 到 70b。\n实验结果表明尽管 LLaMA2 仍然落后于 GPT-4 等最先进的模型，\n这些与现有的开源聊天模型相比已经具有同等竞争力，在一些数据集上与某些专有模型的能力相当。\u003c/p\u003e\n\n\u003cp\u003e本文详细阐述了实现 LLaMA2 所用的方法和技术，并重点如何与 helpfulness and safety 原则对齐。\n为更有意义地为社会做出贡献并促进研究步伐，我们负责任地开放了 LLaMA2 和 LLaMA2-Chat。\n对透明度和安全性的持续承诺，将使我们在未来工作中进一步改进 LLaMA2-Chat。\u003c/p\u003e\n\n\u003ch1 id=\"参考文献略\"\u003e参考文献（略）\u003c/h1\u003e\n\n\u003ch1 id=\"附录略\"\u003e附录（略）\u003c/h1\u003e\n\n\n  \u003c!-- POST NAVIGATION --\u003e\n  \u003cdiv class=\"postNav clearfix\"\u003e\n     \n      \u003ca class=\"prev\" href=\"/blog/llm-practical-guide-zh/\"\u003e\u003cspan\u003e« [译][论文] 大语言模型（LLM）综述与实用指南（Amazon，2023）\u003c/span\u003e\n      \n    \u003c/a\u003e\n      \n      \n      \u003ca class=\"next\" href=\"/blog/understanding-gpu-performance/\"\u003e\u003cspan\u003eUnderstanding NVIDIA GPU Performance: Utilization vs. Saturation (2023) »\u003c/span\u003e\n       \n      \u003c/a\u003e\n     \n  \u003c/div\u003e\n\u003c/div\u003e",
  "Date": "2023-08-06T00:00:00Z",
  "Author": "Arthur Chiao"
}