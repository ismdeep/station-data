{
  "Source": "arthurchiao.art",
  "Title": "云计算时代携程的网络架构变迁（2019）",
  "Link": "https://arthurchiao.art/blog/ctrip-network-arch-evolution-zh/",
  "Content": "\u003cdiv class=\"post\"\u003e\n  \n  \u003ch1 class=\"postTitle\"\u003e云计算时代携程的网络架构变迁（2019）\u003c/h1\u003e\n  \u003cp class=\"meta\"\u003ePublished at 2019-04-27 | Last Update 2019-04-27\u003c/p\u003e\n  \n  \u003cp\u003eThis post also provides an \u003ca href=\"/blog/ctrip-network-arch-evolution/\"\u003eEnglish version\u003c/a\u003e.\u003c/p\u003e\n\n\u003ch3 id=\"前言\"\u003e前言\u003c/h3\u003e\n\n\u003cp\u003e本文来自我在 \u003ca href=\"https://www.bagevent.com/event/GOPS2019-shenzhen\"\u003eGOPS 2019 深圳站\u003c/a\u003e \n的分享，并对内容做了少量更新。\u003c/p\u003e\n\n\u003chr/\u003e\n\n\u003cul id=\"markdown-toc\"\u003e\n  \u003cli\u003e\u003ca href=\"#前言\" id=\"markdown-toc-前言\"\u003e前言\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#0-携程云平台简介\" id=\"markdown-toc-0-携程云平台简介\"\u003e0 携程云平台简介\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#网络演进时间线\" id=\"markdown-toc-网络演进时间线\"\u003e网络演进时间线\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#1-基于-vlan-的二层网络\" id=\"markdown-toc-1-基于-vlan-的二层网络\"\u003e1 基于 VLAN 的二层网络\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#11-需求\" id=\"markdown-toc-11-需求\"\u003e1.1 需求\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#12-解决方案openstack-provider-network-模型\" id=\"markdown-toc-12-解决方案openstack-provider-network-模型\"\u003e1.2 解决方案：OpenStack Provider Network 模型\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#13-硬件网络拓扑\" id=\"markdown-toc-13-硬件网络拓扑\"\u003e1.3 硬件网络拓扑\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#14-宿主机内部网络拓扑\" id=\"markdown-toc-14-宿主机内部网络拓扑\"\u003e1.4 宿主机内部网络拓扑\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#15-小结\" id=\"markdown-toc-15-小结\"\u003e1.5 小结\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#优点\" id=\"markdown-toc-优点\"\u003e优点\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#缺点\" id=\"markdown-toc-缺点\"\u003e缺点\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#2-基于-sdn-的大二层网络\" id=\"markdown-toc-2-基于-sdn-的大二层网络\"\u003e2 基于 SDN 的大二层网络\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#21-面临的新问题\" id=\"markdown-toc-21-面临的新问题\"\u003e2.1 面临的新问题\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#22-解决方案-openstack--sdn\" id=\"markdown-toc-22-解决方案-openstack--sdn\"\u003e2.2 解决方案: OpenStack + SDN\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#硬件拓扑\" id=\"markdown-toc-硬件拓扑\"\u003e硬件拓扑\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#sdn-控制平面和数据平面\" id=\"markdown-toc-sdn-控制平面和数据平面\"\u003eSDN: 控制平面和数据平面\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#sdn-组件和实现\" id=\"markdown-toc-sdn-组件和实现\"\u003eSDN: 组件和实现\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#23-软件硬件网络拓扑\" id=\"markdown-toc-23-软件硬件网络拓扑\"\u003e2.3 软件+硬件网络拓扑\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#24-创建实例涉及的网络配置流程\" id=\"markdown-toc-24-创建实例涉及的网络配置流程\"\u003e2.4 创建实例涉及的网络配置流程\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#25-小结\" id=\"markdown-toc-25-小结\"\u003e2.5 小结\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#硬件\" id=\"markdown-toc-硬件\"\u003e硬件\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#软件\" id=\"markdown-toc-软件\"\u003e软件\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#多租户\" id=\"markdown-toc-多租户\"\u003e多租户\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#3-容器和混合云网络\" id=\"markdown-toc-3-容器和混合云网络\"\u003e3 容器和混合云网络\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#31-私有云的-k8s-网络方案\" id=\"markdown-toc-31-私有云的-k8s-网络方案\"\u003e3.1 私有云的 K8S 网络方案\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#311-网络需求\" id=\"markdown-toc-311-网络需求\"\u003e3.1.1 网络需求\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#312-解决方案扩展现有-sdn-方案接入-mesosk8s\" id=\"markdown-toc-312-解决方案扩展现有-sdn-方案接入-mesosk8s\"\u003e3.1.2 解决方案：扩展现有 SDN 方案，接入 Mesos/K8S\u003c/a\u003e            \u003cul\u003e\n              \u003cli\u003e\u003ca href=\"#neutron-改动\" id=\"markdown-toc-neutron-改动\"\u003eNeutron 改动\u003c/a\u003e\u003c/li\u003e\n              \u003cli\u003e\u003ca href=\"#k8s-cni-for-neutron-插件\" id=\"markdown-toc-k8s-cni-for-neutron-插件\"\u003eK8S CNI for Neutron 插件\u003c/a\u003e\u003c/li\u003e\n              \u003cli\u003e\u003ca href=\"#基础网络服务升级\" id=\"markdown-toc-基础网络服务升级\"\u003e基础网络服务升级\u003c/a\u003e\u003c/li\u003e\n            \u003c/ul\u003e\n          \u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#313-容器漂移\" id=\"markdown-toc-313-容器漂移\"\u003e3.1.3 容器漂移\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#314-小结\" id=\"markdown-toc-314-小结\"\u003e3.1.4 小结\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#315-进一步演进方向\" id=\"markdown-toc-315-进一步演进方向\"\u003e3.1.5 进一步演进方向\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#32-公有云上的-k8s\" id=\"markdown-toc-32-公有云上的-k8s\"\u003e3.2 公有云上的 K8S\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#321-需求\" id=\"markdown-toc-321-需求\"\u003e3.2.1 需求\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#322-aws-上的-k8s-网络方案\" id=\"markdown-toc-322-aws-上的-k8s-网络方案\"\u003e3.2.2 AWS 上的 K8S 网络方案\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#323-全球-vpc-拓扑\" id=\"markdown-toc-323-全球-vpc-拓扑\"\u003e3.2.3 全球 VPC 拓扑\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#4-cloud-native-方案探索\" id=\"markdown-toc-4-cloud-native-方案探索\"\u003e4 Cloud Native 方案探索\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#41-cilium-overview\" id=\"markdown-toc-41-cilium-overview\"\u003e4.1 Cilium Overview\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#42-宿主机内部网络通信host-networking\" id=\"markdown-toc-42-宿主机内部网络通信host-networking\"\u003e4.2 宿主机内部网络通信（Host Networking）\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#43-跨宿主机网络通信multi-host-networking\" id=\"markdown-toc-43-跨宿主机网络通信multi-host-networking\"\u003e4.3 跨宿主机网络通信（Multi-Host Networking）\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#44-优劣势比较pros--cons\" id=\"markdown-toc-44-优劣势比较pros--cons\"\u003e4.4 优劣势比较（Pros \u0026amp; Cons）\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#pros\" id=\"markdown-toc-pros\"\u003ePros\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#cons\" id=\"markdown-toc-cons\"\u003eCons\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#references\" id=\"markdown-toc-references\"\u003eReferences\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003chr/\u003e\n\n\u003cp\u003e本文介绍云计算时代以来携程在私有云和公有云上的几代网络解决方案。希望这些内容可以\n给业内同行，尤其是那些设计和维护同等规模网络的团队提供一些参考。\u003c/p\u003e\n\n\u003cp\u003e本文将首先简单介绍携程的云平台，然后依次介绍我们经历过的几代网络模型：从传统的基\n于 VLAN 的二层网络，到基于 SDN 的大二层网络，再到容器和混合云场景下的网络，最后是\ncloud native 时代的一些探索。\u003c/p\u003e\n\n\u003ch1 id=\"0-携程云平台简介\"\u003e0 携程云平台简介\u003c/h1\u003e\n\n\u003cp\u003e携程 Cloud 团队成立于 2013 年左右，最初是基于 OpenStack 做私有云，后来又开发了自\n己的 baremetal（BM）系统，集成到 OpenStack，最近几年又陆续落地了 Mesos 和 K8S 平\n台，并接入了公有云。目前，我们已经将所有 cloud 服务打造成一个 \u003cstrong\u003eCDOS —\n携程数据中心操作系统\u003c/strong\u003e的混合云平台，统一管理我们在私有云和公有云上的计算、网络\n、存储资源。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/ctrip-net-evolution/1.jpg\" width=\"40%\" height=\"40%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 1. Ctrip Datacenter Operation System (CDOS)\u003c/p\u003e\n\n\u003cp\u003e在私有云上，我们有虚拟机、应用物理机和容器。在公有云上，接入了亚马逊、腾讯云、\nUCloud 等供应商，给应用部门提供虚拟机和容器。所有这些资源都通过 CDOS API 统一管\n理。\u003c/p\u003e\n\n\u003ch2 id=\"网络演进时间线\"\u003e网络演进时间线\u003c/h2\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/ctrip-net-evolution/2.jpg\" width=\"70%\" height=\"70%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 2. Timeline of the Network Architecture Evolution\u003c/p\u003e\n\n\u003cp\u003e图 2 我们网络架构演进的大致时间线。\u003c/p\u003e\n\n\u003cp\u003e最开始做 OpenStack 时采用的是简单的 VLAN 二层网络，硬件网络基于传统的三层网络模\n型。\u003c/p\u003e\n\n\u003cp\u003e随着网络规模的扩大，这种架构的不足逐渐显现出来。因此，在 2016 年自研了基于 SDN\n的大二层网络来解决面临的问题，其中硬件架构换成了 Spine-Leaf。\u003c/p\u003e\n\n\u003cp\u003e2017 年，我们开始在私有云和公有云上落地容器平台。在私有云上，对 SDN 方案进行了扩\n展和优化，接入了 Mesos 和 K8S 平台，单套网络方案同时管理了虚拟机、应用物理机和容\n器网络。公有云上也设计了自己的网络方案，打通了混合云。\u003c/p\u003e\n\n\u003cp\u003e最后是 2019 年，针对 Cloud Native 时代面临的一些新挑战，我们在调研一些新方案。\u003c/p\u003e\n\n\u003ch1 id=\"1-基于-vlan-的二层网络\"\u003e1 基于 VLAN 的二层网络\u003c/h1\u003e\n\n\u003cp\u003e2013 年我们开始基于 OpenStack 做私有云，给公司的业务部门提供虚拟机和应用物理机资\n源。\u003c/p\u003e\n\n\u003ch2 id=\"11-需求\"\u003e1.1 需求\u003c/h2\u003e\n\n\u003cp\u003e网络方面的需求有：\u003c/p\u003e\n\n\u003cp\u003e首先，\u003cstrong\u003e性能不能太差\u003c/strong\u003e，衡量指标包括 instance-to-instance 延迟、吞吐量等等。\u003c/p\u003e\n\n\u003cp\u003e第二，二层要有必要的隔离，防止二层网络的一些常见问题，例如广播泛洪。\u003c/p\u003e\n\n\u003cp\u003e第三，实例的 IP 要可路由，这点比较重要。这也决定了在宿主机内部不能使用隧道\n技术。\u003c/p\u003e\n\n\u003cp\u003e第四，安全的优先级可以稍微放低一点。如果可以通过牺牲一点安全性带来比较大的性能提\n升，在当时也是可以接受的。而且在私有云上，还是有其他方式可以弥补安全性的不足。\u003c/p\u003e\n\n\u003ch2 id=\"12-解决方案openstack-provider-network-模型\"\u003e1.2 解决方案：OpenStack Provider Network 模型\u003c/h2\u003e\n\n\u003cp\u003e经过一些调研，我们最后选择了 OpenStack \u003cstrong\u003e\u003cem\u003eprovider network\u003c/em\u003e\u003c/strong\u003e 模型 [1]。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/ctrip-net-evolution/3.jpg\" width=\"25%\" height=\"25%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 3. OpenStack Provider Network Model\u003c/p\u003e\n\n\u003cp\u003e如图 3 所示。宿主机内部走二层软交换，可以是 OVS、Linux Bridge、或者特定厂商的方案\n；宿主机外面，二层走交换，三层走路由，没有 overlay 封装。\u003c/p\u003e\n\n\u003cp\u003e这种方案有如下特点:\u003c/p\u003e\n\n\u003cp\u003e首先，租户网络的网关要配置在硬件设备上，因此需要硬件网络的配合，而并不是一个纯软\n件的方案；\u003c/p\u003e\n\n\u003cp\u003e第二，实例的 IP 是可路由的，不需要走隧道；\u003c/p\u003e\n\n\u003cp\u003e第三，和纯软件的方案相比，性能更好，因为不需要隧道的封装和解封装，而且跨网段的路\n由都是由硬件交换机完成的。\u003c/p\u003e\n\n\u003cp\u003e方案的一些其他方面：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e二层使用 VLAN 做隔离\u003c/li\u003e\n  \u003cli\u003eML2 选用 OVS，相应的 L2 agent 就是 neutron ovs agent\u003c/li\u003e\n  \u003cli\u003e因为网关在硬件交换机上，所以我们不需要 L3 agent（OpenStack 软件实现的虚拟路由器）来做路由转发\u003c/li\u003e\n  \u003cli\u003e不用 DHCP\u003c/li\u003e\n  \u003cli\u003e没有 floating ip 的需求\u003c/li\u003e\n  \u003cli\u003e出于性能考虑，我们去掉了 security group\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch2 id=\"13-硬件网络拓扑\"\u003e1.3 硬件网络拓扑\u003c/h2\u003e\n\n\u003cp\u003e图 4 是我们的物理网络拓扑，最下面是服务器机柜，上面的网络是典型的\u003cstrong\u003e接入-汇聚-核\n心\u003c/strong\u003e三层架构。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/ctrip-net-evolution/4.png\" width=\"60%\" height=\"60%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 4. Physical Network Topology in the Datacenter\u003c/p\u003e\n\n\u003cp\u003e特点：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e每个服务器两个物理网卡，直连到两个置顶交换机做物理高可用\u003c/li\u003e\n  \u003cli\u003e汇聚层和接入层走二层交换，和核心层走三层路由\u003c/li\u003e\n  \u003cli\u003e所有 OpenStack 网关配置在核心层路由器\u003c/li\u003e\n  \u003cli\u003e防火墙和核心路由器直连，做一些安全策略\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch2 id=\"14-宿主机内部网络拓扑\"\u003e1.4 宿主机内部网络拓扑\u003c/h2\u003e\n\n\u003cp\u003e再来看宿主机内部的网络拓扑。图 5 是一个计算节点内部的拓扑。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/ctrip-net-evolution/5.png\" width=\"75%\" height=\"75%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 5. Designed Virtual Network Topology within A Compute Node\u003c/p\u003e\n\n\u003cp\u003e特点：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e首先，在宿主机内有两个 OVS bridge：\u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebr-int\u003c/code\u003e 和 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebr-bond\u003c/code\u003e，两个 bridge 直连\u003c/li\u003e\n  \u003cli\u003e有两个物理网卡，通过 OVS 做 bond。宿主机的 IP 配置在 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebr-bond\u003c/code\u003e 上作为管理 IP\u003c/li\u003e\n  \u003cli\u003e所有实例连接到 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebr-int\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e图中的两个实例属于不同网段，这些标数字的（虚拟和物理）设备连接起来，就是\u003cstrong\u003e两个跨\n网段的实例之间\u003c/strong\u003e通信的路径：\u003ccode class=\"language-plaintext highlighter-rouge\"\u003einst1\u003c/code\u003e 出来的包经 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebr-int\u003c/code\u003e 到达 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebr-bond\u003c/code\u003e，再经物理\n网卡出宿主机，然后到达交换机，最后到达路由器（网关）；路由转发之后，包再经类似路\n径回到 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003einst2\u003c/code\u003e，总共是 18 跳。\u003c/p\u003e\n\n\u003cp\u003e作为对比，图 6 是原生的 OpenStack \u003cstrong\u003e\u003cem\u003eprovider network\u003c/em\u003e\u003c/strong\u003e 模型。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/ctrip-net-evolution/6.png\" width=\"75%\" height=\"75%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 6. Virtual Network Topology within A Compute Node in Legacy OpenStack\u003c/p\u003e\n\n\u003cp\u003e这里最大的区别就是每个实例和 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebr-int\u003c/code\u003e 之间都多出一个 Linux bridge。这是因为原生的\nOpenStack 支持通过 security group 对实例做安全策略，而 security group 底层是基于\niptables 的。OVS port 不支持 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eiptables\u003c/code\u003e 规则，而 Linux bridge port 支持，因此\nOpenStack 在每个实例和 OVS 之间都插入了一个 Linux Bridge。在这种情况下，\u003ccode class=\"language-plaintext highlighter-rouge\"\u003einst1\n-\u0026gt; inst2\u003c/code\u003e 总共是 24 跳，比刚才多出 6 跳。\u003c/p\u003e\n\n\u003ch2 id=\"15-小结\"\u003e1.5 小结\u003c/h2\u003e\n\n\u003cp\u003e最后总结一下我们第一代网络方案。\u003c/p\u003e\n\n\u003ch3 id=\"优点\"\u003e优点\u003c/h3\u003e\n\n\u003cp\u003e首先，我们去掉了一些不用的 OpenStack 组件，例如 L3 agent、HDCP agent, Neutron\nmeta agent 等等，简化了系统架构。对于一个刚开始做 OpenStack、经验还不是很丰\n富的团队来说，开发和运维成本比较低。\u003c/p\u003e\n\n\u003cp\u003e第二，上面已经看到，我们去掉了 Linux Bridge，简化了宿主机内部的网络拓扑，这使得\n转发路径更短，实例的网络延迟更低。\u003c/p\u003e\n\n\u003cp\u003e第三，网关在硬件设备上，和 OpenStack 的纯软件方案相比，性能更好。\u003c/p\u003e\n\n\u003cp\u003e第四，实例的 IP 可路由，给跟踪、监控等外围系统带来很大便利。\u003c/p\u003e\n\n\u003ch3 id=\"缺点\"\u003e缺点\u003c/h3\u003e\n\n\u003cp\u003e首先，去掉了 security group，没有了主机防火墙的功能，因此安全性变弱。我们通\n过硬件防火墙部分地补偿了这一问题。\u003c/p\u003e\n\n\u003cp\u003e第二，网络资源交付过程还没有完全自动化，并且存在较大的运维风险。\n\u003cstrong\u003e\u003cem\u003eprovider network\u003c/em\u003e\u003c/strong\u003e 模型要求网关配置在硬件设备上，在我们的方案中就是核心路由\n器上。因此，每次在 OpenStack 里创建或删除网络时，都需要手动去核心交换机上做配置\n。虽然说这种操作频率还是很低的，但操作核心路由器风险很大，核心发生故障会影响整张\n网络。\u003c/p\u003e\n\n\u003ch1 id=\"2-基于-sdn-的大二层网络\"\u003e2 基于 SDN 的大二层网络\u003c/h1\u003e\n\n\u003cp\u003e以上就是我们在云计算时代的第一代网络方案，设计上比较简单直接，相应地，功能也比较\n少。随着网络规模的扩大和近几年我们内部微服务化的推进，这套方案遇到了一些问题。\u003c/p\u003e\n\n\u003ch2 id=\"21-面临的新问题\"\u003e2.1 面临的新问题\u003c/h2\u003e\n\n\u003cp\u003e首先来自硬件。做\u003cstrong\u003e数据中心网络\u003c/strong\u003e的同学比较清楚，三层网络架构的可扩展性比较差，\n而且我们所有的 OpenStack 网关都配置在核心上，使得核心成为潜在的性能瓶颈，而核心\n挂了会影响整张网络。\u003c/p\u003e\n\n\u003cp\u003e第二，很大的 VLAN 网络内部的泛洪，以及 VLAN 最多只有 4096 个的限制。\u003c/p\u003e\n\n\u003cp\u003e第三，宿主机网卡比较旧，都是 1Gbps，也很容易达到瓶颈。\u003c/p\u003e\n\n\u003cp\u003e另外我们也有一些新的需求：\u003c/p\u003e\n\n\u003cp\u003e首先，携程在这期间收购了一些公司，会有将这些收购来的公司的网络与携程的网络打通的\n需求。在网络层面，我们想把它们当作租户对待，因此有多租户和 VPC 的需求。\u003c/p\u003e\n\n\u003cp\u003e另外，我们想让网络配置和网络资源交付更加自动化，减少运维成本与运维风险。\u003c/p\u003e\n\n\u003ch2 id=\"22-解决方案-openstack--sdn\"\u003e2.2 解决方案: OpenStack + SDN\u003c/h2\u003e\n\n\u003cp\u003e针对以上问题和需求，数据中心网络团队和我们 cloud 网络团队一起设计了第二代网络方\n案：一套\u003cstrong\u003e基于软件+硬件、OpenStack+SDN\u003c/strong\u003e 的方案，从二层网络演进到大二层网络。\u003c/p\u003e\n\n\u003ch3 id=\"硬件拓扑\"\u003e硬件拓扑\u003c/h3\u003e\n\n\u003cp\u003e在硬件拓扑上，从传统三层网络模型换成了近几年比较流行的 Spine-Leaf 架构，如图 7\n所示。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/ctrip-net-evolution/7.png\" width=\"70%\" height=\"70%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 7. Spine-Leaf Topology in the New Datacenter\u003c/p\u003e\n\n\u003cp\u003eSpine-Leaf 是 full-mesh 连接，它可以带来如下几个好处：\u003c/p\u003e\n\n\u003cp\u003e第一，转发路径更短。以图 7 的 Spine-Leaf（两级 Clos 架构）为例，任何两台服务器经\n过三跳（Leaf1 -\u0026gt; Spine -\u0026gt; Leaf2）就可以到达，延迟更低，并且可保障（可以按跳数精\n确计算出来）。\u003c/p\u003e\n\n\u003cp\u003e第二，水平可扩展性更好，任何一层有带宽或性能瓶颈，只需新加一台设备，然后跟另一层\n的所有设备直连。\u003c/p\u003e\n\n\u003cp\u003e第三，所有设备都是 active 的，一个设备挂掉之后，影响面比三层模型里挂掉一个设备小\n得多。\u003c/p\u003e\n\n\u003cp\u003e宿主机方面，我们升级到了 10G 和 25G 的网卡。\u003c/p\u003e\n\n\u003ch3 id=\"sdn-控制平面和数据平面\"\u003eSDN: 控制平面和数据平面\u003c/h3\u003e\n\n\u003cp\u003e数据平面基于 VxLAN，控制平面基于 MP-BGP EVPN 协议，在设备之间同步控制平面信息。\n网关是分布式的，每个 leaf 节点都是网关。VxLAN 和 MP-BGP EVPN 都是 RFC 标准协议，\n更多信息参考 [2]。\u003c/p\u003e\n\n\u003cp\u003eVxLAN 的封装和解封装都在 leaf 完成，leaf 以下是 VLAN 网络，以上是 VxLAN 网络。\u003c/p\u003e\n\n\u003cp\u003e另外，这套方案在物理上支持真正的租户隔离。\u003c/p\u003e\n\n\u003ch3 id=\"sdn-组件和实现\"\u003eSDN: 组件和实现\u003c/h3\u003e\n\n\u003cp\u003e开发集中在以下几个方面。\u003c/p\u003e\n\n\u003cp\u003e首先是自研了一个 SDN 控制器，称作\u003cstrong\u003e携程网络控制器\u003c/strong\u003e（Ctrip Network Controller）\n，缩写 CNC。CNC 是一个集中式控制器，管理网络内所有 spine 和 leaf 节点，通过\nneutron plugin 与 OpenStack Neutron 集成，能够动态向交换机下发配置。\u003c/p\u003e\n\n\u003cp\u003eNeutron 的主要改造：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e添加了 ML2 和 L3 两个 plugin 与 CNC 集成\u003c/li\u003e\n  \u003cli\u003e设计了新的 port 状态机，因为原来的 port 只对 underlay 进行了建模，我们现在有\nunderlay 和 overlay 两个平面\u003c/li\u003e\n  \u003cli\u003e添加了一下新的 API，用于和 CNC 交互\u003c/li\u003e\n  \u003cli\u003e扩展了一些表结构等等\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e图 8 就是我们对 neutron port 状态的一个监控。如果一个 IP（port）不通，我们很容易\n从它的状态看出问题是出在 underlay 还是 overlay。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/ctrip-net-evolution/8.png\" width=\"40%\" height=\"40%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 8. Monitoring Panel for Neutron Port States\u003c/p\u003e\n\n\u003ch2 id=\"23-软件硬件网络拓扑\"\u003e2.3 软件+硬件网络拓扑\u003c/h2\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/ctrip-net-evolution/9.png\" width=\"90%\" height=\"90%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 9. HW + SW Topology of the Designed SDN Solution\u003c/p\u003e\n\n\u003cp\u003e图 9 是我们软件+硬件的网络拓扑：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e以 leaf 为边界，leaf 以下是 underlay，走 VLAN；上面 overlay，走 VxLAN\u003c/li\u003e\n  \u003cli\u003eunderlay 由 neutron、OVS 和 neutron OVS agent 控制；overlay 是 CNC 控制\u003c/li\u003e\n  \u003cli\u003eNeutron 和 CNC 之间通过 plugin 集成\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch2 id=\"24-创建实例涉及的网络配置流程\"\u003e2.4 创建实例涉及的网络配置流程\u003c/h2\u003e\n\n\u003cp\u003e这里简单来看一下创建一个实例后，它的网络是怎么通的。图 10 中黑色的线是 OpenStack\n原有的逻辑，蓝色的是我们新加的。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/ctrip-net-evolution/10.png\" width=\"90%\" height=\"90%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 10. Flow of Spawn An Instance\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e控制节点：从 nova 发起一个创建实例请求，指定从哪个网络分配 IP 给这个实例。\nnova 调度器将任务调度到某台计算节点\u003c/li\u003e\n  \u003cli\u003e计算节点：nova compute 开始创建实例，其中一步是向 neutron 发起一个 create\nport 请求，简单认为就是申请一个 IP 地址\u003c/li\u003e\n  \u003cli\u003eNeutron Server: neutron 创建一个 port，然后经 create port 的 postcommit 方法\n到达 CNC ML2 plugin；plugin 进一步将 port 信息同步给 CNC，CNC 将其存储到自己\n的 DB\u003c/li\u003e\n  \u003cli\u003e计算节点：port 信息从 neutron server 返回给 nova-compute\u003c/li\u003e\n  \u003cli\u003e计算节点：nova-compute 拿到 port 信息，为实例创建虚拟网卡，配置 IP 地址等参数\n，并将其 attach 到 OVS\u003c/li\u003e\n  \u003cli\u003e计算节点：ovs agent 检测到新的 device 后，就会为这个 device 配置 OVS，添加\nflow 等，\u003cstrong\u003e这时候 underlay 就通了\u003c/strong\u003e，它会将 underlay 状态上报给 neutron\nserver\u003c/li\u003e\n  \u003cli\u003e计算节点：nova-compute 做完网络配置后，会发送一个 update port 消息给 neutron\nserver，其中带着 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ehost_id\u003c/code\u003e 信息，表示这个 port 现在在哪台计算节点上\u003c/li\u003e\n  \u003cli\u003eNeutron Server: 请求会通过 postcommit 发给 CNC\u003c/li\u003e\n  \u003cli\u003eCNC：CNC 根据 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ehost_id\u003c/code\u003e 找到这台计算节点所连接的 leaf 的端口，然后向这些端口\n动态下发配置，\u003cstrong\u003e这时候 overlay 就通了\u003c/strong\u003e，最后将 overlay 状态上报给 neutron\nserver\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e在我们的系统里看，这时 port 就是一个 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eACTIVE_ACTIVE\u003c/code\u003e 的状态，表示 underlay 和\noverlay 配置都是正常的，网络应该是通的。\u003c/p\u003e\n\n\u003ch2 id=\"25-小结\"\u003e2.5 小结\u003c/h2\u003e\n\n\u003cp\u003e总结一下我们这套方案。\u003c/p\u003e\n\n\u003ch3 id=\"硬件\"\u003e硬件\u003c/h3\u003e\n\n\u003cp\u003e首先，我们从三层网络架构演进到 Spine-Leaf 两级架构。Spine-Leaf 的 full-mesh 使得\n服务器之间延迟更低、容错性更好、更易水平扩展。另外，Spine-Leaf 还支持分布式网\n关，缓解了集中式网关的性能瓶颈和单点问题。\u003c/p\u003e\n\n\u003ch3 id=\"软件\"\u003e软件\u003c/h3\u003e\n\n\u003cp\u003e自研 SDN 控制器并与 OpenStack 集成，实现了网络的动态配置。\u003c/p\u003e\n\n\u003cp\u003e这套方案同时支持虚拟机和应用物理机部署系统，限于篇幅这里只介绍了虚拟机。\u003c/p\u003e\n\n\u003ch3 id=\"多租户\"\u003e多租户\u003c/h3\u003e\n\n\u003cp\u003e有硬多租户（hard-multitenancy）支持能力。\u003c/p\u003e\n\n\u003ch1 id=\"3-容器和混合云网络\"\u003e3 容器和混合云网络\u003c/h1\u003e\n\n\u003cp\u003e以上方案最开始还是针对虚拟机和应用物理机设计的。到了 2017 年，我们开始在私有云和\n公有云上落地容器平台，将一部分应用从虚拟机或应用物理机迁移到容器。\u003c/p\u003e\n\n\u003cp\u003e容器平台（K8S、Mesos 等）有不同于虚拟机平台的一些特点，例如：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e实例的规模很大，单个集群 10k～100k 个容器是很常见的\u003c/li\u003e\n  \u003cli\u003e很高的发布频率，实例会频繁地创建和销毁\u003c/li\u003e\n  \u003cli\u003e实例创建和销毁时间很短，比传统的虚拟机低至少一个数量级\u003c/li\u003e\n  \u003cli\u003e容器的失败是很常见，总会因为各种各样的原因导致容器挂掉。容器编排引擎在设计的\n时候已经把失败当做预期情况处理，例如将挂掉的容器在本机或其他宿主机再拉起来，\n后者就是一次漂移\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch2 id=\"31-私有云的-k8s-网络方案\"\u003e3.1 私有云的 K8S 网络方案\u003c/h2\u003e\n\n\u003cp\u003e容器平台的这些特点对网络提出了新的需求。\u003c/p\u003e\n\n\u003ch3 id=\"311-网络需求\"\u003e3.1.1 网络需求\u003c/h3\u003e\n\n\u003cp\u003e首先，网络服务的 API 必须要快，而且支持较大的并发。\u003c/p\u003e\n\n\u003cp\u003e第二，不管是 agent 还是可执行文件，为容器创建和删除网络（虚拟网络及相应配置）也要快。\u003c/p\u003e\n\n\u003cp\u003e最后是一个工程问题：\u003cstrong\u003e新系统要想快速落地，就必须与很多线上系统保持前向兼容\u003c/strong\u003e。这\n给我们网络提出一个需求就是：\u003cstrong\u003e容器漂移时，IP 要保持不变\u003c/strong\u003e。因为 OpenStack 时代，\n虚拟机迁移 IP 是不变的，所以很多外围系统都认为 IP 是实例生命周期中的一个不变量，\n如果我们突然要改成 IP 可变，就会涉及大量的外围系统（例如 SOA）改造，这其中很多不\n是我们所能控制的。因此为了实现容器的快速落地，就必须考虑这个需求。而流行的 K8S\n网络方案都是无法支持这个功能的，因为在容器平台的设计哲学里，IP 地址已经是一个被\n弱化的概念，用户更\u003cstrong\u003e应该关心的是实例暴露的服务，而不是 IP\u003c/strong\u003e。\u003c/p\u003e\n\n\u003ch3 id=\"312-解决方案扩展现有-sdn-方案接入-mesosk8s\"\u003e3.1.2 解决方案：扩展现有 SDN 方案，接入 Mesos/K8S\u003c/h3\u003e\n\n\u003cp\u003e在私有云中，我们最终决定对现有的为虚拟机和应用物理机设计的 SDN 方案进行扩展，将\n容器网络也统一由 Neutron/CNC 管理。具体来说，会复用已有的网络基础设施，包括\nNeutron、CNC、OVS、Neutron-OVS-Agent 等待，然后开发一个针对 Neutron 的 CNI 插件\n（对于 K8S）。\u003c/p\u003e\n\n\u003cp\u003e一些核心改动或优化如下。\u003c/p\u003e\n\n\u003ch4 id=\"neutron-改动\"\u003eNeutron 改动\u003c/h4\u003e\n\n\u003cp\u003e首先是增加了一些新的 API。比如，原来的 neutron 是按 network id 分配 IP，我们给\nnetwork 添加了 label 属性，相同 label 的 network 我们认为是无差别的。这样，CNI 申\n请 IP 的时候，只需要说\u003cstrong\u003e“我需要一个 ‘prod-env’ 网段的 IP”\u003c/strong\u003e，neutron 就会从打了“\nprod-env” label 的 network 中任选一个还没用完的，从中分一个 IP。这样既将外部系统\n与 OpenStack 细节解耦，又提高了可扩展性，因为一个 label 可以对应任意多个 network\n。\u003c/p\u003e\n\n\u003cp\u003e另外做了一些性能优化，例如增加批量分配 IP 接口、API 异步化、数据库操作优化等待。\u003c/p\u003e\n\n\u003cp\u003e还有就是 backport 一些新 feature 到 neutron，我们的 OpenStack 已经不随社区一起升\n级了，都是按需 backport。例如，其中一个对运维和 trouble shooting 非常友好的功能\n是 Graceful OVS agent restart。\u003c/p\u003e\n\n\u003ch4 id=\"k8s-cni-for-neutron-插件\"\u003eK8S CNI for Neutron 插件\u003c/h4\u003e\n\n\u003cp\u003e开发了一个 CNI plugin 对接 neutron。CNI 插件的功能比较常规：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e为容器创建 veth pairt，并 attach 到 OVS\u003c/li\u003e\n  \u003cli\u003e为容器配置 MAC、IP、路由等信息\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e但有两个特殊之处：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e向 neutron（global IPAM）申请分配和释放 IP，而不是宿主机的本地服务分配（local IPAM）\u003c/li\u003e\n  \u003cli\u003e将 port 信息更新到 neutron server\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch4 id=\"基础网络服务升级\"\u003e基础网络服务升级\u003c/h4\u003e\n\n\u003cp\u003e另外进行了一些基础架构的升级，比如 OVS 在过去几年的使用过程中发现老版本的几个\nbug，后来发现这几个问题在社区也是有记录的：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003evswitchd\u003c/code\u003e CPU 100% [3]\u003c/li\u003e\n  \u003cli\u003e流量镜像丢包 [4]\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e注意到最新的 LTS 版本已经解决了这些问题，因此我们将 OVS 升级到了最新的 LTS。\n大家如果有遇到类似问题，可以参考 [3, 4]。\u003c/p\u003e\n\n\u003ch3 id=\"313-容器漂移\"\u003e3.1.3 容器漂移\u003c/h3\u003e\n\n\u003cp\u003e创建一个容器后，容器网络配置的流程和图 10 类似，Nova 和 K8S 只需做如下的组件对应：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003enova\u003c/code\u003e -\u0026gt; \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ekube master\u003c/code\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003enova-compute -\u0026gt; kubelet\u003c/code\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003enova network driver -\u0026gt; CNI\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e其流程不再详细介绍。这里重点介绍一下容器漂移时 IP 是如何保持不变的。\u003c/p\u003e\n\n\u003cp\u003e如图 11 所示，保持 IP 不变的关键是：CNI 插件能够根据容器的 labels 推导出 port\nname，然后拿 name 去 neutron 里获取 port 详细信息。port name 是唯一的，这个是我\n们改过的，原生的 OpenStack 并不唯一。\u003c/p\u003e\n\n\u003cp\u003e第二个宿主机的 CNI plugin 会根据 name 找到 port 信息，配置完成后，会将新的\n\u003ccode class=\"language-plaintext highlighter-rouge\"\u003ehost_id\u003c/code\u003e 更新到 netron server；neutron 通知到 CNC，CNC 去原来的交换机上删除配置\n，并向新的交换机下发配置。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/ctrip-net-evolution/11.png\" width=\"80%\" height=\"80%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 11. Pod drifting with the same IP within a K8S cluster \u003c/p\u003e\n\n\u003ch3 id=\"314-小结\"\u003e3.1.4 小结\u003c/h3\u003e\n\n\u003cp\u003e简单总结一下：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e在保持基础设施不变的情况下，我们快速地将容器平台的网络接入到已有系统\u003c/li\u003e\n  \u003cli\u003e一个 IPAM 同时管理了虚拟机、应用物理机和容器的网络\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e目前这套新方案的部署规模：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e4 个可用区\u003c/li\u003e\n  \u003cli\u003e最大的可用区有超过 500 个节点（VM/BM/Container 宿主机），其中主要是 K8S 节点\u003c/li\u003e\n  \u003cli\u003e单个 K8S 节点最多会有 500+ pod（测试环境的超分比较高）\u003c/li\u003e\n  \u003cli\u003e最大的可用区有 2+ 万个实例，其中主要是容器实例\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch3 id=\"315-进一步演进方向\"\u003e3.1.5 进一步演进方向\u003c/h3\u003e\n\n\u003cp\u003e以上就是到目前为止我们私有云上的网络方案演讲，下面这张图是我们希望将来能达到的一\n个架构。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/ctrip-net-evolution/12.png\" width=\"85%\" height=\"85%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 12. Layered view of the future network architecture\u003c/p\u003e\n\n\u003cp\u003e首先会有 underlay 和 overlay 两个平面。underlay 部署各种基础设施，包括 Openstack\n控制器、计算节点、SDN 控制器等，以及各种需要运行在 underlay 的物理设备；\n在 overlay 创建 VPC，在 VPC 里部署虚拟机、应用物理机实例等。\u003c/p\u003e\n\n\u003cp\u003e在 VPC 内创建 K8S 集群，单个 K8S 集群只会属于一个 VPC，所有跨 K8S 集群的访问都走\n服务接口，例如 Ingress，现在我们还没有做到这一步，因为涉及到很多老环境的软件和硬\n件改造。\u003c/p\u003e\n\n\u003ch2 id=\"32-公有云上的-k8s\"\u003e3.2 公有云上的 K8S\u003c/h2\u003e\n\n\u003cp\u003e接下来看一下我们在公有云上的网络。\u003c/p\u003e\n\n\u003ch3 id=\"321-需求\"\u003e3.2.1 需求\u003c/h3\u003e\n\n\u003cp\u003e随着携程国际化战略的开展，我们需要具备在海外部署应用的能力。自建数据中心肯定是来\n不及的，因此我们选择在公有云上购买虚拟机或 baremetal 机器，并搭建和维护自己的\nK8S 集群（非厂商托管方案，例如 AWS EKS [10]）。\n在外层，我们通过 CDOS API 封装不同厂商的差异，给应用部门提供统一的接口。这样，我\n们的私有云演进到了混合云的阶段。\u003c/p\u003e\n\n\u003cp\u003e网络方面主要涉及两方面工作：一是 K8S 的网络方案，这个可能会因厂商而已，因为不同\n厂商提供的网络模型和功能可能不同；二是打通私有云和公有云。\u003c/p\u003e\n\n\u003ch3 id=\"322-aws-上的-k8s-网络方案\"\u003e3.2.2 AWS 上的 K8S 网络方案\u003c/h3\u003e\n\n\u003cp\u003e以 AWS 为例来看下我们在公有云上的 K8S 网络方案。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/ctrip-net-evolution/13.png\" width=\"70%\" height=\"70%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 13. K8S network solution on public cloud vendor (AWS)\u003c/p\u003e\n\n\u003cp\u003e首先，起 EC2 实例作为 K8S node，我们自己开发一个 CNI 插件，动态向 EC2 插拔 ENI，\n并把 ENI 作为网卡给容器使用。这一部分借鉴了 Lyft 和 Netflix 在 AWS 上经验 [5, 6]。\u003c/p\u003e\n\n\u003cp\u003e在 VPC 内，有一个全局的 IPAM，管理整个 K8S 集群的网络资源，角色和私有云中的\nneutron 类似。它会调用 AWS API 实现网络资源的申请、释放和管理。\u003c/p\u003e\n\n\u003cp\u003e另外，我们的 CNI 还支持 attach/detach floating IP 到容器。还有就是和私有云一样，\n容器漂移的时候 IP 保持不变。\u003c/p\u003e\n\n\u003ch3 id=\"323-全球-vpc-拓扑\"\u003e3.2.3 全球 VPC 拓扑\u003c/h3\u003e\n\n\u003cp\u003e图 14 是我们现在在全球的 VPC 分布示意图。\u003c/p\u003e\n\n\u003cp\u003e在上海和南通有我们的私有云 VPC；在海外，例如首尔、莫斯科、法兰克福、加州（美\n西）、香港、墨尔本等地方有公有云上的 VPC，这里画的不全，实际不止这几个 region。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/ctrip-net-evolution/14.png\" width=\"70%\" height=\"70%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 14. VPCs distributed over the globe \u003c/p\u003e\n\n\u003cp\u003e这些 VPC 使用的网段是经过规划的，目前不会跟内网网段重合。因此通过专线打通后，IP\n可以做到可路由。\u003c/p\u003e\n\n\u003ch1 id=\"4-cloud-native-方案探索\"\u003e4 Cloud Native 方案探索\u003c/h1\u003e\n\n\u003cp\u003e以上就是我们在私有云和混合云场景下的网络方案演进。目前的方案可以支持\n业务未来一段的发展，但也有一些新的挑战。\u003c/p\u003e\n\n\u003cp\u003e首先，中心式的 IPAM 逐渐成为性能瓶颈。做过 OpenStack 的同学应该很清楚，neutron 不\n是为性能设计的，而是为发布频率很低、性能要求不高的虚拟机设计的。没有优化过的话，\n一次 neutron API 调用百毫秒级是很正常的，高负载的时候更慢。另外，中心式的 IPAM\n也不符合容器网络的设计哲学。Cloud native 方案都倾向于 local IPAM（去中心化），即\n每个 node 上有一个 IPAM，自己管理本节点的网络资源分配，calico、flannel 等流行的\n网络方案都是这样的。\u003c/p\u003e\n\n\u003cp\u003e第二，现在我们的 IP 在整张网络都是可漂移的，因此故障范围特别大。\u003c/p\u003e\n\n\u003cp\u003e第三，容器的高部署密度会给大二层网络的交换机表项带来压力，这里面有一些大二层设计\n本身以及硬件的限制。\u003c/p\u003e\n\n\u003cp\u003e另外，近年来安全受到越来越高度重视，我们有越来越强的 3-7 层主机防火墙需求。目前\n基于 OVS 的方案与主流的 K8S 方案差异很大，导致 Service 等很多 K8S 原生功能用不了。\u003c/p\u003e\n\n\u003cp\u003e针对以上问题和需求，我们进行了一些新方案的调研，包括 Calico，Cilium 等等。Calico\n大家应该已经比较熟悉了，这里介绍下 Cilium。\u003c/p\u003e\n\n\u003ch2 id=\"41-cilium-overview\"\u003e4.1 Cilium Overview\u003c/h2\u003e\n\n\u003cp\u003eCilium [7]是近两年新出现的网络方案，它使用了很多内核新技术，因此对内核版本要求比\n较高，需要 4.8 以上支持。\u003c/p\u003e\n\n\u003cp\u003eCilium 的核心功能依赖 BPF/eBPF，这是内核里的一个沙盒虚拟机。应用程序可以通过\nBPF 动态的向内核注入程序来完成很多高级功能，例如系统调用跟踪、性能分析、网络拦截\n等等。Cilium 基于 BPF 做网络的连通和安全，提供 3-7 层的安策略。\u003c/p\u003e\n\n\u003cp\u003eCilium 组件：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eCLI\u003c/li\u003e\n  \u003cli\u003e存储安全策略的 repository，一般是 etcd\u003c/li\u003e\n  \u003cli\u003e与编排引擎集成的插件：提供了 plugin 与主流的编排系统（K8S、Mesos 等）集成\u003c/li\u003e\n  \u003cli\u003eAgent，运行在每台宿主机，其中集成了 Local IPAM 功能\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/ctrip-net-evolution/15.png\" width=\"60%\" height=\"60%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 15. Cilium\u003c/p\u003e\n\n\u003ch2 id=\"42-宿主机内部网络通信host-networking\"\u003e4.2 宿主机内部网络通信（Host Networking）\u003c/h2\u003e\n\n\u003cp\u003e每个网络方案都需要解决两个主要问题：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e宿主机内部的通信：实例之间，实例和宿主机通信\u003c/li\u003e\n  \u003cli\u003e宿主机之间的通信：跨宿主机的实例之间的通信\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e先来看 Cilium 宿主机内部的网络通信。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/ctrip-net-evolution/16.png\" width=\"45%\" height=\"45%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eFig 16. Cilium host-networking\u003c/p\u003e\n\n\u003cp\u003eAgent 首先会创建一个 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ecilium_host \u0026lt;---\u0026gt; cilium_net\u003c/code\u003e 的 veth pair，然后将它管理的\nCIDR 的第一个 IP 作为网关，配置在 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ecilium_host\u003c/code\u003e 上。对于每个容器，CNI 插件\n会承担创建 veth pair、配置 IP、生成 BPF 规则 等工作。\u003c/p\u003e\n\n\u003cp\u003e同宿主机内部的容器之间的连通性靠内核协议栈二层转发和 BPF 程序。比如 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003einst1\u003c/code\u003e 到\n\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eisnt2\u003c/code\u003e，包首先从 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eeth0\u003c/code\u003e 经协议栈到达 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003elxc11\u003c/code\u003e，中间再经过 BPF 规则到达 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003elxc22\u003c/code\u003e，\n然后再经协议栈转发到达 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003einst2\u003c/code\u003e 的 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eeth0\u003c/code\u003e。\u003c/p\u003e\n\n\u003cp\u003e以传统的网络观念来看，\u003ccode class=\"language-plaintext highlighter-rouge\"\u003elxc11\u003c/code\u003e 到 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003elxc22\u003c/code\u003e 这一跳非常怪，因为没有既没有 OVS 或\nLinux bridge 这样的二层转发设备，也没有 iptables 规则或者 ARP 表项，包就神奇的到\n达了另一个地方，并且 MAC 地址还被修改了。\u003c/p\u003e\n\n\u003cp\u003e类似地，容器和宿主机的通信走宿主机内部的三层路由和 BPF 转发，其中 BPF 程序连接容\n器的 veth pair 和它的网关设备，因为容器和宿主机是两个网段。\u003c/p\u003e\n\n\u003ch2 id=\"43-跨宿主机网络通信multi-host-networking\"\u003e4.3 跨宿主机网络通信（Multi-Host Networking）\u003c/h2\u003e\n\n\u003cp\u003e跨宿主机的通信和主流的方案一样，支持两种常见方式：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eVxLAN 隧道\u003c/li\u003e\n  \u003cli\u003eBGP 直接路由\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e如果使用 VxLAN 方式，Cilium 会创建一个名为 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ecilium_vxlan\u003c/code\u003e 的 device 作为 VTEP，\n负责封装和解封装。这种方案需要评估软件 VxLAN 的性能能否接受，以及是否需要\noffload 到硬件加速。一般来说，软件 VxLAN 的方式性能较差，而且实例 IP 不可路由。\u003c/p\u003e\n\n\u003cp\u003eBGP 方案性能更好，而且 IP 可路由，但需要底层网络支持。这种方案需要在每个\nnode 上起一个 BGP agent 来和外部网络交换路由，涉及 BGP agent 的选型、AS（自治系\n统）的设计等额外工作。如果是内网，一般就是 BGP agent 与硬件网络做 peering；\u003cdel\u003e如果\n是在 AWS 之类的公有云上，还可以调用厂商提供的 BGP API\u003c/del\u003e。\u003c/p\u003e\n\n\u003ch2 id=\"44-优劣势比较pros--cons\"\u003e4.4 优劣势比较（Pros \u0026amp; Cons）\u003c/h2\u003e\n\n\u003cp\u003e最后总结一下 Cilium 方案的优劣势。\u003c/p\u003e\n\n\u003ch3 id=\"pros\"\u003ePros\u003c/h3\u003e\n\n\u003cp\u003e首先，原生支持 K8S L4-L7 安全策略，例如在 yaml 指定期望的安全效果，Cilium 会自动\n将其转化为 BPF 规则。\u003c/p\u003e\n\n\u003cp\u003e第二，高性能策略下发（控制平面）。Calico/iptables 最大的问题之一就是集群规模大了\n之后，新策略生效非常慢。iptables 是链式设计，复杂度是 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eO(n)\u003c/code\u003e；而 Cilium 的复杂度\n是 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eO(1)\u003c/code\u003e [11]，因此性能非常好。\u003c/p\u003e\n\n\u003cp\u003e第三，高性能数据平面 (veth pair, IPVLAN)。\u003c/p\u003e\n\n\u003cp\u003e第四，原生支持双栈 (IPv4/IPv6)。事实上 Cilium 最开始只支持 IPv6，后面才添加了对\nIPv4 的支持，因为他们一开始就是作为下一代技术为超大规模集群设计的。\u003c/p\u003e\n\n\u003cp\u003e第五，支持运行在 flannel 之上：flannel 负责网络连通性，Cilium 负责安全策略。如果\n你的集群现在是 flannel 模式，迁移到 Cilium 会比较方便。\u003c/p\u003e\n\n\u003cp\u003e最后，非常活跃的社区。Cilium 背后是一家公司在支持，一部分核心开发者来自内核社区\n，而且同时也是 eBPF 的开发者。\u003c/p\u003e\n\n\u003ch3 id=\"cons\"\u003eCons\u003c/h3\u003e\n\n\u003cp\u003e首先是内核版本要求比较高，至少 4.8+，最好 4.14+，相信很多公司的内核版本是没有\n这么高的。\u003c/p\u003e\n\n\u003cp\u003e第二，方案比较新，还没有哪家比较有名或有说服力的大厂在较大规模的生产环境部署了这\n种方案，因此不知道里面有没有大坑。\u003c/p\u003e\n\n\u003cp\u003e第三，如果要对代码有把控（稍大规模的公司应该都有这种要求），而不仅仅是做一个用户\n，那对内核有一定的要求，例如要熟悉协议栈、包的收发路径、内核协议栈数据结构、\n不错的 C 语言功底（BPF 程序是 C 写的）等等，开发和运维成本比基于 iptables 的方案\n（例如 Calico）要高。\u003c/p\u003e\n\n\u003cp\u003e总体来说，Cilium/eBPF 是近几年出现的最令人激动的项目之一，而且还在快速发展之中。\n我推荐大家有机会都上手玩一玩，发现其中的乐趣。\u003c/p\u003e\n\n\u003ch1 id=\"references\"\u003eReferences\u003c/h1\u003e\n\n\u003col\u003e\n  \u003cli\u003e\u003ca href=\"https://docs.openstack.org/neutron/rocky/admin/intro-os-networking.html\"\u003eOpenStack Doc: Networking Concepts\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://www.cisco.com/c/en/us/products/collateral/switches/nexus-7000-series-switches/white-paper-c11-737022.pdf\"\u003eCisco Data Center Spine-and-Leaf Architecture: Design Overview\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://mail.openvswitch.org/pipermail/ovs-dev/2014-October/290600.html\"\u003eovs-vswitchd: Fix high cpu utilization when acquire idle lock fails\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://bugs.launchpad.net/cloud-archive/+bug/1639273\"\u003eopenvswitch port mirroring only mirrors egress traffic\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://github.com/lyft/cni-ipvlan-vpc-k8s\"\u003eLyft CNI plugin\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://www.slideshare.net/aspyker/container-world-2018\"\u003eNetflix: run container at scale\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://cilium.io/\"\u003eCilium Project\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://arthurchiao.github.io/blog/cilium-cheat-sheet/\"\u003eCilium Cheat Sheet\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://arthurchiao.github.io/blog/cilium-code-walk-through-create-network/\"\u003eCilium Code Walk Through: CNI Create Network\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://aws.amazon.com/eks/\"\u003eAmazon EKS - Managed Kubernetes Service\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://www.slideshare.net/ThomasGraf5/cilium-bringing-the-bpf-revolution-to-kubernetes-networking-and-security\"\u003eCilium: API Aware Networking \u0026amp; Network Security for Microservices using BPF \u0026amp; XDP\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\n\n  \u003c!-- POST NAVIGATION --\u003e\n  \u003cdiv class=\"postNav clearfix\"\u003e\n     \n      \u003ca class=\"prev\" href=\"/blog/gobgp-cheat-sheet/\"\u003e\u003cspan\u003e« GoBGP Cheat Sheet\u003c/span\u003e\n      \n    \u003c/a\u003e\n      \n      \n      \u003ca class=\"next\" href=\"/blog/awesome-bpf/\"\u003e\u003cspan\u003eAwesome BPF Resources »\u003c/span\u003e\n       \n      \u003c/a\u003e\n     \n  \u003c/div\u003e\n\u003c/div\u003e",
  "Date": "2019-04-27T00:00:00Z",
  "Author": "Arthur Chiao"
}