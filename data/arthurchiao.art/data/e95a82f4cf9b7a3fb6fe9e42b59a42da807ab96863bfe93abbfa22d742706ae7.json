{
  "Source": "arthurchiao.art",
  "Title": "Understanding NVIDIA GPU Performance: Utilization vs. Saturation (2023)",
  "Link": "https://arthurchiao.art/blog/understanding-gpu-performance/",
  "Content": "\u003cdiv class=\"post\"\u003e\n  \n  \u003ch1 class=\"postTitle\"\u003eUnderstanding NVIDIA GPU Performance: Utilization vs. Saturation (2023)\u003c/h1\u003e\n  \u003cp class=\"meta\"\u003ePublished at 2023-08-27 | Last Update 2023-08-27\u003c/p\u003e\n  \n  \u003cp\u003eGPU performance metrics reported by tools like \u003ccode class=\"language-plaintext highlighter-rouge\"\u003envidia-smi\u003c/code\u003e may be misleading.\nThis blog delves into the underlying issue to provide a deeper understanding.\u003c/p\u003e\n\n\u003chr/\u003e\n\n\u003cul id=\"markdown-toc\"\u003e\n  \u003cli\u003e\u003ca href=\"#1-nvidia-gpu-util-a-confusing-phenomenon\" id=\"markdown-toc-1-nvidia-gpu-util-a-confusing-phenomenon\"\u003e1 NVIDIA \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e\u0026#34;GPU util\u0026#34;\u003c/code\u003e: a confusing phenomenon\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#2-gpu-util-a-misleading-term\" id=\"markdown-toc-2-gpu-util-a-misleading-term\"\u003e2 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eGPU Util\u003c/code\u003e: a misleading term?\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#21-definition-from-official-documentation\" id=\"markdown-toc-21-definition-from-official-documentation\"\u003e2.1 Definition from official documentation\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#22-exploring-the-code\" id=\"markdown-toc-22-exploring-the-code\"\u003e2.2 Exploring the code\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#23-explanation\" id=\"markdown-toc-23-explanation\"\u003e2.3 Explanation\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#24-the-use-methodology\" id=\"markdown-toc-24-the-use-methodology\"\u003e2.4 The “USE” methodology\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#25-two-metric-sources-nvml--dcgm\" id=\"markdown-toc-25-two-metric-sources-nvml--dcgm\"\u003e2.5 Two metric sources: NVML / DCGM\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#3-conclusion-and-general-advice\" id=\"markdown-toc-3-conclusion-and-general-advice\"\u003e3 Conclusion and general advice\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#31-utilization-vs-saturation\" id=\"markdown-toc-31-utilization-vs-saturation\"\u003e3.1 “Utilization” vs. saturation\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#32-general-advice-prioritizing-saturation-metrics\" id=\"markdown-toc-32-general-advice-prioritizing-saturation-metrics\"\u003e3.2 General advice: prioritizing saturation metrics\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003chr/\u003e\n\n\u003ch1 id=\"1-nvidia-gpu-util-a-confusing-phenomenon\"\u003e1 NVIDIA \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e\u0026#34;GPU util\u0026#34;\u003c/code\u003e: a confusing phenomenon\u003c/h1\u003e\n\n\u003cp\u003eEven when there is only a single task that is running on a small portion of a GPU,\nthe \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e\u0026#34;GPU util\u0026#34;\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e metric reported by tools such as\n\u003ccode class=\"language-plaintext highlighter-rouge\"\u003envidia-smi\u003c/code\u003e or other nvml-based tools may indicate that the device is fully occupied,\nwhich is rather confusing for users.\u003c/p\u003e\n\n\u003cp\u003eTo provide a clearer understanding, consider the example from the \u003ca href=\"https://forums.developer.nvidia.com/t/some-questions-on-gpu-utilization/191025\"\u003eNVIDIA\ndeveloper forum\u003c/a\u003e:\u003c/p\u003e\n\n\u003cdiv class=\"language-c highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"n\"\u003e__global__\u003c/span\u003e \u003cspan class=\"kt\"\u003evoid\u003c/span\u003e \u003cspan class=\"nf\"\u003esimple_kernel\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \u003cspan class=\"k\"\u003ewhile\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"nb\"\u003etrue\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{}\u003c/span\u003e\n\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\n\u003cspan class=\"kt\"\u003eint\u003c/span\u003e \u003cspan class=\"nf\"\u003emain\u003c/span\u003e\u003cspan class=\"p\"\u003e()\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \u003cspan class=\"n\"\u003esimple_kernel\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026lt;\u0026lt;\u0026lt;\u003c/span\u003e\u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026gt;\u0026gt;\u0026gt;\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\n    \u003cspan class=\"n\"\u003ecudaDeviceSynchronize\u003c/span\u003e\u003cspan class=\"p\"\u003e();\u003c/span\u003e\n\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eThis code snippet will launch a specified kernel (thread) on a single\nStreaming Multiprocessor (SM). Based on conventional understanding, the\n“utilization” of the GPU should be calculated as \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e1 / num_sm * 100%\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e.\nFor instance:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eIf there are 10 SMs on the GPU, the “GPU utilization” should be 10%.\u003c/li\u003e\n  \u003cli\u003eIf there are 20 SMs on the GPU, the “GPU utilization” should be 5%.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eHowever, it has been observed that nvidia-smi may report a \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e\u0026#34;GPU-Util\u0026#34;\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e\nof 100%, as demonstrated in the following example output:\u003c/p\u003e\n\n\u003cdiv class=\"language-shell highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"nv\"\u003e$ \u003c/span\u003envidia-smi\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|\u003cspan class=\"o\"\u003e===============================\u003c/span\u003e+\u003cspan class=\"o\"\u003e======================\u003c/span\u003e+\u003cspan class=\"o\"\u003e======================\u003c/span\u003e|\n|   0  Tesla V100-SXM2...  Off  | 00000000:1A:00.0 Off |                    0 |\n| N/A   42C    P0    67W / 300W |   2602MiB / 32510MiB |    100%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eWhat’s the problem? Let’s find the answer.\u003c/p\u003e\n\n\u003ch1 id=\"2-gpu-util-a-misleading-term\"\u003e2 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eGPU Util\u003c/code\u003e: a misleading term?\u003c/h1\u003e\n\n\u003cp\u003eWe first do some searches to gain a better understanding.\u003c/p\u003e\n\n\u003ch2 id=\"21-definition-from-official-documentation\"\u003e2.1 Definition from official documentation\u003c/h2\u003e\n\n\u003cp\u003eThe \u003ccode class=\"language-plaintext highlighter-rouge\"\u003envidia-smi\u003c/code\u003e command line tool is based on the NVIDIA Management Library\n(NVML), which unfortunately is not open-sourced. To find some clarifications,\nwe refer to the official \u003ca href=\"https://developer.nvidia.com/nvidia-management-library-nvml\"\u003eNVML documentation\u003c/a\u003e.\nAccording to the documentation,\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eGPU utilization: Current utilization rates are reported for both the compute resources of the GPU and the memory interface.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThis information does not provide the desired clarity.\nSo, move on.\u003c/p\u003e\n\n\u003ch2 id=\"22-exploring-the-code\"\u003e2.2 Exploring the code\u003c/h2\u003e\n\n\u003cp\u003eAlthough the NVML library itself is not open-sourced, we discovered that there\nare open-source language bindings available for it. This means that we can at\nleast access the \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003estructure and field definitions\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e, typically provided in a C/C++\nheader file. Here we choose the \u003ccode class=\"language-plaintext highlighter-rouge\"\u003egonvml\u003c/code\u003e project, which\nprovides a Golang binding for NVML. Here is an excerpt from the NVML header\nfile that defines the terms \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e\u0026#34;GPU Util\u0026#34;\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e and “Memory Util”:\u003c/p\u003e\n\n\u003cdiv class=\"language-c highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"c1\"\u003e// https://github.com/NVIDIA/go-nvml/blob/v0.12.0-1/gen/nvml/nvml.h#L210\u003c/span\u003e\n\n\u003cspan class=\"cm\"\u003e/**\n * Utilization information for a device.\n * Each sample period may be between 1 second and 1/6 second, depending on the product being queried.\n */\u003c/span\u003e\n\u003cspan class=\"k\"\u003etypedef\u003c/span\u003e \u003cspan class=\"k\"\u003estruct\u003c/span\u003e \u003cspan class=\"n\"\u003envmlUtilization_st\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n    \u003cspan class=\"kt\"\u003eunsigned\u003c/span\u003e \u003cspan class=\"kt\"\u003eint\u003c/span\u003e \u003cspan class=\"n\"\u003egpu\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e                \u003cspan class=\"c1\"\u003e//!\u0026lt; Percent of time over the past sample period during which one or more kernels was executing on the GPU\u003c/span\u003e\n    \u003cspan class=\"kt\"\u003eunsigned\u003c/span\u003e \u003cspan class=\"kt\"\u003eint\u003c/span\u003e \u003cspan class=\"n\"\u003ememory\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e             \u003cspan class=\"c1\"\u003e//!\u0026lt; Percent of time over the past sample period during which global (device) memory was being read or written\u003c/span\u003e\n\u003cspan class=\"p\"\u003e}\u003c/span\u003e \u003cspan class=\"n\"\u003envmlUtilization_t\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eWith the comments above, we get the answer.\u003c/p\u003e\n\n\u003ch2 id=\"23-explanation\"\u003e2.3 Explanation\u003c/h2\u003e\n\n\u003cp\u003eAccording to the definition provided by NVML, “utilization” refers to the\n\u003cstrong\u003e\u003cmark\u003epercentage of time that certain activities occurred during the past sample period\u003c/mark\u003e\u003c/strong\u003e. Specifically:\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003cstrong\u003eGPU utilization\u003c/strong\u003e: This represents the percentage of time during which one or more kernels were executing on the GPU.\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eMemory utilization\u003c/strong\u003e: This represents the percentage of time during which global (device) memory was being read from or written to.\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003eIn other words, the concept of “utilization” defined by NVML may \u003cstrong\u003e\u003cmark\u003enot align with\nour common understanding\u003c/mark\u003e\u003c/strong\u003e. It solely measures the portion of time that the\ndevice is being used within the given sampling period, without considering the\nnumber of streaming multiprocessors (SMs) being utilized during that time.\nTypically, we perceive “utilization” as the portion of GPU processors being\nused.\u003c/p\u003e\n\n\u003cp\u003eI’m not sure why NVIDIA’s defines “utilization” in this unconventional way.\nBut, it may be related to the “utilization” definition\nwithin the “USE” (Utilization/Saturation/Error) methodology.\u003c/p\u003e\n\n\u003ch2 id=\"24-the-use-methodology\"\u003e2.4 The “USE” methodology\u003c/h2\u003e\n\n\u003cp\u003eIf you are familiar with the book “Systems Performance: Enterprise and the\nCloud”, you may recall the \u003ca href=\"https://www.brendangregg.com/usemethod.html\"\u003e“USE”\u003c/a\u003e\nmethodology introduced by Brendan Gregg. This\nmethodology focuses on three key metrics: Utilization, Saturation, and Errors.\nAccording to the “USE” blog, the terminology definitions are as follows:\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cul\u003e\n    \u003cli\u003eutilization: \u003cstrong\u003e\u003cmark\u003ethe average time\u003c/mark\u003e\u003c/strong\u003e that the resource was busy servicing work [2]\u003c/li\u003e\n    \u003cli\u003esaturation: the degree to which the resource has extra work which it can’t service, often queued\u003c/li\u003e\n    \u003cli\u003eerrors: the count of error events\u003c/li\u003e\n  \u003c/ul\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eThe “USE” methodology provides an extra explanation for “utilization”:\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e\u003cstrong\u003e\u003cmark\u003eThere is another definition\u003c/mark\u003e\u003c/strong\u003e where utilization describes\n\u003cstrong\u003e\u003cmark\u003ethe proportion of a resource that is used\u003c/mark\u003e\u003c/strong\u003e, and so 100% utilization means no more work can be\naccepted, \u003cstrong\u003e\u003cmark\u003eunlike with the \u0026#34;busy\u0026#34; definition above\u003c/mark\u003e\u003c/strong\u003e.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003eIn summary, within the “USE” methodology, “utilization” refers to the\n\u003cstrong\u003e\u003cmark\u003eportion of time a resource is actively serving or working, without considering the\nallocated capacity\u003c/mark\u003e\u003c/strong\u003e. For the latter, the term “saturation” is used. While the\n“USE” methodology offers valuable insights into resource usage evaluation,\nredefining a well-established term like “utilization” can lead to confusion.\nMany people still prefer the concept of “utilization” representing capacity\nusage or saturation.\u003c/p\u003e\n\n\u003cp\u003eIf necessary, an alternative term to replace “utilization” could be \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e\u0026#34;used-frequency\u0026#34;\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e,\nindicating \u003cstrong\u003e\u003cmark\u003ehow frequently the device is being utilized\u003c/mark\u003e\u003c/strong\u003e.\u003c/p\u003e\n\n\u003ch2 id=\"25-two-metric-sources-nvml--dcgm\"\u003e2.5 Two metric sources: NVML / DCGM\u003c/h2\u003e\n\n\u003cp\u003eIn most cases, the metrics that we are primarily concerned with are related to\n“saturation”. So, where can we find these GPU metrics?\u003c/p\u003e\n\n\u003cp\u003eThere are two popular methods for collecting GPU performance metrics:\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\n    \u003cp\u003eUsing command line tools such as \u003ccode class=\"language-plaintext highlighter-rouge\"\u003envidia-smi\u003c/code\u003e, which can output data in formats like pretty-print and \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003exml\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e.\u003c/p\u003e\n\n    \u003cul\u003e\n      \u003cli\u003eThis tool is based on NVML (NVIDIA Management Library) internally.\u003c/li\u003e\n      \u003cli\u003eIt collects high-level metrics such as GPU and memory “utilization” (used-frequency), device temperature, power usage, etc.\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eUsing services like \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003edcgm-exporter\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e, which can output data in Prometheus format.\u003c/p\u003e\n\n    \u003cul\u003e\n      \u003cli\u003eThis service is based on DCGM (Data Center GPU Management) internally.\u003c/li\u003e\n      \u003cli\u003eIn addition to high-level metrics, it can also perform profiling and collect detailed \u003cstrong\u003e\u003cmark\u003esaturation data\u003c/mark\u003e\u003c/strong\u003e about the GPU devices.\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003eBelow are two collections of dashboards that show the metrics collected from \u003ccode class=\"language-plaintext highlighter-rouge\"\u003envidia-smi\u003c/code\u003e and \u003ccode class=\"language-plaintext highlighter-rouge\"\u003edcgm-exporter\u003c/code\u003e:\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/understanding-gpu-performance/nvidia-smi-metrics.png\" width=\"100%\" height=\"100%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eMetrics from nvidia-smi\u003c/p\u003e\n\n\u003cp\u003eNote that GPU util is 100%. Below are the metrics collected from \u003ccode class=\"language-plaintext highlighter-rouge\"\u003edcgm-exporter\u003c/code\u003e:\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/understanding-gpu-performance/dcgm-metrics.png\" width=\"100%\" height=\"100%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eMetrics from dcgm-exporter\u003c/p\u003e\n\n\u003cp\u003eWe can see that SM occupancy is very low (\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e\u0026lt;20%\u003c/code\u003e), and floating point operations (FP32/FP16/TensorCore)\nalso stays in a very low percent, indicating that the GPU is not saturated.\u003c/p\u003e\n\n\u003ch1 id=\"3-conclusion-and-general-advice\"\u003e3 Conclusion and general advice\u003c/h1\u003e\n\n\u003ch2 id=\"31-utilization-vs-saturation\"\u003e3.1 “Utilization” vs. saturation\u003c/h2\u003e\n\n\u003cp\u003eNot sure if the NVML designer intentionally adopted the “USE”\nmethodology mentioned above, but it seems that its definition of\n“utilization” (including GPU and memory utilization) aligns with the “USE”\nstandard. The reported “utilization” simply indicates the frequency (in\npercentage of time) at which the device is being used, without considering the\namount of capacity being utilized.\u003c/p\u003e\n\n\u003ch2 id=\"32-general-advice-prioritizing-saturation-metrics\"\u003e3.2 General advice: prioritizing saturation metrics\u003c/h2\u003e\n\n\u003cp\u003eWhile \u003ccode class=\"language-plaintext highlighter-rouge\"\u003envidia-smi\u003c/code\u003e is a commonly used and convenient tool, it is not the\noptimal choice for performance measurements. For GPU applications in real\ndeployment, it is recommended to utilize metrics based on DCGM, such as those\nprovided by \u003ccode class=\"language-plaintext highlighter-rouge\"\u003edcgm-exporter\u003c/code\u003e.\u003c/p\u003e\n\n\u003cp\u003eFurthermore, paying closer attention to saturation metrics can be beneficial.\nThese metrics include FP64/FP32/FP16 activation, tensor core activation\npercentage, NVLINK bandwidth, GPU memory bandwidth percentage, and more.\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/understanding-gpu-performance/dcgm-metrics-2.png\" width=\"100%\" height=\"100%\"/\u003e\u003c/p\u003e\n\u003cp align=\"center\"\u003eMetrics from dcgm-exporter\u003c/p\u003e\n\n\n  \u003c!-- POST NAVIGATION --\u003e\n  \u003cdiv class=\"postNav clearfix\"\u003e\n     \n      \u003ca class=\"prev\" href=\"/blog/llama2-paper-zh/\"\u003e\u003cspan\u003e« [译][论文] LLaMA 2：开放基础和微调聊天模型（Meta/Facebook，2023）\u003c/span\u003e\n      \n    \u003c/a\u003e\n      \n      \n      \u003ca class=\"next\" href=\"/blog/how-to-train-a-gpt-assistant-zh/\"\u003e\u003cspan\u003e[译] 如何训练一个企业级 GPT 助手（OpenAI，2023） »\u003c/span\u003e\n       \n      \u003c/a\u003e\n     \n  \u003c/div\u003e\n\u003c/div\u003e",
  "Date": "2023-08-27T00:00:00Z",
  "Author": "Arthur Chiao"
}