{
  "Source": "arthurchiao.art",
  "Title": "[译] 基于 BPF/XDP 实现 K8s Service 负载均衡 (LPC, 2020)",
  "Link": "https://arthurchiao.art/blog/cilium-k8s-service-lb-zh/",
  "Content": "\u003cdiv class=\"post\"\u003e\n  \n  \u003ch1 class=\"postTitle\"\u003e[译] 基于 BPF/XDP 实现 K8s Service 负载均衡 (LPC, 2020)\u003c/h1\u003e\n  \u003cp class=\"meta\"\u003ePublished at 2020-11-24 | Last Update 2021-02-03\u003c/p\u003e\n  \n  \u003ch3 id=\"译者序\"\u003e译者序\u003c/h3\u003e\n\n\u003cp\u003e本文翻译自 2020 年 Daniel Borkmann 和 Martynas Pumputis 在 Linux Plumbers Conference 的一篇分享:\n\u003ca href=\"https://linuxplumbersconf.org/event/7/contributions/674/\"\u003eK8s Service Load Balancing with BPF \u0026amp; XDP\u003c/a\u003e。\u003c/p\u003e\n\n\u003cp\u003e文章介绍了 K8s 的一些核心网络模型和设计、Cilium 对 K8s Service 的实现、BPF/XDP\n性能优化，以及他们从中得到的一些实践经验，全是干货。\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e由于译者水平有限，本文不免存在遗漏或错误之处。如有疑问，请查阅原文。\u003c/strong\u003e\u003c/p\u003e\n\n\u003chr/\u003e\n\n\u003cul id=\"markdown-toc\"\u003e\n  \u003cli\u003e\u003ca href=\"#译者序\" id=\"markdown-toc-译者序\"\u003e译者序\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#1-k8s-网络基础访问集群内服务的几种方式\" id=\"markdown-toc-1-k8s-网络基础访问集群内服务的几种方式\"\u003e1 K8s 网络基础：访问集群内服务的几种方式\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#11-podip直连容器-ip\" id=\"markdown-toc-11-podip直连容器-ip\"\u003e1.1 PodIP（直连容器 IP）\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#12-hostport宿主机端口映射\" id=\"markdown-toc-12-hostport宿主机端口映射\"\u003e1.2 HostPort（宿主机端口映射）\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#13-nodeport-service\" id=\"markdown-toc-13-nodeport-service\"\u003e1.3 NodePort Service\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#14-externalips-service\" id=\"markdown-toc-14-externalips-service\"\u003e1.4 ExternalIPs Service\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#15-loadbalancer-service\" id=\"markdown-toc-15-loadbalancer-service\"\u003e1.5 LoadBalancer Service\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#151-私有云\" id=\"markdown-toc-151-私有云\"\u003e1.5.1 私有云\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#152-公有云\" id=\"markdown-toc-152-公有云\"\u003e1.5.2 公有云\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#16-clusterip-service\" id=\"markdown-toc-16-clusterip-service\"\u003e1.6 ClusterIP Service\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#2-k8s-service-负载均衡cilium-基于-bpfxdp-的实现\" id=\"markdown-toc-2-k8s-service-负载均衡cilium-基于-bpfxdp-的实现\"\u003e2 K8s Service 负载均衡：Cilium 基于 BPF/XDP 的实现\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#21-socket-层负载均衡东西向流量\" id=\"markdown-toc-21-socket-层负载均衡东西向流量\"\u003e2.1 Socket 层负载均衡（东西向流量）\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#实现\" id=\"markdown-toc-实现\"\u003e实现\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#查找后端-pods\" id=\"markdown-toc-查找后端-pods\"\u003e查找后端 pods\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#好处\" id=\"markdown-toc-好处\"\u003e好处\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#22-tc--xdp-层负载均衡南北向流量\" id=\"markdown-toc-22-tc--xdp-层负载均衡南北向流量\"\u003e2.2 TC \u0026amp; XDP 层负载均衡（南北向流量）\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#23-xdp-相关优化\" id=\"markdown-toc-23-xdp-相关优化\"\u003e2.3 XDP 相关优化\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#bpfxdp-context-通用化\" id=\"markdown-toc-bpfxdp-context-通用化\"\u003eBPF/XDP context 通用化\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#内联汇编绕过编译器自动优化\" id=\"markdown-toc-内联汇编绕过编译器自动优化\"\u003e内联汇编：绕过编译器自动优化\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#避免在用户侧使用-generic-xdp\" id=\"markdown-toc-避免在用户侧使用-generic-xdp\"\u003e避免在用户侧使用 generic XDP\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#自定义内存操作函数\" id=\"markdown-toc-自定义内存操作函数\"\u003e自定义内存操作函数\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#cb-control-buffer\" id=\"markdown-toc-cb-control-buffer\"\u003ecb (control buffer)\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#bpf_map_update_elem\" id=\"markdown-toc-bpf_map_update_elem\"\u003ebpf_map_update_elem()\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#bpf_fib_lookup\" id=\"markdown-toc-bpf_fib_lookup\"\u003ebpf_fib_lookup()\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#静态-key\" id=\"markdown-toc-静态-key\"\u003e静态 key\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#24-xdp-转发性能\" id=\"markdown-toc-24-xdp-转发性能\"\u003e2.4 XDP 转发性能\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#3-新的-bpf-内核扩展\" id=\"markdown-toc-3-新的-bpf-内核扩展\"\u003e3 新的 BPF 内核扩展\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#31-避免穿越内核协议栈\" id=\"markdown-toc-31-避免穿越内核协议栈\"\u003e3.1 避免穿越内核协议栈\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#32-redirection-helpers\" id=\"markdown-toc-32-redirection-helpers\"\u003e3.2 Redirection helpers\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#321-pod-egressbpf_redirect_neigh\" id=\"markdown-toc-321-pod-egressbpf_redirect_neigh\"\u003e3.2.1 Pod egress：\u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebpf_redirect_neigh()\u003c/code\u003e\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#322-pod-ingressbpf_redirect_peer\" id=\"markdown-toc-322-pod-ingressbpf_redirect_peer\"\u003e3.2.2 Pod ingress：\u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebpf_redirect_peer()\u003c/code\u003e\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#323-veth-to-veth\" id=\"markdown-toc-323-veth-to-veth\"\u003e3.2.3 veth to veth\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#324-bpf-redirection-性能\" id=\"markdown-toc-324-bpf-redirection-性能\"\u003e3.2.4 BPF redirection 性能\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#4-结束语\" id=\"markdown-toc-4-结束语\"\u003e4 结束语\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e以下是译文。\u003c/p\u003e\n\n\u003chr/\u003e\n\n\u003cp\u003e去年我们也参加了这个大会（LPC），并做了题为\n\u003ca href=\"https://linuxplumbersconf.org/event/4/contributions/458/\"\u003eMaking the Kubernetes Service Abstraction Scale using eBPF\u003c/a\u003e \n（中译 \u003ca href=\"/blog/cilium-scale-k8s-service-with-bpf-zh/\"\u003e利用 eBPF 支撑大规模 K8s Service (LPC, 2019)\u003c/a\u003e）\n的分享。\n今天的内容是去年内容的延续，具体分为三个部分：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eKubernetes 网络模型\u003c/li\u003e\n  \u003cli\u003eCilium 对 K8s Service 负载均衡的实现，以及我们的一些实践经验\u003c/li\u003e\n  \u003cli\u003e一些新的 BPF 内核扩展\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch1 id=\"1-k8s-网络基础访问集群内服务的几种方式\"\u003e1 K8s 网络基础：访问集群内服务的几种方式\u003c/h1\u003e\n\n\u003cp\u003eKubernetes 是一个分布式容器调度器，最小调度单位是 Pod。从网络的角度来说，可以认为\n一个 pod 就是\u003cstrong\u003e网络命名空间的一个实例\u003c/strong\u003e（an instance of network namespace）。\n一个 pod 内可能会有多个容器，因此，\u003cstrong\u003e多个容器可以共存于同一个网络命名空间\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e需要注意的是：\u003cstrong\u003eK8s 只定义了网络模型，具体实现则是交给所谓的 CNI 插件\u003c/strong\u003e\n，后者完成 pod 网络的创建和销毁。本文接下来将以 Cilium CNI 插件作为例子。\u003c/p\u003e\n\n\u003cp\u003eK8s 规定了\u003cstrong\u003e每个 pod 的 IP 在集群内要能访问\u003c/strong\u003e，这是通过 CNI 来完成的：CNI\n插件负责为 pod 分配 IP 地址，然后为其创建和打通网络。\n\u003cstrong\u003e除此之外，K8s 没有对 CNI 插件做任何限制\u003c/strong\u003e。尤其是，K8s 没有对\u003cstrong\u003e从集群外访问\npod 的行为做任何规定\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e接下来我们就来看看如何访问 K8s 集群里的一个\u003cstrong\u003e服务\u003c/strong\u003e（通常会对应多个 backend pods）。\u003c/p\u003e\n\n\u003ch2 id=\"11-podip直连容器-ip\"\u003e1.1 PodIP（直连容器 IP）\u003c/h2\u003e\n\n\u003cp\u003e第一种方式是\u003cstrong\u003e通过 PodIP 直接访问\u003c/strong\u003e，这是最简单的方式。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cilium-service-lb/pod-ip.png\" width=\"90%\" height=\"90%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e如上图所示，这个服务的 3 个 backend pods 分别位于两个 node 上。当集群外的客户端\n访问这个服务时，它会\u003cstrong\u003e直接通过某个具体的 PodIP 来访问\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e假设客户端和 Pod 之间的网络是可达的，那这种访问是没问题的。\u003c/p\u003e\n\n\u003cp\u003e但这种方式有几个\u003cstrong\u003e缺点\u003c/strong\u003e：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003epod 会因为某些原因重建，而 K8s \u003cstrong\u003e无法保证它每次都会分到同一个 IP 地址\u003c/strong\u003e\n。例如，\u003cmark\u003e如果 node 重启了，pod 很可能就会分到不同的 IP 地址\u003c/mark\u003e，这\n对客户端来说个大麻烦。\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e没有内置的负载均衡\u003c/strong\u003e。即，客户端选择一个 PodIP 后，所有的请求都会发送到这个\npod，而不是分散到不同的后端 pod。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch2 id=\"12-hostport宿主机端口映射\"\u003e1.2 HostPort（宿主机端口映射）\u003c/h2\u003e\n\n\u003cp\u003e第二种方式是使用所谓的 HostPort。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cilium-service-lb/host-port.png\" width=\"75%\" height=\"75%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e如上图所示，\u003cstrong\u003e在宿主机的 netns 分配一个端口\u003c/strong\u003e，并将这个端口的所有流量转发到\n后端 pod。\u003c/p\u003e\n\n\u003cp\u003e这种情况下，\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e客户端通过 Pod 所在的宿主机的 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eHostIP:HostPort\u003c/code\u003e 访问服务，例如上图中访问 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e10.0.0.1:10000\u003c/code\u003e；\u003c/li\u003e\n  \u003cli\u003e宿主机先\u003cstrong\u003e对流量进行 DNAT\u003c/strong\u003e，然后转发给 Pod。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e这种方式的\u003cstrong\u003e缺点\u003c/strong\u003e：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e宿主机的端口资源是所有 Pod 共享的，任何一个端口只能被一个 pod 使用\n，因此\u003cstrong\u003e在每台 node 上，任何一个服务最多只能有一个 pod\u003c/strong\u003e（每个 backend 都是一\n致的，因此需要使用相同的 HostPort）。对用户非常不友好。\u003c/li\u003e\n  \u003cli\u003e和 PodIP 方式一样，没有内置的负载均衡。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch2 id=\"13-nodeport-service\"\u003e1.3 NodePort Service\u003c/h2\u003e\n\n\u003cp\u003eNodePort 和上面的 HostPort 有点像（可以认为是 HostPort 的增强版），也是将 Pod 暴\n露到宿主机 netns 的某个端口，但此时，\u003cstrong\u003e集群内的每个 Node 上都会为这个服务的 pods\n预留这个端口，并且将流量负载均衡到这些 pods\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e如下图所示，假设这里的 NodePort 是 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e30001\u003c/code\u003e。当客户端请求到达任意一台 node 的\n\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e30001\u003c/code\u003e 端口时，它可以对请求做 DNAT 然后转发给本节点内的 Pod，如下图所示，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cilium-service-lb/node-port.png\" width=\"75%\" height=\"75%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e也可以 DNAT 之后将请求转发给其他节点上的 pod，如下图所示：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cilium-service-lb/node-port-2.png\" width=\"75%\" height=\"75%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e注意在后面跨宿主机转发的情况下，\u003cstrong\u003e除了做 DNAT 还需要做 SNAT\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e优点：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\u003cstrong\u003e已经有了服务（service）的概念\u003c/strong\u003e，多个 pod 属于同一个 service，挂掉一个时其\n他 pod 还能继续提供服务。\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e客户端不用关心 pod 在哪个 node 上\u003c/strong\u003e，因为集群内的所有 node 上都开了这个端\n口并监听在那里，它们对全局的 backend 有一致的视图。\u003c/li\u003e\n  \u003cli\u003e已经\u003cstrong\u003e有了负载均衡，每个 node 都是 LB\u003c/strong\u003e。\u003c/li\u003e\n  \u003cli\u003e在宿主机 netns 内访问这些服务时，通过 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003elocalhost:NodePort\u003c/code\u003e 就行了，无需 DNS 解析。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e缺点：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\u003cstrong\u003e大部分实现都是基于 SNAT\u003c/strong\u003e，当 pod 不在本节点时，导致 packet 中的\u003cstrong\u003e真实客户端 IP 地址\u003c/strong\u003e信息丢失，监控、排障等不方便。\u003c/li\u003e\n  \u003cli\u003eNode 做转发使得\u003cstrong\u003e转发路径多了一跳，延时变大\u003c/strong\u003e。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch2 id=\"14-externalips-service\"\u003e1.4 ExternalIPs Service\u003c/h2\u003e\n\n\u003cp\u003e第四种从集群外访问 service 的方式是 external IP。\u003c/p\u003e\n\n\u003cp\u003e如果有外部可达的 IP ，即\u003cstrong\u003e集群外能通过这个 IP 访问到集群内特定的 nodes\u003c/strong\u003e，那我\n们就可以通过这些 nodes 将流量转发到 service 的后端 pods，并提供负载均衡。\u003c/p\u003e\n\n\u003cp\u003e如下图所示，\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e1.1.1.1\u003c/code\u003e 是一个 external IP，所有目的 IP 地址是 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e1.1.1.1\u003c/code\u003e 的流量会\n被底层的网络（K8s 控制之外）转发到 node1。\u003ccode class=\"language-plaintext highlighter-rouge\"\u003e1.1.1.1:8080\u003c/code\u003e 在 K8s 里定义了一个\nService，如果它将流量转发到本机内的 backend pod，需要做一次 DNAT：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cilium-service-lb/external-ip.png\" width=\"75%\" height=\"75%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e同样，这里的后端 Pod 也可以在其他 node 上，这时除了做 DNAT 还需要做一次 SNAT，\n如下图所示：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cilium-service-lb/external-ip-2.png\" width=\"75%\" height=\"75%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e优点\u003c/strong\u003e：可以\u003cstrong\u003e使用任何外部可达的 IP 地址来定义 Service 入口\u003c/strong\u003e，只要用这个 IP\n地址能访问集群内的至少一台机器即可。\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e缺点\u003c/strong\u003e：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\u003cmark\u003eExternal IP 在 k8s 的控制范围之外\u003c/mark\u003e，是由底层的网络平台提供的。例\n如，底层网络通过 BGP 宣告，使得 IP 能到达某些 nodes。\u003c/li\u003e\n  \u003cli\u003e由于这个 IP 是在 k8s 的控制之外，对 k8s 来说就是黑盒，因此\n\u003cmark\u003e从集群内访问 external IP 是存在安全隐患的\u003c/mark\u003e，例如 external IP 上\n可能运行了恶意服务，能够进行中间人攻击。因此，Cilium 目前不支持在集群内通过\nexternal IP访问 Service。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch2 id=\"15-loadbalancer-service\"\u003e1.5 LoadBalancer Service\u003c/h2\u003e\n\n\u003cp\u003e第五种访问方式是所谓的 LoadBalancer 模式。针对公有云还是私有云，LoadBalancer 又分为两种。\u003c/p\u003e\n\n\u003ch3 id=\"151-私有云\"\u003e1.5.1 私有云\u003c/h3\u003e\n\n\u003cp\u003e如果是私有云，可以考虑实现一个自己的 cloud provider，或者直接使用 MetalLB。\u003c/p\u003e\n\n\u003cp\u003e如下图所示，\u003cstrong\u003e这种模式和 externalIPs 模式非常相似\u003c/strong\u003e，local 转发：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cilium-service-lb/load-balancer.png\" width=\"75%\" height=\"75%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eremote 转发：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cilium-service-lb/load-balancer-2.png\" width=\"75%\" height=\"75%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e但是，\u003cmark\u003e二者有重要区别\u003c/mark\u003e：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\u003cstrong\u003eexternalIPs 在 K8s 的控制之外\u003c/strong\u003e，使用方式是从某个地方申请一个 external IP，\n然后填到 Service 的 Spec 里；这个 external IP 是存在安全隐患的，因为并不是\nK8s 分配和控制的；\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eLoadBalancer 在 K8s 的控制之内\u003c/strong\u003e，只需要声明\n这是一个 LoadBalancer 类型的 Service，K8s 的 cloud-provider 组件\n就会自动给这个 Service 分配一个外部可达的 IP，本质上 cloud-provider 做的事\n情就是从某个 LB 分配一个受信任的 VIP 然后填到 Service 的 Spec 里。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e\u003cstrong\u003e优点\u003c/strong\u003e：LoadBalancer 分配的 IP 是归 K8s 管的，\u003cstrong\u003e用户无法直接配置这些 IP\u003c/strong\u003e，因\n此也就避免了前面 external IP 的流量欺骗（traffic spoofing）风险。\u003c/p\u003e\n\n\u003cp\u003e但\u003cstrong\u003e注意这些 IP 不是由 CNI 分配的，而是由 LoadBalancer 实现分配\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003eMetalLB 能完成 LoadBalancer IP 的分配，然后\u003cstrong\u003e基于 ARP/NDP 或 BGP 宣告 IP 的可达性\u003c/strong\u003e。\n此外，\u003cstrong\u003eMetalLB 本身并不在 critical fast path\u003c/strong\u003e 上（可以认为它只是控制平面，完成\nLoadBalancer IP 的生效，接下来的请求和响应流量，即数据平面，都不经过它），因此不\n影响 XDP 的使用。\u003c/p\u003e\n\n\u003ch3 id=\"152-公有云\"\u003e1.5.2 公有云\u003c/h3\u003e\n\n\u003cp\u003e主流的云厂商都实现了 LoadBalancer，在它们提供的托管 K8s 内可以直接使用。\u003c/p\u003e\n\n\u003cp\u003e特点：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e有专门的 LB 节点作为统一入口。\u003c/li\u003e\n  \u003cli\u003eLB 节点再将流量转发到 NodePort。\u003c/li\u003e\n  \u003cli\u003eNodePort 再将流量转发到 backend pods。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e如下图所示，local 转发：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cilium-service-lb/load-balancer-cloud.png\" width=\"75%\" height=\"75%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eremote 转发：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cilium-service-lb/load-balancer-cloud-2.png\" width=\"75%\" height=\"75%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e优点：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eLoadBalancer 由云厂商实现，无需用户安装 BGP 软件、配置 BGP 协议等来宣告 VIP 可达性。\u003c/li\u003e\n  \u003cli\u003e开箱即用，主流云厂商都针对它们的托管 K8s 集群实现了这样的功能。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e在这种情况下，\u003cstrong\u003eCloud LB 负责检测后端 node（注意不是后端 pod）的健康状态\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e缺点：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e存在两层 LB：LB 节点转发和 node 转发。\u003c/li\u003e\n  \u003cli\u003e使用方式因厂商而已，例如各厂商的 annotations 并没有标准化到 K8s 中，跨云使用会有一些麻烦。\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eCloud API 非常慢\u003c/strong\u003e，调用厂商的 API 来做拉入拉出非常受影响。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch2 id=\"16-clusterip-service\"\u003e1.6 ClusterIP Service\u003c/h2\u003e\n\n\u003cp\u003e最后一种是\u003cstrong\u003e集群内访问 Service 的方式\u003c/strong\u003e：ClusterIP 方式。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cilium-service-lb/cluster-ip.png\" width=\"75%\" height=\"75%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eClusterIP 也是 Service 的一种 VIP，但这种方式\u003cmark\u003e只适用于从集群内访问 Service\u003c/mark\u003e，\n例如从一个 Pod 访问相同集群内的一个 Service。\u003c/p\u003e\n\n\u003cp\u003eClusterIP 的特点：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eClusterIP 使用的 IP 地址段是\u003cstrong\u003e在创建 K8s 集群之前就预留好的\u003c/strong\u003e；\u003c/li\u003e\n  \u003cli\u003eClusterIP \u003cstrong\u003e不可路由\u003c/strong\u003e（会在出宿主机之前被拦截，然后 DNAT 成具体的 PodIP）；\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e只能在集群内访问\u003c/strong\u003e（For in-cluster access only）。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e实际上，\u003cstrong\u003e当创建一个 LoadBalancer 类型的 Service 时，K8s 会为我们自动创建三种类\n型的 Service\u003c/strong\u003e：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eLoadBalancer\u003c/li\u003e\n  \u003cli\u003eNodePort\u003c/li\u003e\n  \u003cli\u003eClusterIP\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e这三种类型的 Service 对应着同一组 backend pods。\u003c/p\u003e\n\n\u003cp\u003e我们此次分享的第一部分，K8s 网络基础至此就要结束了，实际上还有很多与 Service 相\n关的 K8s 特性，例如 sessionAffinity 和 externalTrafficPolicy，但这里就不展开了，\n有兴趣可以参考附录。\u003c/p\u003e\n\n\u003ch1 id=\"2-k8s-service-负载均衡cilium-基于-bpfxdp-的实现\"\u003e2 K8s Service 负载均衡：Cilium 基于 BPF/XDP 的实现\u003c/h1\u003e\n\n\u003cp\u003e\u003cstrong\u003eCilium 基于 eBPF/XDP 实现了前面提到的所有类型的 K8s Service\u003c/strong\u003e。实现方式是：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e在每个 node 上运行一个 cilium-agent；\u003c/li\u003e\n  \u003cli\u003ecilium-agent 监听 K8s apiserver，因此能够感知到 K8s 里 Service 的变化；\u003c/li\u003e\n  \u003cli\u003e根据 Service 的变化动态更新 BPF 配置。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cilium-service-lb/bpf-lb-layers.png\" width=\"70%\" height=\"70%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e如上图所示，Service 的实现由两个主要部分组成：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e运行在 \u003cstrong\u003esocket\u003c/strong\u003e 层的 BPF 程序\u003c/li\u003e\n  \u003cli\u003e运行在 \u003cstrong\u003etc/XDP\u003c/strong\u003e 层的 BPF 程序\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e以上两者共享 service map 等资源，其中存储了 service 及其 backend pods 的映射关系。\u003c/p\u003e\n\n\u003ch2 id=\"21-socket-层负载均衡东西向流量\"\u003e2.1 Socket 层负载均衡（东西向流量）\u003c/h2\u003e\n\n\u003cp\u003eSocket 层 BPF 负载均衡负责处理\u003cstrong\u003e集群内的东西向流量\u003c/strong\u003e。\u003c/p\u003e\n\n\u003ch3 id=\"实现\"\u003e实现\u003c/h3\u003e\n\n\u003cp\u003e实现方式是：\u003cstrong\u003e将 BPF 程序 attach 到 socket 的系统调用 hooks，使客户端直接和后端\npod 建连和通信\u003c/strong\u003e，如下图所示，这里能 hook 的系统调用包括 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003econnect()\u003c/code\u003e、\u003ccode class=\"language-plaintext highlighter-rouge\"\u003esendmsg()\u003c/code\u003e、\n\u003ccode class=\"language-plaintext highlighter-rouge\"\u003erecvmsg()\u003c/code\u003e、\u003ccode class=\"language-plaintext highlighter-rouge\"\u003egetpeername()\u003c/code\u003e、\u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebind()\u003c/code\u003e 等，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cilium-service-lb/e-w-lb.png\" width=\"75%\" height=\"75%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e这里的一个问题是，\u003cstrong\u003e\u003cmark\u003eK8s 使用的还是 cgroup v1，但这个功能需要使用 v2\u003c/mark\u003e\u003c/strong\u003e，\n而由于兼容性问题，v2 完全替换 v1 还需要很长时间。所以我们目前所能做的就是\n支持 v1 和 v2 的混合模式。这也是为什么 Cilium 会 mount 自己的 cgroup v2 instance\n的原因（将宿主机 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e/var/run/cilium/cgroupv2\u003c/code\u003e mount 到 cilium-agent 容器内。另外，\n启用这个功能需要设置 \u003cstrong\u003e\u003cmark\u003e\u003ccode\u003e--sockops-enable=true\u003c/code\u003e\u003c/mark\u003e\u003c/strong\u003e 及高版本内核，译注）。\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eCilium mounts cgroup v2, attaches BPF to root cgroup. Hybrid use works well for root v2.\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e具体到实现上，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003econnect + sendmsg\u003c/code\u003e 做\u003cstrong\u003e正向变换\u003c/strong\u003e（translation）\u003c/li\u003e\n  \u003cli\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003erecvmsg + getpeername\u003c/code\u003e 做\u003cstrong\u003e反向\u003c/strong\u003e变换，\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e这个变换或转换是\u003cstrong\u003e基于 socket structure 的，此时还没有创建 packet\u003c/strong\u003e，因此\n\u003cstrong\u003e不存在 packet 级别的 NAT！\u003c/strong\u003e目前已经支持 TCP/UDP v4/v6, v4-in-v6。\n\u003cstrong\u003e应用对此是无感知的，它以为自己连接到的还是 Service IP，但其实是 PodIP\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e\u003cmark\u003esocket-level translation 具体是如何实现的\u003c/mark\u003e，\n可参考 \u003ca href=\"/blog/cracking-k8s-node-proxy/\"\u003eCracking kubernetes node proxy (aka kube-proxy)\u003c/a\u003e，\n其中有一个 20 多行 bpf 代码实现的例子，可认为是极度简化的 Cilium 相关代码。译注。\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch3 id=\"查找后端-pods\"\u003e查找后端 pods\u003c/h3\u003e\n\n\u003cp\u003eService lookup \u003cstrong\u003e不一定能选到所有的 backend pods\u003c/strong\u003e（scoped lookup），我们将\nbackend pods 拆成不同的集合。\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e这样设计的好处\u003c/strong\u003e：可以根据\u003cstrong\u003e流量类型\u003c/strong\u003e，例如是来自集群内还是集群外（\ninternal/external），\u003cstrong\u003e来选择不同的 backends\u003c/strong\u003e。例如，如果是到达 node 的 external\ntraffic，我们可以限制它只能选择本机上的 backend pods，这样相比于转发到其他 node\n上的 backend 就少了一跳。\u003c/p\u003e\n\n\u003cp\u003e另外，还支持通配符（wildcard）匹配，这样就能将 Service 暴露到 localhost 或者\nloopback 地址，能在宿主机 netns 访问 Service。但这种方式不会将 Service 暴露到宿\n主机外面。\u003c/p\u003e\n\n\u003ch3 id=\"好处\"\u003e好处\u003c/h3\u003e\n\n\u003cp\u003e显然，这种 \u003cstrong\u003esocket 级别的转换是非常高效和实用的\u003c/strong\u003e，它可以\u003cstrong\u003e\u003cmark\u003e直接将客户端\npod 连接到某个 backend pod\u003c/mark\u003e\u003c/strong\u003e，与 kube-proxy 这样的实现相比，转发路径少了好几跳。\u003c/p\u003e\n\n\u003cp\u003e此外，\u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebind\u003c/code\u003e BPF 程序在 NodePort 冲突时会\u003cstrong\u003e直接拒绝应用的请求\u003c/strong\u003e，因此相比产生流\n量（packet）然后在后面的协议栈中被拒绝，bind 这里要更加高效，\u003cstrong\u003e因为此时\n流量（packet）都还没有产生\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e对这一功能至关重要的两个函数：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\n    \u003cp\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebpf_get_socket_cookie()\u003c/code\u003e\u003c/p\u003e\n\n    \u003cp\u003e主要用于 UDP sockets，我们希望每个 UDP flow 都能选中相同的 backend pods。\u003c/p\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebpf_get_netns_cookie()\u003c/code\u003e\u003c/p\u003e\n\n    \u003cp\u003e用在两个地方：\u003c/p\u003e\n\n    \u003col\u003e\n      \u003cli\u003e用于区分 host netns 和  pod  netns，例如检测到在 host netns 执行 bind 时，直接拒绝（reject）；\u003c/li\u003e\n      \u003cli\u003e用于 serviceSessionAffinity，实现在某段时间内永远选择相同的 backend pods。\u003c/li\u003e\n    \u003c/ol\u003e\n\n    \u003cp\u003e由于 \u003cstrong\u003e\u003cmark\u003ecgroupv2 不感知 netns\u003c/mark\u003e\u003c/strong\u003e，因此在这个 context 中我们没有\nPod 源 IP 信息，通过这个 helper 能让它感知到源 IP，并以此作为它的 source identifier。\u003c/p\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 id=\"22-tc--xdp-层负载均衡南北向流量\"\u003e2.2 TC \u0026amp; XDP 层负载均衡（南北向流量）\u003c/h2\u003e\n\n\u003cp\u003e第二种是进出集群的流量，称为南北向流量，在宿主机 tc 或 XDP hook 里处理。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cilium-service-lb/n-s-lb.png\" width=\"75%\" height=\"75%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eBPF 做的事情，将入向流量转发到后端 Pod，\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e如果 Pod 在本节点，做 DNAT；\u003c/li\u003e\n  \u003cli\u003e如果在其他节点，还需要做 SNAT 或者 DSR。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e\u003cstrong\u003e这些都是 packet 级别的操作\u003c/strong\u003e。\u003c/p\u003e\n\n\u003ch2 id=\"23-xdp-相关优化\"\u003e2.3 XDP 相关优化\u003c/h2\u003e\n\n\u003cp\u003e在引入 XDP 支持时，为了使 context 的抽象更加通用，我们做了很多事情。下面就其中的\n一些展开讨论。\u003c/p\u003e\n\n\u003ch3 id=\"bpfxdp-context-通用化\"\u003eBPF/XDP context 通用化\u003c/h3\u003e\n\n\u003cp\u003eDNAT/SNAT engine, DSR, conntrack 等等都是在 tc BPF 里实现的。\nBPF 代码中用 context 结构体传递数据包信息。\u003c/p\u003e\n\n\u003cp\u003e支持 XDP 时遇到的一个问题是：到底是将 context 抽象地更通用一些，还是直接实现一个\n支持 XDP 的最小子集。我们最后是花大力气重构了以前几乎所有的 BPF 代码，来使得它更\n加通用。好处是共用一套代码，这样对代码的优化同时适用于 TC 和 XDP 逻辑。\u003c/p\u003e\n\n\u003cp\u003e下面是一个具体例子：\u003c/p\u003e\n\n\u003cp\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003ectx\u003c/code\u003e 是一个通用抽象，具体是什么类型和 include 的头文件有关，基于 cxt 可以同时处\n理 tc BPF 和 XDP BPF 逻辑，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cilium-service-lb/generic-code.png\" width=\"60%\" height=\"60%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e例如对于 XDP 场景，编译时这些宏会被相应的 XDP 实现替换掉：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cilium-service-lb/context-specific-code.png\" width=\"60%\" height=\"60%\"/\u003e\u003c/p\u003e\n\n\u003ch3 id=\"内联汇编绕过编译器自动优化\"\u003e内联汇编：绕过编译器自动优化\u003c/h3\u003e\n\n\u003cp\u003e我们遇到的另一个问题是：tc BPF 中已经为 skb 实现了很多的 helper 函数，由于共用一\n套抽象，因此现在需要为 XDP 实现对应的一套函数集。这些 helpers 都是 inline 函数，\n而 LLVM 会对 inline 函数的自动优化会导致接下来校验器（BPF verifier）失败。\u003c/p\u003e\n\n\u003cp\u003e我们的解决方式是用 \u003cstrong\u003einline asm（内联汇编）来绕过这个问题\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e下面是一个具体例子：\u003ccode class=\"language-plaintext highlighter-rouge\"\u003exdp_load_bytes()\u003c/code\u003e，使用下面这段等价的汇编代码，才能\n让 verifier 认出来：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cilium-service-lb/inline-asm.png\" width=\"60%\" height=\"60%\"/\u003e\u003c/p\u003e\n\n\u003ch3 id=\"避免在用户侧使用-generic-xdp\"\u003e避免在用户侧使用 generic XDP\u003c/h3\u003e\n\n\u003cp\u003e\u003cstrong\u003e\u003cmark\u003e5.6 内核\u003c/mark\u003e\u003c/strong\u003e对 XDP 来说是一个里程碑式的版本（但\u003cstrong\u003e\u003cmark\u003e不是 LTS\u003c/mark\u003e\u003c/strong\u003e 版本，后记），\n这个版本使得 \u003cstrong\u003e\u003cmark\u003eXDP 在公有云上大规模可用了\u003c/mark\u003e\u003c/strong\u003e，例如 AWS ENA 和 Azure \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ehv_netvsc\u003c/code\u003e 驱动。\n但如果想\u003cstrong\u003e跨平台使用 XDP\u003c/strong\u003e，那你只应该使用最基本的一些 API，例如\nXDP_PASS/DROP/TX 等等。\u003c/p\u003e\n\n\u003cp\u003eCilium 在用户侧只使用 native XDP（only supports native XDP on user side），\n我们也用 Generic XDP，但目前只限于 CI 等场景。\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e为什么我们避免在用户侧使用 generic XDP 呢\u003c/strong\u003e？因为这套 LB 逻辑会运行在集群内的\n每个 node 上，目前 linearize skb 以及 bypass GRO 会增加太大的 overhead。\u003c/p\u003e\n\n\u003ch3 id=\"自定义内存操作函数\"\u003e自定义内存操作函数\u003c/h3\u003e\n\n\u003cp\u003e现在回到加载和存储字节相关的辅助函数（load and store bytes helpers）。\u003c/p\u003e\n\n\u003cp\u003e查看 BPF 反汇编代码时，发现内置函数会执行字节级别（byte-wise）的一些操作，因此我\n们实现了\u003cstrong\u003e自己优化过的 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003emem{cpy,zero,cmp,move}()\u003c/code\u003e 函数\u003c/strong\u003e。这一点做起来还是比较容\n易的，因为 \u003cstrong\u003eLLVM 对栈外数据（non-stack data）没有上下文信息\u003c/strong\u003e，例如 packet data\n、map data，因而它无法准确地知道底层的架构是否支持高效的非对齐访问（unaligned\naccess）。\u003c/p\u003e\n\n\u003cp\u003e另外，在基准测试中我们发现，\u003cstrong\u003e大流量的场景下，\u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebpf_ktime_get_ns()\u003c/code\u003e 在 XDP 中的开\n销非常大\u003c/strong\u003e，因此我们将 clock source 变成可选的，Cilium 启动时会执行检查，如果内\n核支持，就\u003cstrong\u003e自动切换到 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebpf_jiffies64()\u003c/code\u003e\u003c/strong\u003e（精度更低，但 conntrack 不需要那么高的\n精度），这使得转发性能增加了大约 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e1.1Mpps\u003c/code\u003e。\u003c/p\u003e\n\n\u003ch3 id=\"cb-control-buffer\"\u003ecb (control buffer)\u003c/h3\u003e\n\n\u003cp\u003etc BPF 中大量使用 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eskb-\u0026gt;cb[]\u003c/code\u003e 来传递数据，显然，XDP 中也是没有这个东西的。\u003c/p\u003e\n\n\u003cp\u003e为了在 XDP 中传递数据，我们最开始使用的是 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003exdp_adjust_meta()\u003c/code\u003e，但有两个缺点：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003emissing driver support\u003c/li\u003e\n  \u003cli\u003ehigh rate of cache-misses\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cstrong\u003e后来换成 per-CPU scratch map\u003c/strong\u003e（每个 CPU 独立的、内容可随意修改的 map）, 增加了大约 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e1.2Mpps\u003c/code\u003e。\u003c/p\u003e\n\n\u003ch3 id=\"bpf_map_update_elem\"\u003ebpf_map_update_elem()\u003c/h3\u003e\n\n\u003cp\u003e在 fast path 中有很多 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebpf_map_update_elem()\u003c/code\u003e 调用，触发了 bucket spinlock。\u003c/p\u003e\n\n\u003cp\u003e如果流量来自多个 CPU，这里可以优化的是：先检查一下是否需要更新（这一步不需要加锁\n），如果原来已经存在，并且需要更新的值并没有变，那就直接返回，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cilium-service-lb/bpf_map_update_ele.png\" width=\"60%\" height=\"60%\"/\u003e\u003c/p\u003e\n\n\u003ch3 id=\"bpf_fib_lookup\"\u003ebpf_fib_lookup()\u003c/h3\u003e\n\n\u003cp\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebpf_fib_lookup()\u003c/code\u003e \u003cstrong\u003e\u003cmark\u003e开销非常大\u003c/mark\u003e\u003c/strong\u003e，但在 XDP 中，例如 hairpin LB 场景，是不需要这个\n函数的，可以在编译时去掉。我们在测试环境的结果显示可以提高 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e1.5Mpps\u003c/code\u003e。\u003c/p\u003e\n\n\u003ch3 id=\"静态-key\"\u003e静态 key\u003c/h3\u003e\n\n\u003cp\u003e作为这次分享的最后一个例子，不要对不确定的 LLVM 行为做任何假设。\u003c/p\u003e\n\n\u003cp\u003e我们在 BPF map 的基础上有大量的尾调用，它们有静态的 keys，能够在编译期间确\n定 key 的大小。我们还实现了一个内联汇编来做静态的尾递归调用，保证 LLVM 不会出现\n尾调用相关的问题。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cilium-service-lb/tail_call_static.png\" width=\"50%\" height=\"50%\"/\u003e\u003c/p\u003e\n\n\u003ch2 id=\"24-xdp-转发性能\"\u003e2.4 XDP 转发性能\u003c/h2\u003e\n\n\u003cp\u003e我们在 K8s 集群测试了 \u003cstrong\u003eXDP 对 K8s Service 的转发\u003c/strong\u003e。用 pktgen 生成 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e10Mpps\u003c/code\u003e 的\n入向处理流量，然后让 node 转发到位于其他节点的 backend pods。来看下几种不同的\n负载均衡实现分别能处理多少。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cilium-service-lb/fwd-performance.png\" width=\"75%\" height=\"75%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e由上图可以看出，\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\u003cstrong\u003eCilium XDP 模式\u003c/strong\u003e：能够处理全部的 10Mpps 入向流量，将它们转发到其他节点上的 backend pods。\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003eCilium TC 模式\u003c/strong\u003e：可以处理大约 2.8Mpps，虽然它的处理逻辑和 Cilium XDP 是类似的（除了 BPF helpers）。\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003ekube-proxy iptables 模式\u003c/strong\u003e：能处理 2.4Mpps，这是 K8s 的默认 Service 负载均衡实现。\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003ekube-proxy IPVS 模式\u003c/strong\u003e：性能更差一些，因为它的 \u003cstrong\u003eper-packet overhead 更大一\n些\u003c/strong\u003e，这里测试的 Service 只对应一个 backend pod。当 Service 数量更多时，\n\u003cstrong\u003eIPVS 的可扩展性更好\u003c/strong\u003e，相比 iptables 模式的 kube-proxy 性能会更好，但仍然没\n法跟我们基于 TC BPF 和 XDP 的实现相比（no comparison at all）。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e\u003cstrong\u003esoftirq 开销\u003c/strong\u003e也是类似的，如下图所示，流量从 1Mpps 到 2Mpps 再到 4Mpps 时，\nXDP 模式下的 softirq 开销都远小于其他几种模式。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cilium-service-lb/fwd-performance-cpu.png\" width=\"60%\" height=\"60%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e特别是 pps 到达某个临界点时，TC 和 Netfilter 实现中 \u003cstrong\u003esoftirq 开销会大到饱和\u003c/strong\u003e ——\n占用几乎全部 CPU。\u003c/p\u003e\n\n\u003ch1 id=\"3-新的-bpf-内核扩展\"\u003e3 新的 BPF 内核扩展\u003c/h1\u003e\n\n\u003cp\u003e下面介绍几个新的 BPF 内核扩展，主要是 Cilium 相关的场景。\u003c/p\u003e\n\n\u003ch2 id=\"31-避免穿越内核协议栈\"\u003e3.1 避免穿越内核协议栈\u003c/h2\u003e\n\n\u003cp\u003e主机收到的包，当其 backend 是本机上的 pod 时，或者包是本机产生的，目的端是一个本\n机端口，这个包需要跨越不同的 netns，例如从宿主机的 netns 进入到容\n器的 netns，\u003cstrong\u003e现在 Cilium 的做法是，将包送到内核协议栈\u003c/strong\u003e，如下图所示：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cilium-service-lb/new-bpf-ext.png\" width=\"50%\" height=\"50%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e将包送到内核协议栈有两个原因（需要）：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eTPROXY 需要由内核协议栈完成：我们目前的 L7 proxy 功能会用到这个功能，\u003c/li\u003e\n  \u003cli\u003eK8s 默认安装了一些 iptables rule，用来检测\u003cstrong\u003e从连接跟踪的角度看是非法的连接\u003c/strong\u003e\n（‘invalid’ connections on asymmetric paths），然后 netfilter 会 drop 这些连接\n的包。我们最开始时曾尝试将包从宿主机 tc 层直接 redirect 到 veth，但应答包却要\n经过协议栈，因此形成了\u003cstrong\u003e非对称路径\u003c/strong\u003e，流量被 drop。因此目前进和出都都要经过协议栈。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e但这样带来两个问题，如下图所示：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cilium-service-lb/new-bpf-ext-3.png\" width=\"60%\" height=\"60%\"/\u003e\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003ePod 的出向流量在进入协议栈后，在 socket buffer 层会丢掉 socket 信息\n（\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eskb-\u0026gt;sk\u003c/code\u003e gets orphaned at \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eip_rcv_core()\u003c/code\u003e），这导致包从主机设备发出去时，\n我们无法在 FQ leaf 获得 TCP 反压（TCP back-pressure）。\u003c/li\u003e\n  \u003cli\u003e转发和处理都是 packet 级别的，因此有 per-packet overhead。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e不久之前，\u003cstrong\u003eBPF TPROXY 已经合并到内核，因此最后一个真正依赖 Netfilter 的东西已经\n解决了\u003c/strong\u003e。因此我们现在可以\u003cstrong\u003e在 TC 层做全部逻辑处理了，无需进入内核协议栈\u003c/strong\u003e，如下图所示：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cilium-service-lb/new-bpf-ext-2.png\" width=\"50%\" height=\"50%\"/\u003e\u003c/p\u003e\n\n\u003ch2 id=\"32-redirection-helpers\"\u003e3.2 Redirection helpers\u003c/h2\u003e\n\n\u003cp\u003e两个用于 redirection 的 TC BPF helpers：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebpf_redirect_neigh()\u003c/code\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebpf_redirect_peer()\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e\u003cstrong\u003e从 IPVLAN driver 中借鉴了一些理念，实现到了 veth 驱动中\u003c/strong\u003e。\u003c/p\u003e\n\n\u003ch3 id=\"321-pod-egressbpf_redirect_neigh\"\u003e3.2.1 Pod egress：\u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebpf_redirect_neigh()\u003c/code\u003e\u003c/h3\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cilium-service-lb/tc-redir-helper.png\" width=\"60%\" height=\"60%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e对于 pod egress 流量，我们会填充 src 和 dst mac 地址，这和原来 neighbor\nsubsystem 做的事情相同；此外，我们还可以保留 skb 的 socket。这些都是由\n\u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebpf_redirect_neigh()\u003c/code\u003e 来完成的：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cilium-service-lb/tc-redir-helper-2.png\" width=\"75%\" height=\"75%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e整个过程大致实现如下，在 veth 主机端的 ingress（对应 pod 的 egress）调用这\n个方法的时候：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e首先会查找路由，\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eip_route_output_flow()\u003c/code\u003e\u003c/li\u003e\n  \u003cli\u003e将 skb 和匹配的路由条目（dst entry）关联起来，\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eskb_dst_set()\u003c/code\u003e\u003c/li\u003e\n  \u003cli\u003e然后调用到 neighbor 子系统，\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eip_finish_output2()\u003c/code\u003e\n    \u003col\u003e\n      \u003cli\u003e填充 neighbor 信息，即 src/dst MAC 地址\u003c/li\u003e\n      \u003cli\u003e保留 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eskb-\u0026gt;sk\u003c/code\u003e 信息，因此物理网卡上的 qdisc 都能访问到这个字段\u003c/li\u003e\n    \u003c/ol\u003e\n  \u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e这就是 pod 出向的处理过程。\u003c/p\u003e\n\n\u003ch3 id=\"322-pod-ingressbpf_redirect_peer\"\u003e3.2.2 Pod ingress：\u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebpf_redirect_peer()\u003c/code\u003e\u003c/h3\u003e\n\n\u003cp\u003e入向流量，\u003cstrong\u003e会有快速 netns 切换\u003c/strong\u003e，从宿主机 netns 直接进入容器的 netns。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cilium-service-lb/tc-redir-helper-3.png\" width=\"60%\" height=\"60%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e这是由 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebpf_redirect_peer()\u003c/code\u003e 完成的。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cilium-service-lb/tc-redir-helper-4.png\" width=\"75%\" height=\"75%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e在主机设备的 ingress 执行这个 helper 的时候，\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e首先会获取对应的 veth pair，\u003ccode class=\"language-plaintext highlighter-rouge\"\u003edev = ops-\u0026gt;ndo_get_peer_dev(dev)\u003c/code\u003e，然后获取 veth\n的对端（在另一个 netns）\u003c/li\u003e\n  \u003cli\u003e然后，\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eskb_scrub_packet()\u003c/code\u003e\u003c/li\u003e\n  \u003cli\u003e设置包的 dev 为容器内的 dev，\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eskb-\u0026gt;dev = dev\u003c/code\u003e\u003c/li\u003e\n  \u003cli\u003e重新调度一次，\u003ccode class=\"language-plaintext highlighter-rouge\"\u003esch_handle_ingress()\u003c/code\u003e，这不会进入 CPU 的 backlog queue:\n    \u003col\u003e\n      \u003cli\u003egoto another_round\u003c/li\u003e\n      \u003cli\u003eno CPU backlog queue\u003c/li\u003e\n    \u003c/ol\u003e\n  \u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch3 id=\"323-veth-to-veth\"\u003e3.2.3 veth to veth\u003c/h3\u003e\n\n\u003cp\u003e同宿主机上的两个 Pod 之间通信时，这两个 helper 也非常有用。\n因为我们已经在主机 netns 的 TC ingress 层了，因此能直接将其 redirect 到另一个容\n器的 ingress 路径。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cilium-service-lb/tc-redir-helper-5.png\" width=\"60%\" height=\"60%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e这里比较好的一点是，需要针对老版本内核所做的兼容性非常少；因此，我们只需要在启动的\n时候检测内核是否有相应的 helper，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e如果有，就用 redirection 功能；\u003c/li\u003e\n  \u003cli\u003e如果没有，就直接返回 TC_OK，走传统的内核协议栈方式，经过内核邻居子系统。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e支持这些功能无需对原有的 BPF datapath 进行大规模重构。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cilium-service-lb/tc-redir-helper-6.png\" width=\"60%\" height=\"60%\"/\u003e\u003c/p\u003e\n\n\u003ch3 id=\"324-bpf-redirection-性能\"\u003e3.2.4 BPF redirection 性能\u003c/h3\u003e\n\n\u003cp\u003e下面看下性能。\u003c/p\u003e\n\n\u003cp\u003eTCP stream 场景，相比 Cilium baseline，转发带宽增加了 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e1.3Gbps\u003c/code\u003e，接近线速：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cilium-service-lb/new-ext-perf.png\" width=\"75%\" height=\"75%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e更有趣的是 TCP_RR 的场景，以 transactions/second 衡量，提升了  \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e2.9\u003c/code\u003e 倍，接近最\n大性能：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cilium-service-lb/new-ext-perf-2.png\" width=\"75%\" height=\"75%\"/\u003e\u003c/p\u003e\n\n\u003ch1 id=\"4-结束语\"\u003e4 结束语\u003c/h1\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cilium-service-lb/try-out.png\" width=\"50%\" height=\"50%\"/\u003e\u003c/p\u003e\n\n\n  \u003c!-- POST NAVIGATION --\u003e\n  \u003cdiv class=\"postNav clearfix\"\u003e\n     \n      \u003ca class=\"prev\" href=\"/blog/network-evolves-zh/\"\u003e\u003cspan\u003e« 计算规模驱动下的网络方案演进\u003c/span\u003e\n      \n    \u003c/a\u003e\n      \n      \n      \u003ca class=\"next\" href=\"/blog/cilium-scale-k8s-service-with-bpf-zh/\"\u003e\u003cspan\u003e[译] 利用 eBPF 支撑大规模 K8s Service (LPC, 2019) »\u003c/span\u003e\n       \n      \u003c/a\u003e\n     \n  \u003c/div\u003e\n\u003c/div\u003e",
  "Date": "2020-11-24T00:00:00Z",
  "Author": "Arthur Chiao"
}