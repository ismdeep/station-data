{
  "Source": "arthurchiao.art",
  "Title": "[译] 利用 eBPF 支撑大规模 K8s Service (LPC, 2019)",
  "Link": "https://arthurchiao.art/blog/cilium-scale-k8s-service-with-bpf-zh/",
  "Content": "\u003cdiv class=\"post\"\u003e\n  \n  \u003ch1 class=\"postTitle\"\u003e[译] 利用 eBPF 支撑大规模 K8s Service (LPC, 2019)\u003c/h1\u003e\n  \u003cp class=\"meta\"\u003ePublished at 2020-11-29 | Last Update 2020-11-29\u003c/p\u003e\n  \n  \u003ch3 id=\"译者序\"\u003e译者序\u003c/h3\u003e\n\n\u003cp\u003e本文翻译自 2019 年 Daniel Borkmann 和 Martynas Pumputis 在 Linux Plumbers Conference 的一篇分享:\n\u003ca href=\"https://linuxplumbersconf.org/event/4/contributions/458/\"\u003eMaking the Kubernetes Service Abstraction Scale using eBPF\u003c/a\u003e 。\n翻译时对大家耳熟能详或已显陈旧的内容（K8s 介绍、Cilium 1.6 之前的版本对 Service\n实现等）略有删减，如有需要请查阅原 PDF。\u003c/p\u003e\n\n\u003cp\u003e实际上，一年之后 Daniel 和 Martynas 又在 LPC 做了一次分享，内容是本文的延续：\n\u003ca href=\"/blog/cilium-k8s-service-lb-zh/\"\u003e基于 BPF/XDP 实现 K8s Service 负载均衡 (LPC, 2020)\u003c/a\u003e。\u003c/p\u003e\n\n\u003cp\u003e其他推荐阅读：\u003ca href=\"/blog/cracking-k8s-node-proxy/\"\u003eCracking kubernetes node proxy (aka kube-proxy)\u003c/a\u003e，\n用五种方式、百来行代码，实现极度简易版 kube-proxy。\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003e由于译者水平有限，本文不免存在遗漏或错误之处。如有疑问，请查阅原文。\u003c/strong\u003e\u003c/p\u003e\n\n\u003chr/\u003e\n\n\u003cul id=\"markdown-toc\"\u003e\n  \u003cli\u003e\u003ca href=\"#译者序\" id=\"markdown-toc-译者序\"\u003e译者序\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#1-k8s-service-类型及默认基于-kube-proxy-的实现\" id=\"markdown-toc-1-k8s-service-类型及默认基于-kube-proxy-的实现\"\u003e1 K8s Service 类型及默认基于 kube-proxy 的实现\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#11-clusterip-service\" id=\"markdown-toc-11-clusterip-service\"\u003e1.1 ClusterIP Service\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#12-nodeport-service\" id=\"markdown-toc-12-nodeport-service\"\u003e1.2 NodePort Service\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#13-小结\" id=\"markdown-toc-13-小结\"\u003e1.3 小结\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#2-用-ciliumbpf-替换-kube-proxy\" id=\"markdown-toc-2-用-ciliumbpf-替换-kube-proxy\"\u003e2 用 Cilium/BPF 替换 kube-proxy\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#21-clusterip-service\" id=\"markdown-toc-21-clusterip-service\"\u003e2.1 ClusterIP Service\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#tcp--connected-udp\" id=\"markdown-toc-tcp--connected-udp\"\u003eTCP \u0026amp; connected UDP\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#某些-udp-应用存在的问题及解决方式\" id=\"markdown-toc-某些-udp-应用存在的问题及解决方式\"\u003e某些 UDP 应用：存在的问题及解决方式\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#22-nodeport-service\" id=\"markdown-toc-22-nodeport-service\"\u003e2.2 NodePort Service\u003c/a\u003e        \u003cul\u003e\n          \u003cli\u003e\u003ca href=\"#221-后端-pod-在本节点\" id=\"markdown-toc-221-后端-pod-在本节点\"\u003e2.2.1 后端 pod 在本节点\u003c/a\u003e\u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#222-后端-pod-在其他节点\" id=\"markdown-toc-222-后端-pod-在其他节点\"\u003e2.2.2 后端 pod 在其他节点\u003c/a\u003e            \u003cul\u003e\n              \u003cli\u003e\u003ca href=\"#snat\" id=\"markdown-toc-snat\"\u003eSNAT\u003c/a\u003e\u003c/li\u003e\n            \u003c/ul\u003e\n          \u003c/li\u003e\n          \u003cli\u003e\u003ca href=\"#223-client-pods-和-backend-pods-在同一节点\" id=\"markdown-toc-223-client-pods-和-backend-pods-在同一节点\"\u003e2.2.3 Client pods 和 backend pods 在同一节点\u003c/a\u003e\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#23-service-规则的规模及请求延迟对比\" id=\"markdown-toc-23-service-规则的规模及请求延迟对比\"\u003e2.3 Service 规则的规模及请求延迟对比\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#3-相关的-ciliumbpf-优化\" id=\"markdown-toc-3-相关的-ciliumbpf-优化\"\u003e3 相关的 Cilium/BPF 优化\u003c/a\u003e    \u003cul\u003e\n      \u003cli\u003e\u003ca href=\"#31-bpf-udp-recvmsg-hook\" id=\"markdown-toc-31-bpf-udp-recvmsg-hook\"\u003e3.1 BPF UDP \u003ccode class=\"language-plaintext highlighter-rouge\"\u003erecvmsg()\u003c/code\u003e hook\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#32-全局唯一-socket-cookie\" id=\"markdown-toc-32-全局唯一-socket-cookie\"\u003e3.2 全局唯一 socket cookie\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#33-维护邻居表\" id=\"markdown-toc-33-维护邻居表\"\u003e3.3 维护邻居表\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#34-lru-bpf-callback-on-entry-eviction\" id=\"markdown-toc-34-lru-bpf-callback-on-entry-eviction\"\u003e3.4 LRU BPF callback on entry eviction\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#35-lru-bpf-eviction-zones\" id=\"markdown-toc-35-lru-bpf-eviction-zones\"\u003e3.5 LRU BPF eviction zones\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#36-bpf-原子操作\" id=\"markdown-toc-36-bpf-原子操作\"\u003e3.6 BPF 原子操作\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#37-bpf-getpeername-hook\" id=\"markdown-toc-37-bpf-getpeername-hook\"\u003e3.7 BPF \u003ccode class=\"language-plaintext highlighter-rouge\"\u003egetpeername\u003c/code\u003e hook\u003c/a\u003e\u003c/li\u003e\n      \u003cli\u003e\u003ca href=\"#38-绕过内核最大-bpf-指令数的限制\" id=\"markdown-toc-38-绕过内核最大-bpf-指令数的限制\"\u003e3.8 绕过内核最大 BPF 指令数的限制\u003c/a\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"#4-cilium-上手用-kubeadm-搭建体验环境\" id=\"markdown-toc-4-cilium-上手用-kubeadm-搭建体验环境\"\u003e4 Cilium 上手：用 kubeadm 搭建体验环境\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e以下是译文。\u003c/p\u003e\n\n\u003chr/\u003e\n\n\u003cp\u003e\u003cstrong\u003eK8s 当前重度依赖 iptables 来实现 Service 的抽象\u003c/strong\u003e。\n对于每个 Service 及其 backend pods，在 K8s 里会生成很多 iptables 规则。\n\u003cstrong\u003e例如 5K 个 Service 时，iptables 规则将达到 25K 条\u003c/strong\u003e，导致的后果：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003cstrong\u003e较高、并且不可预测的转发延迟\u003c/strong\u003e（packet latency），因为每个包都要遍历这些规则\n，直到匹配到某条规则；\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e更新规则的操作非常慢\u003c/strong\u003e：无法单独更新某条 iptables 规则，只能将全部规则读出来\n，更新整个集合，再将新的规则集合下发到宿主机。在动态环境中这一问题尤其明显，因为每\n小时可能都有几千次的 backend pods 创建和销毁。\u003c/li\u003e\n  \u003cli\u003e\u003cstrong\u003e可靠性问题\u003c/strong\u003e：iptables 依赖 Netfilter 和系统的连接跟踪模块（conntrack），在\n大流量场景下会出现一些竞争问题（race conditions）；\u003cstrong\u003eUDP 场景尤其明显\u003c/strong\u003e，会导\n致丢包、应用的负载升高等问题。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e本文将介绍如何基于 Cilium/BPF 来解决这些问题，实现 K8s Service 的大规模扩展。\u003c/p\u003e\n\n\u003ch1 id=\"1-k8s-service-类型及默认基于-kube-proxy-的实现\"\u003e1 K8s Service 类型及默认基于 kube-proxy 的实现\u003c/h1\u003e\n\n\u003cp\u003eK8s 提供了 Service 抽象，可以将多个 backend pods 组织为一个\u003cstrong\u003e逻辑单元\u003c/strong\u003e（logical\nunit）。K8s 会为这个逻辑单元分配 \u003cstrong\u003e虚拟 IP 地址\u003c/strong\u003e（VIP），客户端通过该 VIP 就\n能访问到这些 pods 提供的服务。\u003c/p\u003e\n\n\u003cp\u003e下图是一个具体的例子，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cilium-scale-service/k8s-service.png\" width=\"90%\" height=\"90%\"/\u003e\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\n    \u003cp\u003e右边的 yaml 定义了一个名为 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003enginx\u003c/code\u003e 的 Service，它在 TCP 80 端口提供服务；\u003c/p\u003e\n\n    \u003cul\u003e\n      \u003cli\u003e创建：\u003ccode class=\"language-plaintext highlighter-rouge\"\u003ekubectl -f nginx-svc.yaml\u003c/code\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eK8s 会给每个 Service 分配一个虚拟 IP，这里给 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003enginx\u003c/code\u003e 分的是 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e3.3.3.3\u003c/code\u003e；\u003c/p\u003e\n\n    \u003cul\u003e\n      \u003cli\u003e查看：\u003ccode class=\"language-plaintext highlighter-rouge\"\u003ekubectl get service nginx\u003c/code\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003e左边是 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003enginx\u003c/code\u003e Service 的两个 backend pods（在 K8s 对应两个 endpoint），这里\n位于同一台节点，每个 Pod 有独立的 IP 地址；\u003c/p\u003e\n\n    \u003cul\u003e\n      \u003cli\u003e查看：\u003ccode class=\"language-plaintext highlighter-rouge\"\u003ekubectl get endpoints nginx\u003c/code\u003e\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e上面看到的是所谓的 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eClusterIP\u003c/code\u003e 类型的 Service。实际上，\u003cstrong\u003e在 K8s 里有几种不同类型\n的 Service\u003c/strong\u003e：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eClusterIP\u003c/li\u003e\n  \u003cli\u003eNodePort\u003c/li\u003e\n  \u003cli\u003eLoadBalancer\u003c/li\u003e\n  \u003cli\u003eExternalName\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e本文将主要关注前两种类型。\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eK8s 里实现 Service 的组件是 kube-proxy\u003c/strong\u003e，实现的主要功能就是\u003cstrong\u003e将访问 VIP 的请\n求转发（及负载均衡）到相应的后端 pods\u003c/strong\u003e。前面提到的那些 iptables 规则就是它创建\n和管理的。\u003c/p\u003e\n\n\u003cp\u003e另外，kube-proxy 是 K8s 的可选组件，如果不需要 Service 功能，可以不启用它。\u003c/p\u003e\n\n\u003ch2 id=\"11-clusterip-service\"\u003e1.1 ClusterIP Service\u003c/h2\u003e\n\n\u003cp\u003e这是 \u003cstrong\u003eK8s 的默认 Service 类型\u003c/strong\u003e，使得\u003cstrong\u003e宿主机或 pod 可以通过 VIP 访问一个 Service\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eVirtual IP to any endpoint (pod)\u003c/li\u003e\n  \u003cli\u003eOnly in-cluster access\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003ekube-proxy 是通过如下的 iptables 规则来实现这个功能的：\u003c/p\u003e\n\n\u003cdiv class=\"language-shell highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"nt\"\u003e-t\u003c/span\u003e nat \u003cspan class=\"nt\"\u003e-A\u003c/span\u003e \u003cspan class=\"o\"\u003e{\u003c/span\u003ePREROUTING, OUTPUT\u003cspan class=\"o\"\u003e}\u003c/span\u003e \u003cspan class=\"nt\"\u003e-m\u003c/span\u003e conntrack \u003cspan class=\"nt\"\u003e--ctstate\u003c/span\u003e NEW \u003cspan class=\"nt\"\u003e-j\u003c/span\u003e KUBE-SERVICES\n\n\u003cspan class=\"c\"\u003e# 宿主机访问 nginx Service 的流量，同时满足 4 个条件：\u003c/span\u003e\n\u003cspan class=\"c\"\u003e# 1. src_ip 不是 Pod 网段\u003c/span\u003e\n\u003cspan class=\"c\"\u003e# 2. dst_ip=3.3.3.3/32 (ClusterIP)\u003c/span\u003e\n\u003cspan class=\"c\"\u003e# 3. proto=TCP\u003c/span\u003e\n\u003cspan class=\"c\"\u003e# 4. dport=80\u003c/span\u003e\n\u003cspan class=\"c\"\u003e# 如果匹配成功，直接跳转到 KUBE-MARK-MASQ；否则，继续匹配下面一条（iptables 是链式规则，高优先级在前）\u003c/span\u003e\n\u003cspan class=\"c\"\u003e# 跳转到 KUBE-MARK-MASQ 是为了保证这些包出宿主机时，src_ip 用的是宿主机 IP。\u003c/span\u003e\n\u003cspan class=\"nt\"\u003e-A\u003c/span\u003e KUBE-SERVICES \u003cspan class=\"o\"\u003e!\u003c/span\u003e \u003cspan class=\"nt\"\u003e-s\u003c/span\u003e 1.1.0.0/16 \u003cspan class=\"nt\"\u003e-d\u003c/span\u003e 3.3.3.3/32 \u003cspan class=\"nt\"\u003e-p\u003c/span\u003e tcp \u003cspan class=\"nt\"\u003e-m\u003c/span\u003e tcp \u003cspan class=\"nt\"\u003e--dport\u003c/span\u003e 80 \u003cspan class=\"nt\"\u003e-j\u003c/span\u003e KUBE-MARK-MASQ\n\u003cspan class=\"c\"\u003e# Pod 访问 nginx Service 的流量：同时满足 4 个条件：\u003c/span\u003e\n\u003cspan class=\"c\"\u003e# 1. 没有匹配到前一条的，（说明 src_ip 是 Pod 网段）\u003c/span\u003e\n\u003cspan class=\"c\"\u003e# 2. dst_ip=3.3.3.3/32 (ClusterIP)\u003c/span\u003e\n\u003cspan class=\"c\"\u003e# 3. proto=TCP\u003c/span\u003e\n\u003cspan class=\"c\"\u003e# 4. dport=80\u003c/span\u003e\n\u003cspan class=\"nt\"\u003e-A\u003c/span\u003e KUBE-SERVICES \u003cspan class=\"nt\"\u003e-d\u003c/span\u003e 3.3.3.3/32 \u003cspan class=\"nt\"\u003e-p\u003c/span\u003e tcp \u003cspan class=\"nt\"\u003e-m\u003c/span\u003e tcp \u003cspan class=\"nt\"\u003e--dport\u003c/span\u003e 80 \u003cspan class=\"nt\"\u003e-j\u003c/span\u003e KUBE-SVC-NGINX\n\n\u003cspan class=\"c\"\u003e# 以 50% 的概率跳转到 KUBE-SEP-NGINX1\u003c/span\u003e\n\u003cspan class=\"nt\"\u003e-A\u003c/span\u003e KUBE-SVC-NGINX \u003cspan class=\"nt\"\u003e-m\u003c/span\u003e statistic \u003cspan class=\"nt\"\u003e--mode\u003c/span\u003e random \u003cspan class=\"nt\"\u003e--probability\u003c/span\u003e 0.50 \u003cspan class=\"nt\"\u003e-j\u003c/span\u003e KUBE-SEP-NGINX1\n\u003cspan class=\"c\"\u003e# 如果没有命中上面一条，则以 100% 的概率跳转到 KUBE-SEP-NGINX2\u003c/span\u003e\n\u003cspan class=\"nt\"\u003e-A\u003c/span\u003e KUBE-SVC-NGINX \u003cspan class=\"nt\"\u003e-j\u003c/span\u003e KUBE-SEP-NGINX2\n\n\u003cspan class=\"c\"\u003e# 如果 src_ip=1.1.1.1/32，说明是 Service-\u0026gt;client 流量，则\u003c/span\u003e\n\u003cspan class=\"c\"\u003e# 需要做 SNAT（MASQ 是动态版的 SNAT），替换 src_ip -\u0026gt; svc_ip，这样客户端收到包时，\u003c/span\u003e\n\u003cspan class=\"c\"\u003e# 看到就是从 svc_ip 回的包，跟它期望的是一致的。\u003c/span\u003e\n\u003cspan class=\"nt\"\u003e-A\u003c/span\u003e KUBE-SEP-NGINX1 \u003cspan class=\"nt\"\u003e-s\u003c/span\u003e 1.1.1.1/32 \u003cspan class=\"nt\"\u003e-j\u003c/span\u003e KUBE-MARK-MASQ\n\u003cspan class=\"c\"\u003e# 如果没有命令上面一条，说明 src_ip != 1.1.1.1/32，则说明是 client-\u0026gt; Service 流量，\u003c/span\u003e\n\u003cspan class=\"c\"\u003e# 需要做 DNAT，将 svc_ip -\u0026gt; pod1_ip，\u003c/span\u003e\n\u003cspan class=\"nt\"\u003e-A\u003c/span\u003e KUBE-SEP-NGINX1 \u003cspan class=\"nt\"\u003e-p\u003c/span\u003e tcp \u003cspan class=\"nt\"\u003e-m\u003c/span\u003e tcp \u003cspan class=\"nt\"\u003e-j\u003c/span\u003e DNAT \u003cspan class=\"nt\"\u003e--to-destination\u003c/span\u003e 1.1.1.1:80\n\u003cspan class=\"c\"\u003e# 同理，见上面两条的注释\u003c/span\u003e\n\u003cspan class=\"nt\"\u003e-A\u003c/span\u003e KUBE-SEP-NGINX2 \u003cspan class=\"nt\"\u003e-s\u003c/span\u003e 1.1.1.2/32 \u003cspan class=\"nt\"\u003e-j\u003c/span\u003e KUBE-MARK-MASQ\n\u003cspan class=\"nt\"\u003e-A\u003c/span\u003e KUBE-SEP-NGINX2 \u003cspan class=\"nt\"\u003e-p\u003c/span\u003e tcp \u003cspan class=\"nt\"\u003e-m\u003c/span\u003e tcp \u003cspan class=\"nt\"\u003e-j\u003c/span\u003e DNAT \u003cspan class=\"nt\"\u003e--to-destination\u003c/span\u003e 1.1.1.2:80\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003col\u003e\n  \u003cli\u003eService 既要能被宿主机访问，又要能被 pod 访问（\u003cstrong\u003e二者位于不同的 netns\u003c/strong\u003e），\n因此需要在 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ePREROUTING\u003c/code\u003e 和 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eOUTPUT\u003c/code\u003e 两个 hook 点拦截请求，然后跳转到自定义的\n\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eKUBE-SERVICES\u003c/code\u003e chain；\u003c/li\u003e\n  \u003cli\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eKUBE-SERVICES\u003c/code\u003e chain \u003cstrong\u003e执行真正的 Service 匹配\u003c/strong\u003e，依据协议类型、目的 IP\n和目的端口号。当匹配到某个 Service 后，就会跳转到专门针对这个 Service 创\n建的 chain，命名格式为 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eKUBE-SVC-\u0026lt;Service\u0026gt;\u003c/code\u003e。\u003c/li\u003e\n  \u003cli\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eKUBE-SVC-\u0026lt;Service\u0026gt;\u003c/code\u003e chain \u003cstrong\u003e根据概率选择某个后端 pod\u003c/strong\u003e 然后将请\n求转发过去。这其实是一种\u003cstrong\u003e穷人的负载均衡器\u003c/strong\u003e —— 基于 iptables。选中某个 pod\n后，会跳转到这个 pod 相关的一条 iptables chain \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eKUBE-SEP-\u0026lt;POD\u0026gt;\u003c/code\u003e。\u003c/li\u003e\n  \u003cli\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eKUBE-SEP-\u0026lt;POD\u0026gt;\u003c/code\u003e chain 会\u003cstrong\u003e执行 DNAT\u003c/strong\u003e，将 VIP 换成 PodIP。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e译注：以上解释并不是非常详细和直观，因为这不是本文重点。想更深入地理解基于\niptables 的实现，可参考网上其他一些文章，例如下面这张图所出自的博客\n\u003ca href=\"https://www.stackrox.com/post/2020/01/kubernetes-networking-demystified/\"\u003eKubernetes Networking Demystified: A Brief Guide\u003c/a\u003e，\u003c/p\u003e\n\n  \u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cilium-scale-service/k8s-net-demystified-svc-lb.png\" width=\"90%\" height=\"90%\"/\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch2 id=\"12-nodeport-service\"\u003e1.2 NodePort Service\u003c/h2\u003e\n\n\u003cp\u003e这种类型的 Service 也能被宿主机和 pod 访问，但与 ClusterIP 不同的是，\u003cstrong\u003e它还能被\n集群外的服务访问\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eExternal node IP + port in NodePort range to any endpoint (pod), e.g. 10.0.0.1:31000\u003c/li\u003e\n  \u003cli\u003eEnables access from outside\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e实现上，kube-apiserver 会\u003cstrong\u003e从预留的端口范围内分配一个端口给 Service\u003c/strong\u003e，然后\n\u003cstrong\u003e每个宿主机上的 kube-proxy 都会创建以下规则\u003c/strong\u003e：\u003c/p\u003e\n\n\u003cdiv class=\"language-shell highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"nt\"\u003e-t\u003c/span\u003e nat \u003cspan class=\"nt\"\u003e-A\u003c/span\u003e \u003cspan class=\"o\"\u003e{\u003c/span\u003ePREROUTING, OUTPUT\u003cspan class=\"o\"\u003e}\u003c/span\u003e \u003cspan class=\"nt\"\u003e-m\u003c/span\u003e conntrack \u003cspan class=\"nt\"\u003e--ctstate\u003c/span\u003e NEW \u003cspan class=\"nt\"\u003e-j\u003c/span\u003e KUBE-SERVICES\n\n\u003cspan class=\"nt\"\u003e-A\u003c/span\u003e KUBE-SERVICES \u003cspan class=\"o\"\u003e!\u003c/span\u003e \u003cspan class=\"nt\"\u003e-s\u003c/span\u003e 1.1.0.0/16 \u003cspan class=\"nt\"\u003e-d\u003c/span\u003e 3.3.3.3/32 \u003cspan class=\"nt\"\u003e-p\u003c/span\u003e tcp \u003cspan class=\"nt\"\u003e-m\u003c/span\u003e tcp \u003cspan class=\"nt\"\u003e--dport\u003c/span\u003e 80 \u003cspan class=\"nt\"\u003e-j\u003c/span\u003e KUBE-MARK-MASQ\n\u003cspan class=\"nt\"\u003e-A\u003c/span\u003e KUBE-SERVICES \u003cspan class=\"nt\"\u003e-d\u003c/span\u003e 3.3.3.3/32 \u003cspan class=\"nt\"\u003e-p\u003c/span\u003e tcp \u003cspan class=\"nt\"\u003e-m\u003c/span\u003e tcp \u003cspan class=\"nt\"\u003e--dport\u003c/span\u003e 80 \u003cspan class=\"nt\"\u003e-j\u003c/span\u003e KUBE-SVC-NGINX\n\u003cspan class=\"c\"\u003e# 如果前面两条都没匹配到（说明不是 ClusterIP service 流量），并且 dst 是 LOCAL，跳转到 KUBE-NODEPORTS\u003c/span\u003e\n\u003cspan class=\"nt\"\u003e-A\u003c/span\u003e KUBE-SERVICES \u003cspan class=\"nt\"\u003e-m\u003c/span\u003e addrtype \u003cspan class=\"nt\"\u003e--dst-type\u003c/span\u003e LOCAL \u003cspan class=\"nt\"\u003e-j\u003c/span\u003e KUBE-NODEPORTS\n\n\u003cspan class=\"nt\"\u003e-A\u003c/span\u003e KUBE-NODEPORTS \u003cspan class=\"nt\"\u003e-p\u003c/span\u003e tcp \u003cspan class=\"nt\"\u003e-m\u003c/span\u003e tcp \u003cspan class=\"nt\"\u003e--dport\u003c/span\u003e 31000 \u003cspan class=\"nt\"\u003e-j\u003c/span\u003e KUBE-MARK-MASQ\n\u003cspan class=\"nt\"\u003e-A\u003c/span\u003e KUBE-NODEPORTS \u003cspan class=\"nt\"\u003e-p\u003c/span\u003e tcp \u003cspan class=\"nt\"\u003e-m\u003c/span\u003e tcp \u003cspan class=\"nt\"\u003e--dport\u003c/span\u003e 31000 \u003cspan class=\"nt\"\u003e-j\u003c/span\u003e KUBE-SVC-NGINX\n\n\u003cspan class=\"nt\"\u003e-A\u003c/span\u003e KUBE-SVC-NGINX \u003cspan class=\"nt\"\u003e-m\u003c/span\u003e statistic \u003cspan class=\"nt\"\u003e--mode\u003c/span\u003e random \u003cspan class=\"nt\"\u003e--probability\u003c/span\u003e 0.50 \u003cspan class=\"nt\"\u003e-j\u003c/span\u003e KUBE-SEP-NGINX1\n\u003cspan class=\"nt\"\u003e-A\u003c/span\u003e KUBE-SVC-NGINX \u003cspan class=\"nt\"\u003e-j\u003c/span\u003e KUBE-SEP-NGINX2\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003col\u003e\n  \u003cli\u003e前面几步和 ClusterIP Service 一样；如果没匹配到 ClusterIP 规则，则跳转到 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eKUBE-NODEPORTS\u003c/code\u003e chain。\u003c/li\u003e\n  \u003cli\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eKUBE-NODEPORTS\u003c/code\u003e chain 里做 Service 匹配，但\u003cstrong\u003e这次只匹配协议类型和目的端口号\u003c/strong\u003e。\u003c/li\u003e\n  \u003cli\u003e匹配成功后，转到对应的 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eKUBE-SVC-\u0026lt;Service\u0026gt;\u003c/code\u003e chain，后面的过程跟 ClusterIP 是一样的。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch2 id=\"13-小结\"\u003e1.3 小结\u003c/h2\u003e\n\n\u003cp\u003e以上可以看到，每个 Service 会对应多条 iptables 规则。\u003c/p\u003e\n\n\u003cp\u003eService 数量不断增长时，\u003cstrong\u003eiptables 规则的数量增长会更快\u003c/strong\u003e。而且，\u003cstrong\u003e每个包都需要\n遍历这些规则\u003c/strong\u003e，直到最终匹配到一条相应的规则。如果不幸匹配到最后一条规则才命中，\n那相比其他流量，这些包就会有\u003cstrong\u003e很高的延迟\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e有了这些背景知识，我们来看如何用 BPF/Cilium 来替换掉 kube-proxy，也可以说是\n重新实现 kube-proxy 的逻辑。\u003c/p\u003e\n\n\u003ch1 id=\"2-用-ciliumbpf-替换-kube-proxy\"\u003e2 用 Cilium/BPF 替换 kube-proxy\u003c/h1\u003e\n\n\u003cp\u003e我们从 Cilium 早起版本开始，已经逐步用 BPF 实现 Service 功能，但其中仍然有些\n地方需要用到 iptables。在这一时期，每台 node 上会同时运行 cilium-agent 和 kube-proxy。\u003c/p\u003e\n\n\u003cp\u003e到了 Cilium 1.6，我们已经能\u003cstrong\u003e完全基于 BPF 实现，不再依赖 iptables，也不再需要 kube-proxy\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cilium-scale-service/cilium-cluster-ip.png\" width=\"75%\" height=\"75%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e这里有一些实现上的考虑：相比于在 TC ingress 层做 Service 转换，我们优先利用\n\u003cmark\u003ecgroupv2 hooks\u003c/mark\u003e，\u003cstrong\u003e在 socket BPF 层直接做这种转换\u003c/strong\u003e（需要高版本内核\n支持，如果不支持则 fallback 回 TC ingress 方式）。\u003c/p\u003e\n\n\u003ch2 id=\"21-clusterip-service\"\u003e2.1 ClusterIP Service\u003c/h2\u003e\n\n\u003cp\u003e对于 ClusterIP，我们在 BPF 里\u003cstrong\u003e拦截 socket 的 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003econnect\u003c/code\u003e 和 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003esend\u003c/code\u003e 系统调用\u003c/strong\u003e；\n这些 BPF 执行时，\u003cstrong\u003e协议层还没开始执行\u003c/strong\u003e（这些系统调用 handlers）。\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003eAttach on the cgroupv2 root mount \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eBPF_PROG_TYPE_CGROUP_SOCK_ADDR\u003c/code\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eBPF_CGROUP_INET{4,6}_CONNECT\u003c/code\u003e - TCP, connected UDP\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch3 id=\"tcp--connected-udp\"\u003eTCP \u0026amp; connected UDP\u003c/h3\u003e\n\n\u003cp\u003e对于 TCP 和 connected UDP 场景，执行的是下面一段逻辑，\u003c/p\u003e\n\n\u003cdiv class=\"language-c highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"kt\"\u003eint\u003c/span\u003e \u003cspan class=\"nf\"\u003esock4_xlate\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"k\"\u003estruct\u003c/span\u003e \u003cspan class=\"n\"\u003ebpf_sock_addr\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e\u003cspan class=\"n\"\u003ectx\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\t\u003cspan class=\"k\"\u003estruct\u003c/span\u003e \u003cspan class=\"n\"\u003elb4_svc_key\u003c/span\u003e \u003cspan class=\"n\"\u003ekey\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e \u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003edip\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ectx\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003euser_ip4\u003c/span\u003e\u003cspan class=\"p\"\u003e,\u003c/span\u003e \u003cspan class=\"p\"\u003e.\u003c/span\u003e\u003cspan class=\"n\"\u003edport\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003ectx\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003euser_port\u003c/span\u003e \u003cspan class=\"p\"\u003e};\u003c/span\u003e\n\t\u003cspan class=\"n\"\u003esvc\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003elb4_lookup_svc\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"o\"\u003e\u0026amp;\u003c/span\u003e\u003cspan class=\"n\"\u003ekey\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\n\t\t\u003cspan class=\"k\"\u003eif\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003esvc\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\t\t\t\u003cspan class=\"n\"\u003ectx\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003euser_ip4\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003esvc\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003eendpoint_addr\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\t\t\t\u003cspan class=\"n\"\u003ectx\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003euser_port\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"n\"\u003esvc\u003c/span\u003e\u003cspan class=\"o\"\u003e-\u0026gt;\u003c/span\u003e\u003cspan class=\"n\"\u003eendpoint_port\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\t\t\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\t\u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"mi\"\u003e1\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e\n\u003cspan class=\"p\"\u003e}\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003e所做的事情：在 BPF map 中查找 Service，然后做地址转换。但这里的重点是（相比于 TC\ningress BPF 实现）：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003e\u003cstrong\u003e不经过连接跟踪（conntrack）模块，也不需要修改包头\u003c/strong\u003e（实际上这时候还没有包\n），也不再 mangle 包。这也意味着，\u003cstrong\u003e不需要重新计算包的 checksum\u003c/strong\u003e。\u003c/li\u003e\n  \u003cli\u003e对于 TCP 和 connected UDP，\u003cstrong\u003e负载均衡的开销是一次性的\u003c/strong\u003e，只需要在 socket 建立\n时做一次转换，后面都不需要了，\u003cstrong\u003e不存在包级别的转换\u003c/strong\u003e。\u003c/li\u003e\n  \u003cli\u003e这种方式是对宿主机 netns 上的 socket 和 pod netns 内的 socket 都是适用的。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003ch3 id=\"某些-udp-应用存在的问题及解决方式\"\u003e某些 UDP 应用：存在的问题及解决方式\u003c/h3\u003e\n\n\u003cp\u003e但这种方式\u003cstrong\u003e对某些 UDP 应用是不适用的\u003c/strong\u003e，因为这些 UDP 应用会检查包的源地址，以及\n会调用 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003erecvmsg\u003c/code\u003e 系统调用。\u003c/p\u003e\n\n\u003cp\u003e针对这个问题，我们引入了新的 BPF attach 类型：\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eBPF_CGROUP_UDP4_RECVMSG\u003c/code\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eBPF_CGROUP_UDP6_RECVMSG\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e另外还引入了用于 NAT 的 UDP map、rev-NAT map：\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e              BPF rev NAT map\nCookie   EndpointIP  Port =\u0026gt; ServiceID  IP       Port\n-----------------------------------------------------\n42       1.1.1.1     80   =\u0026gt; 1          3.3.3.30 80\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cul\u003e\n  \u003cli\u003e\n    \u003cp\u003e通过 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ebpf_get_socket_cookie()\u003c/code\u003e 创建 socket cookie。\u003c/p\u003e\n\n    \u003cp\u003e除了 Service 访问方式，还会有一些\u003cstrong\u003e客户端通过 PodIP 直连的方式建立 UDP 连接，\n  cookie 就是为了防止对这些类型的流量做 rev-NAT\u003c/strong\u003e。\u003c/p\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003e在 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003econnect(2)\u003c/code\u003e 和 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003esendmsg(2)\u003c/code\u003e 时更新 map。\u003c/p\u003e\n  \u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003e在 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003erecvmsg(2)\u003c/code\u003e 时做 rev-NAT。\u003c/p\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\n\n\u003ch2 id=\"22-nodeport-service\"\u003e2.2 NodePort Service\u003c/h2\u003e\n\n\u003cp\u003eNodePort 会更复杂一些，我们先从最简单的场景看起。\u003c/p\u003e\n\n\u003ch3 id=\"221-后端-pod-在本节点\"\u003e2.2.1 后端 pod 在本节点\u003c/h3\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cilium-scale-service/cilium-node-port.png\" width=\"90%\" height=\"90%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e后端 pod 在本节点时，只需要\u003cstrong\u003e在宿主机的网络设备上 attach 一段 tc ingress bpf\n程序\u003c/strong\u003e，这段程序做的事情：\u003c/p\u003e\n\n\u003col\u003e\n  \u003cli\u003eService 查找\u003c/li\u003e\n  \u003cli\u003eDNAT\u003c/li\u003e\n  \u003cli\u003eredirect 到容器的 lxc0。\u003c/li\u003e\n\u003c/ol\u003e\n\n\u003cp\u003e对于应答包，lxc0 负责 rev-NAT，FIB 查找（因为我们需要设置 L2 地址，否则会被 drop），\n然后将其 redirect 回客户端。\u003c/p\u003e\n\n\u003ch3 id=\"222-后端-pod-在其他节点\"\u003e2.2.2 后端 pod 在其他节点\u003c/h3\u003e\n\n\u003cp\u003e后端 pod 在其他节点时，会复杂一些，因为要转发到其他节点。这种情况下，\u003cstrong\u003e需要在 BPF\n做 SNAT\u003c/strong\u003e，否则 pod 会直接回包给客户端，而由于不同 node 之间没有做连接跟踪（\nconntrack）同步，因此直接回给客户端的包出 pod 后就会被 drop 掉。\u003c/p\u003e\n\n\u003cp\u003e所以需要\u003cstrong\u003e在当前节点做一次 SNAT\u003c/strong\u003e（\u003ccode class=\"language-plaintext highlighter-rouge\"\u003esrc_ip\u003c/code\u003e 从原来的 ClientIP 替换为 NodeIP），让回包也经过\n当前节点，然后在这里再做 rev-SNAT（\u003ccode class=\"language-plaintext highlighter-rouge\"\u003edst_ip\u003c/code\u003e 从原来的 NodeIP 替换为 ClientIP）。\u003c/p\u003e\n\n\u003cp\u003e具体来说，在 \u003cstrong\u003eTC ingress\u003c/strong\u003e 插入一段 BPF 代码，然后依次执行：Service 查找、DNAT、\n选择合适的 egress interface、SNAT、FIB lookup，最后发送给相应的 node，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cilium-scale-service/cilium-node-port-2.png\" width=\"85%\" height=\"85%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e反向路径是类似的，也是回到这个 node，TC ingress BPF 先执行 rev-SNAT，然后\nrev-DNAT，FIB lookup，最后再发送回客户端，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cilium-scale-service/cilium-node-port-3.png\" width=\"85%\" height=\"85%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e现在跨宿主机转发是 SNAT 模式，但将来我们打算支持 \u003cstrong\u003eDSR 模式\u003c/strong\u003e（译注，Cilium 1.8+\n已经支持了）。DSR 的好处是 \u003cstrong\u003ebackend pods 直接将包回给客户端\u003c/strong\u003e，回包不再经过当前\n节点转发。\u003c/p\u003e\n\n\u003cp\u003e另外，现在 Service 的处理是在 TC ingress 做的，\u003cstrong\u003e这些逻辑其实也能够在 XDP 层实现\u003c/strong\u003e，\n那将会是另一件激动人心的事情（译注，Cilium 1.8+ 已经支持了，性能大幅提升）。\u003c/p\u003e\n\n\u003ch4 id=\"snat\"\u003eSNAT\u003c/h4\u003e\n\n\u003cp\u003e当前基于 BPF 的 SNAT 实现中，用一个 LRU BPF map 存放 Service 和 backend pods 的映\n射信息。\u003c/p\u003e\n\n\u003cp\u003e需要说明的是，SNAT \u003cstrong\u003e除了替换 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003esrc_ip\u003c/code\u003e，还可能会替换 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003esrc_port\u003c/code\u003e\u003c/strong\u003e：不同客户端的\nsrc_port 可能是相同的，如果只替换 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003esrc_ip\u003c/code\u003e，不同客户端的应答包在反向转换时就会失\n败。因此这种情况下需要做 src_port 转换。现在的做法是，先进行哈希，如果哈希失败，\n就调用 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eprandom()\u003c/code\u003e 随机选择一个端口。\u003c/p\u003e\n\n\u003cp\u003e此外，我们还需要跟踪宿主机上的流（local flows）信息，因此在 Cilium 里\u003cstrong\u003e基于 BPF\n实现了一个连接跟踪器\u003c/strong\u003e（connection tracker），它会监听宿主机的主物理网络设备（\nmain physical device）；我们也会对宿主机上的应用执行 NAT，pod 流量 NAT 之后使用的\n是宿主机的 src_port，而宿主机上的应用使用的也是同一个 src_port 空间，它们可能会\n有冲突，因此需要在这里处理。\u003c/p\u003e\n\n\u003cp\u003e这就是 NodePort Service 类型的流量到达一台节点后，我们在 BPF 所做的事情。\u003c/p\u003e\n\n\u003ch3 id=\"223-client-pods-和-backend-pods-在同一节点\"\u003e2.2.3 Client pods 和 backend pods 在同一节点\u003c/h3\u003e\n\n\u003cp\u003e另外一种情况是：本机上的 pod 访问某个 NodePort Service，而且 backend pods 也在本机。\u003c/p\u003e\n\n\u003cp\u003e这种情况下，流量会从 loopback 口转发到 backend pods，中间会经历路由和转发过程，\n整个过程对应用是透明的 —— 我们可以\u003cstrong\u003e在应用无感知的情况下，修改二者之间的通信方式\u003c/strong\u003e，\n只要流量能被双方正确地接受就行。因此，我们在这里\u003cstrong\u003e使用了 ClusterIP，并对其进\n行了一点扩展\u003c/strong\u003e，只要连接的 Service 是 loopback 地址或者其他 local 地址，它都能正\n确地转发到本机 pods。\u003c/p\u003e\n\n\u003cp\u003e另外，比较好的一点是，这种实现方式是基于 cgroups 的，因此独立于 netns。这意味着\n我们不需要进入到每个 pod 的 netns 来做这种转换。\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cilium-scale-service/cilium-snat.png\" width=\"75%\" height=\"75%\"/\u003e\u003c/p\u003e\n\n\u003ch2 id=\"23-service-规则的规模及请求延迟对比\"\u003e2.3 Service 规则的规模及请求延迟对比\u003c/h2\u003e\n\n\u003cp\u003e有了以上功能，基本上就可以避免 kube-proxy 那样 per-service 的 iptables 规则了，\n每个节点上只留下了少数几条由 Kubernetes 自己创建的 iptables 规则：\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e$ iptables-save | grep ‘\\-A KUBE’ | wc -l:\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cul\u003e\n  \u003cli\u003eWith kube-proxy: 25401\u003c/li\u003e\n  \u003cli\u003eWith BPF: 4\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e在将来，我们有希望连这几条规则也不需要，完全绕开 Netfilter 框架（译注：新版本已经做到了）。\u003c/p\u003e\n\n\u003cp\u003e此外，我们做了一些初步的基准测试，如下图所示，\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cilium-scale-service/performance.png\" width=\"75%\" height=\"75%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e可以看到，随着 Service 数量从 1 增加到 2000+，\u003cstrong\u003ekube-proxy/iptables 的请求延\n迟增加了将近一倍\u003c/strong\u003e，而 Cilium/eBPF 的延迟几乎没有任何增加。\u003c/p\u003e\n\n\u003ch1 id=\"3-相关的-ciliumbpf-优化\"\u003e3 相关的 Cilium/BPF 优化\u003c/h1\u003e\n\n\u003cp\u003e接下来介绍一些我们在实现 Service 过程中的优化工作，以及一些未来可能会做的事情。\u003c/p\u003e\n\n\u003ch2 id=\"31-bpf-udp-recvmsg-hook\"\u003e3.1 BPF UDP \u003ccode class=\"language-plaintext highlighter-rouge\"\u003erecvmsg()\u003c/code\u003e hook\u003c/h2\u003e\n\n\u003cp\u003e实现 socket 层 UDP Service 转换时，我们发现如果只对 UDP \u003ccode class=\"language-plaintext highlighter-rouge\"\u003esendmsg\u003c/code\u003e 做 hook\n，会导致 \u003cstrong\u003eDNS 等应用无法正常工作\u003c/strong\u003e，会出现下面这种错误：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cilium-scale-service/udp-recvmsg-before.png\" width=\"75%\" height=\"75%\"/\u003e\u003c/p\u003e\n\n\u003cp\u003e深入分析发现，\u003ccode class=\"language-plaintext highlighter-rouge\"\u003enslookup\u003c/code\u003e 及其他一些工具会检查 \u003cstrong\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003econnect()\u003c/code\u003e 时用的 IP 地址\u003c/strong\u003e和\n\u003cstrong\u003e\u003ccode class=\"language-plaintext highlighter-rouge\"\u003erecvmsg()\u003c/code\u003e 读到的 reply message 里的 IP 地址\u003c/strong\u003e是否一致。如果不一致，就会\n报上面的错误。\u003c/p\u003e\n\n\u003cp\u003e原因清楚之后，解决就比较简单了：我们引入了一个做反向映射的 BPF hook，对\n\u003ccode class=\"language-plaintext highlighter-rouge\"\u003erecvmsg()\u003c/code\u003e 做额外处理，这个问题就解决了：\u003c/p\u003e\n\n\u003cp align=\"center\"\u003e\u003cimg src=\"/assets/img/cilium-scale-service/udp-recvmsg-after.png\" width=\"75%\" height=\"75%\"/\u003e\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e\u003ca href=\"https://github.com/torvalds/linux/commit/983695fa6765\"\u003e983695fa6765\u003c/a\u003e  bpf: fix unconnected udp hooks。\u003c/p\u003e\n\n  \u003cp\u003e这个 patch 能在不重写包（without packet rewrite）的前提下，会对 BPF ClusterIP 做反向映射（reverse mapping）。\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch2 id=\"32-全局唯一-socket-cookie\"\u003e3.2 全局唯一 socket cookie\u003c/h2\u003e\n\n\u003cp\u003eBPF ClusterIP Service 为 UDP 维护了一个 LRU 反向映射表（reverse mapping table）。\u003c/p\u003e\n\n\u003cp\u003e\u003cstrong\u003eSocket cookie 是这个映射表的 key 的一部分\u003c/strong\u003e，但\u003cstrong\u003e这个 cookie 只在每个 netns 内唯一\u003c/strong\u003e，\n其背后的实现比较简单：每次调用 BPF cookie helper，它都会增加计数器，然后将\ncookie 存储到 socket。因此不同 netns 内分配出来的 cookie 值可能会一样，导致冲突。\u003c/p\u003e\n\n\u003cp\u003e为解决这个问题，我们将 cookie generator 改成了全局的，见下面的 commit。\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e\u003ca href=\"https://github.com/torvalds/linux/commit/cd48bdda4fb8\"\u003ecd48bdda4fb8\u003c/a\u003e sock: make cookie generation global instead of per netns。\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch2 id=\"33-维护邻居表\"\u003e3.3 维护邻居表\u003c/h2\u003e\n\n\u003cp\u003eCilium agent 从 K8s apiserver 收到 Service 事件时，\n会将 backend entry 更新到 datapath 中的 Service backend 列表。\u003c/p\u003e\n\n\u003cp\u003e前面已经看到，当 Service 是 NodePort 类型并且 backend 是 remote 时，需要转发到其\n他节点（TC ingress BPF \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eredirect()\u003c/code\u003e）。\u003c/p\u003e\n\n\u003cp\u003e我们发现\u003cstrong\u003e在某些直接路由（direct routing）的场景下，会出现 fib 查找失败的问题\u003c/strong\u003e\n（\u003ccode class=\"language-plaintext highlighter-rouge\"\u003efib_lookup()\u003c/code\u003e），原因是系统中没有对应 backend 的 neighbor entry（IP-\u0026gt;MAC 映射\n信息），并且接下来\u003cstrong\u003e不会主动做 ARP 探测\u003c/strong\u003e（ARP probe）。\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003eTunneling 模式下这个问题可以忽略，因为本来发送端的 BPF 程\n序就会将 src/dst mac 清零，另一台节点对收到的包做处理时， VxLAN 设备上的另一段\nBPF 程序会能够正确的转发这个包，因此这种方式更像是 L3 方式。\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003cp\u003e我们目前 workaround 了这个问题，解决方式有点丑陋：Cilium 解析 backend，然后直接\n将 neighbor entry 永久性地（\u003ccode class=\"language-plaintext highlighter-rouge\"\u003eNUD_PERMANENT\u003c/code\u003e）插入邻居表中。\u003c/p\u003e\n\n\u003cp\u003e目前这样做是没问题的，因为邻居的数量是固定或者可控的（fixed/controlled number of\nentries）。但后面我们想尝试的是让内核来做这些事情，因为它能以最好的方式处理这个\n问题。实现方式就是引入一些新的 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eNUD_*\u003c/code\u003e 类型，只需要传 L3 地址，然后内核自己将解\n析 L2 地址，并负责这个地址的维护。这样 Cilium 就不需要再处理 L2 地址的事情了。\n但到今天为止，我并没有看到这种方式的可能性。\u003c/p\u003e\n\n\u003cp\u003e对于从集群外来的访问 NodePort Service 的请求，也存在类似的问题，\n因为最后将响应流量回给客户端也需要邻居表。由于这些流量都是在 pre-routing，因此我\n们现在的处理方式是：自己维护了一个小的 BPF LRU map（L3-\u0026gt;L2 mapping in\nBPF LRU map）；由于这是主处理逻辑（转发路径），流量可能很高，因此将这种映射放到\nBPF LRU 是更合适的，不会导致邻居表的 overflow。\u003c/p\u003e\n\n\u003ch2 id=\"34-lru-bpf-callback-on-entry-eviction\"\u003e3.4 LRU BPF callback on entry eviction\u003c/h2\u003e\n\n\u003cp\u003e我们想讨论的另一件事情是：在每个 LRU entry 被 eviction（驱逐）时，能有一个\ncallback 将会更好。为什么呢？\u003c/p\u003e\n\n\u003cp\u003eCilium 中现在有一个 BPF conntrack table，我们支持到了一些非常老的内核版本\n，例如 4.9。Cilium 在启动时会检查内核版本，优先选择使用 LRU，没有 LRU 再\nfallback 到普通的哈希表（Hash Table）。\u003cstrong\u003e对于哈希表，就需要一个不断 GC 的过程\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003eCilium \u003cstrong\u003e有意将 NAT map 与 CT map 独立开来\u003c/strong\u003e，因\n为我们要求在 \u003cstrong\u003ecilium-agent 升级或降级过程中，现有的连接/流量不能受影响\u003c/strong\u003e。\n如果二者是耦合在一起的，假如 CT 相关的东西有很大改动，那升级时那要么\n是将当前的连接状态全部删掉重新开始；要么就是服务中断，临时不可用，升级完成后再将\n老状态迁移到新状态表，但我认为，要轻松、正确地实现这件事情非常困难。\n这就是为什么将它们分开的原因。但实际上，GC 在回收 CT entry 的同时，\n也会顺便回收 NAT entry。\u003c/p\u003e\n\n\u003cp\u003e另外一个问题：\u003cstrong\u003e每次从 userspace 操作 conntrack entry 都会破坏 LRU\n的正常工作流程\u003c/strong\u003e（因为不恰当地更新了所有 entry 的时间戳）。我们通过下面的 commit\n解决了这个问题，但要彻底避免这个问题，\u003cstrong\u003e最好有一个 GC 以 callback 的方式在第一时\n间清理掉这些被 evicted entry\u003c/strong\u003e，例如在 CT entry 被 evict 之后，顺便也清理掉 NAT\n映射。这是我们正在做的事情（译注，Cilium 1.9+ 已经实现了）。\u003c/p\u003e\n\n\u003cblockquote\u003e\n  \u003cp\u003e\u003ca href=\"https://github.com/torvalds/linux/commit/50b045a8c0cc\"\u003e50b045a8c0cc\u003c/a\u003e (“bpf, lru: avoid messing with eviction heuristics upon syscall lookup”) fixed map walking from user space\u003c/p\u003e\n\u003c/blockquote\u003e\n\n\u003ch2 id=\"35-lru-bpf-eviction-zones\"\u003e3.5 LRU BPF eviction zones\u003c/h2\u003e\n\n\u003cp\u003e另一件跟 CT map 相关的比较有意思的探讨：未来\u003cstrong\u003e是否能根据流量类型，将 LRU\neviction 分割为不同的 zone\u003c/strong\u003e？例如，\u003c/p\u003e\n\n\u003cul\u003e\n  \u003cli\u003e东西向流量分到 zone1：处理 ClusterIP service 流量，都是 pod-{pod,host} 流量，\n比较大；\u003c/li\u003e\n  \u003cli\u003e南北向流量分到 zone2：处理 NodePort 和 ExternalName service 流量，相对比较小。\u003c/li\u003e\n\u003c/ul\u003e\n\n\u003cp\u003e这样的好处是：当\u003cstrong\u003e对南北向流量 CT 进行操作时，占大头的东西向流量不会受影响\u003c/strong\u003e。\u003c/p\u003e\n\n\u003cp\u003e理想的情况是这种隔离是有保障的，例如：可以安全地假设，如果正在清理 zone1 内的 entries，\n那预期不会对 zone2 内的 entry 有任何影响。不过，虽然分为了多个 zones，但在全局，\n只有一个 map。\u003c/p\u003e\n\n\u003ch2 id=\"36-bpf-原子操作\"\u003e3.6 BPF 原子操作\u003c/h2\u003e\n\n\u003cp\u003e另一个要讨论的内容是原子操作。\u003c/p\u003e\n\n\u003cp\u003e使用场景之一是\u003cstrong\u003e过期 NAT entry 的快速重复利用\u003c/strong\u003e（fast recycling）。\n例如，结合前面的 GC 过程，如果一个连接断开时， 不是直接删除对应的 entry，而是更\n新一个标记，表明这条 entry 过期了；接下来如果有新的连接刚好命中了这个 entry，就\n直接将其标记为正常（非过期），重复利用（循环）这个 entry，而不是像之前一样从新创\n建。\u003c/p\u003e\n\n\u003cp\u003e现在基于 BPF spinlock 可以实现做这个功能，但并不是最优的方式，因为如果有合适的原\n子操作，我们就能节省两次辅助函数调用，然后将 spinlock 移到 map 里。将 spinlock\n放到 map 结构体的额外好处是，每个结构体都有自己独立的结构（互相解耦），因此更能\n够避免升级/降低导致的问题。\u003c/p\u003e\n\n\u003cp\u003e当前内核只有 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eBPF_XADD\u003c/code\u003e 指令，我认为它主要适用于计数（counting），因为它并不像原\n子递增（inc）函数一样返回一个值。此外内核中还有的就是针对 maps 的 spinlock。\u003c/p\u003e\n\n\u003cp\u003e我觉得如果有 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eREAD_ONCE/WRITE_ONCE\u003c/code\u003e 语义将会带来很大便利，现在的 BPF 代码中其实已\n经有了一些这样功能的、自己实现的代码。此外，我们还需要 \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eBPF_XCHG, BPF_CMPXCHG\u003c/code\u003e 指\n令，这也将带来很大帮助。\u003c/p\u003e\n\n\u003ch2 id=\"37-bpf-getpeername-hook\"\u003e3.7 BPF \u003ccode class=\"language-plaintext highlighter-rouge\"\u003egetpeername\u003c/code\u003e hook\u003c/h2\u003e\n\n\u003cp\u003e还有一个 hook —— \u003ccode class=\"language-plaintext highlighter-rouge\"\u003egetpeername()\u003c/code\u003e —— 没有讨论到，它\u003cstrong\u003e用在 TCP 和 connected UDP 场\n景\u003c/strong\u003e，对应用是透明的。\u003c/p\u003e\n\n\u003cp\u003e这里的想法是：永远返回 Service IP 而不是 backend pod IP，这样对应用来说，它看到\n就是和 Service IP 建立的连接，而不是和某个具体的 backend pod。\u003c/p\u003e\n\n\u003cp\u003e现在返回的是 backend IP 而不是 service IP。从应用的角度看，它连接到的对端并不是\n它期望的。\u003c/p\u003e\n\n\u003ch2 id=\"38-绕过内核最大-bpf-指令数的限制\"\u003e3.8 绕过内核最大 BPF 指令数的限制\u003c/h2\u003e\n\n\u003cp\u003e最后再讨论几个非内核的改动（non-kernel changes）。\u003c/p\u003e\n\n\u003cp\u003e内核对 \u003cstrong\u003eBPF 最大指令数有 4K 条\u003c/strong\u003e的限制，现在这个限制已经放大到 \u003cstrong\u003e1M\u003c/strong\u003e（一百万）\n条（但需要 5.1+ 内核，或者稍低版本的内核 + 相应 patch）。\u003c/p\u003e\n\n\u003cp\u003e我们的 BPF 程序中包含了 NAT 引擎，因此肯定是超过这个限制的。\n但 Cilium 这边，我们目前还并未用到这个新的最大限制，而是通过“外包”的方式将 BPF\n切分成了子 BPF 程序，然后通过尾调用（tail call）跳转过去，以此来绕过这个 4K 的限\n制。\u003c/p\u003e\n\n\u003cp\u003e另外，我们当前使用的是 BPF tail call，而不是 BPF-to-BPF call，因为\u003cstrong\u003e二者不能同时\n使用\u003c/strong\u003e。更好的方式是，Cilium agent 在启动时进行检查，如果内核支持 1M BPF\ninsns/complexity limit + bounded loops（我们用于 NAT mappings 查询优化），就用这\n些新特性；否则回退到尾调用的方式。\u003c/p\u003e\n\n\u003ch1 id=\"4-cilium-上手用-kubeadm-搭建体验环境\"\u003e4 Cilium 上手：用 kubeadm 搭建体验环境\u003c/h1\u003e\n\n\u003cp\u003e有兴趣尝试 Cilium，可以参考下面的快速安装命令：\u003c/p\u003e\n\n\u003cdiv class=\"language-shell highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e\u003cspan class=\"nv\"\u003e$ \u003c/span\u003ekubeadm init \u003cspan class=\"nt\"\u003e--pod-network-cidr\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003e10.217.0.0/16 \u003cspan class=\"nt\"\u003e--skip-phases\u003c/span\u003e\u003cspan class=\"o\"\u003e=\u003c/span\u003eaddon/kube-proxy\n\u003cspan class=\"nv\"\u003e$ \u003c/span\u003ekubeadm \u003cspan class=\"nb\"\u003ejoin\u003c/span\u003e \u003cspan class=\"o\"\u003e[\u003c/span\u003e...]\n\u003cspan class=\"nv\"\u003e$ \u003c/span\u003ehelm template cilium \u003cspan class=\"se\"\u003e\\\u003c/span\u003e\n\t\t \u003cspan class=\"nt\"\u003e--namespace\u003c/span\u003e kube-system \u003cspan class=\"nt\"\u003e--set\u003c/span\u003e global.nodePort.enabled\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"nb\"\u003etrue\u003c/span\u003e \u003cspan class=\"se\"\u003e\\\u003c/span\u003e\n\t\t \u003cspan class=\"nt\"\u003e--set\u003c/span\u003e global.k8sServiceHost\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"nv\"\u003e$API_SERVER_IP\u003c/span\u003e \u003cspan class=\"se\"\u003e\\\u003c/span\u003e\n\t\t \u003cspan class=\"nt\"\u003e--set\u003c/span\u003e global.k8sServicePort\u003cspan class=\"o\"\u003e=\u003c/span\u003e\u003cspan class=\"nv\"\u003e$API_SERVER_PORT\u003c/span\u003e \u003cspan class=\"se\"\u003e\\\u003c/span\u003e\n\t\t \u003cspan class=\"nt\"\u003e--set\u003c/span\u003e global.tag\u003cspan class=\"o\"\u003e=\u003c/span\u003ev1.6.1 \u003cspan class=\"o\"\u003e\u0026gt;\u003c/span\u003e cilium.yaml\n\t\t kubectl apply \u003cspan class=\"nt\"\u003e-f\u003c/span\u003e cilium.yaml\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\n  \u003c!-- POST NAVIGATION --\u003e\n  \u003cdiv class=\"postNav clearfix\"\u003e\n     \n      \u003ca class=\"prev\" href=\"/blog/cilium-k8s-service-lb-zh/\"\u003e\u003cspan\u003e« [译] 基于 BPF/XDP 实现 K8s Service 负载均衡 (LPC, 2020)\u003c/span\u003e\n      \n    \u003c/a\u003e\n      \n      \n      \u003ca class=\"next\" href=\"/blog/packet-mark-in-a-cloud-native-world-zh/\"\u003e\u003cspan\u003e[译] 云原生世界中的数据包标记（packet mark）(LPC, 2020) »\u003c/span\u003e\n       \n      \u003c/a\u003e\n     \n  \u003c/div\u003e\n\u003c/div\u003e",
  "Date": "2020-11-29T00:00:00Z",
  "Author": "Arthur Chiao"
}