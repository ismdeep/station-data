{
  "Source": "dunwu",
  "Title": "MapReduce",
  "Link": "https://dunwu.github.io/blog/pages/7644aa/",
  "Content": "\u003cdiv class=\"post-body\" itemprop=\"articleBody\"\u003e\u003ch1 id=\"MapReduce\"\u003e\u003ca href=\"#MapReduce\" class=\"headerlink\" title=\"MapReduce\"\u003e\u003c/a\u003eMapReduce\u003c/h1\u003e\u003ch2 id=\"MapReduce-简介\"\u003e\u003ca href=\"#MapReduce-简介\" class=\"headerlink\" title=\"MapReduce 简介\"\u003e\u003c/a\u003eMapReduce 简介\u003c/h2\u003e\u003cblockquote\u003e\n\u003cp\u003eHadoop MapReduce 是一个分布式计算框架，用于编写批处理应用程序。编写好的程序可以提交到 Hadoop 集群上用于并行处理大规模的数据集。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eMapReduce 的设计思路是：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e分而治之，并行计算\u003c/li\u003e\n\u003cli\u003e移动计算，而非移动数据\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eMapReduce 作业通过将输入的数据集拆分为独立的块，这些块由 \u003ccode\u003emap\u003c/code\u003e 以并行的方式处理，框架对 \u003ccode\u003emap\u003c/code\u003e 的输出进行排序，然后输入到 \u003ccode\u003ereduce\u003c/code\u003e 中。MapReduce 框架专门用于 \u003ccode\u003e\u0026lt;key，value\u0026gt;\u003c/code\u003e 键值对处理，它将作业的输入视为一组 \u003ccode\u003e\u0026lt;key，value\u0026gt;\u003c/code\u003e 对，并生成一组 \u003ccode\u003e\u0026lt;key，value\u0026gt;\u003c/code\u003e 对作为输出。输出和输出的 \u003ccode\u003ekey\u003c/code\u003e 和 \u003ccode\u003evalue\u003c/code\u003e 都必须实现\u003ca target=\"_blank\" rel=\"noopener\" href=\"http://hadoop.apache.org/docs/stable/api/org/apache/hadoop/io/Writable.html\"\u003eWritable\u003c/a\u003e 接口。\u003c/p\u003e\n\u003cfigure class=\"highlight xl\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e(\u003cspan class=\"function\"\u003e\u003cspan class=\"title\"\u003einput\u003c/span\u003e) \u0026lt;k1, v1\u0026gt; -\u0026gt;\u003c/span\u003e \u003cspan class=\"function\"\u003e\u003cspan class=\"title\"\u003emap\u003c/span\u003e -\u0026gt;\u003c/span\u003e \u0026lt;\u003cspan class=\"function\"\u003e\u003cspan class=\"title\"\u003ek2\u003c/span\u003e, v2\u0026gt; -\u0026gt;\u003c/span\u003e \u003cspan class=\"function\"\u003e\u003cspan class=\"title\"\u003ecombine\u003c/span\u003e -\u0026gt;\u003c/span\u003e \u0026lt;\u003cspan class=\"function\"\u003e\u003cspan class=\"title\"\u003ek2\u003c/span\u003e, v2\u0026gt; -\u0026gt;\u003c/span\u003e \u003cspan class=\"function\"\u003e\u003cspan class=\"title\"\u003ereduce\u003c/span\u003e -\u0026gt;\u003c/span\u003e \u0026lt;k3, v3\u0026gt; (output)\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003ch3 id=\"特点\"\u003e\u003ca href=\"#特点\" class=\"headerlink\" title=\"特点\"\u003e\u003c/a\u003e特点\u003c/h3\u003e\u003cul\u003e\n\u003cli\u003e计算跟着数据走\u003c/li\u003e\n\u003cli\u003e良好的扩展性：计算能力随着节点数增加，近似线性递增\u003c/li\u003e\n\u003cli\u003e高容错\u003c/li\u003e\n\u003cli\u003e状态监控\u003c/li\u003e\n\u003cli\u003e适合海量数据的离线批处理\u003c/li\u003e\n\u003cli\u003e降低了分布式编程的门槛\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"应用场景\"\u003e\u003ca href=\"#应用场景\" class=\"headerlink\" title=\"应用场景\"\u003e\u003c/a\u003e应用场景\u003c/h3\u003e\u003cp\u003e适用场景：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e数据统计，如：网站的 PV、UV 统计\u003c/li\u003e\n\u003cli\u003e搜索引擎构建索引\u003c/li\u003e\n\u003cli\u003e海量数据查询\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e不适用场景：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOLAP\u003cul\u003e\n\u003cli\u003e要求毫秒或秒级返回结果\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e流计算\u003cul\u003e\n\u003cli\u003e流计算的输入数据集是动态的，而 MapReduce 是静态的\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eDAG 计算\u003cul\u003e\n\u003cli\u003e多个作业存在依赖关系，后一个的输入是前一个的输出，构成有向无环图 DAG\u003c/li\u003e\n\u003cli\u003e每个 MapReduce 作业的输出结果都会落盘，造成大量磁盘 IO，导致性能非常低下\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"MapReduce-编程模型\"\u003e\u003ca href=\"#MapReduce-编程模型\" class=\"headerlink\" title=\"MapReduce 编程模型\"\u003e\u003c/a\u003eMapReduce 编程模型\u003c/h2\u003e\u003cp\u003eMapReduce 编程模型：MapReduce 程序被分为 Map（映射）阶段和 Reduce（化简）阶段。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/dunwu/images/master/snap/20200601162305.png\" alt=\"img\"/\u003e\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003einput\u003c/strong\u003e : 读取文本文件；\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003esplitting\u003c/strong\u003e : 将文件按照行进行拆分，此时得到的 \u003ccode\u003eK1\u003c/code\u003e 行数，\u003ccode\u003eV1\u003c/code\u003e 表示对应行的文本内容；\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003emapping\u003c/strong\u003e : 并行将每一行按照空格进行拆分，拆分得到的 \u003ccode\u003eList(K2,V2)\u003c/code\u003e，其中 \u003ccode\u003eK2\u003c/code\u003e 代表每一个单词，由于是做词频统计，所以 \u003ccode\u003eV2\u003c/code\u003e 的值为 1，代表出现 1 次；\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eshuffling\u003c/strong\u003e：由于 \u003ccode\u003eMapping\u003c/code\u003e 操作可能是在不同的机器上并行处理的，所以需要通过 \u003ccode\u003eshuffling\u003c/code\u003e 将相同 \u003ccode\u003ekey\u003c/code\u003e 值的数据分发到同一个节点上去合并，这样才能统计出最终的结果，此时得到 \u003ccode\u003eK2\u003c/code\u003e 为每一个单词，\u003ccode\u003eList(V2)\u003c/code\u003e 为可迭代集合，\u003ccode\u003eV2\u003c/code\u003e 就是 Mapping 中的 V2；\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eReducing\u003c/strong\u003e : 这里的案例是统计单词出现的总次数，所以 \u003ccode\u003eReducing\u003c/code\u003e 对 \u003ccode\u003eList(V2)\u003c/code\u003e 进行归约求和操作，最终输出。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eMapReduce 编程模型中 \u003ccode\u003esplitting\u003c/code\u003e 和 \u003ccode\u003eshuffing\u003c/code\u003e 操作都是由框架实现的，需要我们自己编程实现的只有 \u003ccode\u003emapping\u003c/code\u003e 和 \u003ccode\u003ereducing\u003c/code\u003e，这也就是 MapReduce 这个称呼的来源。\u003c/p\u003e\n\u003ch2 id=\"combiner-partitioner\"\u003e\u003ca href=\"#combiner-partitioner\" class=\"headerlink\" title=\"combiner \u0026amp; partitioner\"\u003e\u003c/a\u003ecombiner \u0026amp; partitioner\u003c/h2\u003e\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/dunwu/images/master/snap/20200601163846.png\" alt=\"img\"/\u003e\u003c/p\u003e\n\u003ch3 id=\"InputFormat-RecordReaders\"\u003e\u003ca href=\"#InputFormat-RecordReaders\" class=\"headerlink\" title=\"InputFormat \u0026amp; RecordReaders\"\u003e\u003c/a\u003eInputFormat \u0026amp; RecordReaders\u003c/h3\u003e\u003cp\u003e\u003ccode\u003eInputFormat\u003c/code\u003e 将输出文件拆分为多个 \u003ccode\u003eInputSplit\u003c/code\u003e，并由 \u003ccode\u003eRecordReaders\u003c/code\u003e 将 \u003ccode\u003eInputSplit\u003c/code\u003e 转换为标准的\u0026lt;key，value\u0026gt;键值对，作为 map 的输出。这一步的意义在于只有先进行逻辑拆分并转为标准的键值对格式后，才能为多个 \u003ccode\u003emap\u003c/code\u003e 提供输入，以便进行并行处理。\u003c/p\u003e\n\u003ch3 id=\"Combiner\"\u003e\u003ca href=\"#Combiner\" class=\"headerlink\" title=\"Combiner\"\u003e\u003c/a\u003eCombiner\u003c/h3\u003e\u003cp\u003e\u003ccode\u003ecombiner\u003c/code\u003e 是 \u003ccode\u003emap\u003c/code\u003e 运算后的可选操作，它实际上是一个本地化的 \u003ccode\u003ereduce\u003c/code\u003e 操作，它主要是在 \u003ccode\u003emap\u003c/code\u003e 计算出中间文件后做一个简单的合并重复 \u003ccode\u003ekey\u003c/code\u003e 值的操作。这里以词频统计为例：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003emap\u003c/code\u003e 在遇到一个 hadoop 的单词时就会记录为 1，但是这篇文章里 hadoop 可能会出现 n 多次，那么 \u003ccode\u003emap\u003c/code\u003e 输出文件冗余就会很多，因此在 \u003ccode\u003ereduce\u003c/code\u003e 计算前对相同的 key 做一个合并操作，那么需要传输的数据量就会减少，传输效率就可以得到提升。\u003c/p\u003e\n\u003cp\u003e但并非所有场景都适合使用 \u003ccode\u003ecombiner\u003c/code\u003e，使用它的原则是 \u003ccode\u003ecombiner\u003c/code\u003e 的输出不会影响到 \u003ccode\u003ereduce\u003c/code\u003e 计算的最终输入，例如：求总数，最大值，最小值时都可以使用 \u003ccode\u003ecombiner\u003c/code\u003e，但是做平均值计算则不能使用 \u003ccode\u003ecombiner\u003c/code\u003e。\u003c/p\u003e\n\u003cp\u003e不使用 combiner 的情况：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/dunwu/images/master/snap/20200601164709.png\" alt=\"img\"/\u003e\u003c/p\u003e\n\u003cp\u003e使用 combiner 的情况：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/dunwu/images/master/snap/20200601164804.png\" alt=\"img\"/\u003e\u003c/p\u003e\n\u003cp\u003e可以看到使用 combiner 的时候，需要传输到 reducer 中的数据由 12keys，降低到 10keys。降低的幅度取决于你 keys 的重复率，下文词频统计案例会演示用 combiner 降低数百倍的传输量。\u003c/p\u003e\n\u003ch2 id=\"MapReduce-词频统计案例\"\u003e\u003ca href=\"#MapReduce-词频统计案例\" class=\"headerlink\" title=\"MapReduce 词频统计案例\"\u003e\u003c/a\u003eMapReduce 词频统计案例\u003c/h2\u003e\u003ch3 id=\"项目简介\"\u003e\u003ca href=\"#项目简介\" class=\"headerlink\" title=\"项目简介\"\u003e\u003c/a\u003e项目简介\u003c/h3\u003e\u003cp\u003e这里给出一个经典的词频统计的案例：统计如下样本数据中每个单词出现的次数。\u003c/p\u003e\n\u003cfigure class=\"highlight ebnf\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e2\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e3\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e4\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e5\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e6\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e7\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e8\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e9\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e10\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e11\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e12\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e13\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"attribute\"\u003eSpark\tHBase\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"attribute\"\u003eHive\tFlink\tStorm\tHadoop\tHBase\tSpark\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"attribute\"\u003eFlink\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"attribute\"\u003eHBase\tStorm\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"attribute\"\u003eHBase\tHadoop\tHive\tFlink\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"attribute\"\u003eHBase\tFlink\tHive\tStorm\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"attribute\"\u003eHive\tFlink\tHadoop\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"attribute\"\u003eHBase\tHive\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"attribute\"\u003eHadoop\tSpark\tHBase\tStorm\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"attribute\"\u003eHBase\tHadoop\tHive\tFlink\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"attribute\"\u003eHBase\tFlink\tHive\tStorm\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"attribute\"\u003eHive\tFlink\tHadoop\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"attribute\"\u003eHBase\tHive\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003cp\u003e为方便大家开发，我在项目源码中放置了一个工具类 \u003ccode\u003eWordCountDataUtils\u003c/code\u003e，用于模拟产生词频统计的样本，生成的文件支持输出到本地或者直接写到 HDFS 上。\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e项目完整源码下载地址：\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://github.com/heibaiying/BigData-Notes/tree/master/code/Hadoop/hadoop-word-count\"\u003ehadoop-word-count\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch3 id=\"项目依赖\"\u003e\u003ca href=\"#项目依赖\" class=\"headerlink\" title=\"项目依赖\"\u003e\u003c/a\u003e项目依赖\u003c/h3\u003e\u003cp\u003e想要进行 MapReduce 编程，需要导入 \u003ccode\u003ehadoop-client\u003c/code\u003e 依赖：\u003c/p\u003e\n\u003cfigure class=\"highlight xml\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e2\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e3\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e4\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e5\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"tag\"\u003e\u0026lt;\u003cspan class=\"name\"\u003edependency\u003c/span\u003e\u0026gt;\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e    \u003cspan class=\"tag\"\u003e\u0026lt;\u003cspan class=\"name\"\u003egroupId\u003c/span\u003e\u0026gt;\u003c/span\u003eorg.apache.hadoop\u003cspan class=\"tag\"\u003e\u0026lt;/\u003cspan class=\"name\"\u003egroupId\u003c/span\u003e\u0026gt;\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e    \u003cspan class=\"tag\"\u003e\u0026lt;\u003cspan class=\"name\"\u003eartifactId\u003c/span\u003e\u0026gt;\u003c/span\u003ehadoop-client\u003cspan class=\"tag\"\u003e\u0026lt;/\u003cspan class=\"name\"\u003eartifactId\u003c/span\u003e\u0026gt;\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e    \u003cspan class=\"tag\"\u003e\u0026lt;\u003cspan class=\"name\"\u003eversion\u003c/span\u003e\u0026gt;\u003c/span\u003e${hadoop.version}\u003cspan class=\"tag\"\u003e\u0026lt;/\u003cspan class=\"name\"\u003eversion\u003c/span\u003e\u0026gt;\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"tag\"\u003e\u0026lt;/\u003cspan class=\"name\"\u003edependency\u003c/span\u003e\u0026gt;\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003ch3 id=\"WordCountMapper\"\u003e\u003ca href=\"#WordCountMapper\" class=\"headerlink\" title=\"WordCountMapper\"\u003e\u003c/a\u003eWordCountMapper\u003c/h3\u003e\u003cp\u003e将每行数据按照指定分隔符进行拆分。这里需要注意在 MapReduce 中必须使用 Hadoop 定义的类型，因为 Hadoop 预定义的类型都是可序列化，可比较的，所有类型均实现了 \u003ccode\u003eWritableComparable\u003c/code\u003e 接口。\u003c/p\u003e\n\u003cfigure class=\"highlight java\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e2\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e3\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e4\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e5\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e6\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e7\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e8\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e9\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e10\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e11\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e12\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"keyword\"\u003epublic\u003c/span\u003e \u003cspan class=\"keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"title class_\"\u003eWordCountMapper\u003c/span\u003e \u003cspan class=\"keyword\"\u003eextends\u003c/span\u003e \u003cspan class=\"title class_\"\u003eMapper\u003c/span\u003e\u0026lt;LongWritable, Text, Text, IntWritable\u0026gt; {\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e    \u003cspan class=\"meta\"\u003e@Override\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e    \u003cspan class=\"keyword\"\u003eprotected\u003c/span\u003e \u003cspan class=\"keyword\"\u003evoid\u003c/span\u003e \u003cspan class=\"title function_\"\u003emap\u003c/span\u003e\u003cspan class=\"params\"\u003e(LongWritable key, Text value, Context context)\u003c/span\u003e \u003cspan class=\"keyword\"\u003ethrows\u003c/span\u003e IOException,\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e                                                                      InterruptedException {\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        String[] words = value.toString().split(\u003cspan class=\"string\"\u003e\u0026#34;\\t\u0026#34;\u003c/span\u003e);\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        \u003cspan class=\"keyword\"\u003efor\u003c/span\u003e (String word : words) {\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e            context.write(\u003cspan class=\"keyword\"\u003enew\u003c/span\u003e \u003cspan class=\"title class_\"\u003eText\u003c/span\u003e(word), \u003cspan class=\"keyword\"\u003enew\u003c/span\u003e \u003cspan class=\"title class_\"\u003eIntWritable\u003c/span\u003e(\u003cspan class=\"number\"\u003e1\u003c/span\u003e));\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        }\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e    }\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e}\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003cp\u003e\u003ccode\u003eWordCountMapper\u003c/code\u003e 对应下图的 Mapping 操作：\u003c/p\u003e\n\u003cp\u003e\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://camo.githubusercontent.com/fcf3ac016579c1fbb050d97309660aaa3b9550cc/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f702d636f64652d6d617070696e672e706e67\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/fcf3ac016579c1fbb050d97309660aaa3b9550cc/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f702d636f64652d6d617070696e672e706e67\" alt=\"img\"/\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003eWordCountMapper\u003c/code\u003e 继承自 \u003ccode\u003eMappe\u003c/code\u003e 类，这是一个泛型类，定义如下：\u003c/p\u003e\n\u003cfigure class=\"highlight java\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e2\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e3\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e4\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e5\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003eWordCountMapper \u003cspan class=\"keyword\"\u003eextends\u003c/span\u003e \u003cspan class=\"title class_\"\u003eMapper\u003c/span\u003e\u0026lt;LongWritable, Text, Text, IntWritable\u0026gt;\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"keyword\"\u003epublic\u003c/span\u003e \u003cspan class=\"keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"title class_\"\u003eMapper\u003c/span\u003e\u0026lt;KEYIN, VALUEIN, KEYOUT, VALUEOUT\u0026gt; {\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e   ......\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e}\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eKEYIN\u003c/strong\u003e : \u003ccode\u003emapping\u003c/code\u003e 输入 key 的类型，即每行的偏移量 (每行第一个字符在整个文本中的位置)，\u003ccode\u003eLong\u003c/code\u003e 类型，对应 Hadoop 中的 \u003ccode\u003eLongWritable\u003c/code\u003e 类型；\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eVALUEIN\u003c/strong\u003e : \u003ccode\u003emapping\u003c/code\u003e 输入 value 的类型，即每行数据；\u003ccode\u003eString\u003c/code\u003e 类型，对应 Hadoop 中 \u003ccode\u003eText\u003c/code\u003e 类型；\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eKEYOUT\u003c/strong\u003e ：\u003ccode\u003emapping\u003c/code\u003e 输出的 key 的类型，即每个单词；\u003ccode\u003eString\u003c/code\u003e 类型，对应 Hadoop 中 \u003ccode\u003eText\u003c/code\u003e 类型；\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003eVALUEOUT\u003c/strong\u003e：\u003ccode\u003emapping\u003c/code\u003e 输出 value 的类型，即每个单词出现的次数；这里用 \u003ccode\u003eint\u003c/code\u003e 类型，对应 \u003ccode\u003eIntWritable\u003c/code\u003e 类型。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"WordCountReducer\"\u003e\u003ca href=\"#WordCountReducer\" class=\"headerlink\" title=\"WordCountReducer\"\u003e\u003c/a\u003eWordCountReducer\u003c/h3\u003e\u003cp\u003e在 Reduce 中进行单词出现次数的统计：\u003c/p\u003e\n\u003cfigure class=\"highlight java\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e2\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e3\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e4\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e5\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e6\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e7\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e8\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e9\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e10\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e11\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e12\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"keyword\"\u003epublic\u003c/span\u003e \u003cspan class=\"keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"title class_\"\u003eWordCountReducer\u003c/span\u003e \u003cspan class=\"keyword\"\u003eextends\u003c/span\u003e \u003cspan class=\"title class_\"\u003eReducer\u003c/span\u003e\u0026lt;Text, IntWritable, Text, IntWritable\u0026gt; {\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e    \u003cspan class=\"meta\"\u003e@Override\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e    \u003cspan class=\"keyword\"\u003eprotected\u003c/span\u003e \u003cspan class=\"keyword\"\u003evoid\u003c/span\u003e \u003cspan class=\"title function_\"\u003ereduce\u003c/span\u003e\u003cspan class=\"params\"\u003e(Text key, Iterable\u0026lt;IntWritable\u0026gt; values, Context context)\u003c/span\u003e \u003cspan class=\"keyword\"\u003ethrows\u003c/span\u003e IOException,\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e                                                                                  InterruptedException {\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        \u003cspan class=\"type\"\u003eint\u003c/span\u003e \u003cspan class=\"variable\"\u003ecount\u003c/span\u003e \u003cspan class=\"operator\"\u003e=\u003c/span\u003e \u003cspan class=\"number\"\u003e0\u003c/span\u003e;\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        \u003cspan class=\"keyword\"\u003efor\u003c/span\u003e (IntWritable value : values) {\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e            count += value.get();\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        }\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        context.write(key, \u003cspan class=\"keyword\"\u003enew\u003c/span\u003e \u003cspan class=\"title class_\"\u003eIntWritable\u003c/span\u003e(count));\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e    }\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e}\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003cp\u003e如下图，\u003ccode\u003eshuffling\u003c/code\u003e 的输出是 reduce 的输入。这里的 key 是每个单词，values 是一个可迭代的数据类型，类似 \u003ccode\u003e(1,1,1,...)\u003c/code\u003e。\u003c/p\u003e\n\u003cp\u003e\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://camo.githubusercontent.com/bf6ad9c970812b1db6f6f86d12d783db75eaa9fb/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f702d636f64652d726564756365722e706e67\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/bf6ad9c970812b1db6f6f86d12d783db75eaa9fb/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f702d636f64652d726564756365722e706e67\" alt=\"img\"/\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"WordCountApp\"\u003e\u003ca href=\"#WordCountApp\" class=\"headerlink\" title=\"WordCountApp\"\u003e\u003c/a\u003eWordCountApp\u003c/h3\u003e\u003cp\u003e组装 MapReduce 作业，并提交到服务器运行，代码如下：\u003c/p\u003e\n\u003cfigure class=\"highlight java\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e2\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e3\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e4\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e5\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e6\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e7\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e8\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e9\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e10\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e11\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e12\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e13\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e14\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e15\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e16\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e17\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e18\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e19\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e20\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e21\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e22\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e23\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e24\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e25\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e26\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e27\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e28\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e29\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e30\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e31\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e32\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e33\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e34\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e35\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e36\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e37\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e38\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e39\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e40\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e41\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e42\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e43\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e44\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e45\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e46\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e47\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e48\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e49\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e50\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e51\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e52\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e53\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e54\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e55\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e56\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e57\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e58\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e59\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e60\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e61\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e62\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e63\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e64\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e65\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"comment\"\u003e/**\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"comment\"\u003e * 组装作业 并提交到集群运行\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"comment\"\u003e */\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"keyword\"\u003epublic\u003c/span\u003e \u003cspan class=\"keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"title class_\"\u003eWordCountApp\u003c/span\u003e {\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e    \u003cspan class=\"comment\"\u003e// 这里为了直观显示参数 使用了硬编码，实际开发中可以通过外部传参\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e    \u003cspan class=\"keyword\"\u003eprivate\u003c/span\u003e \u003cspan class=\"keyword\"\u003estatic\u003c/span\u003e \u003cspan class=\"keyword\"\u003efinal\u003c/span\u003e \u003cspan class=\"type\"\u003eString\u003c/span\u003e \u003cspan class=\"variable\"\u003eHDFS_URL\u003c/span\u003e \u003cspan class=\"operator\"\u003e=\u003c/span\u003e \u003cspan class=\"string\"\u003e\u0026#34;hdfs://192.168.0.107:8020\u0026#34;\u003c/span\u003e;\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e    \u003cspan class=\"keyword\"\u003eprivate\u003c/span\u003e \u003cspan class=\"keyword\"\u003estatic\u003c/span\u003e \u003cspan class=\"keyword\"\u003efinal\u003c/span\u003e \u003cspan class=\"type\"\u003eString\u003c/span\u003e \u003cspan class=\"variable\"\u003eHADOOP_USER_NAME\u003c/span\u003e \u003cspan class=\"operator\"\u003e=\u003c/span\u003e \u003cspan class=\"string\"\u003e\u0026#34;root\u0026#34;\u003c/span\u003e;\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e    \u003cspan class=\"keyword\"\u003epublic\u003c/span\u003e \u003cspan class=\"keyword\"\u003estatic\u003c/span\u003e \u003cspan class=\"keyword\"\u003evoid\u003c/span\u003e \u003cspan class=\"title function_\"\u003emain\u003c/span\u003e\u003cspan class=\"params\"\u003e(String[] args)\u003c/span\u003e \u003cspan class=\"keyword\"\u003ethrows\u003c/span\u003e Exception {\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        \u003cspan class=\"comment\"\u003e//  文件输入路径和输出路径由外部传参指定\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        \u003cspan class=\"keyword\"\u003eif\u003c/span\u003e (args.length \u0026lt; \u003cspan class=\"number\"\u003e2\u003c/span\u003e) {\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e            System.out.println(\u003cspan class=\"string\"\u003e\u0026#34;Input and output paths are necessary!\u0026#34;\u003c/span\u003e);\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e            \u003cspan class=\"keyword\"\u003ereturn\u003c/span\u003e;\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        }\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        \u003cspan class=\"comment\"\u003e// 需要指明 hadoop 用户名，否则在 HDFS 上创建目录时可能会抛出权限不足的异常\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        System.setProperty(\u003cspan class=\"string\"\u003e\u0026#34;HADOOP_USER_NAME\u0026#34;\u003c/span\u003e, HADOOP_USER_NAME);\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        \u003cspan class=\"type\"\u003eConfiguration\u003c/span\u003e \u003cspan class=\"variable\"\u003econfiguration\u003c/span\u003e \u003cspan class=\"operator\"\u003e=\u003c/span\u003e \u003cspan class=\"keyword\"\u003enew\u003c/span\u003e \u003cspan class=\"title class_\"\u003eConfiguration\u003c/span\u003e();\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        \u003cspan class=\"comment\"\u003e// 指明 HDFS 的地址\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        configuration.set(\u003cspan class=\"string\"\u003e\u0026#34;fs.defaultFS\u0026#34;\u003c/span\u003e, HDFS_URL);\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        \u003cspan class=\"comment\"\u003e// 创建一个 Job\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        \u003cspan class=\"type\"\u003eJob\u003c/span\u003e \u003cspan class=\"variable\"\u003ejob\u003c/span\u003e \u003cspan class=\"operator\"\u003e=\u003c/span\u003e Job.getInstance(configuration);\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        \u003cspan class=\"comment\"\u003e// 设置运行的主类\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        job.setJarByClass(WordCountApp.class);\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        \u003cspan class=\"comment\"\u003e// 设置 Mapper 和 Reducer\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        job.setMapperClass(WordCountMapper.class);\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        job.setReducerClass(WordCountReducer.class);\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        \u003cspan class=\"comment\"\u003e// 设置 Mapper 输出 key 和 value 的类型\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        job.setMapOutputKeyClass(Text.class);\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        job.setMapOutputValueClass(IntWritable.class);\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        \u003cspan class=\"comment\"\u003e// 设置 Reducer 输出 key 和 value 的类型\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        job.setOutputKeyClass(Text.class);\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        job.setOutputValueClass(IntWritable.class);\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        \u003cspan class=\"comment\"\u003e// 如果输出目录已经存在，则必须先删除，否则重复运行程序时会抛出异常\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        \u003cspan class=\"type\"\u003eFileSystem\u003c/span\u003e \u003cspan class=\"variable\"\u003efileSystem\u003c/span\u003e \u003cspan class=\"operator\"\u003e=\u003c/span\u003e FileSystem.get(\u003cspan class=\"keyword\"\u003enew\u003c/span\u003e \u003cspan class=\"title class_\"\u003eURI\u003c/span\u003e(HDFS_URL), configuration, HADOOP_USER_NAME);\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        \u003cspan class=\"type\"\u003ePath\u003c/span\u003e \u003cspan class=\"variable\"\u003eoutputPath\u003c/span\u003e \u003cspan class=\"operator\"\u003e=\u003c/span\u003e \u003cspan class=\"keyword\"\u003enew\u003c/span\u003e \u003cspan class=\"title class_\"\u003ePath\u003c/span\u003e(args[\u003cspan class=\"number\"\u003e1\u003c/span\u003e]);\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        \u003cspan class=\"keyword\"\u003eif\u003c/span\u003e (fileSystem.exists(outputPath)) {\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e            fileSystem.delete(outputPath, \u003cspan class=\"literal\"\u003etrue\u003c/span\u003e);\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        }\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        \u003cspan class=\"comment\"\u003e// 设置作业输入文件和输出文件的路径\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        FileInputFormat.setInputPaths(job, \u003cspan class=\"keyword\"\u003enew\u003c/span\u003e \u003cspan class=\"title class_\"\u003ePath\u003c/span\u003e(args[\u003cspan class=\"number\"\u003e0\u003c/span\u003e]));\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        FileOutputFormat.setOutputPath(job, outputPath);\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        \u003cspan class=\"comment\"\u003e// 将作业提交到群集并等待它完成，参数设置为 true 代表打印显示对应的进度\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        \u003cspan class=\"type\"\u003eboolean\u003c/span\u003e \u003cspan class=\"variable\"\u003eresult\u003c/span\u003e \u003cspan class=\"operator\"\u003e=\u003c/span\u003e job.waitForCompletion(\u003cspan class=\"literal\"\u003etrue\u003c/span\u003e);\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        \u003cspan class=\"comment\"\u003e// 关闭之前创建的 fileSystem\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        fileSystem.close();\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        \u003cspan class=\"comment\"\u003e// 根据作业结果,终止当前运行的 Java 虚拟机,退出程序\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        System.exit(result ? \u003cspan class=\"number\"\u003e0\u003c/span\u003e : -\u003cspan class=\"number\"\u003e1\u003c/span\u003e);\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e    }\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e}\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003cp\u003e需要注意的是：如果不设置 \u003ccode\u003eMapper\u003c/code\u003e 操作的输出类型，则程序默认它和 \u003ccode\u003eReducer\u003c/code\u003e 操作输出的类型相同。\u003c/p\u003e\n\u003ch3 id=\"提交到服务器运行\"\u003e\u003ca href=\"#提交到服务器运行\" class=\"headerlink\" title=\"提交到服务器运行\"\u003e\u003c/a\u003e提交到服务器运行\u003c/h3\u003e\u003cp\u003e在实际开发中，可以在本机配置 hadoop 开发环境，直接在 IDE 中启动进行测试。这里主要介绍一下打包提交到服务器运行。由于本项目没有使用除 Hadoop 外的第三方依赖，直接打包即可：\u003c/p\u003e\n\u003cfigure class=\"highlight 1c\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"meta\"\u003e# mvn clean package\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003cp\u003e使用以下命令提交作业：\u003c/p\u003e\n\u003cfigure class=\"highlight stylus\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e2\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e3\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003ehadoop jar /usr/appjar/hadoop-word-count-\u003cspan class=\"number\"\u003e1.0\u003c/span\u003e\u003cspan class=\"selector-class\"\u003e.jar\u003c/span\u003e \\\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003ecom\u003cspan class=\"selector-class\"\u003e.heibaiying\u003c/span\u003e\u003cspan class=\"selector-class\"\u003e.WordCountApp\u003c/span\u003e \\\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e/wordcount/\u003cspan class=\"selector-tag\"\u003einput\u003c/span\u003e\u003cspan class=\"selector-class\"\u003e.txt\u003c/span\u003e /wordcount/output/WordCountApp\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003cp\u003e作业完成后查看 HDFS 上生成目录：\u003c/p\u003e\n\u003cfigure class=\"highlight bash\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e2\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e3\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e4\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e5\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"comment\"\u003e# 查看目录\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003ehadoop fs -\u003cspan class=\"built_in\"\u003els\u003c/span\u003e /wordcount/output/WordCountApp\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"comment\"\u003e# 查看统计结果\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003ehadoop fs -\u003cspan class=\"built_in\"\u003ecat\u003c/span\u003e /wordcount/output/WordCountApp/part-r-00000\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003cp\u003e\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://camo.githubusercontent.com/cecb00eef3b951794fbf92b8308d8b6601faf5a0/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f702d776f7264636f756e746170702e706e67\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/cecb00eef3b951794fbf92b8308d8b6601faf5a0/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f702d776f7264636f756e746170702e706e67\" alt=\"img\"/\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"词频统计案例进阶之-Combiner\"\u003e\u003ca href=\"#词频统计案例进阶之-Combiner\" class=\"headerlink\" title=\"词频统计案例进阶之 Combiner\"\u003e\u003c/a\u003e词频统计案例进阶之 Combiner\u003c/h2\u003e\u003ch3 id=\"代码实现\"\u003e\u003ca href=\"#代码实现\" class=\"headerlink\" title=\"代码实现\"\u003e\u003c/a\u003e代码实现\u003c/h3\u003e\u003cp\u003e想要使用 \u003ccode\u003ecombiner\u003c/code\u003e 功能只要在组装作业时，添加下面一行代码即可：\u003c/p\u003e\n\u003cfigure class=\"highlight cos\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e2\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"comment\"\u003e// 设置 Combiner\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"keyword\"\u003ejob\u003c/span\u003e.setCombinerClass(WordCountReducer.\u003cspan class=\"keyword\"\u003eclass\u003c/span\u003e)\u003cspan class=\"comment\"\u003e;\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003ch3 id=\"执行结果\"\u003e\u003ca href=\"#执行结果\" class=\"headerlink\" title=\"执行结果\"\u003e\u003c/a\u003e执行结果\u003c/h3\u003e\u003cp\u003e加入 \u003ccode\u003ecombiner\u003c/code\u003e 后统计结果是不会有变化的，但是可以从打印的日志看出 \u003ccode\u003ecombiner\u003c/code\u003e 的效果：\u003c/p\u003e\n\u003cp\u003e没有加入 \u003ccode\u003ecombiner\u003c/code\u003e 的打印日志：\u003c/p\u003e\n\u003cp\u003e\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://camo.githubusercontent.com/e4849556db34d3a02b82b546af7296154920dfff/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f702d6e6f2d636f6d62696e65722e706e67\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/e4849556db34d3a02b82b546af7296154920dfff/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f702d6e6f2d636f6d62696e65722e706e67\" alt=\"img\"/\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e加入 \u003ccode\u003ecombiner\u003c/code\u003e 后的打印日志如下：\u003c/p\u003e\n\u003cp\u003e\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://camo.githubusercontent.com/17f20f481bc4bd01252bc3ccb3b2aceb7d0eca63/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f702d636f6d62696e65722e706e67\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/17f20f481bc4bd01252bc3ccb3b2aceb7d0eca63/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f702d636f6d62696e65722e706e67\" alt=\"img\"/\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e这里我们只有一个输入文件并且小于 128M，所以只有一个 Map 进行处理。可以看到经过 combiner 后，records 由 \u003ccode\u003e3519\u003c/code\u003e 降低为 \u003ccode\u003e6\u003c/code\u003e(样本中单词种类就只有 6 种)，在这个用例中 combiner 就能极大地降低需要传输的数据量。\u003c/p\u003e\n\u003ch2 id=\"词频统计案例进阶之-Partitioner\"\u003e\u003ca href=\"#词频统计案例进阶之-Partitioner\" class=\"headerlink\" title=\"词频统计案例进阶之 Partitioner\"\u003e\u003c/a\u003e词频统计案例进阶之 Partitioner\u003c/h2\u003e\u003ch3 id=\"默认的-Partitioner\"\u003e\u003ca href=\"#默认的-Partitioner\" class=\"headerlink\" title=\"默认的 Partitioner\"\u003e\u003c/a\u003e默认的 Partitioner\u003c/h3\u003e\u003cp\u003e这里假设有个需求：将不同单词的统计结果输出到不同文件。这种需求实际上比较常见，比如统计产品的销量时，需要将结果按照产品种类进行拆分。要实现这个功能，就需要用到自定义 \u003ccode\u003ePartitioner\u003c/code\u003e。\u003c/p\u003e\n\u003cp\u003e这里先介绍下 MapReduce 默认的分类规则：在构建 job 时候，如果不指定，默认的使用的是 \u003ccode\u003eHashPartitioner\u003c/code\u003e：对 key 值进行哈希散列并对 \u003ccode\u003enumReduceTasks\u003c/code\u003e 取余。其实现如下：\u003c/p\u003e\n\u003cfigure class=\"highlight angelscript\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e2\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e3\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e4\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e5\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e6\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e7\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e8\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"keyword\"\u003epublic\u003c/span\u003e \u003cspan class=\"keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"symbol\"\u003eHashPartitioner\u003c/span\u003e\u0026lt;\u003cspan class=\"symbol\"\u003eK, \u003cspan class=\"symbol\"\u003eV\u003c/span\u003e\u003c/span\u003e\u0026gt; \u003cspan class=\"symbol\"\u003eextends\u003c/span\u003e \u003cspan class=\"symbol\"\u003ePartitioner\u003c/span\u003e\u0026lt;\u003cspan class=\"symbol\"\u003eK, \u003cspan class=\"symbol\"\u003eV\u003c/span\u003e\u003c/span\u003e\u0026gt; {\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  \u003cspan class=\"keyword\"\u003epublic\u003c/span\u003e \u003cspan class=\"built_in\"\u003eint\u003c/span\u003e getPartition(K key, V value,\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e                          \u003cspan class=\"built_in\"\u003eint\u003c/span\u003e numReduceTasks) {\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e    \u003cspan class=\"keyword\"\u003ereturn\u003c/span\u003e (key.hashCode() \u0026amp; Integer.MAX_VALUE) % numReduceTasks;\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  }\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e}\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003ch3 id=\"自定义-Partitioner\"\u003e\u003ca href=\"#自定义-Partitioner\" class=\"headerlink\" title=\"自定义 Partitioner\"\u003e\u003c/a\u003e自定义 Partitioner\u003c/h3\u003e\u003cp\u003e这里我们继承 \u003ccode\u003ePartitioner\u003c/code\u003e 自定义分类规则，这里按照单词进行分类：\u003c/p\u003e\n\u003cfigure class=\"highlight angelscript\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e2\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e3\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e4\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e5\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e6\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"keyword\"\u003epublic\u003c/span\u003e \u003cspan class=\"keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"symbol\"\u003eCustomPartitioner\u003c/span\u003e \u003cspan class=\"symbol\"\u003eextends\u003c/span\u003e \u003cspan class=\"symbol\"\u003ePartitioner\u003c/span\u003e\u0026lt;\u003cspan class=\"symbol\"\u003eText, \u003cspan class=\"symbol\"\u003eIntWritable\u003c/span\u003e\u003c/span\u003e\u0026gt; {\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e    \u003cspan class=\"keyword\"\u003epublic\u003c/span\u003e \u003cspan class=\"built_in\"\u003eint\u003c/span\u003e getPartition(Text text, IntWritable \u003cspan class=\"built_in\"\u003eint\u003c/span\u003eWritable, \u003cspan class=\"built_in\"\u003eint\u003c/span\u003e numPartitions) {\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        \u003cspan class=\"keyword\"\u003ereturn\u003c/span\u003e WordCountDataUtils.WORD_LIST.indexOf(text.toString());\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e    }\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e}\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003cp\u003e在构建 \u003ccode\u003ejob\u003c/code\u003e 时候指定使用我们自己的分类规则，并设置 \u003ccode\u003ereduce\u003c/code\u003e 的个数：\u003c/p\u003e\n\u003cfigure class=\"highlight cos\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e2\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e3\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e4\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"comment\"\u003e// 设置自定义分区规则\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"keyword\"\u003ejob\u003c/span\u003e.setPartitionerClass(CustomPartitioner.\u003cspan class=\"keyword\"\u003eclass\u003c/span\u003e)\u003cspan class=\"comment\"\u003e;\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"comment\"\u003e// 设置 reduce 个数\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"keyword\"\u003ejob\u003c/span\u003e.setNumReduceTasks(WordCountDataUtils.WORD_LIST.size())\u003cspan class=\"comment\"\u003e;\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003ch3 id=\"执行结果-1\"\u003e\u003ca href=\"#执行结果-1\" class=\"headerlink\" title=\"执行结果\"\u003e\u003c/a\u003e执行结果\u003c/h3\u003e\u003cp\u003e执行结果如下，分别生成 6 个文件，每个文件中为对应单词的统计结果：\u003c/p\u003e\n\u003cp\u003e\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://camo.githubusercontent.com/202b1eb7065e18a513db5b2a50b22ab62a7d6692/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f702d776f7264636f756e74636f6d62696e6572706172746974696f6e2e706e67\"\u003e\u003cimg src=\"https://camo.githubusercontent.com/202b1eb7065e18a513db5b2a50b22ab62a7d6692/68747470733a2f2f67697465652e636f6d2f68656962616979696e672f426967446174612d4e6f7465732f7261772f6d61737465722f70696374757265732f6861646f6f702d776f7264636f756e74636f6d62696e6572706172746974696f6e2e706e67\" alt=\"img\"/\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"参考资料\"\u003e\u003ca href=\"#参考资料\" class=\"headerlink\" title=\"参考资料\"\u003e\u003c/a\u003e参考资料\u003c/h2\u003e\u003cul\u003e\n\u003cli\u003e\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://github.com/heibaiying/BigData-Notes/blob/master/notes/Hadoop-MapReduce.md\"\u003e分布式计算框架——MapReduce\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n    \u003c/div\u003e",
  "Date": "2020-06-21T16:22:25Z",
  "Author": "钝悟 ◾ Dunwu"
}