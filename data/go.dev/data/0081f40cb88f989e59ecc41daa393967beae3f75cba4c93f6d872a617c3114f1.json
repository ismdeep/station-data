{
  "Source": "go.dev",
  "Title": "Text normalization in Go",
  "Link": "https://go.dev/blog/normalization",
  "Content": "\u003cdiv class=\"Article\" data-slug=\"/blog/normalization\"\u003e\n    \n    \u003ch1 class=\"small\"\u003e\u003ca href=\"/blog/\"\u003eThe Go Blog\u003c/a\u003e\u003c/h1\u003e\n    \n\n    \u003ch1\u003eText normalization in Go\u003c/h1\u003e\n      \n      \u003cp class=\"author\"\u003e\n      Marcel van Lohuizen\u003cbr/\u003e\n      26 November 2013\n      \u003c/p\u003e\n      \n      \u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eAn earlier \u003ca href=\"/blog/strings\"\u003epost\u003c/a\u003e talked about strings, bytes\nand characters in Go. I’ve been working on various packages for multilingual\ntext processing for the go.text repository. Several of these packages deserve a\nseparate blog post, but today I want to focus on\n\u003ca href=\"https://pkg.go.dev/golang.org/x/text/unicode/norm\" rel=\"noreferrer\" target=\"_blank\"\u003ego.text/unicode/norm\u003c/a\u003e,\nwhich handles normalization, a topic touched in the\n\u003ca href=\"/blog/strings\"\u003estrings article\u003c/a\u003e and the subject of this\npost. Normalization works at a higher level of abstraction than raw bytes.\u003c/p\u003e\n\u003cp\u003eTo learn pretty much everything you ever wanted to know about normalization\n(and then some), \u003ca href=\"http://unicode.org/reports/tr15/\" rel=\"noreferrer\" target=\"_blank\"\u003eAnnex 15 of the Unicode Standard\u003c/a\u003e\nis a good read. A more approachable article is the corresponding\n\u003ca href=\"http://en.wikipedia.org/wiki/Unicode_equivalence\" rel=\"noreferrer\" target=\"_blank\"\u003eWikipedia page\u003c/a\u003e. Here we\nfocus on how normalization relates to Go.\u003c/p\u003e\n\u003ch2 id=\"what-is-normalization\"\u003eWhat is normalization?\u003c/h2\u003e\n\u003cp\u003eThere are often several ways to represent the same string. For example, an é\n(e-acute) can be represented in a string as a single rune (\u0026#34;\\u00e9\u0026#34;) or an ’e\u0026#39;\nfollowed by an acute accent (“e\\u0301”). According to the Unicode standard,\nthese two are “canonically equivalent” and should be treated as equal.\u003c/p\u003e\n\u003cp\u003eUsing a byte-to-byte comparison to determine equality would clearly not give\nthe right result for these two strings. Unicode defines a set of normal forms\nsuch that if two strings are canonically equivalent and are normalized to the\nsame normal form, their byte representations are the same.\u003c/p\u003e\n\u003cp\u003eUnicode also defines a “compatibility equivalence” to equate characters that\nrepresent the same characters, but may have a different visual appearance. For\nexample, the superscript digit ‘⁹’ and the regular digit ‘9’ are equivalent in\nthis form.\u003c/p\u003e\n\u003cp\u003eFor each of these two equivalence forms, Unicode defines a composing and\ndecomposing form. The former replaces runes that can combine into a single rune\nwith this single rune. The latter breaks runes apart into their components.\nThis table shows the names, all starting with NF, by which the Unicode\nConsortium identifies these forms:\u003c/p\u003e\n\u003cstyle\u003e\n    .padtable td, .padtable th { padding-right: 10px; }\n\u003c/style\u003e\n\u003ctable class=\"codetable padtable\"\u003e\n    \u003ctbody\u003e\u003ctr\u003e\n        \u003cth\u003e \u003c/th\u003e\n        \u003cth\u003eComposing\u003c/th\u003e\n        \u003cth\u003eDecomposing\u003c/th\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003cth\u003eCanonical equivalence\u003c/th\u003e\n        \u003ctd\u003eNFC\u003c/td\u003e\n        \u003ctd\u003eNFD\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003cth\u003eCompatibility equivalence\u003c/th\u003e\n        \u003ctd\u003eNFKC\u003c/td\u003e\n        \u003ctd\u003eNFKD\u003c/td\u003e\n    \u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003ch2 id=\"gos-approach-to-normalization\"\u003eGo’s approach to normalization\u003c/h2\u003e\n\u003cp\u003eAs mentioned in the strings blog post, Go does not guarantee that characters in\na string are normalized. However, the go.text packages can compensate. For\nexample, the\n\u003ca href=\"https://pkg.go.dev/golang.org/x/text/collate\" rel=\"noreferrer\" target=\"_blank\"\u003ecollate\u003c/a\u003e package, which\ncan sort strings in a language-specific way, works correctly even with\nunnormalized strings. The packages in go.text do not always require normalized\ninput, but in general normalization may be necessary for consistent results.\u003c/p\u003e\n\u003cp\u003eNormalization isn’t free but it is fast, particularly for collation and\nsearching or if a string is either in NFD or in NFC and can be converted to NFD\nby decomposing without reordering its bytes. In practice,\n\u003ca href=\"http://www.macchiato.com/unicode/nfc-faq#TOC-How-much-text-is-already-NFC-\" rel=\"noreferrer\" target=\"_blank\"\u003e99.98%\u003c/a\u003e of\nthe web’s HTML page content is in NFC form (not counting markup, in which case\nit would be more). By far most NFC can be decomposed to NFD without the need\nfor reordering (which requires allocation). Also, it is efficient to detect\nwhen reordering is necessary, so we can save time by doing it only for the rare\nsegments that need it.\u003c/p\u003e\n\u003cp\u003eTo make things even better, the collation package typically does not use the\nnorm package directly, but instead uses the norm package to interleave\nnormalization information with its own tables. Interleaving the two problems\nallows for reordering and normalization on the fly with almost no impact on\nperformance. The cost of on-the-fly normalization is compensated by not having\nto normalize text beforehand and ensuring that the normal form is maintained\nupon edits. The latter can be tricky. For instance, the result of concatenating\ntwo NFC-normalized strings is not guaranteed to be in NFC.\u003c/p\u003e\n\u003cp\u003eOf course, we can also avoid the overhead outright if we know in advance that a\nstring is already normalized, which is often the case.\u003c/p\u003e\n\u003ch2 id=\"why-bother\"\u003eWhy bother?\u003c/h2\u003e\n\u003cp\u003eAfter all this discussion about avoiding normalization, you might ask why it’s\nworth worrying about at all. The reason is that there are cases where\nnormalization is required and it is important to understand what those are, and\nin turn how to do it correctly.\u003c/p\u003e\n\u003cp\u003eBefore discussing those, we must first clarify the concept of ‘character’.\u003c/p\u003e\n\u003ch2 id=\"what-is-a-character\"\u003eWhat is a character?\u003c/h2\u003e\n\u003cp\u003eAs was mentioned in the strings blog post, characters can span multiple runes.\nFor example, an ’e’ and ‘◌́’ (acute “\\u0301”) can combine to form ‘é’ (“e\\u0301”\nin NFD).  Together these two runes are one character. The definition of a\ncharacter may vary depending on the application. For normalization we will\ndefine it as a sequence of runes that starts with a starter, a rune that does\nnot modify or combine backwards with any other rune, followed by possibly empty\nsequence of non-starters, that is, runes that do (typically accents). The\nnormalization algorithm processes one character at a time.\u003c/p\u003e\n\u003cp\u003eTheoretically, there is no bound to the number of runes that can make up a\nUnicode character. In fact, there are no restrictions on the number of\nmodifiers that can follow a character and a modifier may be repeated, or\nstacked. Ever seen an ’e’ with three acutes? Here you go: ’é́́’. That is a\nperfectly valid 4-rune character according to the standard.\u003c/p\u003e\n\u003cp\u003eAs a consequence, even at the lowest level, text needs to be processed in\nincrements of unbounded chunk sizes. This is especially awkward with a\nstreaming approach to text processing, as used by Go’s standard Reader and\nWriter interfaces, as that model potentially requires any intermediate buffers\nto have unbounded size as well. Also, a straightforward implementation of\nnormalization will have a O(n²) running time.\u003c/p\u003e\n\u003cp\u003eThere are really no meaningful interpretations for such large sequences of\nmodifiers for practical applications. Unicode defines a Stream-Safe Text\nformat, which allows capping the number of modifiers (non-starters) to at most\n30, more than enough for any practical purpose. Subsequent modifiers will be\nplaced after a freshly inserted Combining Grapheme Joiner (CGJ or U+034F). Go\nadopts this approach for all normalization algorithms. This decision gives up a\nlittle conformance but gains a little safety.\u003c/p\u003e\n\u003ch2 id=\"writing-in-normal-form\"\u003eWriting in normal form\u003c/h2\u003e\n\u003cp\u003eEven if you don’t need to normalize text within your Go code, you might still\nwant to do so when communicating to the outside world. For example, normalizing\nto NFC might compact your text, making it cheaper to send down a wire. For some\nlanguages, like Korean, the savings can be substantial. Also, some external\nAPIs might expect text in a certain normal form. Or you might just want to fit\nin and output your text as NFC like the rest of the world.\u003c/p\u003e\n\u003cp\u003eTo write your text as NFC, use the\n\u003ca href=\"https://pkg.go.dev/golang.org/x/text/unicode/norm\" rel=\"noreferrer\" target=\"_blank\"\u003eunicode/norm\u003c/a\u003e package\nto wrap your \u003ccode\u003eio.Writer\u003c/code\u003e of choice:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ewc := norm.NFC.Writer(w)\ndefer wc.Close()\n// write as before...\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIf you have a small string and want to do a quick conversion, you can use this\nsimpler form:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003enorm.NFC.Bytes(b)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ePackage norm provides various other methods for normalizing text.\nPick the one that suits your needs best.\u003c/p\u003e\n\u003ch2 id=\"catching-look-alikes\"\u003eCatching look-alikes\u003c/h2\u003e\n\u003cp\u003eCan you tell the difference between ‘K’ (\u0026#34;\\u004B\u0026#34;) and ‘K’ (Kelvin sign\n“\\u212A”) or ‘Ω’ (\u0026#34;\\u03a9\u0026#34;) and ‘Ω’ (Ohm sign “\\u2126”)? It is easy to overlook\nthe sometimes minute differences between variants of the same underlying\ncharacter. It is generally a good idea to disallow such variants in identifiers\nor anything where deceiving users with such look-alikes can pose a security\nhazard.\u003c/p\u003e\n\u003cp\u003eThe compatibility normal forms, NFKC and NFKD, will map many visually nearly\nidentical forms to a single value. Note that it will not do so when two symbols\nlook alike, but are really from two different alphabets. For example the Latin\n‘o’, Greek ‘ο’, and Cyrillic ‘о’ are still different characters as defined by\nthese forms.\u003c/p\u003e\n\u003ch2 id=\"correct-text-modifications\"\u003eCorrect text modifications\u003c/h2\u003e\n\u003cp\u003eThe norm package might also come to the rescue when one needs to modify text.\nConsider a case where you want to search and replace the word “cafe” with its\nplural form “cafes”.  A code snippet could look like this.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003es := \u0026#34;We went to eat at multiple cafe\u0026#34;\ncafe := \u0026#34;cafe\u0026#34;\nif p := strings.Index(s, cafe); p != -1 {\n    p += len(cafe)\n    s = s[:p] + \u0026#34;s\u0026#34; + s[p:]\n}\nfmt.Println(s)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis prints “We went to eat at multiple cafes” as desired and expected. Now\nconsider our text contains the French spelling “café” in NFD form:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003es := \u0026#34;We went to eat at multiple cafe\\u0301\u0026#34;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eUsing the same code from above, the plural “s” would still be inserted after\nthe ’e’, but before the acute, resulting in  “We went to eat at multiple\ncafeś”.  This behavior is undesirable.\u003c/p\u003e\n\u003cp\u003eThe problem is that the code does not respect the boundaries between multi-rune\ncharacters and inserts a rune in the middle of a character.  Using the norm\npackage, we can rewrite this piece of code as follows:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003es := \u0026#34;We went to eat at multiple cafe\\u0301\u0026#34;\ncafe := \u0026#34;cafe\u0026#34;\nif p := strings.Index(s, cafe); p != -1 {\n    p += len(cafe)\n    if bp := norm.FirstBoundary(s[p:]); bp \u0026gt; 0 {\n        p += bp\n    }\n    s = s[:p] + \u0026#34;s\u0026#34; + s[p:]\n}\nfmt.Println(s)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis may be a contrived example, but the gist should be clear. Be mindful of\nthe fact that characters can span multiple runes. Generally these kinds of\nproblems can be avoided by using search functionality that respects character\nboundaries (such as the planned go.text/search package.)\u003c/p\u003e\n\u003ch2 id=\"iteration\"\u003eIteration\u003c/h2\u003e\n\u003cp\u003eAnother tool provided by the norm package that may help dealing with character\nboundaries is its iterator,\n\u003ca href=\"https://pkg.go.dev/golang.org/x/text/unicode/norm#Iter\" rel=\"noreferrer\" target=\"_blank\"\u003e\u003ccode\u003enorm.Iter\u003c/code\u003e\u003c/a\u003e.\nIt iterates over characters one at a time in the normal form of choice.\u003c/p\u003e\n\u003ch2 id=\"performing-magic\"\u003ePerforming magic\u003c/h2\u003e\n\u003cp\u003eAs mentioned earlier, most text is in NFC form, where base characters and\nmodifiers are combined into a single rune whenever possible.  For the purpose\nof analyzing characters, it is often easier to handle runes after decomposition\ninto their smallest components. This is where the NFD form comes in handy. For\nexample, the following piece of code creates a \u003ccode\u003etransform.Transformer\u003c/code\u003e that\ndecomposes text into its smallest parts, removes all accents, and then\nrecomposes the text into NFC:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eimport (\n    \u0026#34;unicode\u0026#34;\n\n    \u0026#34;golang.org/x/text/transform\u0026#34;\n    \u0026#34;golang.org/x/text/unicode/norm\u0026#34;\n)\n\nisMn := func(r rune) bool {\n    return unicode.Is(unicode.Mn, r) // Mn: nonspacing marks\n}\nt := transform.Chain(norm.NFD, transform.RemoveFunc(isMn), norm.NFC)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe resulting \u003ccode\u003eTransformer\u003c/code\u003e can be used to remove accents from an \u003ccode\u003eio.Reader\u003c/code\u003e\nof choice as follows:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003er = transform.NewReader(r, t)\n// read as before ...\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis will, for example, convert any mention of “cafés” in the text to “cafes”,\nregardless of the normal form in which the original text was encoded.\u003c/p\u003e\n\u003ch2 id=\"normalization-info\"\u003eNormalization info\u003c/h2\u003e\n\u003cp\u003eAs mentioned earlier, some packages precompute normalizations into their tables\nto minimize the need for normalization at run time. The type \u003ccode\u003enorm.Properties\u003c/code\u003e\nprovides access to the per-rune information needed by these packages, most\nnotably the Canonical Combining Class and decomposition information. Read the\n\u003ca href=\"https://pkg.go.dev/golang.org/x/text/unicode/norm#Properties\" rel=\"noreferrer\" target=\"_blank\"\u003edocumentation\u003c/a\u003e\nfor this type if you want to dig deeper.\u003c/p\u003e\n\u003ch2 id=\"performance\"\u003ePerformance\u003c/h2\u003e\n\u003cp\u003eTo give an idea of the performance of normalization, we compare it against the\nperformance of strings.ToLower. The sample in the first row is both lowercase\nand NFC and can in every case be returned as is. The second sample is neither\nand requires writing a new version.\u003c/p\u003e\n\u003ctable class=\"codetable padtable\"\u003e\n    \u003ctbody\u003e\u003ctr\u003e\n        \u003cth\u003eInput\u003c/th\u003e\n        \u003cth\u003eToLower\u003c/th\u003e\n        \u003cth\u003eNFC Append\u003c/th\u003e\n        \u003cth\u003eNFC Transform\u003c/th\u003e\n        \u003cth\u003eNFC Iter\u003c/th\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd\u003enörmalization\u003c/td\u003e\n        \u003ctd\u003e199 ns\u003c/td\u003e\n        \u003ctd\u003e137 ns\u003c/td\u003e\n        \u003ctd\u003e133 ns\u003c/td\u003e\n        \u003ctd\u003e251 ns (621 ns)\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n        \u003ctd\u003eNo\\u0308rmalization\u003c/td\u003e\n        \u003ctd\u003e427 ns\u003c/td\u003e\n        \u003ctd\u003e836 ns\u003c/td\u003e\n        \u003ctd\u003e845 ns\u003c/td\u003e\n        \u003ctd\u003e573 ns (948 ns)\u003c/td\u003e\n    \u003c/tr\u003e\n\u003c/tbody\u003e\u003c/table\u003e\n\u003cp\u003eThe column with the results for the iterator shows both the measurement with\nand without initialization of the iterator, which contain buffers that don’t\nneed to be reinitialized upon reuse.\u003c/p\u003e\n\u003cp\u003eAs you can see, detecting whether a string is normalized can be quite\nefficient. A lot of the cost of normalizing in the second row is for the\ninitialization of buffers, the cost of which is amortized when one is\nprocessing larger strings. As it turns out, these buffers are rarely needed, so\nwe may change the implementation at some point to speed up the common case for\nsmall strings even further.\u003c/p\u003e\n\u003ch2 id=\"conclusion\"\u003eConclusion\u003c/h2\u003e\n\u003cp\u003eIf you’re dealing with text inside Go, you generally do not have to use the\nunicode/norm package to normalize your text. The package may still be useful\nfor things like ensuring that strings are normalized before sending them out or\nto do advanced text manipulation.\u003c/p\u003e\n\u003cp\u003eThis article briefly mentioned the existence of other go.text packages as well\nas multilingual text processing and it may have raised more questions than it\nhas given answers. The discussion of these topics, however, will have to wait\nuntil another day.\u003c/p\u003e\n\n    \u003c/div\u003e",
  "Date": "2013-11-26T00:00:00Z",
  "Author": "Marcel van Lohuizen"
}