{
  "Source": "go.dev",
  "Title": "Go runtime: 4 years later",
  "Link": "https://go.dev/blog/go119runtime",
  "Content": "\u003cdiv class=\"Article\" data-slug=\"/blog/go119runtime\"\u003e\n    \n    \u003ch1 class=\"small\"\u003e\u003ca href=\"/blog/\"\u003eThe Go Blog\u003c/a\u003e\u003c/h1\u003e\n    \n\n    \u003ch1\u003eGo runtime: 4 years later\u003c/h1\u003e\n      \n      \u003cp class=\"author\"\u003e\n      Michael Knyszek\u003cbr/\u003e\n      26 September 2022\n      \u003c/p\u003e\n      \n      \u003cp\u003eSince our \u003ca href=\"/blog/ismmkeynote\"\u003elast blog post about the Go GC in 2018\u003c/a\u003e the\nGo GC, and the Go runtime more broadly, has been steadily improving.\nWe’ve tackled some large projects, motivated by real-world Go programs and real\nchallenges facing Go users.\nLet’s catch you up on the highlights!\u003c/p\u003e\n\u003ch3 id=\"whats-new\"\u003eWhat’s new?\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003ccode\u003esync.Pool\u003c/code\u003e, a GC-aware tool for reusing memory, has a \u003ca href=\"/cl/166960\"\u003elower latency\nimpact\u003c/a\u003e and \u003ca href=\"/cl/166961\"\u003erecycles memory much more\neffectively\u003c/a\u003e than before.\n(Go 1.13)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe Go runtime returns unneeded memory back to the operating system \u003ca href=\"/issue/30333\"\u003emuch\nmore proactively\u003c/a\u003e, reducing excess memory\nconsumption and the chance of out-of-memory errors.\nThis reduces idle memory consumption by up to 20%.\n(Go 1.13 and 1.14)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe Go runtime is able to preempt goroutines more readily in many cases,\nreducing stop-the-world latencies up to 90%.\n\u003ca href=\"https://www.youtube.com/watch?v=1I1WmeSjRSw\" rel=\"noreferrer\" target=\"_blank\"\u003eWatch the talk from Gophercon\n2020 here.\u003c/a\u003e\n(Go 1.14)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe Go runtime \u003ca href=\"/cl/171883\"\u003emanages timers more efficiently than\nbefore\u003c/a\u003e, especially on machines with many CPU cores.\n(Go 1.14)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eFunction calls that have been deferred with the \u003ccode\u003edefer\u003c/code\u003e statement now cost as\nlittle as a regular function call in most cases.\n\u003ca href=\"https://www.youtube.com/watch?v=DHVeUsrKcbM\" rel=\"noreferrer\" target=\"_blank\"\u003eWatch the talk from Gophercon 2020\nhere.\u003c/a\u003e\n(Go 1.14)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe memory allocator’s slow path \u003ca href=\"/issue/35112\"\u003escales\u003c/a\u003e\n\u003ca href=\"/issue/37487\"\u003ebetter\u003c/a\u003e with CPU cores, increasing throughput up\nto 10% and decreasing tail latencies up to 30%, especially in highly-parallel\nprograms.\n(Go 1.14 and 1.15)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eGo memory statistics are now accessible in a more granular, flexible, and\nefficient API, the \u003ca href=\"https://pkg.go.dev/runtime/metrics\" rel=\"noreferrer\" target=\"_blank\"\u003eruntime/metrics\u003c/a\u003e\npackage.\nThis reduces latency of obtaining runtime statistics by two orders of\nmagnitude (milliseconds to microseconds).\n(Go 1.16)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe Go scheduler spends up to \u003ca href=\"/issue/43997\"\u003e30% less CPU time spinning to find new\nwork\u003c/a\u003e.\n(Go 1.17)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eGo code now follows a \u003ca href=\"/issues/40724\"\u003eregister-based calling\nconvention\u003c/a\u003e on amd64, arm64, and ppc64, improving\nCPU efficiency by up to 15%.\n(Go 1.17 and Go 1.18)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe Go GC’s internal accounting and scheduling has been\n\u003ca href=\"/issue/44167\"\u003eredesigned\u003c/a\u003e, resolving a variety of long-standing\nissues related to efficiency and robustness.\nThis results in a significant decrease in application tail latency (up to 66%)\nfor applications where goroutines stacks are a substantial portion of memory\nuse.\n(Go 1.18)\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003eThe Go GC now limits \u003ca href=\"/issue/44163\"\u003eits own CPU use when the application is\nidle\u003c/a\u003e.\nThis results in 75% lower CPU utilization during a GC cycle in very idle\napplications, reducing CPU spikes that can confuse job shapers.\n(Go 1.19)\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThese changes have been mostly invisible to users: the Go code they’ve come to\nknow and love runs better, just by upgrading Go.\u003c/p\u003e\n\u003ch3 id=\"a-new-knob\"\u003eA new knob\u003c/h3\u003e\n\u003cp\u003eWith Go 1.19 comes an long-requested feature that requires a little extra work\nto use, but carries a lot of potential: \u003ca href=\"https://pkg.go.dev/runtime/debug#SetMemoryLimit\" rel=\"noreferrer\" target=\"_blank\"\u003ethe Go runtime’s soft memory\nlimit\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eFor years, the Go GC has had only one tuning parameter: \u003ccode\u003eGOGC\u003c/code\u003e.\n\u003ccode\u003eGOGC\u003c/code\u003e lets the user adjust \u003ca href=\"https://pkg.go.dev/runtime/debug#SetGCPercent\" rel=\"noreferrer\" target=\"_blank\"\u003ethe trade-off between CPU overhead and memory\noverhead made by the Go GC\u003c/a\u003e.\nFor years, this “knob” has served the Go community well, capturing a wide\nvariety of use-cases.\u003c/p\u003e\n\u003cp\u003eThe Go runtime team has been reluctant to add new knobs to the Go runtime,\nwith good reason: every new knob represents a new \u003cem\u003edimension\u003c/em\u003e in the space of\nconfigurations that we need to test and maintain, potentially forever.\nThe proliferation of knobs also places a burden on Go developers to understand\nand use them effectively, which becomes more difficult with more knobs.\nHence, the Go runtime has always leaned into behaving reasonably with minimal\nconfiguration.\u003c/p\u003e\n\u003cp\u003eSo why add a memory limit knob?\u003c/p\u003e\n\u003cp\u003eMemory is not as fungible as CPU time.\nWith CPU time, there’s always more of it in the future, if you just wait a bit.\nBut with memory, there’s a limit to what you have.\u003c/p\u003e\n\u003cp\u003eThe memory limit solves two problems.\u003c/p\u003e\n\u003cp\u003eThe first is that when the peak memory use of an application is unpredictable,\n\u003ccode\u003eGOGC\u003c/code\u003e alone offers virtually no protection from running out of memory.\nWith just \u003ccode\u003eGOGC\u003c/code\u003e, the Go runtime is simply unaware of how much memory it has\navailable to it.\nSetting a memory limit enables the runtime to be robust against transient,\nrecoverable load spikes by making it aware of when it needs to work harder to\nreduce memory overhead.\u003c/p\u003e\n\u003cp\u003eThe second is that to avoid out-of-memory errors without using the memory limit,\n\u003ccode\u003eGOGC\u003c/code\u003e must be tuned according to peak memory, resulting in higher GC CPU\noverheads to maintain low memory overheads, even when the application is not at\npeak memory use and there is plenty of memory available.\nThis is especially relevant in our containerized world, where programs are\nplaced in boxes with specific and isolated memory reservations; we might as\nwell make use of them!\nBy offering protection from load spikes, setting a memory limit allows for\n\u003ccode\u003eGOGC\u003c/code\u003e to be tuned much more aggressively with respect to CPU overheads.\u003c/p\u003e\n\u003cp\u003eThe memory limit is designed to be easy to adopt and robust.\nFor example, it’s a limit on the whole memory footprint of the Go parts of an\napplication, not just the Go heap, so users don’t have to worry about accounting\nfor Go runtime overheads.\nThe runtime also adjusts its memory scavenging policy in response to the memory\nlimit so it returns memory to the OS more proactively in response to memory\npressure.\u003c/p\u003e\n\u003cp\u003eBut while the memory limit is a powerful tool, it must still be used with some\ncare.\nOne big caveat is that it opens up your program to GC thrashing: a state in\nwhich a program spends too much time running the GC, resulting in not enough\ntime spent making meaningful progress.\nFor example, a Go program might thrash if the memory limit is set too low for\nhow much memory the program actually needs.\nGC thrashing is something that was unlikely previously, unless \u003ccode\u003eGOGC\u003c/code\u003e was\nexplicitly tuned heavily in favor of memory use.\nWe chose to favor running out of memory over thrashing, so as a mitigation, the\nruntime will limit the GC to 50% of total CPU time, even if this means exceeding\nthe memory limit.\u003c/p\u003e\n\u003cp\u003eAll of this is a lot to consider, so as a part of this work, we released \u003ca href=\"/doc/gc-guide\"\u003ea\nshiny new GC guide\u003c/a\u003e, complete with interactive visualizations to\nhelp you understand GC costs and how to manipulate them.\u003c/p\u003e\n\u003ch3 id=\"conclusion\"\u003eConclusion\u003c/h3\u003e\n\u003cp\u003eTry out the memory limit!\nUse it in production!\nRead the \u003ca href=\"/doc/gc-guide\"\u003eGC guide\u003c/a\u003e!\u003c/p\u003e\n\u003cp\u003eWe’re always looking for feedback on how to improve Go, but it also helps to\nhear about when it just works for you.\n\u003ca href=\"https://groups.google.com/g/golang-dev\" rel=\"noreferrer\" target=\"_blank\"\u003eSend us feedback\u003c/a\u003e!\u003c/p\u003e\n\n    \u003c/div\u003e",
  "Date": "2022-09-26T00:00:00Z",
  "Author": "Michael Knyszek"
}