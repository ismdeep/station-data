{
  "Source": "go.dev",
  "Title": "The cover story",
  "Link": "https://go.dev/blog/cover",
  "Content": "\u003cdiv class=\"Article\" data-slug=\"/blog/cover\"\u003e\n    \n    \u003ch1 class=\"small\"\u003e\u003ca href=\"/blog/\"\u003eThe Go Blog\u003c/a\u003e\u003c/h1\u003e\n    \n\n    \u003ch1\u003eThe cover story\u003c/h1\u003e\n      \n      \u003cp class=\"author\"\u003e\n      Rob Pike\u003cbr/\u003e\n      2 December 2013\n      \u003c/p\u003e\n      \n      \u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eFrom the beginning of the project, Go was designed with tools in mind.\nThose tools include some of the most iconic pieces of Go technology such as\nthe documentation presentation tool\n\u003ca href=\"/cmd/godoc\"\u003egodoc\u003c/a\u003e,\nthe code formatting tool\n\u003ca href=\"/cmd/gofmt\"\u003egofmt\u003c/a\u003e,\nand the API rewriter\n\u003ca href=\"/cmd/fix\"\u003egofix\u003c/a\u003e.\nPerhaps most important of all is the\n\u003ca href=\"/cmd/go\"\u003e\u003ccode\u003ego\u003c/code\u003e command\u003c/a\u003e,\nthe program that automatically installs, builds, and tests Go programs\nusing nothing more than the source code as the build specification.\u003c/p\u003e\n\u003cp\u003eThe release of Go 1.2 introduces a new tool for test coverage that takes an\nunusual approach to the way it generates coverage statistics, an approach\nthat builds on the technology laid down by godoc and friends.\u003c/p\u003e\n\u003ch2 id=\"support-for-tools\"\u003eSupport for tools\u003c/h2\u003e\n\u003cp\u003eFirst, some background: What does it mean for a\n\u003ca href=\"/talks/2012/splash.article#TOC_17.\"\u003elanguage to support good tooling\u003c/a\u003e?\nIt means that the language makes it easy to write good tools and that its ecosystem\nsupports the construction of tools of all flavors.\u003c/p\u003e\n\u003cp\u003eThere are a number of properties of Go that make it suitable for tooling.\nFor starters, Go has a regular syntax that is easy to parse.\nThe grammar aims to be free of special cases that require complex machinery to analyze.\u003c/p\u003e\n\u003cp\u003eWhere possible, Go uses lexical and syntactic constructs to make semantic properties\neasy to understand.\nExamples include the use of upper-case letters to define exported names\nand the radically simplified scoping rules compared to other languages in the C tradition.\u003c/p\u003e\n\u003cp\u003eFinally, the standard library comes with production-quality packages to lex and parse Go source code.\nThey also include, more unusually, a production-quality package to pretty-print Go syntax trees.\u003c/p\u003e\n\u003cp\u003eThese packages in combination form the core of the gofmt tool, but the pretty-printer is worth singling out.\nBecause it can take an arbitrary Go syntax tree and output standard-format, human-readable, correct\ncode, it creates the possibility to build tools that transform the parse tree and output modified but\ncorrect and easy-to-read code.\u003c/p\u003e\n\u003cp\u003eOne example is the gofix tool, which automates the\nrewriting of code to use new language features or updated libraries.\nGofix let us make fundamental changes to the language and libraries in the\n\u003ca href=\"/blog/the-path-to-go-1\"\u003erun-up to Go 1.0\u003c/a\u003e,\nwith the confidence that users could just run the tool to update their source to the newest version.\u003c/p\u003e\n\u003cp\u003eInside Google, we have used gofix to make sweeping changes in a huge code repository that would be almost\nunthinkable in the other languages we use.\nThere’s no need any more to support multiple versions of some API; we can use gofix to update\nthe entire company in one operation.\u003c/p\u003e\n\u003cp\u003eIt’s not just these big tools that these packages enable, of course.\nThey also make it easy to write more modest programs such as IDE plugins, for instance.\nAll these items build on each other, making the Go environment\nmore productive by automating many tasks.\u003c/p\u003e\n\u003ch2 id=\"test-coverage\"\u003eTest coverage\u003c/h2\u003e\n\u003cp\u003eTest coverage is a term that describes how much of a package’s code is exercised by running the package’s tests.\nIf executing the test suite causes 80% of the package’s source statements to be run, we say that the test coverage is 80%.\u003c/p\u003e\n\u003cp\u003eThe program that provides test coverage in Go 1.2 is the latest to exploit the tooling support in the Go ecosystem.\u003c/p\u003e\n\u003cp\u003eThe usual way to compute test coverage is to instrument the binary.\nFor instance, the GNU \u003ca href=\"http://gcc.gnu.org/onlinedocs/gcc/Gcov.html\" rel=\"noreferrer\" target=\"_blank\"\u003egcov\u003c/a\u003e program sets breakpoints at branches\nexecuted by the binary.\nAs each branch executes, the breakpoint is cleared and the target statements of the branch are marked as ‘covered’.\u003c/p\u003e\n\u003cp\u003eThis approach is successful and widely used. An early test coverage tool for Go even worked the same way.\nBut it has problems.\nIt is difficult to implement, as analysis of the execution of binaries is challenging.\nIt also requires a reliable way of tying the execution trace back to the source code, which can also be difficult,\nas any user of a source-level debugger can attest.\nProblems there include inaccurate debugging information and issues such as in-lined functions complicating\nthe analysis.\nMost important, this approach is very non-portable.\nIt needs to be done afresh for every architecture, and to some extent for every\noperating system since debugging support varies greatly from system to system.\u003c/p\u003e\n\u003cp\u003eIt does work, though, and for instance if you are a user of gccgo, the gcov tool can give you test coverage\ninformation.\nHowever If you’re a user of gc, the more commonly used Go compiler suite, until Go 1.2 you were out of luck.\u003c/p\u003e\n\u003ch2 id=\"test-coverage-for-go\"\u003eTest coverage for Go\u003c/h2\u003e\n\u003cp\u003eFor the new test coverage tool for Go, we took a different approach that avoids dynamic debugging.\nThe idea is simple: Rewrite the package’s source code before compilation to add instrumentation,\ncompile and run the modified source, and dump the statistics.\nThe rewriting is easy to arrange because the \u003ccode\u003ego\u003c/code\u003e command controls the flow\nfrom source to test to execution.\u003c/p\u003e\n\u003cp\u003eHere’s an example. Say we have a simple, one-file package like this:\u003c/p\u003e\n\u003cdiv class=\"code\"\u003e\n\u003cpre\u003epackage size\n\nfunc Size(a int) string {\n    switch {\n    case a \u0026lt; 0:\n        return \u0026#34;negative\u0026#34;\n    case a == 0:\n        return \u0026#34;zero\u0026#34;\n    case a \u0026lt; 10:\n        return \u0026#34;small\u0026#34;\n    case a \u0026lt; 100:\n        return \u0026#34;big\u0026#34;\n    case a \u0026lt; 1000:\n        return \u0026#34;huge\u0026#34;\n    }\n    return \u0026#34;enormous\u0026#34;\n}\n\u003c/pre\u003e\n\u003c/div\u003e\n\u003cp\u003eand this test:\u003c/p\u003e\n\u003cdiv class=\"code\"\u003e\n\u003cpre\u003epackage size\n\nimport \u0026#34;testing\u0026#34;\n\ntype Test struct {\n    in  int\n    out string\n}\n\nvar tests = []Test{\n    {-1, \u0026#34;negative\u0026#34;},\n    {5, \u0026#34;small\u0026#34;},\n}\n\nfunc TestSize(t *testing.T) {\n    for i, test := range tests {\n        size := Size(test.in)\n        if size != test.out {\n            t.Errorf(\u0026#34;#%d: Size(%d)=%s; want %s\u0026#34;, i, test.in, size, test.out)\n        }\n    }\n}\n\u003c/pre\u003e\n\u003c/div\u003e\n\u003cp\u003eTo get the test coverage for the package,\nwe run the test with coverage enabled by providing the \u003ccode\u003e-cover\u003c/code\u003e flag to \u003ccode\u003ego\u003c/code\u003e \u003ccode\u003etest\u003c/code\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e% go test -cover\nPASS\ncoverage: 42.9% of statements\nok      size    0.026s\n%\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eNotice that the coverage is 42.9%, which isn’t very good.\nBefore we ask how to raise that number, let’s see how that was computed.\u003c/p\u003e\n\u003cp\u003eWhen test coverage is enabled, \u003ccode\u003ego\u003c/code\u003e \u003ccode\u003etest\u003c/code\u003e runs the “cover” tool, a separate program included\nwith the distribution, to rewrite the source code before compilation. Here’s what the rewritten\n\u003ccode\u003eSize\u003c/code\u003e function looks like:\u003c/p\u003e\n\u003cdiv class=\"code\"\u003e\n\u003cpre\u003efunc Size(a int) string {\n    GoCover.Count[0] = 1\n    switch {\n    case a \u0026lt; 0:\n        GoCover.Count[2] = 1\n        return \u0026#34;negative\u0026#34;\n    case a == 0:\n        GoCover.Count[3] = 1\n        return \u0026#34;zero\u0026#34;\n    case a \u0026lt; 10:\n        GoCover.Count[4] = 1\n        return \u0026#34;small\u0026#34;\n    case a \u0026lt; 100:\n        GoCover.Count[5] = 1\n        return \u0026#34;big\u0026#34;\n    case a \u0026lt; 1000:\n        GoCover.Count[6] = 1\n        return \u0026#34;huge\u0026#34;\n    }\n    GoCover.Count[1] = 1\n    return \u0026#34;enormous\u0026#34;\n}\n\u003c/pre\u003e\n\u003c/div\u003e\n\u003cp\u003eEach executable section of the program is annotated with an assignment statement that,\nwhen executed, records that that section ran.\nThe counter is tied to the original source position of the statements it counts\nthrough a second read-only data structure that is also generated by the cover tool.\nWhen the test run completes, the counters are collected and the percentage is computed\nby seeing how many were set.\u003c/p\u003e\n\u003cp\u003eAlthough that annotating assignment might look expensive, it compiles to a single “move” instruction.\nIts run-time overhead is therefore modest, adding only about 3% when running a typical (more realistic) test.\nThat makes it reasonable to include test coverage as part of the standard development pipeline.\u003c/p\u003e\n\u003ch2 id=\"viewing-the-results\"\u003eViewing the results\u003c/h2\u003e\n\u003cp\u003eThe test coverage for our example was poor.\nTo discover why, we ask \u003ccode\u003ego\u003c/code\u003e \u003ccode\u003etest\u003c/code\u003e to write a “coverage profile” for us, a file that holds\nthe collected statistics so we can study them in more detail.\nThat’s easy to do: use the \u003ccode\u003e-coverprofile\u003c/code\u003e flag to specify a file for the output:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e% go test -coverprofile=coverage.out\nPASS\ncoverage: 42.9% of statements\nok      size    0.030s\n%\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e(The \u003ccode\u003e-coverprofile\u003c/code\u003e flag automatically sets \u003ccode\u003e-cover\u003c/code\u003e to enable coverage analysis.)\nThe test runs just as before, but the results are saved in a file.\nTo study them, we run the test coverage tool ourselves, without \u003ccode\u003ego\u003c/code\u003e \u003ccode\u003etest\u003c/code\u003e.\nAs a start, we can ask for the coverage to be broken down by function,\nalthough that’s not going to illuminate much in this case since there’s\nonly one function:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e% go tool cover -func=coverage.out\nsize.go:    Size          42.9%\ntotal:      (statements)  42.9%\n%\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eA much more interesting way to see the data is to get an HTML presentation\nof the source code decorated with coverage information.\nThis display is invoked by the \u003ccode\u003e-html\u003c/code\u003e flag:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e$ go tool cover -html=coverage.out\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eWhen this command is run, a browser window pops up, showing the covered (green),\nuncovered (red), and uninstrumented (grey) source.\nHere’s a screen dump:\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"cover/set.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eWith this presentation, it’s obvious what’s wrong: we neglected to test several\nof the cases!\nAnd we can see exactly which ones they are, which makes it easy to\nimprove our test coverage.\u003c/p\u003e\n\u003ch2 id=\"heat-maps\"\u003eHeat maps\u003c/h2\u003e\n\u003cp\u003eA big advantage of this source-level approach to test coverage is that it’s\neasy to instrument the code in different ways.\nFor instance, we can ask not only whether a statement has been executed,\nbut how many times.\u003c/p\u003e\n\u003cp\u003eThe \u003ccode\u003ego\u003c/code\u003e \u003ccode\u003etest\u003c/code\u003e command accepts a \u003ccode\u003e-covermode\u003c/code\u003e flag to set the coverage mode\nto one of three settings:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eset:    did each statement run?\u003c/li\u003e\n\u003cli\u003ecount:  how many times did each statement run?\u003c/li\u003e\n\u003cli\u003eatomic: like count, but counts precisely in parallel programs\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe default is ‘set’, which we’ve already seen.\nThe \u003ccode\u003eatomic\u003c/code\u003e setting is needed only when accurate counts are required\nwhen running parallel algorithms. It uses atomic operations from the\n\u003ca href=\"/pkg/sync/atomic/\"\u003esync/atomic\u003c/a\u003e package,\nwhich can be quite expensive.\nFor most purposes, though, the \u003ccode\u003ecount\u003c/code\u003e mode works fine and, like\nthe default \u003ccode\u003eset\u003c/code\u003e mode, is very cheap.\u003c/p\u003e\n\u003cp\u003eLet’s try counting statement execution for a standard package, the \u003ccode\u003efmt\u003c/code\u003e formatting package.\nWe run the test and write out a coverage profile so we can present the information\nnicely afterwards.\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e% go test -covermode=count -coverprofile=count.out fmt\nok      fmt 0.056s  coverage: 91.7% of statements\n%\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThat’s a much better test coverage ratio than for our previous example.\n(The coverage ratio is not affected by the coverage mode.)\nWe can display the function breakdown:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e% go tool cover -func=count.out\nfmt/format.go: init              100.0%\nfmt/format.go: clearflags        100.0%\nfmt/format.go: init              100.0%\nfmt/format.go: computePadding     84.6%\nfmt/format.go: writePadding      100.0%\nfmt/format.go: pad               100.0%\n...\nfmt/scan.go:   advance            96.2%\nfmt/scan.go:   doScanf            96.8%\ntotal:         (statements)       91.7%\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThe big payoff happens in the HTML output:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e% go tool cover -html=count.out\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eHere’s what the \u003ccode\u003epad\u003c/code\u003e function looks like in that presentation:\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"cover/count.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eNotice how the intensity of the green changes. Brighter-green\nstatements have higher execution counts; less saturated greens\nrepresent lower execution counts.\nYou can even hover the mouse over the statements to see the\nactual counts pop up in a tool tip.\nAt the time of writing, the counts come out like this\n(we’ve moved the counts from the tool tips to beginning-of-line\nmarkers to make them easier to show):\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e2933    if !f.widPresent || f.wid == 0 {\n2985        f.buf.Write(b)\n2985        return\n2985    }\n  56    padding, left, right := f.computePadding(len(b))\n  56    if left \u0026gt; 0 {\n  37        f.writePadding(left, padding)\n  37    }\n  56    f.buf.Write(b)\n  56    if right \u0026gt; 0 {\n  13        f.writePadding(right, padding)\n  13    }\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThat’s a lot of information about the execution of the function,\ninformation that might be useful in profiling.\u003c/p\u003e\n\u003ch2 id=\"basic-blocks\"\u003eBasic blocks\u003c/h2\u003e\n\u003cp\u003eYou might have noticed that the counts in the previous example\nwere not what you expected on the lines with closing braces.\nThat’s because, as always, test coverage is an inexact science.\u003c/p\u003e\n\u003cp\u003eWhat’s going on here is worth explaining, though. We’d like the\ncoverage annotations to be demarcated by branches in the program,\nthe way they are when the binary is instrumented in the traditional\nmethod.\nIt’s hard to do that by rewriting the source, though, since\nthe branches don’t appear explicitly in the source.\u003c/p\u003e\n\u003cp\u003eWhat the coverage annotation does is instrument blocks, which\nare typically bounded by brace brackets.\nGetting this right in general is very hard.\nA consequence of the algorithm used is that the closing\nbrace looks like it belongs to the block it closes, while the\nopening brace looks like it belongs outside the block.\nA more interesting consequence is that in an expression like\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ef() \u0026amp;\u0026amp; g()\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003ethere is no attempt to separately instrument the calls to \u003ccode\u003ef\u003c/code\u003e and \u003ccode\u003eg\u003c/code\u003e, Regardless of\nthe facts it will always look like they both ran the same\nnumber of times, the number of times \u003ccode\u003ef\u003c/code\u003e ran.\u003c/p\u003e\n\u003cp\u003eTo be fair, even \u003ccode\u003egcov\u003c/code\u003e has trouble here. That tool gets the\ninstrumentation right but the presentation is line-based and\ncan therefore miss some nuances.\u003c/p\u003e\n\u003ch2 id=\"the-big-picture\"\u003eThe big picture\u003c/h2\u003e\n\u003cp\u003eThat’s the story about test coverage in Go 1.2.\nA new tool with an interesting implementation enables not only\ntest coverage statistics, but easy-to-interpret presentations\nof them and even the possibility to extract profiling information.\u003c/p\u003e\n\u003cp\u003eTesting is an important part of software development and test\ncoverage a simple way to add discipline to your testing strategy.\nGo forth, test, and cover.\u003c/p\u003e\n\n    \u003c/div\u003e",
  "Date": "2013-12-02T00:00:00Z",
  "Author": "Rob Pike"
}