{
  "Source": "go.dev",
  "Title": "Getting to Go: The Journey of Go's Garbage Collector",
  "Link": "https://go.dev/blog/ismmkeynote",
  "Content": "\u003cdiv class=\"Article\" data-slug=\"/blog/ismmkeynote\"\u003e\n    \n    \u003ch1 class=\"small\"\u003e\u003ca href=\"/blog/\"\u003eThe Go Blog\u003c/a\u003e\u003c/h1\u003e\n    \n\n    \u003ch1\u003eGetting to Go: The Journey of Go\u0026#39;s Garbage Collector\u003c/h1\u003e\n      \n      \u003cp class=\"author\"\u003e\n      Rick Hudson\u003cbr/\u003e\n      12 July 2018\n      \u003c/p\u003e\n      \n      \u003cp\u003eThis is the transcript from the keynote I gave at the International Symposium\non Memory Management (ISMM) on June 18, 2018.\nFor the past 25 years ISMM has been the premier venue for publishing memory\nmanagement and garbage collection papers and it was an honor to have been\ninvited to give the keynote.\u003c/p\u003e\n\u003ch2 id=\"abstract\"\u003eAbstract\u003c/h2\u003e\n\u003cp\u003eThe Go language features, goals, and use cases have forced us to rethink\nthe entire garbage collection stack and have led us to a surprising place.\nThe journey has been exhilarating. This talk describes our journey.\nIt is a journey motivated by open source and Google’s production demands.\nIncluded are side hikes into dead end box canyons where numbers guided us home.\nThis talk will provide insight into the how and the why of our journey,\nwhere we are in 2018, and Go’s preparation for the next part of the journey.\u003c/p\u003e\n\u003ch2 id=\"bio\"\u003eBio\u003c/h2\u003e\n\u003cp\u003eRichard L. Hudson (Rick) is best known for his work in memory management\nincluding the invention of the Train,\nSapphire, and Mississippi Delta algorithms as well as GC stack maps which\nenabled garbage collection in statically typed languages such as Modula-3, Java, C#, and Go.\nRick is currently a member of Google’s Go team where he is working on Go’s\ngarbage collection and runtime issues.\u003c/p\u003e\n\u003cp\u003eContact: rlh@golang.org\u003c/p\u003e\n\u003cp\u003eComments: See \u003ca href=\"https://groups.google.com/forum/#!topic/golang-dev/UuDv7W1Hsns\" rel=\"noreferrer\" target=\"_blank\"\u003ethe discussion on golang-dev\u003c/a\u003e.\u003c/p\u003e\n\u003ch2 id=\"the-transcript\"\u003eThe Transcript\u003c/h2\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image63.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eRick Hudson here.\u003c/p\u003e\n\u003cp\u003eThis is a talk about the Go runtime and in particular the garbage collector.\nI have about 45 or 50 minutes of prepared material and after that we will\nhave time for discussion and I’ll be around so feel free to come up afterwards.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image24.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eBefore I get started I want to acknowledge some people.\u003c/p\u003e\n\u003cp\u003eA lot of the good stuff in the talk was done by Austin Clements.\nOther people on the Cambridge Go team, Russ,\nThan, Cherry, and David have been an engaging,\nexciting, and fun group to work with.\u003c/p\u003e\n\u003cp\u003eWe also want to thank the 1.6 million Go users worldwide for giving us interesting problems to solve.\nWithout them a lot of these problems would never come to light.\u003c/p\u003e\n\u003cp\u003eAnd finally I want to acknowledge Renee French for all these nice Gophers\nthat she has been producing over the years.\nYou will see several of them throughout the talk.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image38.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eBefore we get revved up and going on this stuff we really have to show what GC’s view of Go looks like.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image32.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eWell first of all Go programs have hundreds of thousands of stacks.\nThey are managed by the Go scheduler and are always preempted at GC safepoints.\nThe Go scheduler multiplexes Go routines onto OS threads which hopefully\nrun with one OS thread per HW thread.\nWe manage the stacks and their size by copying them and updating pointers in the stack.\nIt’s a local operation so it scales fairly well.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image22.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eThe next thing that is important is the fact that Go is a value-oriented\nlanguage in the tradition of C-like systems languages rather than reference-oriented\nlanguage in the tradition of most managed runtime languages.\nFor example, this shows how a type from the tar package is laid out in memory.\nAll of the fields are embedded directly in the Reader value.\nThis gives programmers more control over memory layout when they need it.\nOne can collocate fields that have related values which helps with cache locality.\u003c/p\u003e\n\u003cp\u003eValue-orientation also helps with the foreign function interfaces.\nWe have a fast FFI with C and C++. Obviously Google has a tremendous number\nof facilities available but they are written in C++.\nGo couldn’t wait to reimplement all of these things in Go so Go had to have\naccess to these systems through the foreign function interface.\u003c/p\u003e\n\u003cp\u003eThis one design decision has led to some of the more amazing things that\nhave to go on with the runtime.\nIt is probably the most important thing that differentiates Go from other GCed languages.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image60.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eOf course Go can have pointers and in fact they can have interior pointers.\nSuch pointers keep the entire value live and they are fairly common.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image29.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eWe also have a way ahead of time compilation system so the binary contains the entire runtime.\u003c/p\u003e\n\u003cp\u003eThere is no JIT recompilation. There are pluses and minuses to this.\nFirst of all, reproducibility of program execution is a lot easier which\nmakes moving forward with compiler improvements much faster.\u003c/p\u003e\n\u003cp\u003eOn the sad side of it we don’t have the chance to do feedback optimizations as you would with a JITed system.\u003c/p\u003e\n\u003cp\u003eSo there are pluses and minuses.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image13.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eGo comes with two knobs to control the GC.\nThe first one is GCPercent. Basically this is a knob that adjusts how much\nCPU you want to use and how much memory you want to use.\nThe default is 100 which means that half the heap is dedicated to live memory\nand half the heap is dedicated to allocation.\nYou can modify this in either direction.\u003c/p\u003e\n\u003cp\u003eMaxHeap, which is not yet released but is being used and evaluated internally,\nlets the programmer set what the maximum heap size should be.\nOut of memory, OOMs, are tough on Go; temporary spikes in memory usage should\nbe handled by increasing CPU costs, not by aborting.\nBasically if the GC sees memory pressure it informs the application that\nit should shed load.\nOnce things are back to normal the GC informs the application that it can\ngo back to its regular load.\nMaxHeap also provides a lot more flexibility in scheduling.\nInstead of always being paranoid about how much memory is available the\nruntime can size the heap up to the MaxHeap.\u003c/p\u003e\n\u003cp\u003eThis wraps up our discussion on the pieces of Go that are important to the garbage collector.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image3.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eSo now let’s talk about the Go runtime and how did we get here, how we got to where we are.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image59.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eSo it’s 2014. If Go does not solve this GC latency problem somehow then\nGo isn’t going to be successful. That was clear.\u003c/p\u003e\n\u003cp\u003eOther new languages were facing the same problem.\nLanguages like Rust went a different way but we are going to talk about\nthe path that Go took.\u003c/p\u003e\n\u003cp\u003eWhy is latency so important?\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image7.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eThe math is completely unforgiving on this.\u003c/p\u003e\n\u003cp\u003eA 99%ile isolated GC latency service level objective (SLO),\nsuch as 99% of the time a GC cycle takes \u0026lt; 10ms,\njust simply doesn’t scale.\nWhat matters is latency during an entire session or through the course of\nusing an app many times in a day.\nAssume a session that browses several web pages ends up making 100 server\nrequests during a session or it makes 20 requests and you have 5 sessions\npacked up during the day.\nIn that situation only 37% of users will have a consistent sub 10ms experience\nacross the entire session.\u003c/p\u003e\n\u003cp\u003eIf you want 99% of those users to have a sub 10ms experience,\nas we are suggesting, the math says you really need to target 4 9s or the 99.99%ile.\u003c/p\u003e\n\u003cp\u003eSo it’s 2014 and Jeff Dean had just come out with his paper called ‘The\nTail at Scale’ which this digs into this further.\nIt was being widely read around Google since it had serious ramifications\nfor Google going forward and trying to scale at Google scale.\u003c/p\u003e\n\u003cp\u003eWe call this problem the tyranny of the 9s.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image36.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eSo how do you fight the tyranny of the 9s?\u003c/p\u003e\n\u003cp\u003eA lot of things were being done in 2014.\u003c/p\u003e\n\u003cp\u003eIf you want 10 answers ask for several more and take the first 10 and those\nare the answers you put on your search page.\nIf the request exceeds 50%ile reissue or forward the request to another server.\nIf GC is about to run, refuse new requests or forward the requests to another\nserver until GC is done.\nAnd so forth and so on.\u003c/p\u003e\n\u003cp\u003eAll these are workarounds come from very clever people with very real problems\nbut they didn’t tackle the root problem of GC latency.\nAt Google scale we had to tackle the root problem. Why?\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image48.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eRedundancy wasn’t going to scale, redundancy costs a lot. It costs new server farms.\u003c/p\u003e\n\u003cp\u003eWe hoped we could solve this problem and saw it as an opportunity to improve\nthe server ecosystem and in the process save some of the endangered corn\nfields and give some kernel of corn the chance to be knee high by the fourth\nof July and reach its full potential.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image56.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eSo here is the 2014 SLO. Yes, it was true that I was sandbagging,\nI was new on the team, it was a new process to me,\nand I didn’t want to over promise.\u003c/p\u003e\n\u003cp\u003eFurthermore presentations about GC latency in other languages were just plain scary.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image67.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eThe original plan was to do a read barrier free concurrent copying GC.\nThat was the long term plan. There was a lot of uncertainty about the overhead\nof read barriers so Go wanted to avoid them.\u003c/p\u003e\n\u003cp\u003eBut short term 2014 we had to get our act together.\nWe had to convert all of the runtime and compiler to Go.\nThey were written in C at the time. No more C,\nno long tail of bugs due to C coders not understanding GC but having a cool\nidea about how to copy strings.\nWe also needed something quickly and focused on latency but the performance\nhit had to be less than the speedups provided by the compiler.\nSo we were limited. We had basically a year of compiler performance improvements\nthat we could eat up by making the GC concurrent.\nBut that was it. We couldn’t slow down Go programs.\nThat would have been untenable in 2014.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image28.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eSo we backed off a bit. We weren’t going to do the copying part.\u003c/p\u003e\n\u003cp\u003eThe decision was to do a tri-color concurrent algorithm.\nEarlier in my career Eliot Moss and I had done the journal proofs showing\nthat Dijkstra’s algorithm worked with multiple application threads.\nWe also showed we could knock off the STW problems,\nand we had proofs that it could be done.\u003c/p\u003e\n\u003cp\u003eWe were also concerned about compiler speed,\nthat is the code the compiler generated.\nIf we kept the write barrier turned off most of the time the compiler optimizations\nwould be minimally impacted and the compiler team could move forward rapidly.\nGo also desperately needed short term success in 2015.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image55.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eSo let’s look at some of the things we did.\u003c/p\u003e\n\u003cp\u003eWe went with a size segregated span. Interior pointers were a problem.\u003c/p\u003e\n\u003cp\u003eThe garbage collector needs to efficiently find the start of the object.\nIf it knows the size of the objects in a span it simply rounds down to that\nsize and that will be the start of the object.\u003c/p\u003e\n\u003cp\u003eOf course size segregated spans have some other advantages.\u003c/p\u003e\n\u003cp\u003eLow fragmentation: Experience with C, besides Google’s TCMalloc and Hoard,\nI was intimately involved with Intel’s Scalable Malloc and that work gave\nus confidence that fragmentation was not going to be a problem with non-moving allocators.\u003c/p\u003e\n\u003cp\u003eInternal structures: We fully understood and had experience with them.\nWe understood how to do size segregated spans,\nwe understood how to do low or zero contention allocation paths.\u003c/p\u003e\n\u003cp\u003eSpeed: Non-copy did not concern us, allocation admittedly might be slower\nbut still in the order of C.\nIt might not be as fast as bump pointer but that was OK.\u003c/p\u003e\n\u003cp\u003eWe also had this foreign function interface issue.\nIf we didn’t move our objects then we didn’t have to deal with the long\ntail of bugs you might encounter if you had a moving collector as you attempt\nto pin objects and put levels of indirection between C and the Go object\nyou are working with.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image5.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eThe next design choice was where to put the object’s metadata.\nWe needed to have some information about the objects since we didn’t have headers.\nMark bits are kept on the side and used for marking as well as allocation.\nEach word has 2 bits associated with it to tell you if it was a scalar or\na pointer inside that word.\nIt also encoded whether there were more pointers in the object so we could\nstop scanning objects sooner than later.\nWe also had an extra bit encoding that we could use as an extra mark bit\nor to do other debugging things.\nThis was really valuable for getting this stuff running and finding bugs.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image19.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eSo what about write barriers? The write barrier is on only during the GC.\nAt other times the compiled code loads a global variable and looks at it.\nSince the GC was typically off the hardware correctly speculates to branch\naround the write barrier.\nWhen we are inside the GC that variable is different,\nand the write barrier is responsible for ensuring that no reachable objects\nget lost during the tri-color operations.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image50.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eThe other piece of this code is the GC Pacer.\nIt is some of the great work that Austin did.\nIt is basically based on a feedback loop that determines when to best start a GC cycle.\nIf the system is in a steady state and not in a phase change,\nmarking will end just about the time memory runs out.\u003c/p\u003e\n\u003cp\u003eThat might not be the case so the Pacer also has to monitor the marking\nprogress and ensure allocation doesn’t overrun the concurrent marking.\u003c/p\u003e\n\u003cp\u003eIf need be, the Pacer slows down allocation while speeding up marking.\nAt a high level the Pacer stops the Goroutine,\nwhich is doing a lot of the allocation, and puts it to work doing marking.\nThe amount of work is proportional to the Goroutine’s allocation.\nThis speeds up the garbage collector while slowing down the mutator.\u003c/p\u003e\n\u003cp\u003eWhen all of this is done the Pacer takes what it has learnt from this GC\ncycle as well as previous ones and projects when to start the next GC.\u003c/p\u003e\n\u003cp\u003eIt does much more than this but that is the basic approach.\u003c/p\u003e\n\u003cp\u003eThe math is absolutely fascinating, ping me for the design docs.\nIf you are doing a concurrent GC you really owe it to yourself to look at\nthis math and see if it’s the same as your math.\nIf you have any suggestions let us know.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"/s/go15gcpacing\"\u003e*Go 1.5 concurrent garbage collector pacing\u003c/a\u003e\nand \u003ca href=\"https://github.com/golang/proposal/blob/master/design/14951-soft-heap-limit.md\" rel=\"noreferrer\" target=\"_blank\"\u003eProposal: Separate soft and hard heap size goal\u003c/a\u003e\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image40.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eYes, so we had successes, lots of them. A younger crazier Rick would have\ntaken some of these graphs and tattooed them on my shoulder I was so proud of them.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image20.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eThis is a series of graphs that was done for a production server at Twitter.\nWe of course had nothing to do with that production server.\nBrian Hatfield did these measurements and oddly enough tweeted about them.\u003c/p\u003e\n\u003cp\u003eOn the Y axis we have GC latency in milliseconds.\nOn the X axis we have time. Each of the points is a stop the world pause\ntime during that GC.\u003c/p\u003e\n\u003cp\u003eOn our first release, which was in August of 2015,\nwe saw a drop from around 300 - 400 milliseconds down to 30 or 40 milliseconds.\nThis was good, order of magnitude good.\u003c/p\u003e\n\u003cp\u003eWe are going to change the Y-axis here radically from 0 to 400 milliseconds down to 0 to 50 milliseconds.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image54.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eThis is 6 months later. The improvement was largely due to systematically\neliminating all the O(heap) things we were doing during the stop the world time.\nThis was our second order of magnitude improvement as we went from 40 milliseconds down to 4 or 5.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image1.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eThere were some bugs in there that we had to clean up and we did this during\na minor release 1.6.3.\nThis dropped latency down to well under 10 milliseconds, which was our SLO.\u003c/p\u003e\n\u003cp\u003eWe are about to change our Y-axis again, this time down to 0 to 5 milliseconds.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image68.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eSo here we are, this is August of 2016, a year after the first release.\nAgain we kept knocking off these O(heap size) stop the world processes.\nWe are talking about an 18Gbyte heap here.\nWe had much larger heaps and as we knocked off these O(heap size) stop the world pauses,\nthe size of the heap could obviously grow considerable without impacting latency.\nSo this was a bit of a help in 1.7.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image58.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eThe next release was in March of 2017. We had the last of our large latency\ndrops which was due to figuring out how to avoid the stop the world stack\nscanning at the end of the GC cycle.\nThat dropped us into the sub-millisecond range.\nAgain the Y axis is about to change to 1.5 milliseconds and we see our third\norder of magnitude improvement.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image45.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eThe August 2017 release saw little improvement.\nWe know what is causing the remaining pauses.\nThe SLO whisper number here is around 100-200 microseconds and we will push towards that.\nIf you see anything over a couple hundred microseconds then we really want\nto talk to you and figure out whether it fits into the stuff we know about\nor whether it is something new we haven’t looked into.\nIn any case there seems to be little call for lower latency.\nIt is important to note these latency levels can happen for a wide variety\nof non-GC reasons and as the saying goes “You don’t have to be faster than the bear,\nyou just have to be faster than the guy next to you.”\u003c/p\u003e\n\u003cp\u003eThere was no substantial change in the Feb\u0026#39;18 1.10 release just some clean-up and chasing corner cases.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image6.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eSo a new year and a new SLO This is our 2018 SLO.\u003c/p\u003e\n\u003cp\u003eWe have dropped total CPU to CPU used during a GC cycle.\u003c/p\u003e\n\u003cp\u003eThe heap is still at 2x.\u003c/p\u003e\n\u003cp\u003eWe now have an objective of 500 microseconds stop the world pause per GC cycle. Perhaps a little sandbagging here.\u003c/p\u003e\n\u003cp\u003eThe allocation would continue to be proportional to the GC assists.\u003c/p\u003e\n\u003cp\u003eThe Pacer had gotten much better so we looked to see minimal GC assists in a steady state.\u003c/p\u003e\n\u003cp\u003eWe were pretty happy with this. Again this is not an SLA but an SLO so it’s an objective,\nnot an agreement, since we can’t control such things as the OS.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image64.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eThat’s the good stuff. Let’s shift and start talking about our failures.\nThese are our scars; they are sort of like tattoos and everyone gets them.\nAnyway they come with better stories so let’s do some of those stories.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image46.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eOur first attempt was to do something called the request oriented collector or ROC. The hypothesis can be seen here.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image34.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eSo what does this mean?\u003c/p\u003e\n\u003cp\u003eGoroutines are lightweight threads that look like Gophers,\nso here we have two Goroutines.\nThey share some stuff such as the two blue objects there in the middle.\nThey have their own private stacks and their own selection of private objects.\nSay the guy on the left wants to share the green object.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image9.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eThe goroutine puts it in the shared area so the other Goroutine can access it.\nThey can hook it to something in the shared heap or assign it to a global\nvariable and the other Goroutine can see it.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image26.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eFinally the Goroutine on the left goes to its death bed, it’s about to die, sad.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image14.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eAs you know you can’t take your objects with you when you die.\nYou can’t take your stack either. The stack is actually empty at this time\nand the objects are unreachable so you can simply reclaim them.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image2.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eThe important thing here is that all actions were local and did not require\nany global synchronization.\nThis is fundamentally different than approaches like a generational GC,\nand the hope was that the scaling we would get from not having to do that\nsynchronization would be sufficient for us to have a win.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image27.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eThe other issue that was going on with this system was that the write barrier was always on.\nWhenever there was a write, we would have to see if it was writing a pointer\nto a private object into a public object.\nIf so, we would have to make the referent object public and then do a transitive\nwalk of reachable objects making sure they were also public.\nThat was a pretty expensive write barrier that could cause many cache misses.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image30.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eThat said, wow, we had some pretty good successes.\u003c/p\u003e\n\u003cp\u003eThis is an end-to-end RPC benchmark. The mislabeled Y axis goes from 0 to\n5 milliseconds (lower is better),\nanyway that is just what it is.\nThe X axis is basically the ballast or how big the in-core database is.\u003c/p\u003e\n\u003cp\u003eAs you can see if you have ROC on and not a lot of sharing,\nthings actually scale quite nicely.\nIf you don’t have ROC on it wasn’t nearly as good.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image35.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eBut that wasn’t good enough, we also had to make sure that ROC didn’t slow\ndown other pieces of the system.\nAt that point there was a lot of concern about our compiler and we could\nnot slow down our compilers.\nUnfortunately the compilers were exactly the programs that ROC did not do well at.\nWe were seeing 30, 40, 50% and more slowdowns and that was unacceptable.\nGo is proud of how fast its compiler is so we couldn’t slow the compiler down,\ncertainly not this much.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image61.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eWe then went and looked at some other programs.\nThese are our performance benchmarks. We have a corpus of 200 or 300 benchmarks\nand these were the ones the compiler folks had decided were important for\nthem to work on and improve.\nThese weren’t selected by the GC folks at all.\nThe numbers were uniformly bad and ROC wasn’t going to become a winner.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image44.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eIt’s true we scaled but we only had 4 to 12 hardware thread system so we\ncouldn’t overcome the write barrier tax.\nPerhaps in the future when we have 128 core systems and Go is taking advantage of them,\nthe scaling properties of ROC might be a win.\nWhen that happens we might come back and revisit this,\nbut for now ROC was a losing proposition.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image66.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eSo what were we going to do next? Let’s try the generational GC.\nIt’s an oldie but a goodie. ROC didn’t work so let’s go back to stuff we\nhave a lot more experience with.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image41.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eWe weren’t going to give up our latency, we weren’t going to give up the\nfact that we were non-moving.\nSo we needed a non-moving generational GC.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image27.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eSo could we do this? Yes, but with a generational GC,\nthe write barrier is always on.\nWhen the GC cycle is running we use the same write barrier we use today,\nbut when GC is off we use a fast GC write barrier that buffers the pointers\nand then flushes the buffer to a card mark table when it overflows.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image4.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eSo how is this going to work in a non-moving situation? Here is the mark / allocation map.\nBasically you maintain a current pointer.\nWhen you are allocating you look for the next zero and when you find that\nzero you allocate an object in that space.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image51.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eYou then update the current pointer to the next 0.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image17.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eYou continue until at some point it is time to do a generation GC.\nYou will notice that if there is a one in the mark/allocation vector then\nthat object was alive at the last GC so it is mature.\nIf it is zero and you reach it then you know it is young.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image53.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eSo how do you do promoting. If you find something marked with a 1 pointing\nto something marked with a 0 then you promote the referent simply by setting that zero to a one.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image49.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eYou have to do a transitive walk to make sure all reachable objects are promoted.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image69.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eWhen all reachable objects have been promoted the minor GC terminates.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image62.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eFinally, to finish your generational GC cycle you simply set the current\npointer back to the start of the vector and you can continue.\nAll the zeros weren’t reached during that GC cycle so are free and can be reused.\nAs many of you know this is called ‘sticky bits’ and was invented by Hans\nBoehm and his colleagues.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image21.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eSo what did the performance look like? It wasn’t bad for the large heaps.\nThese were the benchmarks that the GC should do well on. This was all good.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image65.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eWe then ran it on our performance benchmarks and things didn’t go as well. So what was going on?\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image43.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eThe write barrier was fast but it simply wasn’t fast enough.\nFurthermore it was hard to optimize for. For example,\nwrite barrier elision can happen if there is an initializing write between\nwhen the object was allocated and the next safepoint.\nBut we were having to move to a system where we have a GC safepoint at every\ninstruction so there really wasn’t any write barrier that we could elide going forward.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image47.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eWe also had escape analysis and it was getting better and better.\nRemember the value-oriented stuff we were talking about? Instead of passing\na pointer to a function we would pass the actual value.\nBecause we were passing a value, escape analysis would only have to do intraprocedural\nescape analysis and not interprocedural analysis.\u003c/p\u003e\n\u003cp\u003eOf course in the case where a pointer to the local object escapes, the object would be heap allocated.\u003c/p\u003e\n\u003cp\u003eIt isn’t that the generational hypothesis isn’t true for Go,\nit’s just that the young objects live and die young on the stack.\nThe result is that generational collection is much less effective than you\nmight find in other managed runtime languages.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image10.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eSo these forces against the write barrier were starting to gather.\nToday, our compiler is much better than it was in 2014.\nEscape analysis is picking up a lot of those objects and sticking them on\nthe stack-objects that the generational collector would have helped with.\nWe started creating tools to help our users find objects that escaped and\nif it was minor they could make changes to the code and help the compiler\nallocate on the stack.\u003c/p\u003e\n\u003cp\u003eUsers are getting more clever about embracing value-oriented approaches\nand the number of pointers is being reduced.\nArrays and maps hold values and not pointers to structs. Everything is good.\u003c/p\u003e\n\u003cp\u003eBut that’s not the main compelling reason why write barriers in Go have an uphill fight going forward.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image8.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eLet’s look at this graph. It’s just an analytical graph of mark costs.\nEach line represents a different application that might have a mark cost.\nSay your mark cost is 20%, which is pretty high but it’s possible.\nThe red line is 10%, which is still high.\nThe lower line is 5% which is about what a write barrier costs these days.\nSo what happens if you double the heap size? That’s the point on the right.\nThe cumulative cost of the mark phase drops considerably since GC cycles are less frequent.\nThe write barrier costs are constant so the cost of increasing the heap\nsize will drive that marking cost underneath the cost of the write barrier.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image39.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eHere is a more common cost for a write barrier,\nwhich is 4%, and we see that even with that we can drive the cost of the\nmark barrier down below the cost of the write barrier by simply increasing the heap size.\u003c/p\u003e\n\u003cp\u003eThe real value of generational GC is that,\nwhen looking at GC times, the write barrier costs are ignored since they\nare smeared across the mutator.\nThis is generational GC’s great advantage,\nit greatly reduces the long STW times of full GC cycles but it doesn’t necessarily improve throughput.\nGo doesn’t have this stop the world problem so it had to look more closely\nat the throughput problems and that is what we did.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image23.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eThat’s a lot of failure and with such failure comes food and lunch.\nI’m doing my usual whining “Gee wouldn’t this be great if it wasn’t for the write barrier.”\u003c/p\u003e\n\u003cp\u003eMeanwhile Austin has just spent an hour talking to some of the HW GC folks\nat Google and he was saying we should talk to them and try and figure out\nhow to get HW GC support that might help.\nThen I started telling war stories about zero-fill cache lines,\nrestartable atomic sequences, and other things that didn’t fly when I was\nworking for a large hardware company.\nSure we got some stuff into a chip called the Itanium,\nbut we couldn’t get them into the more popular chips of today.\nSo the moral of the story is simply to use the HW we have.\u003c/p\u003e\n\u003cp\u003eAnyway that got us talking, what about something crazy?\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image25.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eWhat about card marking without a write barrier? It turns out that Austin\nhas these files and he writes into these files all of his crazy ideas that\nfor some reason he doesn’t tell me about.\nI figure it is some sort of therapeutic thing.\nI used to do the same thing with Eliot. New ideas are easily smashed and\none needs to protect them and make them stronger before you let them out into the world.\nWell anyway he pulls this idea out.\u003c/p\u003e\n\u003cp\u003eThe idea is that you maintain a hash of mature pointers in each card.\nIf pointers are written into a card, the hash will change and the card will\nbe considered marked.\nThis would trade the cost of write barrier off for cost of hashing.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image31.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eBut more importantly it’s hardware aligned.\u003c/p\u003e\n\u003cp\u003eToday’s modern architectures have AES (Advanced Encryption Standard) instructions.\nOne of those instructions can do encryption-grade hashing and with encryption-grade\nhashing we don’t have to worry about collisions if we also follow standard\nencryption policies.\nSo hashing is not going to cost us much but we have to load up what we are going to hash.\nFortunately we are walking through memory sequentially so we get really\ngood memory and cache performance.\nIf you have a DIMM and you hit sequential addresses,\nthen it’s a win because they will be faster than hitting random addresses.\nThe hardware prefetchers will kick in and that will also help.\nAnyway we have 50 years, 60 years of designing hardware to run Fortran,\nto run C, and to run the SPECint benchmarks.\nIt’s no surprise that the result is hardware that runs this kind of stuff fast.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image12.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eWe took the measurement. This is pretty good. This is the benchmark suite for large heaps which should be good.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image18.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eWe then said what does it look like for the performance benchmark? Not so good,\na couple of outliers.\nBut now we have moved the write barrier from always being on in the mutator\nto running as part of the GC cycle.\nNow making a decision about whether we are going to do a generational GC\nis delayed until the start of the GC cycle.\nWe have more control there since we have localized the card work.\nNow that we have the tools we can turn it over to the Pacer,\nand it could do a good job of dynamically cutting off programs that fall\nto the right and do not benefit from generational GC.\nBut is this going to win going forward? We have to know or at least think\nabout what hardware is going to look like going forward.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image52.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eWhat are the memories of the future?\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image11.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eLet’s take a look at this graph. This is your classic Moore’s law graph.\nYou have a log scale on the Y axis showing the number of transistors in a single chip.\nThe X-axis is the years between 1971 and 2016.\nI will note that these are the years when someone somewhere predicted that\nMoore’s law was dead.\u003c/p\u003e\n\u003cp\u003eDennard scaling had ended frequency improvements ten years or so ago.\nNew processes are taking longer to ramp. So instead of 2 years they are\nnow 4 years or more.\nSo it’s pretty clear that we are entering an era of the slowing of Moore’s law.\u003c/p\u003e\n\u003cp\u003eLet’s just look at the chips in the red circle. These are the chips that are the best at sustaining Moore’s law.\u003c/p\u003e\n\u003cp\u003eThey are chips where the logic is increasingly simple and duplicated many times.\nLots of identical cores, multiple memory controllers and caches,\nGPUs, TPUs, and so forth.\u003c/p\u003e\n\u003cp\u003eAs we continue to simplify and increase duplication we asymptotically end\nup with a couple of wires,\na transistor, and a capacitor.\nIn other words a DRAM memory cell.\u003c/p\u003e\n\u003cp\u003ePut another way, we think that doubling memory is going to be a better value than doubling cores.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.kurzweilai.net/ask-ray-the-future-of-moores-law\" rel=\"noreferrer\" target=\"_blank\"\u003eOriginal graph\u003c/a\u003e\nat \u003ca href=\"http://www.kurzweilai.net/ask-ray-the-future-of-moores-law\" rel=\"noreferrer\" target=\"_blank\"\u003ewww.kurzweilai.net/ask-ray-the-future-of-moores-law\u003c/a\u003e.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image57.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eLet’s look at another graph focused on DRAM.\nThese are numbers from a recent PhD thesis from CMU.\nIf we look at this we see that Moore’s law is the blue line.\nThe red line is capacity and it seems to be following Moore’s law.\nOddly enough I saw a graph that goes all the way back to 1939 when we were\nusing drum memory and that capacity and Moore’s law were chugging along\ntogether so this graph has been going on for a long time,\ncertainly longer than probably anybody in this room has been alive.\u003c/p\u003e\n\u003cp\u003eIf we compare this graph to CPU frequency or the various Moore’s-law-is-dead graphs,\nwe are led to the conclusion that memory,\nor at least chip capacity, will follow Moore’s law longer than CPUs.\nBandwidth, the yellow line, is related not only to the frequency of the\nmemory but also to the number of pins one can get off of the chip so it’s\nnot keeping up as well but it’s not doing badly.\u003c/p\u003e\n\u003cp\u003eLatency, the green line, is doing very poorly,\nthough I will note that latency for sequential accesses does better than\nlatency for random access.\u003c/p\u003e\n\u003cp\u003e(Data from “Understanding and Improving the Latency of DRAM-Based Memory\nSystems Submitted in partial fulfillment of the requirements for the degree\nof Doctor of Philosophy in Electrical and Computer Engineering Kevin K.\nChang M.S., Electrical \u0026amp; Computer Engineering,\nCarnegie Mellon University B.S., Electrical \u0026amp; Computer Engineering,\nCarnegie Mellon University Carnegie Mellon University Pittsburgh, PA May, 2017”.\nSee \u003ca href=\"http://repository.cmu.edu/cgi/viewcontent.cgi?article%3D1946%26context%3Ddissertations\u0026amp;sa=D\u0026amp;ust=1531164842660000\" rel=\"noreferrer\" target=\"_blank\"\u003eKevin K. Chang’s thesis.\u003c/a\u003e\nThe original graph in the introduction was not in a form that I could draw\na Moore’s law line on it easily so I changed the X-axis to be more uniform.)\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image15.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eLet’s go to where the rubber meets the road.\nThis is actual DRAM pricing and it has generally declined from 2005 to 2016.\nI chose 2005 since that is around the time when Dennard scaling ended and\nalong with it frequency improvements.\u003c/p\u003e\n\u003cp\u003eIf you look at the red circle, which is basically the time our work to reduce\nGo’s GC latency has been going on,\nwe see that for the first couple of years prices did well.\nLately, not so good, as demand has exceeded supply leading to price increases\nover the last two years.\nOf course, transistors haven’t gotten bigger and in some cases chip capacity\nhas increased so this is driven by market forces.\nRAMBUS and other chip manufacturers say that moving forward we will see\nour next process shrink in the 2019-2020 time frame.\u003c/p\u003e\n\u003cp\u003eI will refrain from speculating on global market forces in the memory industry\nbeyond noting that pricing is cyclic and in the long term supply has a tendency to meet demand.\u003c/p\u003e\n\u003cp\u003eLong term, it is our belief that memory pricing will drop at a rate that is much faster than CPU pricing.\u003c/p\u003e\n\u003cp\u003e(Sources \u003ca href=\"https://hblok.net/blog/\" rel=\"noreferrer\" target=\"_blank\"\u003ehttps://hblok.net/blog/\u003c/a\u003e and \u003ca href=\"https://hblok.net/storage_data/storage_memory_prices_2005-2017-12.png\" rel=\"noreferrer\" target=\"_blank\"\u003ehttps://hblok.net/storage_data/storage_memory_prices_2005-2017-12.png\u003c/a\u003e)\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image37.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eLet’s look at this other line. Gee it would be nice if we were on this line.\nThis is the SSD line. It is doing a better job of keeping prices low.\nThe material physics of these chips is much more complicated that with DRAM.\nThe logic is more complex, instead of a one transistor per cell there are half a dozen or so.\u003c/p\u003e\n\u003cp\u003eGoing forward there is a line between DRAM and SSD where NVRAM such as Intel’s\n3D XPoint and Phase Change Memory (PCM) will live.\nOver the next decade increased availability of this type of memory is likely\nto become more mainstream and this will only reinforce the idea that adding\nmemory is the cheap way to add value to our servers.\u003c/p\u003e\n\u003cp\u003eMore importantly we can expect to see other competing alternatives to DRAM.\nI won’t pretend to know which one will be favored in five or ten years but\nthe competition will be fierce and heap memory will move closer to the highlighted blue SSD line here.\u003c/p\u003e\n\u003cp\u003eAll of this reinforces our decision to avoid always-on barriers in favor of increasing memory.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image16.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eSo what does all this mean for Go going forward?\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image42.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eWe intend to make the runtime more flexible and robust as we look at corner\ncases that come in from our users.\nThe hope is to tighten the scheduler down and get better determinism and\nfairness but we don’t want to sacrifice any of our performance.\u003c/p\u003e\n\u003cp\u003eWe also do not intend to increase the GC API surface.\nWe’ve had almost a decade now and we have two knobs and that feels about right.\nThere is not an application that is important enough for us to add a new flag.\u003c/p\u003e\n\u003cp\u003eWe will also be looking into how to improve our already pretty good escape\nanalysis and optimize for Go’s value-oriented programming.\nNot only in the programming but in the tools we provide our users.\u003c/p\u003e\n\u003cp\u003eAlgorithmically, we will focus on parts of the design space that minimize\nthe use of barriers,\nparticularly those that are turned on all the time.\u003c/p\u003e\n\u003cp\u003eFinally, and most importantly, we hope to ride Moore’s law’s tendency to\nfavor RAM over CPU certainly for the next 5 years and hopefully for the next decade.\u003c/p\u003e\n\u003cp\u003eSo that’s it. Thank you.\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\n  \u003cimg src=\"ismmkeynote/image33.png\" alt=\"\"/\u003e\n\u003c/div\u003e\n\u003cp\u003eP.S. The Go team is looking to hire engineers to help develop and maintain the Go runtime and compiler toolchain.\u003c/p\u003e\n\u003cp\u003eInterested? Have a look at our \u003ca href=\"https://go-jobs-at-goog.firebaseapp.com\" rel=\"noreferrer\" target=\"_blank\"\u003eopen positions\u003c/a\u003e.\u003c/p\u003e\n\n    \u003c/div\u003e",
  "Date": "2018-07-12T00:00:00Z",
  "Author": "Rick Hudson"
}