{
  "Source": "go.dev",
  "Title": "Building LLM-powered applications in Go",
  "Link": "https://go.dev/blog/llmpowered",
  "Content": "\u003cdiv class=\"Article\" data-slug=\"/blog/llmpowered\"\u003e\n    \n    \u003ch1 class=\"small\"\u003e\u003ca href=\"/blog/\"\u003eThe Go Blog\u003c/a\u003e\u003c/h1\u003e\n    \n\n    \u003ch1\u003eBuilding LLM-powered applications in Go\u003c/h1\u003e\n      \n      \u003cp class=\"author\"\u003e\n      Eli Bendersky\u003cbr/\u003e\n      12 September 2024\n      \u003c/p\u003e\n      \n      \u003cp\u003eAs the capabilities of LLMs (Large Language Models) and adjacent tools like\nembedding models grew significantly over the past year, more and more developers\nare considering integrating LLMs into their applications.\u003c/p\u003e\n\u003cp\u003eSince LLMs often require dedicated hardware and significant compute resources,\nthey are most commonly packaged as network services that provide APIs for\naccess. This is how the APIs for leading LLMs like OpenAI or Google Gemini work;\neven run-your-own-LLM tools like \u003ca href=\"https://ollama.com/\" rel=\"noreferrer\" target=\"_blank\"\u003eOllama\u003c/a\u003e wrap\nthe LLM in a REST API for local consumption. Moreover, developers who take\nadvantage of LLMs in their applications often require supplementary tools like\nVector Databases, which are most commonly deployed as network services as\nwell.\u003c/p\u003e\n\u003cp\u003eIn other words, LLM-powered applications are a lot like other modern\ncloud-native applications: they require excellent support for REST and RPC\nprotocols, concurrency and performance. These just so happen to be the areas\nwhere Go excels, making it a fantastic language for writing LLM-powered\napplications.\u003c/p\u003e\n\u003cp\u003eThis blog post works through an example of using Go for a simple LLM-powered\napplication. It starts by describing the problem the demo application is\nsolving, and proceeds by presenting several variants of the application that\nall accomplish the same task, but use different packages to implement it. All\nthe code for the demos of this post\n\u003ca href=\"https://github.com/golang/example/tree/master/ragserver\" rel=\"noreferrer\" target=\"_blank\"\u003eis available online\u003c/a\u003e.\u003c/p\u003e\n\u003ch2 id=\"a-rag-server-for-qa\"\u003eA RAG server for Q\u0026amp;A\u003c/h2\u003e\n\u003cp\u003eA common LLM-powered application technique is RAG -\n\u003ca href=\"https://en.wikipedia.org/wiki/Retrieval-augmented_generation\" rel=\"noreferrer\" target=\"_blank\"\u003eRetrieval Augmented Generation\u003c/a\u003e.\nRAG is one of the most scalable ways of customizing an LLM’s knowledge base\nfor domain-specific interactions.\u003c/p\u003e\n\u003cp\u003eWe’re going to build a \u003cem\u003eRAG server\u003c/em\u003e in Go. This is an HTTP server that provides\ntwo operations to users:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAdd a document to the knowledge base\u003c/li\u003e\n\u003cli\u003eAsk an LLM a question about this knowledge base\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIn a typical real-world scenario, users would add a corpus of documents to\nthe server, and proceed to ask it questions. For example, a company can fill up\nthe RAG server’s knowledge base with internal documentation and use it to\nprovide LLM-powered Q\u0026amp;A capabilities to internal users.\u003c/p\u003e\n\u003cp\u003eHere’s a diagram showing the interactions of our server with the external\nworld:\u003c/p\u003e\n\u003cdiv class=\"image\"\u003e\u003cdiv class=\"centered\"\u003e\n\u003cfigure\u003e\n\u003cimg src=\"llmpowered/rag-server-diagram.png\" alt=\"RAG server diagram\"/\u003e\n\u003c/figure\u003e\n\u003c/div\u003e\u003c/div\u003e\n\u003cp\u003eIn addition to the user sending HTTP requests (the two operations described\nabove), the server interacts with:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAn embedding model to calculate \u003ca href=\"https://en.wikipedia.org/wiki/Sentence_embedding\" rel=\"noreferrer\" target=\"_blank\"\u003evector embeddings\u003c/a\u003e\nfor the submitted documents and for user questions.\u003c/li\u003e\n\u003cli\u003eA Vector Database for storing and retrieving embeddings efficiently.\u003c/li\u003e\n\u003cli\u003eAn LLM for asking questions based on context collected from the knowledge\nbase.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eConcretely, the server exposes two HTTP endpoints to users:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e/add/: POST {\u0026#34;documents\u0026#34;: [{\u0026#34;text\u0026#34;: \u0026#34;...\u0026#34;}, {\u0026#34;text\u0026#34;: \u0026#34;...\u0026#34;}, ...]}\u003c/code\u003e: submits\na sequence of text documents to the server, to be added to its knowledge base.\nFor this request, the server:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eCalculates a vector embedding for each document using the embedding model.\u003c/li\u003e\n\u003cli\u003eStores the documents along with their vector embeddings in the vector DB.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003ccode\u003e/query/: POST {\u0026#34;content\u0026#34;: \u0026#34;...\u0026#34;}\u003c/code\u003e: submits a question to the server. For this\nrequest, the server:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eCalculates the question’s vector embedding using the embedding model.\u003c/li\u003e\n\u003cli\u003eUses the vector DB’s similarity search to find the most relevant documents\nto the question in the knowledge database.\u003c/li\u003e\n\u003cli\u003eUses simple prompt engineering to reformulate the question with the most\nrelevant documents found in step (2) as context, and sends it to the LLM,\nreturning its answer to the user.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThe services used by our demo are:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://ai.google.dev/\" rel=\"noreferrer\" target=\"_blank\"\u003eGoogle Gemini API\u003c/a\u003e for the LLM and embedding model.\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://weaviate.io/\" rel=\"noreferrer\" target=\"_blank\"\u003eWeaviate\u003c/a\u003e for a locally-hosted vector DB; Weaviate\nis an open-source vector database\n\u003ca href=\"https://github.com/weaviate/weaviate\" rel=\"noreferrer\" target=\"_blank\"\u003eimplemented in Go\u003c/a\u003e.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eIt should be very simple to replace these by other, equivalent services. In\nfact, this is what the second and third variants of the server are all about!\nWe’ll start with the first variant which uses these tools directly.\u003c/p\u003e\n\u003ch2 id=\"using-the-gemini-api-and-weaviate-directly\"\u003eUsing the Gemini API and Weaviate directly\u003c/h2\u003e\n\u003cp\u003eBoth the Gemini API and Weaviate have convenient Go SDKs (client libraries),\nand our first server variant uses these directly. The full code of this\nvariant is \u003ca href=\"https://github.com/golang/example/tree/master/ragserver/ragserver\" rel=\"noreferrer\" target=\"_blank\"\u003ein this directory\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eWe won’t reproduce the entire code in this blog post, but here are some notes\nto keep in mind while reading it:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eStructure\u003c/strong\u003e: the code structure will be familiar to anyone who’s written an\nHTTP server in Go. Client libraries for Gemini and for Weaviate are initialized\nand the clients are stored in a state value that’s passed to HTTP handlers.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eRoute registration\u003c/strong\u003e: the HTTP routes for our server are trivial to set up\nusing the \u003ca href=\"/blog/routing-enhancements\"\u003erouting enhancements\u003c/a\u003e introduced in\nGo 1.22:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-Go\"\u003emux := http.NewServeMux()\nmux.HandleFunc(\u0026#34;POST /add/\u0026#34;, server.addDocumentsHandler)\nmux.HandleFunc(\u0026#34;POST /query/\u0026#34;, server.queryHandler)\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e\u003cstrong\u003eConcurrency\u003c/strong\u003e: the HTTP handlers of our server reach out\nto other services over the network and wait for a response. This isn’t a problem\nfor Go, since each HTTP handler runs concurrently in its own goroutine. This\nRAG server can handle a large number of concurrent requests, and the code of\neach handler is linear and synchronous.\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eBatch APIs\u003c/strong\u003e: since an \u003ccode\u003e/add/\u003c/code\u003e request may provide a large number of documents\nto add to the knowledge base, the server leverages \u003cem\u003ebatch APIs\u003c/em\u003e for both\nembeddings (\u003ccode\u003eembModel.BatchEmbedContents\u003c/code\u003e) and the Weaviate DB\n(\u003ccode\u003ers.wvClient.Batch\u003c/code\u003e) for efficiency.\u003c/p\u003e\n\u003ch2 id=\"using-langchain-for-go\"\u003eUsing LangChain for Go\u003c/h2\u003e\n\u003cp\u003eOur second RAG server variant uses LangChainGo to accomplish the same task.\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://www.langchain.com/\" rel=\"noreferrer\" target=\"_blank\"\u003eLangChain\u003c/a\u003e is a popular Python framework for\nbuilding LLM-powered applications.\n\u003ca href=\"https://github.com/tmc/langchaingo\" rel=\"noreferrer\" target=\"_blank\"\u003eLangChainGo\u003c/a\u003e is its Go equivalent. The\nframework has some tools to build applications out of modular components, and\nsupports many LLM providers and vector databases in a common API. This allows\ndevelopers to write code that may work with any provider and change providers\nvery easily.\u003c/p\u003e\n\u003cp\u003eThe full code for this variant is \u003ca href=\"https://github.com/golang/example/tree/master/ragserver/ragserver-langchaingo\" rel=\"noreferrer\" target=\"_blank\"\u003ein this directory\u003c/a\u003e.\nYou’ll notice two things when reading the code:\u003c/p\u003e\n\u003cp\u003eFirst, it’s somewhat shorter than the previous variant. LangChainGo takes care\nof wrapping the full APIs of vector databases in common interfaces, and less\ncode is needed to initialize and deal with Weaviate.\u003c/p\u003e\n\u003cp\u003eSecond, the LangChainGo API makes it fairly easy to switch providers. Let’s say\nwe want to replace Weaviate by another vector DB; in our previous variant, we’d\nhave to rewrite all the code interfacing the vector DB to use a new API. With\na framework like LangChainGo, we no longer need to do so. As long as LangChainGo\nsupports the new vector DB we’re interested in, we should be able to replace\njust a few lines of code in our server, since all the DBs implement a\n\u003ca href=\"https://pkg.go.dev/github.com/tmc/langchaingo@v0.1.12/vectorstores#VectorStore\" rel=\"noreferrer\" target=\"_blank\"\u003ecommon interface\u003c/a\u003e:\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-Go\"\u003etype VectorStore interface {\n    AddDocuments(ctx context.Context, docs []schema.Document, options ...Option) ([]string, error)\n    SimilaritySearch(ctx context.Context, query string, numDocuments int, options ...Option) ([]schema.Document, error)\n}\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id=\"using-genkit-for-go\"\u003eUsing Genkit for Go\u003c/h2\u003e\n\u003cp\u003eEarlier this year, Google introduced \u003ca href=\"https://developers.googleblog.com/en/introducing-genkit-for-go-build-scalable-ai-powered-apps-in-go/\" rel=\"noreferrer\" target=\"_blank\"\u003eGenkit for Go\u003c/a\u003e -\na new open-source framework for building LLM-powered applications. Genkit shares\nsome characteristics with LangChain, but diverges in other aspects.\u003c/p\u003e\n\u003cp\u003eLike LangChain, it provides common interfaces that may be implemented by\ndifferent providers (as plugins), and thus makes switching from one to the other\nsimpler. However, it doesn’t try to prescribe how different LLM components\ninteract; instead, it focuses on production features like prompt management and\nengineering, and deployment with integrated developer tooling.\u003c/p\u003e\n\u003cp\u003eOur third RAG server variant uses Genkit for Go to accomplish the same task.\nIts full code is \u003ca href=\"https://github.com/golang/example/tree/master/ragserver/ragserver-genkit\" rel=\"noreferrer\" target=\"_blank\"\u003ein this directory\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThis variant is fairly similar to the LangChainGo one - common interfaces for\nLLMs, embedders and vector DBs are used instead of direct provider APIs, making\nit easier to switch from one to another. In addition, deploying an LLM-powered\napplication to production is much easier with Genkit; we don’t implement this\nin our variant, but feel free to read \u003ca href=\"https://firebase.google.com/docs/genkit-go/get-started-go\" rel=\"noreferrer\" target=\"_blank\"\u003ethe documentation\u003c/a\u003e\nif you’re interested.\u003c/p\u003e\n\u003ch2 id=\"summary---go-for-llm-powered-applications\"\u003eSummary - Go for LLM-powered applications\u003c/h2\u003e\n\u003cp\u003eThe samples in this post provide just a taste of what’s possible for building\nLLM-powered applications in Go. It demonstrates how simple it is to build\na powerful RAG server with relatively little code; most important, the samples\npack a significant degree of production readiness because of some fundamental\nGo features.\u003c/p\u003e\n\u003cp\u003eWorking with LLM services often means sending REST or RPC requests to a network\nservice, waiting for the response, sending new requests to other services based\non that and so on. Go excels at all of these, providing great tools for managing\nconcurrency and the complexity of juggling network services.\u003c/p\u003e\n\u003cp\u003eIn addition, Go’s great performance and reliability as a Cloud-native language\nmakes it a natural choice for implementing the more fundamental building blocks\nof the LLM ecosystem. For some examples, see projects like\n\u003ca href=\"https://ollama.com/\" rel=\"noreferrer\" target=\"_blank\"\u003eOllama\u003c/a\u003e, \u003ca href=\"https://localai.io/\" rel=\"noreferrer\" target=\"_blank\"\u003eLocalAI\u003c/a\u003e,\n\u003ca href=\"https://weaviate.io/\" rel=\"noreferrer\" target=\"_blank\"\u003eWeaviate\u003c/a\u003e or \u003ca href=\"https://zilliz.com/what-is-milvus\" rel=\"noreferrer\" target=\"_blank\"\u003eMilvus\u003c/a\u003e.\u003c/p\u003e\n\n    \u003c/div\u003e",
  "Date": "2024-09-12T00:00:00Z",
  "Author": "Eli Bendersky"
}