{
  "Source": "io-meter",
  "Title": "集群资源调度系统设计架构总结",
  "Link": "https://io-meter.com/2018/02/09/A-summary-of-designing-schedulers/",
  "Content": "\u003cdiv class=\"entry\"\u003e\n      \u003cp\u003e之前为完成《\u003ca href=\"/2017/10/13/kylin-aws-scheduler-system/\"\u003eAWS 下 Kylin 调度系统的设计\u003c/a\u003e》，阅读了大量\n集群资源管理和任务调度的资料和论文。了解了如\n\u003ca href=\"https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html\" target=\"_blank\" rel=\"noopener\"\u003eHadoop YARN\u003c/a\u003e、\n\u003ca href=\"http://mesos.apache.org/\" target=\"_blank\" rel=\"noopener\"\u003eMesos\u003c/a\u003e、\n\u003ca href=\"https://github.com/amplab/drizzle-spark\" target=\"_blank\" rel=\"noopener\"\u003eSpark Drizzle\u003c/a\u003e、\n\u003ca href=\"https://research.google.com/pubs/pub43438.html\" target=\"_blank\" rel=\"noopener\"\u003eBorg/Kubernetes\u003c/a\u003e 和\n\u003ca href=\"https://research.google.com/pubs/pub41684.html\" target=\"_blank\" rel=\"noopener\"\u003eOmega\u003c/a\u003e\n等系统的调度器设计架构，在这篇文章里我将试图从这些架构案例中总结出此类系统一般的设计模式。\u003c/p\u003e\n\u003ca id=\"more\"\u003e\u003c/a\u003e\n\u003ch2 id=\"u8C03_u5EA6_u5668_u7684_u5B9A_u4E49\"\u003e\u003ca href=\"#u8C03_u5EA6_u5668_u7684_u5B9A_u4E49\" class=\"headerlink\" title=\"调度器的定义\"\u003e\u003c/a\u003e调度器的定义\u003c/h2\u003e\u003cp\u003e无论是在单机系统还是分布式系统当中，调度器其实都是非常核心和普遍的组件，其内涵也比较宽广和模糊。\u003c/p\u003e\n\u003cp\u003e一般来说，下面提到的几种类型的模块都可以认为是调度器：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e早期计算机系统当中的批处理调度系统\u003c/li\u003e\n\u003cli\u003e现代计算机系统当中的抢占式进程调度系统和内存分配系统\u003c/li\u003e\n\u003cli\u003e某些系统或程序提供或实现的，定时激发某些类型操作的工具(如 crontab、\u003ca href=\"http://www.quartz-scheduler.org/\" target=\"_blank\" rel=\"noopener\"\u003eQuartz\u003c/a\u003e 等)\u003c/li\u003e\n\u003cli\u003e某些编程语言的 Runtime 提供的线程/纤程/协程调度器(如 Golang 内置的 Goroutine 调度器)\u003c/li\u003e\n\u003cli\u003e分布式系统当中的任务关系管理和调度执行系统，(如 Hadoop YARN, \u003ca href=\"https://airflow.apache.org/\" target=\"_blank\" rel=\"noopener\"\u003eAirflow\u003c/a\u003e 等)\u003c/li\u003e\n\u003cli\u003e分布式系统当中的资源管理和调度系统(如 Mesos、Borg、Kubernetes 的调度器等)\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e可以被称为调度器的工具涵盖的范围非常广，他们有的提供定时激发任务的能力，有的提供资源管理的能力，\n有的负责维护任务的依赖关系和执行顺序。甚至有的系统还集成了任务监控和各种指标度量的工具。\u003c/p\u003e\n\u003cp\u003e这篇文章主要涉及的是管理系统资源和调度任务执行相关方面的架构和模型，\n具体的资源分配策略和任务调度策略不在我们讨论的范围内。\u003c/p\u003e\n\u003ch2 id=\"u8C03_u5EA6_u5668_u8BBE_u8BA1_u6982_u8FF0\"\u003e\u003ca href=\"#u8C03_u5EA6_u5668_u8BBE_u8BA1_u6982_u8FF0\" class=\"headerlink\" title=\"调度器设计概述\"\u003e\u003c/a\u003e调度器设计概述\u003c/h2\u003e\u003cp\u003e在系统设计领域研究比较多的朋友可以容易地得出一个结论，那就是我们的系统设计——无论是小到一个嵌入式的系统，\n还是大到好几百个机器的集群，在设计抽象上都是在不同的层次上重复自己。比如说，如果我们着眼于一个 CPU，\n他包括计算单元和一系列的用来加速数据访问的缓存 L1、L2、L3 等，每种缓存具有不同的访问速度。\n当我们的视野扩大到整个机器，CPU 又可以被当成一个单元，我们又有内存和硬盘两个层次的储存系统用于加速数据载入。\n而在分布式系统中，如果我们把 HDFS 或 S3 看作硬盘，也存在像 \u003ca href=\"https://www.alluxio.org/\" target=\"_blank\" rel=\"noopener\"\u003eAlluxio\u003c/a\u003e \n这样发挥着类似内存作用的系统。\u003c/p\u003e\n\u003cp\u003e既然系统在设计上的基本原则都是类似的，那为什么大规模分布式系统的设计这么困难呢？\n这是因为当问题的规模变化了，原先不显著或者容易解决的问题可能会变的难以解决。举例来说，\n当我们谈论起进程间通信或者同一个 CPU 不同内核之间的通信时，我们往往不考虑通讯不稳定所带来的问题:\n我们无法想象如果一个 CPU 内核无法发送消息到另一个内核的状况。\n然而在通过网络通讯的多机机群当中这是无法回避的问题，Paxos、Raft、Zab 等算法被设计出来的原因也在于此。\u003c/p\u003e\n\u003cp\u003e我们先看一看单机操作系统调度器的发展路线:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e最早的调度器是批处理调度器，这种调度器批量调度和执行任务，通过对计算机资源的分时复用来增加资源的利用率。\n一般具有较高的吞吐量\u003c/li\u003e\n\u003cli\u003e某些与外界交互次数频繁的系统对响应时间具有较强的要求，因此发展出了实时操作系统。\n实时操作系统的调度器具备低延迟相应外部信号的能力\u003c/li\u003e\n\u003cli\u003e我们常用的操作系统基本上以批处理的方式调度任务，又通过\u003cstrong\u003e中断\u003c/strong\u003e等机制提供实时性的保证，\n通过提供灵活的调度策略，在吞吐量和延迟时间当中获得平衡\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e在分布式系统中的调度器的设计也是相同的。从单机调度器这里我们首先可以总结出调度器设计的三个最基本的需求:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e资源的有效利用\u003c/li\u003e\n\u003cli\u003e信号的实时响应\u003c/li\u003e\n\u003cli\u003e调度策略的灵活配置\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e这三个需求在某种程度上来说是相互矛盾的，在面对不同需求的时候需要做出不一样的取舍。\u003c/p\u003e\n\u003cp\u003e在上述三个需求的基础上，分布式的调度器设计还需要克服很多其他的困难。这些困难往往在单机系统当中并不显著。\n比如说：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e状态的同步问题。在单机系统中，我们一般使用常规的同步方法，如共享内存和锁机制，就可以很好地保证任务的协调运行了。\n这是因为在单机系统上的状态同步比较稳定和容易。然而在分布式系统中，因为网络通讯的不确定性，\n使机群中的各个机器对于周围的状态达成一致是非常困难的任务。实际上，在分布式系统中甚至无法通过网络精确同步所有机器的时间！\u003c/li\u003e\n\u003cli\u003e容错性问题。由于单机系统的处理能力有限，我们运行任务的规模和同时运行任务的数量都比较有限。\n出错的概率和成本都比较低。但是在分布式系统中，由于任务规模变大、任务依赖关系变得更加复杂，\n出错的概率大大增加，错误恢复的成本可能也比较高，因此可能需要调度器快速地识别错误并进行恢复操作。\u003c/li\u003e\n\u003cli\u003e可扩展性的问题。当分布式系统的规模到达一定程度，调度器的可扩展性就可能会成为瓶颈。为了提供高可扩展性，\n调度器不但要可以应对管理上千台机器的挑战，也要能够处理动态增减节点这样的问题。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e现阶段比较流行的分布式调度器可以归纳为三种类型，在接下来的文章里将会结合具体的案例进行介绍。\u003c/p\u003e\n\u003ch2 id=\"u96C6_u4E2D_u5F0F_u8C03_u5EA6_u5668\"\u003e\u003ca href=\"#u96C6_u4E2D_u5F0F_u8C03_u5EA6_u5668\" class=\"headerlink\" title=\"集中式调度器\"\u003e\u003c/a\u003e集中式调度器\u003c/h2\u003e\u003cp\u003e集中式(Centralized)调度器也可以被称为宏(Monolithic)调度器。指的是使用中心化的方式管理资源和调度任务。\n也就是说，调度器本身在系统中只存在单个实例，所有的资源请求和任务调度都通过这一个实例进行。\n下图展示了集中式调度器的一般模型。可以看到，在这一模型中，资源的使用状态和任务的执行状态都由中央调度器管理。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/img/scheduler/centralized-scheduler.png\" alt=\"Centralized Scheduler\"/\u003e\u003c/p\u003e\n\u003cp\u003e按照上面的思路，可以列出集中式调度器在各个方面的表现状况:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e适合批处理任务和吞吐量较大、运行时间较长的任务\u003c/li\u003e\n\u003cli\u003e调度算法只能全部内置在核心调度器当中，灵活性和策略的可扩展性不高\u003c/li\u003e\n\u003cli\u003e状态同步比较容易且稳定，这是因为资源使用和任务执行的状态被统一管理，降低了状态同步和并发控制的难度\u003c/li\u003e\n\u003cli\u003e由于存在单点故障的可能性，集中式调度器的容错性一般，有些系统通过热备份 Master 的方式提高可用性\u003c/li\u003e\n\u003cli\u003e由于所有的资源和任务请求都要由中央调度器处理，集中式调度器的可扩展性较差，容易成为分布式系统吞吐量的瓶颈\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e尽管应用场合比较局限，集中式调度器仍然是普遍使用的调度器，可以被广泛应用于中小规模的数据平台应用。\u003c/p\u003e\n\u003ch3 id=\"u96C6_u4E2D_u5F0F_u8C03_u5EA6_u5668_u6848_u4F8B1_3A__u5355_u673A_u64CD_u4F5C_u7CFB_u7EDF_u7684_u8C03_u5EA6_u5668\"\u003e\u003ca href=\"#u96C6_u4E2D_u5F0F_u8C03_u5EA6_u5668_u6848_u4F8B1_3A__u5355_u673A_u64CD_u4F5C_u7CFB_u7EDF_u7684_u8C03_u5EA6_u5668\" class=\"headerlink\" title=\"集中式调度器案例1: 单机操作系统的调度器\"\u003e\u003c/a\u003e集中式调度器案例1: 单机操作系统的调度器\u003c/h3\u003e\u003cp\u003e单机操作系统，如 Windows、Linux 和 macOS 的进程调度器是典型的集中式调度器。\n当用户请求执行应用之后，由操作系统将进程载入内存。计算机硬件的所有资源，包括 CPU、内存和硬盘等都由操作系统集中式管理。\n当进程需要时，通过系统调用请求操作系统分配资源。如果单机环境中的进程使用了系统级多线程，\n这些线程的调度也由系统一并控制。\u003c/p\u003e\n\u003ch3 id=\"u96C6_u4E2D_u5F0F_u8C03_u5EA6_u5668_u6848_u4F8B2_3A_Hadoop_YARN\"\u003e\u003ca href=\"#u96C6_u4E2D_u5F0F_u8C03_u5EA6_u5668_u6848_u4F8B2_3A_Hadoop_YARN\" class=\"headerlink\" title=\"集中式调度器案例2: Hadoop YARN\"\u003e\u003c/a\u003e集中式调度器案例2: Hadoop YARN\u003c/h3\u003e\u003cp\u003e对于集中式调度器，我们重点介绍 Hadoop YARN 这个案例。下图是将集中式调度器的一般模型替换成 YARN 当中术语之后的示意图:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/img/scheduler/hadoop-yarn-scheduler.png\" alt=\"Hadoop YARN\"/\u003e\u003c/p\u003e\n\u003cp\u003eHadoop YARN 的特点可以总结为:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e集中式的资源管理和调度器被称为 ResourceManager，所有的资源的空闲和使用情况都由 ResourceManager 管理，\nResourceManager 也负责监控任务的执行\u003c/li\u003e\n\u003cli\u003e集群中的每个节点都运行着一个 NodeManager，这个 NodeManager 管理本地的资源占用和任务执行并将这些状态同步给\nResourceManager\u003c/li\u003e\n\u003cli\u003e集群中的任务运行在 Container 当中，YARN 使用 Container 作为资源分配的抽象单位，每个 Container\n会被分配一些本地资源和运算资源等\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e熟悉 YARN 的朋友可能知道 YARN 存在一个 ApplicationMaster 的概念，当一个应用被启动之后，\n一个 ApplicationMaster 会先在集群中被启动，随后 ApplicationMaster 会向 ResourceMaster \n申请新的资源并调度新的任务。这一模型好像看起来和后面介绍的双层调度器特别是 Mesos 的设计有点相似，\n但一般仍认为 YARN 是 Monolithic 设计的调度器。这主要是因为:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eApplicationMaster 其实也是运行在一个 Container 里的 YARN job\u003c/li\u003e\n\u003cli\u003eApplicationMaster 虽然决定 Job 如何被激发，但是仍然需要请求 ResourceMaster 申请资源和启动新的 Job\u003c/li\u003e\n\u003cli\u003eApplicationMaster 启动的 Job 也会由 ResourceMaster 进行监控，其启动所需的本地资源和运算资源都由\nResourceMaster 负责分配并通知 NodeManager 具体执行\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e在 YARN 中，为了防止 ResourceManager 出错退出，可以设计多个 Stand-By Master，\nStand-By Master 一直处于运行状态并和 ResourceManager 注册在同一个 ZooKeeper 集群中。\u003c/p\u003e\n\u003cp\u003eActive 的 ResourceManager 会定期保存自己的状态到 ZooKeeper，当其失败退出后，\n一个 Stand-By Master 会被选举出来成为新的 Manager。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/img/scheduler/yarn-standby-master.png\" alt=\"Hadoop YARN: High Availability\"/\u003e\u003c/p\u003e\n\u003cp\u003e作为一个分布式资源管理和调度器， YARN 与其竞争对手相比功能其实比较薄弱。\n譬如默认情况下 YARN 只能对 Memory 资源施加限制(如果一个 Job 使用了超过许可的 Memory，\nYARN 会直接杀死进程)。尽管其调度接口提供了对 CPU Cores 的抽象，\n但 YARN 默认情况下对任务使用 CPU 核数并没有任何限制。\u003c/p\u003e\n\u003cp\u003e不过若运行在 Linux 环境下， 在较新版本的 YARN 中可以配置 cgroup 限制资源使用。\u003c/p\u003e\n\u003ch2 id=\"u53CC_u5C42_u8C03_u5EA6_u5668\"\u003e\u003ca href=\"#u53CC_u5C42_u8C03_u5EA6_u5668\" class=\"headerlink\" title=\"双层调度器\"\u003e\u003c/a\u003e双层调度器\u003c/h2\u003e\u003cp\u003e前面提到，集中式调度器的主要缺点在于单点模型容错性和可扩展性较差，容易成为性能瓶颈。在一般的数据密集型应用当中，\n解决这一问题的主要方法是分区。下图是双层调度器的一般模型:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/img/scheduler/two-level-scheduler.png\" alt=\"2-level Scheduler\"/\u003e\u003c/p\u003e\n\u003cp\u003e在双层调度器当中，资源的使用状态同时由分区调度器和中央调度器管理，但是中央调度器一般只负责宏观的大规模的资源分配，\n因此业务压力较小。分区调度器负责管理自己分区的所有资源和任务，一般只有当所在分区资源无法满足需求时，\n才将任务冒泡到中央调度器处理。\u003c/p\u003e\n\u003cp\u003e相比集中式调度器，双层调度器某一分区内的资源分配和工作安排可以由具体的任务本身进行定制，\n因此大大增强了使用的灵活性，可以同时对高吞吐和低延迟的两种场景提供良好的支持。每个分区可以独立运行，\n降低了单点故障导致系统崩溃的概率，增加了可用性和可扩展性。但是反过来也导致状态同步和维护变得比较困难。\u003c/p\u003e\n\u003cp\u003e尽管主要思路是一致的，但双层调度器在实现上的变种比较丰富，本文接下来使用案例进行介绍。\u003c/p\u003e\n\u003ch3 id=\"u53CC_u5C42_u8C03_u5EA6_u5668_u6848_u4F8B1_3A__u534F_u7A0B_u8C03_u5EA6_u5668\"\u003e\u003ca href=\"#u53CC_u5C42_u8C03_u5EA6_u5668_u6848_u4F8B1_3A__u534F_u7A0B_u8C03_u5EA6_u5668\" class=\"headerlink\" title=\"双层调度器案例1: 协程调度器\"\u003e\u003c/a\u003e双层调度器案例1: 协程调度器\u003c/h3\u003e\u003cp\u003e单机操作系统的单个进程为了避免系统级多线程上下文切换的成本，可以自行实现进程内的调度器，如 Golang 运行时的 Goroutine\n调度器。在这一模型下，一个进程内部的资源就相当于一个分区，分区内的资源由运行时提供的调度器预先申请并自行管理。\n运行时环境只有当资源耗尽时才会向系统请求新的资源，从而避免频繁的系统调用。\u003c/p\u003e\n\u003cp\u003e提出这个例子的主要目的在于说明类似的优化思路其实也被应用于分布式系统，再次证明了系统设计\u003cstrong\u003e分层重复的特点\u003c/strong\u003e！\u003c/p\u003e\n\u003ch3 id=\"u53CC_u5C42_u8C03_u5EA6_u5668_u6848_u4F8B2_3A_Mesos\"\u003e\u003ca href=\"#u53CC_u5C42_u8C03_u5EA6_u5668_u6848_u4F8B2_3A_Mesos\" class=\"headerlink\" title=\"双层调度器案例2: Mesos\"\u003e\u003c/a\u003e双层调度器案例2: Mesos\u003c/h3\u003e\u003cp\u003eMesos 是和 YARN 几乎同一时间发展起来的任务和资源调度系统。这一调度系统实现了完整的资源调度功能，\n并使用 Linux Container 技术对资源的使用进行限制。和 YARN 一样，Mesos 系统也包括一个独立的 Mesos Master\n和运行在每个节点上的 Mesos Agent，而后者会管理节点上的资源和任务并将状态同步给 Master。\n在 Mesos 里，任务运行在 Executor 里。下图是 Mesos 的主要架构\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/img/scheduler/mesos-scheduler.png\" alt=\"Mesos Scheduler\"/\u003e\u003c/p\u003e\n\u003cp\u003e值得注意的是，Mesos 分区的单位并不是单个节点，是可以将一个节点当中的资源划分到多个区的。\n也就是说，在 Mesos 里，分区是逻辑的和动态的。\u003c/p\u003e\n\u003cp\u003e把 Mesos 看作一种双层的资源调度系统设计主要基于以下几点：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e与一般通过 Master 请求资源不同，Mesos 提出了 Framework 的概念，每个 Framework 相当于一个独立的调度器，\n可以实现自己的调度策略\u003c/li\u003e\n\u003cli\u003eMaster 掌握对整个集群资源的的状态，通过 Offer (而不是被动请求) 的方式通知每个 Framework 可用的资源有哪些\u003c/li\u003e\n\u003cli\u003eFramework 根据自己的需求决定要不要占有 Master Offer 的资源，如果占有了资源，这些资源接下来将完全由 Framework 管理\u003c/li\u003e\n\u003cli\u003eFramework 通过灵活分配自己占有的资源调度任务并执行，并不需要通过 Master 完成这一任务\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e同样，Mesos 也可以通过 Stand-By Master 的方法提供 Master 节点的高可用性。\nMesos 已经被广泛应用于各类集群的管理，但是其 Offer-Accept 的资源申请可能不是特别容易理解。\n对于想要自行编写调度策略的人，Frameworks 的抽象比较并不容易掌握。由于 Framework 要先占有了资源才能使用，\n设计不够良好的 Framework 可能会导致资源浪费和资源竞争/死锁。\u003c/p\u003e\n\u003ch3 id=\"u53CC_u5C42_u8C03_u5EA6_u5668_u6848_u4F8B3_3A_Spark__u548C_Spark_Drizzle\"\u003e\u003ca href=\"#u53CC_u5C42_u8C03_u5EA6_u5668_u6848_u4F8B3_3A_Spark__u548C_Spark_Drizzle\" class=\"headerlink\" title=\"双层调度器案例3: Spark 和 Spark Drizzle\"\u003e\u003c/a\u003e双层调度器案例3: Spark 和 Spark Drizzle\u003c/h3\u003e\u003cp\u003eSpark 为了调度和执行自己基于 DAG 模型的计算，自己实现了一个集中式的调度器，\n这个调度器的 Master 被称为 Driver，当 Driver 运行起来的之后，会向上层的 Scheduler\n申请资源调度起 Executor 进程。Executor 将会一直保持待机，等候 Driver 分配任务并执行，直到任务结束为止。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/img/scheduler/spark-classic-scheduler.png\" alt=\"Spark 传统的调度模型\"/\u003e\u003c/p\u003e\n\u003cp\u003eSpark 和 YARN 这样的集中式调度器放在一起可以认为是通过迂回的方式实现了双层调度器。\n就好像单机进程自己实现协程调度器一样，Spark Driver 预先申请的资源可以认为是在申请分区资源，\n申请到的资源将由 Driver 自行管理和使用。\u003c/p\u003e\n\u003cp\u003e有趣的是，在 2017 年的 SOSP 上，Spark 为了解决流处理计算当中调度延迟较大的问题，\n提出了一种新的调度模型 Drizzle，在原来调度模型的基础上，又再次实现了双层调度。\n下图是 Spark Drizzle 的设计模型\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/img/scheduler/spark-drizzle-scheduler.png\" alt=\"Spark Drizzle 的调度模型\"/\u003e\u003c/p\u003e\n\u003cp\u003eDrizzle 使得调度 Spark Streaming 任务的延迟由最低 500ms 降低到 200ms 左右，让 Spark Streaming\n在低延迟处理的问题上获得了突破性的进展。要搞清楚 Drizzle 提出的目的和解决的问题，\n首先要理解以下几点:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eSpark Streaming 的实现方式实际上是 Micro Batch，也就是说流式输入的数据在这里仍然被切分成一个一个 Batch 进行处理\u003c/li\u003e\n\u003cli\u003e传统的 Spark 调度器会在前序任务完成之后，根据之前任务输出的规模和分布，通过一定的算法有策略地调度新的任务，\n以便于获得更好的处理速度和降低资源浪费\u003c/li\u003e\n\u003cli\u003e在这一过程中，Exector 需要在前续任务完成后通知 Scheduler，之后由 Scheduler 调度新的任务。\n在传统 Batch 处理模式下，这种模型效果很好，但是在 Streaming 的场景下存在很多问题\u003c/li\u003e\n\u003cli\u003e在 Streaming 场景下，每个 Batch 的数据量较小，因此任务可能会需要频繁与 Scheduler 交互，\n因为存在这一交互过程的 Overhead，Streaming 处理的过程中最低的延迟也要 500ms 以上\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e为了得到更低的延迟性且保留 Micro Batch 容错性强且\u003cstrong\u003e易于执行 Checkpoint\u003c/strong\u003e 的优点，Drizzle 在原来的模型上做了一些优化:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e在每个节点运行一个 LocalScheduler\u003c/li\u003e\n\u003cli\u003e中央调度器 Driver 在执行 Streaming 处理任务时，根据计算的 DAG 图模型，预先调度某一个\nJob 的后序 Job，后序 Job 会被放置在 LocalScheduler 上\u003c/li\u003e\n\u003cli\u003e后序 Job 默认在 LocalScheduler 上是沉睡状态，但是前面的 Job 可以知道后序 Job 在哪个节点上，\n因此当前面的任务完成后，可以直接激活后序任务\u003c/li\u003e\n\u003cli\u003e当后序任务被激活之后，前序任务和后序任务可以直接通过网络请求串流结果\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e可以看到 Drizzle 的主要思路就是根据用户程序生成的图模型，预先 Schedule 一些任务，\n使得前序任务知道后序任务的位置，在调度时避免再请求中央调度器 Driver。\n同时 Drizzle 也采取了其他一些方法，比如将多个 Micro Batch 打包在一起，借由 LocalScheduler \n自行本地调度等等方式减少延迟。\u003c/p\u003e\n\u003cp\u003eDrizzle 模型可以说是双层模型的又一种另类体现。然而这种模型主要的缺点是必须要预先知道计算任务的图模型和依赖关系，\n否则就无法发挥作用。\u003c/p\u003e\n\u003ch2 id=\"u5171_u4EAB_u72B6_u6001_u8C03_u5EA6_u5668\"\u003e\u003ca href=\"#u5171_u4EAB_u72B6_u6001_u8C03_u5EA6_u5668\" class=\"headerlink\" title=\"共享状态调度器\"\u003e\u003c/a\u003e共享状态调度器\u003c/h2\u003e\u003cp\u003e通过前面两种模型的介绍，可以发现集群中需要管理的状态主要包括以下两种:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e系统中资源分配和使用的状态\u003c/li\u003e\n\u003cli\u003e系统中任务调度和执行的状态\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e在集中式调度器里，这两个状态都由中心调度器管理，并且一并集成了调度等功能。\n双层调度器模式里，这两个状态分别由中央调度器和次级调度器管理。\n集中式调度器可以容易地保证全局状态的一致性但是可扩展性不够，\n双层调度器对共享状态的管理较难达到好的一致性保证，也不容易检测资源竞争和死锁。\u003c/p\u003e\n\u003cp\u003e为了解决这些问题，一种新的调度器架构被设计出来。\n这种架构基本上沿袭了集中式调度器的模式，通过将中央调度器肢解为多个服务以提供更好的伸缩性。\n这种调度器的核心是共享的集群状态，因此可以被称为\u003cstrong\u003e共享状态调度器\u003c/strong\u003e。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/img/scheduler/shared-state-scheduler.png\" alt=\"共享状态调度器\"/\u003e\u003c/p\u003e\n\u003cp\u003e共享状态调度架构为了提供高可用性和可扩展性，将除共享状态之外的功能剥离出来成为独立的服务。\n这种设计可以类比为单机操作系统的微内核设计。在这种设计中，内核只负责提供最核心的资源管理接口，\n其他的内核功能都被实现为独立的服务，通过调用内核提供的 API 完成工作。\u003c/p\u003e\n\u003cp\u003e共享状态调度器的设计近些年来越来越受欢迎，这两年炙手可热的 Kubernetes 和它的原型 Borg\n都是采用这种架构。最近由加州大学伯克利分校知名实验室 RISELab 提出的号称要取代 Spark 分布式计算系统 Ray\n也是如此。下面将对这些案例进行介绍。\u003c/p\u003e\n\u003ch3 id=\"u5171_u4EAB_u72B6_u6001_u8C03_u5EA6_u5668_u6848_u4F8B1_3A_Borg/Kubernetes\"\u003e\u003ca href=\"#u5171_u4EAB_u72B6_u6001_u8C03_u5EA6_u5668_u6848_u4F8B1_3A_Borg/Kubernetes\" class=\"headerlink\" title=\"共享状态调度器案例1: Borg/Kubernetes\"\u003e\u003c/a\u003e共享状态调度器案例1: Borg/Kubernetes\u003c/h3\u003e\u003cp\u003e根据相关论文，Borg 在初期开发的时候使用的是集中式调度器的设计，所有功能都被集中在 BorgMaster\n当中，之后随着对灵活性和可扩展性的要求，逐步切换到共享状态模型或者说微内核模型上面去。\nGoogle 的工程师们总结了 Borg 的经验教训，将这些概念集合在 Kubernetes 当中开源出来，\n成为了近些年来最炙手可热的资源管理框架。\u003c/p\u003e\n\u003cp\u003e在这里我们依然以 Borg 为例进行介绍，Kubernetes 在具体的设计上是与 Borg 基本一致的。\n下图是 Borg 设计架构示意图:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/img/scheduler/borg-scheduler.png\" alt=\"Borg 资源调度架构\"/\u003e\u003c/p\u003e\n\u003cp\u003eBorg 资源调度架构的设计可以总结为以下几点:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e一个数据中心的集群可以被组织成一个 Borg 当中的 Cell\u003c/li\u003e\n\u003cli\u003e在一个 Borg 的 Cell 当中，资源的管理类似于集中式调度器的设计——集群资源由 BorgMaster 统一管理，\n每一个节点上运行着 Borglet 定时将本机器的状态与 BorgMaster 同步\u003c/li\u003e\n\u003cli\u003e为了增加可用性，BorgMaster 使用了 Stand-By Master 的模式。也就是说同时运行着 BorgMaster 的多个热备份，\n当 Active 的 BorgMaster 出现失败，新的 Master 会被选取出来\u003c/li\u003e\n\u003cli\u003e为了增加可扩展性和灵活性，BorgMaster 的大部分功能被剥离出来成为独立的服务。最终，BorgMaster\n只剩下维护集群资源和任务状态这唯一一个功能，包括 Scheduler 在内的所有其他服务都独立运行\u003c/li\u003e\n\u003cli\u003e独立运行的每个 Scheduler 可以运行自己的调度策略，它们定时从 BorgMaster 同步集群资源状态，\n根据自己的需要做出修改，然后通过 API 同步回 BorgMaster，从而实现调度功能\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e可以看到，Borg 的共享状态调度架构其实是集中式调度的改进，由于承载调度逻辑的调度器都运行在独立的服务里，\n对于 BorgMaster 的请求压力得到了某种程度的缓解。使用微内核设计模式，BorgMaster 自己包含的逻辑就比较简单了，\n系统的鲁棒性、灵活性和可扩展性得到了增强。\u003c/p\u003e\n\u003cp\u003e在 Borg 中，任务的隔离和资源限制使用了 Linux 的 cgroup 机制。在 Kubernetes 当中，这一机制被 Container 技术替代，\n实际上的功能是等价的。\u003c/p\u003e\n\u003cp\u003eBorg 的共享状态设计看似简单，其实具体实现仍然比较复杂。事实上，集中式的状态管理仍然会成为瓶颈。\n随着集群规模的扩展和状态的规模扩大，State Storage 必须使用分布式数据储存机制来保证可用性和低延迟。\u003c/p\u003e\n\u003cp\u003e共享状态架构的设计和双层设计的最大区别是，\n共享状态被抽取出来由一个统一的组件管理。从其他的各种服务的角度来看，\n共享状态提供的调用接口和集中式调度的状态管理是一样的。\n这种设计通过封装内部细节的方式降低了外部服务编写的复杂度，体现了系统设计里\u003cstrong\u003e封装复杂模块\u003c/strong\u003e的思想。\u003c/p\u003e\n\u003ch3 id=\"u5171_u4EAB_u72B6_u6001_u8C03_u5EA6_u5668_u6848_u4F8B2_3A_Omega\"\u003e\u003ca href=\"#u5171_u4EAB_u72B6_u6001_u8C03_u5EA6_u5668_u6848_u4F8B2_3A_Omega\" class=\"headerlink\" title=\"共享状态调度器案例2: Omega\"\u003e\u003c/a\u003e共享状态调度器案例2: Omega\u003c/h3\u003e\u003cp\u003e上面介绍的 Borg 是共享状态最典型的一个示例。尽管 BorgMaster 已经为其他服务的编程提供了简单的接口，\n但是仍然没有降低状态一致性同步的难度——BorgMaster 和服务的编写着仍然需要考虑很多并发控制的方法，\n防止对共享状态的修改出现 Race Condition 或死锁的现象。如何为其他服务和调度策略提供一层简单的抽象，\n使得任务的调度能兼顾吞吐量、延迟和并发安全呢？\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eOmega 使用事务(Transaction)解决共享状态一致性管理的问题\u003c/strong\u003e。这一思路非常直观——如果将数据库储存的数据看作共享状态，\n那么数据库就是是共享状态管理的最成熟、最通用的解决方案！事务更是早已被开发者们熟悉而且证明非常成熟和好用的并发抽象。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/img/scheduler/transaction-scheduler.png\" alt=\"事务调度策略\"/\u003e\u003c/p\u003e\n\u003cp\u003eOmega 将集群中资源的使用和任务的调度看作数据库中的条目，在一个应用执行的过程当中，\n调度器可以分步请求多种资源，当所有资源依次被占用并使任务执行完成，这个 Transaction 就会被成功 Commit。\u003c/p\u003e\n\u003cp\u003eOmega 的设计借鉴了很多数据库设计的思路，比如:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eTransaction 设计保留了一般事务的诸多特性，如嵌套 Transaction 或者 Checkpoint。\n当资源无法获取或任务执行失败，事务将会被回滚到上一个 Checkpoint 那里\u003c/li\u003e\n\u003cli\u003eOmega 可以实现传统数据库的死锁检测机制，如果检测到死锁，可以安全地撤销一个任务或其中的一些步骤\u003c/li\u003e\n\u003cli\u003eOmega 使用了乐观锁，也就是说申请的资源\u003cstrong\u003e不会立刻被加上排他锁\u003c/strong\u003e，只有需要真正分配资源或进行事务提交的时候才会检查锁的状态，\n如果发现出现了 Race Condition 或其他错误，相关事务可以被回滚\u003c/li\u003e\n\u003cli\u003eOmega 可以像主流数据库一样定义 Procedure ，这些 Procedure 可以实现一些简单的逻辑，\n用于对用户的资源请求进行合法性的验证(如优先级保证、一致性校验、资源请求权限管理等)\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eOmega 使用事务管理状态的想法非常新颖，这一设计随着分布式数据库以及分布式事务的逐渐发展和成熟而逐渐变得可行，\n它一度被认为将成为 Google 的下一代调度系统。\u003c/p\u003e\n\u003cp\u003e然而近期的一些消息表明为了达到设计目标，Omega 的实现逻辑变得越来越复杂。\n在原有的 Borg 共享状态模型已经能满足绝大部分需要的情况下，Omega 的前景似乎没有那么乐观。\u003c/p\u003e\n\u003ch1 id=\"u603B_u7ED3\"\u003e\u003ca href=\"#u603B_u7ED3\" class=\"headerlink\" title=\"总结\"\u003e\u003c/a\u003e总结\u003c/h1\u003e\u003cp\u003e这篇文章介绍了多种调度器结构设计的模型并讨论了在相关模型下进行任务调度的一些特点。\n通过这些模型的对比，我想提出自己对调度系统设计的几点看法：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e在小规模的应用和需要自己设计调度器的场景，我们应该尽量采取中心化的调度模型。这是因为这种模型设计和使用都比较简单，\n调度器容易对整个系统的状态有全面的把握，状态同步的困难也不高\u003c/li\u003e\n\u003cli\u003e在机群和应用规模继续扩大或者对调度算法有定制要求的情况下，可以考虑使用双层调度器设计。\n双层调度器调度策略的编写较为复杂，随着新一代共享状态调度器的发展，在未来可能会慢慢退出主流\u003c/li\u003e\n\u003cli\u003e共享状态调度器因为其较为简单的编程接口以及适应多种需要的特点，正随着 Kubernetes 的流行而渐渐变成主流。\n如果应用规模比较大或需要在一个集群上运行多种定制调度策略，这种调度器架构设计是最有前景的\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e最后，通过学习和亲自设计一套调度系统，我深刻的领会到一些个人在编程的时候非常重要的经验：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eKeep things simple 。在实现任何程序的时候，简单的设计往往比复杂的设计更好。\n比如说尽量减少系统中相互独立的各种模块，尽量统一编程语言，尽量减少相互隔离的系统状态。\n这样的设计可以减少 Bug 出现的概率，降低维护状态同步的难度\u003c/li\u003e\n\u003cli\u003eMove fast。在设计复杂系统的时候很容易陷入对细节的不必要追究上，从而导致需要管理的细节越来越多，\n增加了很多心智压力。最后系统完成的进度也是难上加难。更好的办法是先从宏观上进行大概的设计，\n在进行实现的时候忽略具体的细节(比如代码如何组织、函数如何相互调用、代码如何写得好看等)，\n快速迭代并实现功能。当然，在这个过程中也仍然要把握好功能和质量的平衡\u003c/li\u003e\n\u003cli\u003e技术发展的循环上升轨迹。 回忆起当初 Linux 和 Minix 在宏内核和微内核之间的世纪论战，\n尽管以 Linux 这种 Monolithic 内核设计的胜出而告终，但是 Minix\n的作者在其著述的《\u003ca href=\"https://book.douban.com/subject/3017583/\" target=\"_blank\" rel=\"noopener\"\u003eModern Operating System\u003c/a\u003e》\n教科书上指出了这种循环上升的轨迹，预言了微内核设计的归来。看一看共享状态调度系统的设计就会发现，\n这一预言已经应验在了分布式系统上\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch1 id=\"u5C55_u671B\"\u003e\u003ca href=\"#u5C55_u671B\" class=\"headerlink\" title=\"展望\"\u003e\u003c/a\u003e展望\u003c/h1\u003e\u003cp\u003e在本文中，并没有特别涉及到任务调度的具体算法，比如如何准确地定时激发任务，如何更高效地分配资源等等。\n调度算法所要解决的问题本质上只有两个：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e全面掌握当前系统的状态\u003c/li\u003e\n\u003cli\u003e准确预测未来的任务需求\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e很多调度器模型在设计上已经对这两方面有所考量，但调度器算法本身可以说又是一个巨大的主题，\n笔者本身对其了解也非常有限，因此不敢在这篇文章中继续展开。从直觉上讲，上述需求二可能是一个和 AI\n技术相结合很好的切入点，在未来可能会有很多研究。\u003c/p\u003e\n\u003cp\u003e在前文还提到了很多调度器也会附带管理本地文件资源的分发，比如像 Kubernetes 启动任务的时候需要将 Docker\n镜像分发到各个宿主机上。作为其原型，Borg 在这一过程中甚至利用了 P2P 技术加快分发速度和充分利用带宽。\n在开源世界中似乎还没有类似的解决方案。当然，这一需求也只有在机群规模非常大的时候才有价值，\n但未来仍可能成为一个不错的发展方向。\u003c/p\u003e\n\n    \u003c/div\u003e",
  "Date": "2018-02-09T07:45:35Z",
  "Author": "Chase Zhang"
}