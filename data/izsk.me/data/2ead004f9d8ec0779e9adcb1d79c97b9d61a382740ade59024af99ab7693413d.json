{
  "Source": "izsk.me",
  "Title": "Kubernetes学习(flannel深入学习)",
  "Link": "https://izsk.me/2020/01/05/Kubernetes-flannel-details/",
  "Content": "\u003cdiv class=\"post-body\" itemprop=\"articleBody\"\u003e\n\n      \n      \n\n      \n        \u003cp\u003e得益于社区的繁荣, 包括部署工具的完善, 现在几乎没人会从0去搭建kubernetes集群, 随便拿起一个开源工具, 几乎都是一条命令, 喝个咖啡, 然后整个集群就能用了, 非常快捷, 但是方便的时候, 也容易遗漏掉其中的一些知识点, 有些对整个kubernetes的了解及生态还是非常有必要的.\u003c/p\u003e\n\u003cp\u003e由于社区可选的CNI成熟方案非常的多, 而flannel是kubernetes默认的容器网络模型, 还是要重点深入学习下\u003c/p\u003e\n\u003cp\u003e由于flannel涉及到CNI相关的知识, CNI也不会在这里深入探讨, 有机会跟CRI一起更.\u003c/p\u003e\n\u003cspan id=\"more\"\u003e\u003c/span\u003e\n\n\n\n\u003cp\u003e比如有以下涉及容器网络的问题? \u003c/p\u003e\n\u003col\u003e\n\u003cli\u003ekubernetes如何知道集群中采用了哪种CNI, 处理逻辑是什么?\u003c/li\u003e\n\u003cli\u003e使用ip addr看到一个docker0、cni0、flannel.1它们有什么作用, docker0在flannel中扮演什么角色?\u003c/li\u003e\n\u003cli\u003eflannel的通信方式？\u003c/li\u003e\n\u003cli\u003e从使用kubectl创建了一个pod后，期间发生了什么？\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"如何知道使用了哪种CNI-处理逻辑是什么？\"\u003e\u003ca href=\"#如何知道使用了哪种CNI-处理逻辑是什么？\" class=\"headerlink\" title=\"如何知道使用了哪种CNI,处理逻辑是什么？\"\u003e\u003c/a\u003e如何知道使用了哪种CNI,处理逻辑是什么？\u003c/h3\u003e\u003cp\u003e这里并不会对CNI的规范做一个说明, 简单总结来说就是:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003eCNI旨在为容器平台提供网络的标准化，它 本身并不是实现或者代码，可以理解成一个协议，这个协议连接了两个组件：容器管理系统和网络插件。它们之间通过 JSON 格式的文件进行通信，实现容器的网络功能。具体的事情都是插件来实现的，包括：创建容器网络空间（network namespace）、把网络接口（interface）放到对应的网络空间、给网络接口分配 IP 等等\u003c/code\u003e。\u003c/p\u003e\n\u003cp\u003e要知道一个集群使用了哪种CNI, 最好的办法就是顺序数据流向一步一步分析.\u003c/p\u003e\n\u003cp\u003e从上面对CNI的总结看, CNI的相关配置一定是在容器运行时这一端, 因为要创建容器网络空间, 自然是要在client端，那么对于kubernetes来说就是kubelet, 查看kubelet参数(这里kubelet是以容器启动的)\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/20200413131106.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e这3个参数决定了集群使用的哪种CNI, 其中第2个参数指定了使用CNI来组件容器网络, 现在来逐一的查看这些目录\u003c/p\u003e\n\u003ch4 id=\"x2F-etc-x2F-cni-x2F-net-d\"\u003e\u003ca href=\"#x2F-etc-x2F-cni-x2F-net-d\" class=\"headerlink\" title=\"/etc/cni/net.d\"\u003e\u003c/a\u003e/etc/cni/net.d\u003c/h4\u003e\u003cp\u003e该文件下只有一个文件\u003ccode\u003e10-flannel.conflist\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/image-20200413131329691.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e注意\u003ccode\u003etype: flannel\u003c/code\u003e, 那么这里就指定了使用的是flannel, 而不是calico, macvlan等其它CNI\u003c/p\u003e\n\u003ch4 id=\"x2F-opt-x2F-cni-x2F-bin\"\u003e\u003ca href=\"#x2F-opt-x2F-cni-x2F-bin\" class=\"headerlink\" title=\"/opt/cni/bin\"\u003e\u003c/a\u003e/opt/cni/bin\u003c/h4\u003e\u003cp\u003e这个目录下放置着CNI的二进制文件, 因为CNI协议规则，所有实现了CNI的都必须提交二进制的文件供kubelet调用, 并且会调用后端的网络插件\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/20200413131222.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e因此通过指定\u003ccode\u003etype: flannel\u003c/code\u003e来这个目录下调用\u003ccode\u003eflannel\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e这里要区分一下flannel跟flanneld这两个\u003c/p\u003e\n\u003ch4 id=\"flannel\"\u003e\u003ca href=\"#flannel\" class=\"headerlink\" title=\"flannel\"\u003e\u003c/a\u003eflannel\u003c/h4\u003e\u003cp\u003e平时大家说的flannel指的是/opt/cni/bin/flannel, 但是这个二进制的github源码是在\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://github.com/containernetworking/plugins\"\u003eplugins\u003c/a\u003e, 并不是指的flanneld.\u003c/p\u003e\n\u003cp\u003e这个flannel才是真正做为CNI插件实现了\u003ccode\u003eCNI_COMMAND ADD/DEL\u003c/code\u003e方法\u003c/p\u003e\n\u003ch4 id=\"flanneld\"\u003e\u003ca href=\"#flanneld\" class=\"headerlink\" title=\"flanneld\"\u003e\u003c/a\u003eflanneld\u003c/h4\u003e\u003cp\u003e这个flanneld的github源码在\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://github.com/coreos/flannel\"\u003eflannel\u003c/a\u003e, 这个flanneld主要实现overlay网络, 包含给所有node分配subset段, 同步节点间的网络信息等.\u003c/p\u003e\n\u003cp\u003eflanneld可以以容器的方式(daemonset)运行, 查看下配置文件\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/20200413131838.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e从配置文件可以到有两个配置文件\u003c/p\u003e\n\u003ch5 id=\"cni-conf-json\"\u003e\u003ca href=\"#cni-conf-json\" class=\"headerlink\" title=\"cni-conf.json\"\u003e\u003c/a\u003ecni-conf.json\u003c/h5\u003e\u003cp\u003e这个文件就是/etc/cni/net.d/10-flannel.conflist， 指定backend使用flannel\u003c/p\u003e\n\u003ch5 id=\"net-conf-json\"\u003e\u003ca href=\"#net-conf-json\" class=\"headerlink\" title=\"net-conf.json\"\u003e\u003c/a\u003enet-conf.json\u003c/h5\u003e\u003cp\u003e这个文件指定了整个kubernetes网络能够使用的ip地址段(在部署时可以被覆盖), 且指定了flannel使用vxlan模式(flannel本身提供多种模式), 最终这个文件会挂载到\u003ccode\u003e/etc/kube-flannel/net-conf.json\u003c/code\u003e, 提供给cni使用\u003c/p\u003e\n\u003ch5 id=\"subset-env\"\u003e\u003ca href=\"#subset-env\" class=\"headerlink\" title=\"subset.env\"\u003e\u003c/a\u003esubset.env\u003c/h5\u003e\u003cp\u003e还有一个很关键的文件就是\u003ccode\u003e/run/flannel/subnet.env\u003c/code\u003e, 该文件记录了某个node上的flanneld启动后在etcd端分配的ip子段, 也就是说, 这个node上的所有pod只能在这个ip段内分配\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/20200413131511.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e从这里可以看出, 每台机器上最多能够使用的ip有254个, 这个是远远够的, 一般情况下一台机器的pod数不会过百\u003c/p\u003e\n\u003ch5 id=\"flannel-cni容器\"\u003e\u003ca href=\"#flannel-cni容器\" class=\"headerlink\" title=\"flannel-cni容器\"\u003e\u003c/a\u003eflannel-cni容器\u003c/h5\u003e\u003cp\u003e细心的可能会发现flannel的yaml文件还包含了install-cni.sh的容器, 这个容器的逻辑比较简单, 大家可自行查看源码\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://github.com/coreos/flannel-cni\"\u003egithub\u003c/a\u003e\u003c/p\u003e\n\u003ch5 id=\"etcd\"\u003e\u003ca href=\"#etcd\" class=\"headerlink\" title=\"etcd\"\u003e\u003c/a\u003eetcd\u003c/h5\u003e\u003cp\u003eflanneld的启动参数中指定了etcd, etcd在flannel充当数据的角色, 同时使用etcd做为服务注册、 发现, 所有pod/node的ip/mac信息都存储在etcd中, 所有flanneld节点共享， 通过这种方式来避免ip重复分配.\u003c/p\u003e\n\u003ch4 id=\"处理逻辑\"\u003e\u003ca href=\"#处理逻辑\" class=\"headerlink\" title=\"处理逻辑\"\u003e\u003c/a\u003e处理逻辑\u003c/h4\u003e\u003cp\u003e因此, 如果集群中使用了flannel做为CNI, 完整的处理逻辑如上分析, 简单说的话就是:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cul\u003e\n\u003cli\u003e指定整个集群pod的ip CIDR\u003c/li\u003e\n\u003cli\u003e每个node上都会启动flanneld进行, 通过etcd分配该node上所有pod的使用地址, 后续会在整个集群内共享各个node上的pod ip信息, 生成路由信息.\u003c/li\u003e\n\u003cli\u003e所有node通过flannel overlay网络进行通信\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/blockquote\u003e\n\u003ch3 id=\"docker0、cni0、flannel-1\"\u003e\u003ca href=\"#docker0、cni0、flannel-1\" class=\"headerlink\" title=\"docker0、cni0、flannel.1\"\u003e\u003c/a\u003edocker0、cni0、flannel.1\u003c/h3\u003e\u003cp\u003e如果在一台node上使用\u003ccode\u003eip addr\u003c/code\u003e, 会发现有docker0、cni0、flannel.1这些网络接口\u003c/p\u003e\n\u003cp\u003e这三者充当什么角色.\u003c/p\u003e\n\u003cp\u003e首先docker0这个跟flannel及docker的部署顺序有关系, 如果是docker先部署了, 后再部署flannel,部署完成之后也没有重启docker, 那么这种情况docker0是不做为node与pod之间的bridege, 而是使用了默认的cni0来组成veth对\u003c/p\u003e\n\u003cp\u003e可以使用\u003ccode\u003ebrctl show\u003c/code\u003e来查看, 会发现该node上所有的pod都是绑定在cni0上\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/20200413121335.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e而如何在部署flannel时修改了docker的启动参数, docker使用了flannel的bip参数的话,那就会默认使用docker0来组vth对, 即pod都会通过docker0通信.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/20200413173855.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e而flannel.1则是flanneld组成的overlay网络的通道, 它本质上是个VXLAN Tunnel End Point，如上图所示\u003c/p\u003e\n\u003cp\u003e具体的细节下面使用flannel的通信来说明\u003c/p\u003e\n\u003ch3 id=\"flannel通信方式\"\u003e\u003ca href=\"#flannel通信方式\" class=\"headerlink\" title=\"flannel通信方式\"\u003e\u003c/a\u003eflannel通信方式\u003c/h3\u003e\u003ch4 id=\"Kube-dns\"\u003e\u003ca href=\"#Kube-dns\" class=\"headerlink\" title=\"Kube-dns\"\u003e\u003c/a\u003eKube-dns\u003c/h4\u003e\u003cp\u003e这里简单提一下kube-dns, kube-dns负责集群内的域名解析, 因此在集群内访问时才能使用\u003ccode\u003exxx.namespace.svc.cluster.local\u003c/code\u003e这种形式访问.\u003c/p\u003e\n\u003cp\u003ekube-dns也支持上游dns, kube-dns集群内解析不了的可交由上游dns解析. \u003c/p\u003e\n\u003cp\u003e如在kube-dns的pod中挂载宿主要的/etc/resolv.conf文件.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/20200411211019.png\"/\u003e\u003c/p\u003e\n\u003ch3 id=\"路由规则\"\u003e\u003ca href=\"#路由规则\" class=\"headerlink\" title=\"路由规则\"\u003e\u003c/a\u003e路由规则\u003c/h3\u003e\u003cp\u003ekubernets的网络，从设计上来讲是“扁平、直接”的，要实现kubernetes的cni模型, 需要实现3点要求:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cul\u003e\n\u003cli\u003e所有容器可以不使用NAT技术就可以与其他容器通信\u003c/li\u003e\n\u003cli\u003e所有节点(物理机 虚拟机 容器)都可以不使用NAT同容器通信\u003c/li\u003e\n\u003cli\u003e容器看到的IP地址和别的机器看到的IP是一致的\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e那么一条网络报文是怎么从一个容器发送到另外一个容器的呢(跨主机) ？\u003c/p\u003e\n\u003cp\u003e这里以\u003ccode\u003evxlan\u003c/code\u003e为例\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e容器直接使用目标容器的 ip 访问，默认通过容器内部的 eth0 发送出去\u003c/li\u003e\n\u003cli\u003e报文通过 veth pair 被发送到 vethXXX\u003c/li\u003e\n\u003cli\u003evethXXX 是直接连接到虚拟交换机 cni0 的，报文通过虚拟 bridge cni0 发送出去\u003c/li\u003e\n\u003cli\u003e查找路由表，外部容器 ip 的报文都会转发到 flannel.1 虚拟网卡，这是一个 P2P 的虚拟网卡,然后报文就被转发到监听在另一端的 flanneld\u003c/li\u003e\n\u003cli\u003eflanneld 通过 etcd 维护了各个节点之间的路由表，把原来的报文 UDP 封装一层，通过配置的 \u003ccode\u003eiface\u003c/code\u003e 发送出去\u003c/li\u003e\n\u003cli\u003e报文通过主机之间的网络找到目标主机\u003c/li\u003e\n\u003cli\u003e报文继续往上，到传输层，交给监听在 8285 端口的 flanneld 程序处理\u003c/li\u003e\n\u003cli\u003e数据被解包，然后发送给 flannel.1 虚拟网卡\u003c/li\u003e\n\u003cli\u003e查找路由表，发现对应容器的报文要交给 cni0\u003c/li\u003e\n\u003cli\u003ecni0 找到连到自己的容器，把报文发送过去\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e如果是同一node上pod通信, 则直接通过cni0就可以, \u003c/p\u003e\n\u003cp\u003e查看node上的路由信息:\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003eroute -n\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/20200411215227.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e从这里可以看到, \u003ccode\u003e10.42.xx\u003c/code\u003e的使用flannel.1 iface转发, 这也是集群pod的网段.\u003c/p\u003e\n\u003cp\u003e而如果是访问的其它ip,则是通过eth0 iface\u003c/p\u003e\n\u003ch4 id=\"VXLAN\"\u003e\u003ca href=\"#VXLAN\" class=\"headerlink\" title=\"VXLAN\"\u003e\u003c/a\u003eVXLAN\u003c/h4\u003e\u003cp\u003evxlan是Linux上支持的一个隧道的技术，隧道也就是点到点，端到端的两个设备的通信，其实vxlan实现是有一个vtep的设备做数据包的封装与解封装，而现在已经封装到flannel.1这个进程里面了，也就是这个虚拟网卡包含了veth对使用对这个vxlan进行封装和解封装\u003c/p\u003e\n\u003ch4 id=\"为何需要封装\"\u003e\u003ca href=\"#为何需要封装\" class=\"headerlink\" title=\"为何需要封装\"\u003e\u003c/a\u003e为何需要封装\u003c/h4\u003e\u003cp\u003e为什么需要封装呢? 从上面的图可以看到\u003c/p\u003e\n\u003cp\u003e假如现在有如下请求.podA –\u0026gt; podB(\u003ccode\u003e如果是serviceB的clusterIP,则会通过kube-proxy机制随机得到一个后端ip\u003c/code\u003e)\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/20200413192311.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e则根据路由信息, 知道需要从flannel.1网口转发, 但是flannel.1要发送到哪呢？按照常规逻辑, 这个请求包中只包含有如下信息：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003esrc: podA地址\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003edst: podB地址\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e通过flannel生成的路由信息可以知道需要通过NodeA上的flannel.1进行转发.\u003c/p\u003e\n\u003cp\u003e如果仅仅知道上面这些信息是无法将请求转发到NodeB的, 因为现在还不知道NodeB的IP,就无法路由\u003c/p\u003e\n\u003cp\u003e因此, 需要一种机制把NodeB的IP封装到原始包中, 进行路由, 同时NodeB需要支持解封装\u003c/p\u003e\n\u003cp\u003e这就是需要封装的原因, 而vxlan是内核原生支持的特性.性能也比较高\u003c/p\u003e\n\u003cp\u003e当然也许有人会问不是可以通过api-server根据podip来查找所属的NodeIP吗？\u003c/p\u003e\n\u003cp\u003e这完全是没问题的, 但这会交付网络寻址的问题融入到kubernetes的代码中,将会变得异常复杂,同时也违背了\u003ccode\u003e插件化\u003c/code\u003e的理念, 因此路由寻址问题必须在容器网络插件层面解决.\u003c/p\u003e\n\u003ch4 id=\"封包格式\"\u003e\u003ca href=\"#封包格式\" class=\"headerlink\" title=\"封包格式\"\u003e\u003c/a\u003e封包格式\u003c/h4\u003e\u003cp\u003e如上图所示，当NodeB加入flannel网络时，和其他所有backend一样，它会将自己的subnet 10.1.16.0/24和Public IP 192.168.0.101写入etcd中，和其他backend不一样的是，它\u003ccode\u003e还会将vtep设备flannel.1的mac地址也写入etcd中\u003c/code\u003e。\u003c/p\u003e\n\u003cp\u003e之后，NodeA会得到EventAdded事件，并从中获取NodeB添加至etcd的各种信息。这个时候，它会在本机上添加三条信息：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cp\u003e路由信息：所有通往目的地址10.1.16.0/24的封包都通过vtep设备flannel.1设备发出，发往的网关地址为10.1.16.0，即NodeB中的flannel.1设备\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003efdb信息：MAC地址为MAC B的封包，都将通过vxlan首先发往目的地址192.168.0.101，即NodeB\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003earp信息：网关地址10.1.16.0的地址为MAC B\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e最终生成的封装包如下:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/20200413193513.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e可以查看其它机器的flannel.1设备的mac地址\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003ebridge fdb show  dev flannel.1\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/20200413181732.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e通过mac地址查找ip地址\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003ebridge fdb show  dev flannel.1|grep \u0026#39;46:2f:09:f1\u0026#39;\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/20200413181908.png\"/\u003e\u003c/p\u003e\n\u003ch4 id=\"完整流程\"\u003e\u003ca href=\"#完整流程\" class=\"headerlink\" title=\"完整流程\"\u003e\u003c/a\u003e完整流程\u003c/h4\u003e\u003cp\u003e有一个容器网络封包要从A发往容器B，封包首先通过网桥转发到NodeA中。此时通过，查找路由表，该封包应当通过设备flannel.1发往网关10.1.16.0。通过进一步查找arp表，我们知道目的地址10.1.16.0的mac地址为MAC B。到现在为止，vxlan负载部分的数据已经封装完成。由于\u003ccode\u003eflannel.1是vtep设备\u003c/code\u003e，会对通过它发出的数据进行vxlan封装（这一步是由内核完成的），通过查询fdb获取目的mac地址为MAC B的ip地址为192.168.0.101\u003c/p\u003e\n\u003cp\u003e封包到达主机B的eth0，通过内核的vxlan模块解包，容器数据封包将到达vxlan设备flannel.1，封包的目的以太网地址和flannel.1的以太网地址相等，三层封包最终将进入主机B并通过路由转发达到目的容器\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e先通过NodeB的IP在三层网络进行路由, 到达NodeB后, 再在二层网络使用MAC地址发现NodeB上的flannel.1设备, 最终通过PodB的IP进行转发完成整个流程\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e整个流程如下:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/20200413185816.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eflannel不需要Nodes之间在同一个子网, 只需要能够互通就行.\u003c/strong\u003e\u003c/p\u003e\n\u003ch4 id=\"host-gw\"\u003e\u003ca href=\"#host-gw\" class=\"headerlink\" title=\"host-gw\"\u003e\u003c/a\u003ehost-gw\u003c/h4\u003e\u003cp\u003ehostgw是最简单的backend，它的原理非常简单，直接添加路由，将目的主机当做网关，直接路由原始封包。例如，我们从etcd中监听到一个EventAdded事件：subnet为10.1.15.0/24被分配给主机Public IP 192.168.0.100，hostgw要做的工作非常简单，在本主机上添加一条目的地址为10.1.15.0/24，网关地址为192.168.0.100，输出设备为上文中选择的集群间交互的网卡即可。对于EventRemoved事件，删除对应的路由即可\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eHost-gw有一定的限制，就是要求所有的主机都在一个子网内，即二层可达(不需要经过路由)，否则就无法将目的主机当做网关，直接路由\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e至于选择vxlan还是host-gw, 根据实际情况吧, 性能上host-gw肯定是高一点, 但网络上有限制, 而vxlan如果集群规则不是很大的话, 基本跟host-gw的性能差不多.\u003c/p\u003e\n\u003ch3 id=\"参考文章\"\u003e\u003ca href=\"#参考文章\" class=\"headerlink\" title=\"参考文章:\"\u003e\u003c/a\u003e\u003cstrong\u003e参考文章:\u003c/strong\u003e\u003c/h3\u003e\u003cblockquote\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://kubernetes.io/\"\u003ehttps://kubernetes.io\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://github.com/containernetworking/plugins\"\u003ehttps://github.com/containernetworking/plugins\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://github.com/coreos/flannel-cni\"\u003ehttps://github.com/coreos/flannel-cni\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://blog.51cto.com/14143894/2462379\"\u003ehttps://blog.51cto.com/14143894/2462379\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://www.cnblogs.com/YaoDD/p/7681811.html\"\u003ehttps://www.cnblogs.com/YaoDD/p/7681811.html\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://blog.51cto.com/14143894/2462379\"\u003ehttps://blog.51cto.com/14143894/2462379\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://itnext.io/kubernetes-journey-up-and-running-out-of-the-cloud-flannel-c01283308f0e\"\u003ehttps://itnext.io/kubernetes-journey-up-and-running-out-of-the-cloud-flannel-c01283308f0e\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/blockquote\u003e\n\u003ch3 id=\"转载请注明原作者-周淑科-https-izsk-me\"\u003e\u003ca href=\"#转载请注明原作者-周淑科-https-izsk-me\" class=\"headerlink\" title=\"转载请注明原作者: 周淑科(https://izsk.me)\"\u003e\u003c/a\u003e\u003cstrong\u003e转载请注明原作者: 周淑科(\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://izsk.me/\"\u003ehttps://izsk.me\u003c/a\u003e)\u003c/strong\u003e\u003c/h3\u003e\n      \n    \u003c/div\u003e",
  "Date": "2020-01-05T13:30:53+08:00",
  "Author": "Z.S.K."
}