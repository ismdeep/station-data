{
  "Source": "izsk.me",
  "Title": "Ceph学习(架构)",
  "Link": "https://izsk.me/2018/04/28/Ceph%E5%AD%A6%E4%B9%A0(%E6%9E%B6%E6%9E%84)/",
  "Content": "\u003cdiv class=\"post-body\" itemprop=\"articleBody\"\u003e\n\n      \n      \n\n      \n        \u003ch2 id=\"Ceph简介\"\u003e\u003ca href=\"#Ceph简介\" class=\"headerlink\" title=\"Ceph简介\"\u003e\u003c/a\u003eCeph简介\u003c/h2\u003e\u003cblockquote\u003e\n\u003cp\u003eCeph is a unified, distributed storage system designed for excellent performance, reliability and scalability.\u003cbr/\u003eCeph是统一分布式存储系统,具有优异的性能、可靠性、可扩展性.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cul\u003e\n\u003cli\u003e存储系统: 提供对象存储,块存储,文件系统接口等服务\u003c/li\u003e\n\u003cli\u003e分布式:   无中心节点(Monitor-Paxos,类zookeeper)\u003c/li\u003e\n\u003cli\u003e统一:     所有服务基于统一的Rados\u003c/li\u003e\n\u003c/ul\u003e\n\u003cspan id=\"more\"\u003e\u003c/span\u003e\n\n\u003cp\u003eCeph的底层是RADOS(可靠、自动、分布式对象存储),可以通过LIBRADOS直接访问到RADOS的对象存储系统。\u003cbr/\u003e同时基于Rados提供RBD(块设备接口)、RADOS Gateway(对象存储接口)、Ceph File System(POSIX接口)等服务。\u003c/p\u003e\n\u003ch3 id=\"架构介绍\"\u003e\u003ca href=\"#架构介绍\" class=\"headerlink\" title=\"架构介绍\"\u003e\u003c/a\u003e架构介绍\u003c/h3\u003e\u003cp\u003e\u003cimg src=\"http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/ceph-c2-1.jpg\" alt=\"ceph-c2-1\"/\u003e\u003c/p\u003e\n\u003cp\u003eCeph底层提供了Rados存储,用于支撑上层的librados和Rgw,Rbd,cephfs等服务.  \u003c/p\u003e\n\u003ch3 id=\"概念介绍\"\u003e\u003ca href=\"#概念介绍\" class=\"headerlink\" title=\"概念介绍\"\u003e\u003c/a\u003e概念介绍\u003c/h3\u003e\u003cul\u003e\n\u003cli\u003eCRUSH算法: 类一致性哈希,Ceph的基础算法,用于动态计算数据块的存储位置(一个set)\u003cstrong\u003e因为通过算法动态计算,不需要保存所有数据块和存储位置的对应关系\u003c/strong\u003e\u003c/li\u003e\n\u003cli\u003eRados: Reliable, Autonomic Distributed Object Store, Ceph的核心存储,由Monitor及OSD组成.\u003c/li\u003e\n\u003cli\u003eLibrados: 访问Rados的底层库,C/C++ 实现,提供Java/Python等的包装\u003c/li\u003e\n\u003cli\u003eOSD: Object Storage Device, Rados数据读写进程.一般每个Disk或分区对应一个OSD进程.\u003c/li\u003e\n\u003cli\u003eRados Object: 任意保存至OSD的文件\u003c/li\u003e\n\u003cli\u003ePool: 逻辑概念,Rados Object读写操作都是针对Pool的.\u003c/li\u003e\n\u003cli\u003ePG: Placement Group 放置组,Rados操作的最小粒度,由多个PG组成Pool,Object写入Pool时被指定到PG,具体由CRUSH算法计算.PG对应一到多个OSD.\u003c/li\u003e\n\u003cli\u003eMonitor: Ceph中的zookeeper,维护Cluster集群映射关系和配置/Auth等.当各模块有up/down等变化时,通知mon修改对应映射(osdmap,pgmap,poolmap等).\u003cstrong\u003e这些映射和配置保存于Mon本地(leveldb),供客户端CRUSH计算\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr/\u003e\n\u003cul\u003e\n\u003cli\u003eRgw: Rados gateway,使Ceph对象存储兼容S3/Swift接口的进程\u003c/li\u003e\n\u003cli\u003eRbd: Rados block device,Ceph对外提供的块设备服务(无实际进程),基于librados的服务协议\u003c/li\u003e\n\u003cli\u003eLibrbd: 基于Librados的库,用于访问Ceph 块服务\u003c/li\u003e\n\u003cli\u003eMDS: Ceph Metadata Server,CephFS依赖的元数据处理进程\u003c/li\u003e\n\u003cli\u003eCephFS:  Ceph File System,Ceph对外提供的文件系统服务\u003c/li\u003e\n\u003c/ul\u003e\n\u003chr/\u003e\n\u003cul\u003e\n\u003cli\u003eMgr: Manager 将底层C/C++库封装为Python调用,旨在提供Plugin机制,鼓励用户开发插件满足特定需求.Dashboard/Restful-API\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"http://zskgithubblog.oss-cn-beijing.aliyuncs.com/blogpic/ceph-c2-2.jpg\" alt=\"ceph-c2-2\"/\u003e\u003c/p\u003e\n\n      \n    \u003c/div\u003e",
  "Date": "2018-04-28T16:20:53+08:00",
  "Author": "Z.S.K."
}