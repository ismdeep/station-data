{
  "Source": "izsk.me",
  "Title": "Kubernetes学习(Kubernetes踩坑记)",
  "Link": "https://izsk.me/2023/12/04/Kubernetes-prombles/",
  "Content": "\u003cdiv class=\"post-body\" itemprop=\"articleBody\"\u003e\n\n      \n      \n\n      \n        \u003cp\u003e记录在使用Kubernetes中遇到的各种问题及解决方案, 好记性不如烂笔头\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e不定期更新\u003c/strong\u003e\u003c/p\u003e\n\u003cspan id=\"more\"\u003e\u003c/span\u003e\n\n\u003ch3 id=\"kubelet中提示-no-relationship-found-between-node-xxx-and-this-object\"\u003e\u003ca href=\"#kubelet中提示-no-relationship-found-between-node-xxx-and-this-object\" class=\"headerlink\" title=\"kubelet中提示: no relationship found between node xxx and this object\"\u003e\u003c/a\u003ekubelet中提示: no relationship found between node xxx and this object\u003c/h3\u003e\u003cfigure class=\"highlight bash\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003eUser \u003cspan class=\"string\"\u003e\u0026#34;system:node:xxx\u0026#34;\u003c/span\u003e cannot list resource \u003cspan class=\"string\"\u003e\u0026#34;configmap\u0026#34;\u003c/span\u003e \u003cspan class=\"keyword\"\u003ein\u003c/span\u003e API group \u003cspan class=\"keyword\"\u003ein\u003c/span\u003e the namespace \u003cspan class=\"string\"\u003e\u0026#34;xxx\u0026#34;\u003c/span\u003e, no relationship found between node xxx and this object\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003cp\u003e原因: 由于集群开启了Node Authorization Mode,所以节点上的kubelet能操作的权限是跟节点上运行pod相关联的，可以通过以下的方式验证\u003c/p\u003e\n\u003cfigure class=\"highlight bash\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e2\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e3\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"comment\"\u003e# 登录到节点kubectl\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003ekubectl --kubeconfig=/etc/kubernetes/kubelet.conf -n xxx can-i list cm/xxx\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"comment\"\u003e# 如果有权限则提示yes, 没有则为no\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003cp\u003e解决:  如果需要访问，则可通过一个pod挂载对应的cm调度到该节点上，那么在这个node上即可通过kubelet访问到相关的cm\u003c/p\u003e\n\u003cp\u003e参考:  \u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cp\u003e\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://enix.io/fr/blog/kubernetes-tip-and-tricks-node-authorization-mode/\"\u003eKubernetes : Le Node Authorization Mode de l\u0026#39;API-Server\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://blog.frognew.com/2021/05/k8s-apiserver-authorization-mode-node.html\"\u003ekubernetes apiserver的node鉴权 | 青蛙小白\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"kube-controller-manager日志出现-unable-to-retrieve-the-complete-list-of-server-APIs-metrics-k8s-io-x2F-v1beta1\"\u003e\u003ca href=\"#kube-controller-manager日志出现-unable-to-retrieve-the-complete-list-of-server-APIs-metrics-k8s-io-x2F-v1beta1\" class=\"headerlink\" title=\"kube-controller-manager日志出现: unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1\"\u003e\u003c/a\u003ekube-controller-manager日志出现: unable to retrieve the complete list of server APIs: metrics.k8s.io/v1beta1\u003c/h3\u003e\u003cp\u003e原因: apiservice存在Failed的service\u003c/p\u003e\n\u003cp\u003e解决:\u003c/p\u003e\n\u003cfigure class=\"highlight bash\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e2\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e3\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003ekubectl get apiservice\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"comment\"\u003e# 查看是否存在ServiceNotFound, 删除对应的service即可\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003ekubectl delete apiservice {name}\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003ch3 id=\"kubelet启动时提示Failed-to-start-ContainerManager-failed-to-build-map-of-initial-containers-from-runtime-no-PodsandBox-found-with-Id\"\u003e\u003ca href=\"#kubelet启动时提示Failed-to-start-ContainerManager-failed-to-build-map-of-initial-containers-from-runtime-no-PodsandBox-found-with-Id\" class=\"headerlink\" title=\"kubelet启动时提示Failed to start ContainerManager failed to build map of initial containers from runtime: no PodsandBox found with Id\"\u003e\u003c/a\u003ekubelet启动时提示Failed to start ContainerManager failed to build map of initial containers from runtime: no PodsandBox found with Id\u003c/h3\u003e\u003cp\u003e原因: kubelet的数据目录存在脏容器数据\u003c/p\u003e\n\u003cp\u003e解决: 使用以下命令找到脏容器，删除后重启kubelet\u003c/p\u003e\n\u003cfigure class=\"highlight bash\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e2\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e3\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e4\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e5\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e6\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e7\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e8\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"comment\"\u003e# 894f35dca3eda57adef28b69acd0607efdeb34e8814e87e196bc163305576028 是上面报错中的ID\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003edocker ps -a --filter \u003cspan class=\"string\"\u003e\u0026#34;label=io.kubernetes.sandbox.id=894f35dca3eda57adef28b69acd0607efdeb34e8814e87e196bc163305576028\u0026#34;\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"comment\"\u003e# 根据上述ID删除容器\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003edocker \u003cspan class=\"built_in\"\u003erm\u003c/span\u003e ID\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"comment\"\u003e# 重启kubelet\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003esystemctl restart kubelet\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003ch3 id=\"prometheus-logs-compaction-failed-对数的性质uption-in-segment-xxxx-at-yyyy-unexpected-non-zero-byte-in-padded-page\"\u003e\u003ca href=\"#prometheus-logs-compaction-failed-对数的性质uption-in-segment-xxxx-at-yyyy-unexpected-non-zero-byte-in-padded-page\" class=\"headerlink\" title=\"prometheus logs: compaction failed, 对数的性质uption in segment xxxx at yyyy: unexpected non-zero byte in padded page\"\u003e\u003c/a\u003eprometheus logs: compaction failed, 对数的性质uption in segment xxxx at yyyy: unexpected non-zero byte in padded page\u003c/h3\u003e\u003cp\u003e原因: prometheus在对wal进行压缩时出现segment错误，导致创建checkout失败\u003c/p\u003e\n\u003cp\u003e解决: 在prometheus持久化目录下删除上述xxxx的目录，然后重启prometheus\u003c/p\u003e\n\u003cp\u003e重启后可能会出现unexpected gap to last checkpoint, expect: xxx, requested: yyy\u003c/p\u003e\n\u003cp\u003e需要将checkout目录也进行删除，然后重启prometheus实例 \u003c/p\u003e\n\u003ch3 id=\"is-forbidden-User-xxx-cannot-get-resource-“services-x2F-proxy”\"\u003e\u003ca href=\"#is-forbidden-User-xxx-cannot-get-resource-“services-x2F-proxy”\" class=\"headerlink\" title=\"is forbidden: User xxx cannot get resource “services/proxy”\"\u003e\u003c/a\u003eis forbidden: User xxx cannot get resource “services/proxy”\u003c/h3\u003e\u003cp\u003e在rancher中使用非admin用户无法显示grafana的 workload metrics, 请求中提示: services http:rancher-monitoring-grafana:80 is forbidden: User xxx cannot get resource “services/proxy” in API group “” in the namespace cattle-monitoring-system\u003c/p\u003e\n\u003cp\u003e原因: 需要为该用户在cattle-monitoring-system ns中授权 services/proxy权限, 同时该用户所对应的角色需要继承 project-monitoring-view角色，这样非admin用户才能看到metrics菜单\u003c/p\u003e\n\u003cp\u003e参考: \u003ca target=\"_blank\" rel=\"noopener\" href=\"https://forums.rancher.com/t/cluster-member-cant-see-use-grafana-or-monitoring-stuff/15814\"\u003ehttps://forums.rancher.com/t/cluster-member-cant-see-use-grafana-or-monitoring-stuff/15814\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"pod状态提示UnexpectedAdmissionError\"\u003e\u003ca href=\"#pod状态提示UnexpectedAdmissionError\" class=\"headerlink\" title=\"pod状态提示UnexpectedAdmissionError\"\u003e\u003c/a\u003epod状态提示UnexpectedAdmissionError\u003c/h3\u003e\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/20220125180525.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e原因: 在排查的过程中，发现这个问题涉及的东西太多，写了篇专门的文章来说明这个问题，可参考\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://izsk.me/2022/01/27/Kubernetes-pod-status-is-UnexpectedAdmissionError/\"\u003eKubernetes-pod-status-is-UnexpectedAdmissionError\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"nvidia-device-plugin-提示bind-address-already-in-use\"\u003e\u003ca href=\"#nvidia-device-plugin-提示bind-address-already-in-use\" class=\"headerlink\" title=\"nvidia-device-plugin 提示bind: address already in use\"\u003e\u003c/a\u003envidia-device-plugin 提示bind: address already in use\u003c/h3\u003e\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/20220106144313.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e原因: 这个错误提示其实是有歧义的, 一般看到\u003ccode\u003ebind: address already in use\u003c/code\u003e都会认为是不是地址端口被占用了, 在这里其实不是，正常来讲\u003ccode\u003envidia-device-plugin\u003c/code\u003e在正常退出后会将节点上的\u003ccode\u003envidia.sock\u003c/code\u003e文件一起删除，启动时会自动创建该文件, 但如果出现退出后\u003ccode\u003envidia.sock\u003c/code\u003e文件还存在，这个时候启动\u003ccode\u003envidia-device-plugin\u003c/code\u003e就会提示上述报错\u003c/p\u003e\n\u003cp\u003e解决: 手动删除节点上的\u003ccode\u003envidia.sock\u003c/code\u003e,然后重启\u003ccode\u003envidia-device-plugin\u003c/code\u003e即可\u003c/p\u003e\n\u003ch3 id=\"prometheus提示-x2F-metrics-x2F-resource-x2F-v1alpha1-404\"\u003e\u003ca href=\"#prometheus提示-x2F-metrics-x2F-resource-x2F-v1alpha1-404\" class=\"headerlink\" title=\"prometheus提示 /metrics/resource/v1alpha1 404\"\u003e\u003c/a\u003eprometheus提示 /metrics/resource/v1alpha1 404\u003c/h3\u003e\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/20211118152201.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e原因: 这是因为[/metrics/resource/v1alpha1]是在v1.14中才新增的特性，而当前kubelet版本为1.13\u003c/p\u003e\n\u003cp\u003e解决: 升级k8s的版本，这里要注意的是\u003cstrong\u003ekubelet的版本不能为api-server的高，所以不能只升级kubelet.\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"Error-from-server-Forbidden-pods-“xxx”-is-forbidden-cannot-exec-into-or-attach-to-a-privileged-container\"\u003e\u003ca href=\"#Error-from-server-Forbidden-pods-“xxx”-is-forbidden-cannot-exec-into-or-attach-to-a-privileged-container\" class=\"headerlink\" title=\"Error from server (Forbidden): pods “xxx” is forbidden: cannot exec into or attach to a privileged container\"\u003e\u003c/a\u003eError from server (Forbidden): pods “xxx” is forbidden: cannot exec into or attach to a privileged container\u003c/h3\u003e\u003cp\u003e原因: 排查两个方面，是否有psp，第二个是否启用了相关的admission\u003c/p\u003e\n\u003cp\u003e解决: 在本case中，因安全因素，开启了DenyEscalatingExec 这个admission，从api-server的配置–enable-admission-plugins中上去掉DenyEscalatingExec 即可\u003c/p\u003e\n\u003cp\u003e参考: \u003ca target=\"_blank\" rel=\"noopener\" href=\"https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/\"\u003ehttps://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"kubeadm-join提示unable-to-fetch-the-kubeadm-config-ConfigMap\"\u003e\u003ca href=\"#kubeadm-join提示unable-to-fetch-the-kubeadm-config-ConfigMap\" class=\"headerlink\" title=\"kubeadm join提示unable to fetch the kubeadm-config ConfigMap\"\u003e\u003c/a\u003ekubeadm join提示unable to fetch the kubeadm-config ConfigMap\u003c/h3\u003e\u003cfigure class=\"highlight yaml\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e2\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e3\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e4\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e[\u003cspan class=\"string\"\u003ediscovery\u003c/span\u003e] \u003cspan class=\"string\"\u003eSuccessfully\u003c/span\u003e \u003cspan class=\"string\"\u003eestablished\u003c/span\u003e \u003cspan class=\"string\"\u003econnection\u003c/span\u003e \u003cspan class=\"string\"\u003ewith\u003c/span\u003e \u003cspan class=\"string\"\u003eAPI\u003c/span\u003e \u003cspan class=\"string\"\u003eServer\u003c/span\u003e \u003cspan class=\"string\"\u003e\u0026#34;xxx.xxx.xxx.xxx:16443\u0026#34;\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e[\u003cspan class=\"string\"\u003ejoin\u003c/span\u003e] \u003cspan class=\"string\"\u003eReading\u003c/span\u003e \u003cspan class=\"string\"\u003econfiguration\u003c/span\u003e \u003cspan class=\"string\"\u003efrom\u003c/span\u003e \u003cspan class=\"string\"\u003ethe\u003c/span\u003e \u003cspan class=\"string\"\u003ecluster...\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e[\u003cspan class=\"string\"\u003ejoin\u003c/span\u003e] \u003cspan class=\"attr\"\u003eFYI:\u003c/span\u003e \u003cspan class=\"string\"\u003eYou\u003c/span\u003e \u003cspan class=\"string\"\u003ecan\u003c/span\u003e \u003cspan class=\"string\"\u003elook\u003c/span\u003e \u003cspan class=\"string\"\u003eat\u003c/span\u003e \u003cspan class=\"string\"\u003ethis\u003c/span\u003e \u003cspan class=\"string\"\u003econfig\u003c/span\u003e \u003cspan class=\"string\"\u003efile\u003c/span\u003e \u003cspan class=\"string\"\u003ewith\u003c/span\u003e \u003cspan class=\"string\"\u003e\u0026#39;kubectl -n kube-system get cm kubeadm-config -oyaml\u0026#39;\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"attr\"\u003eunable to fetch the kubeadm-config ConfigMap: failed to get config map: Get https://127.0.0.1:16443/api/v1/namespaces/kube-system/configmaps/kubeadm-config: dial tcp 127.0.0.1:16443: connect:\u003c/span\u003e \u003cspan class=\"string\"\u003econnection\u003c/span\u003e \u003cspan class=\"string\"\u003erefused\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003cp\u003e原因: 127.0.0.1:16443是apiserver的VIP,从报错信息来看, 对127.0.0.1:16443的访问被拒绝了, 但是在apiserver本地curl这个地址又是没问题的，还是非常诡异，可以通过以下方式解决了\u003c/p\u003e\n\u003cp\u003e解决: 请确认好kubeadm join时会访问的两个配置文件中的apiserver地址是否正确\u003c/p\u003e\n\u003cfigure class=\"highlight bash\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e2\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e3\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e4\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e5\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003ekubectl -n kube-system get cm kubeadm-config -oyaml\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"comment\"\u003e# 其中的controlPlaneEndpoint地址\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003ekubectl edit cm cluster-info -oyaml -n kube-public\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"comment\"\u003e# 其中的server地址\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003cp\u003e参考: \u003ca target=\"_blank\" rel=\"noopener\" href=\"https://github.com/kubernetes/kubeadm/issues/1596\"\u003ehttps://github.com/kubernetes/kubeadm/issues/1596\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"CRD-spec-versions-Invalid-value\"\u003e\u003ca href=\"#CRD-spec-versions-Invalid-value\" class=\"headerlink\" title=\"CRD spec.versions: Invalid value\"\u003e\u003c/a\u003eCRD spec.versions: Invalid value\u003c/h3\u003e\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/20210622102036.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e原因: CRD yaml文件中apiVersion与versions中的版本不对应\u003c/p\u003e\n\u003cp\u003e参考: \u003ca target=\"_blank\" rel=\"noopener\" href=\"https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/\"\u003ehttps://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"删除namespaces时Terminating，无法强制删除且无法在该ns下创建对象\"\u003e\u003ca href=\"#删除namespaces时Terminating，无法强制删除且无法在该ns下创建对象\" class=\"headerlink\" title=\"删除namespaces时Terminating，无法强制删除且无法在该ns下创建对象\"\u003e\u003c/a\u003e删除namespaces时Terminating，无法强制删除且无法在该ns下创建对象\u003c/h3\u003e\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/20210428190009.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e原因: ns处于terminating时hang住了，使用\u003ccode\u003e--grace-period=0 -- force\u003c/code\u003e强制删除也无效\u003c/p\u003e\n\u003cp\u003e解决:\u003c/p\u003e\n\u003cfigure class=\"highlight bash\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e2\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e3\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e4\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e5\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e6\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e7\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e8\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e9\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"comment\"\u003e# 保存现在的ns json\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003ekubectl get ns xxxx -o json \u0026gt; /tmp/temp.json\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"comment\"\u003e# 编辑temp.json，将其中的spec.finalizer字段删除保存\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"comment\"\u003e# 导出k8s访问密钥\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"built_in\"\u003eecho\u003c/span\u003e $(kubectl config view --raw -oyaml | grep client-cert  |\u003cspan class=\"built_in\"\u003ecut\u003c/span\u003e -d \u003cspan class=\"string\"\u003e\u0026#39; \u0026#39;\u003c/span\u003e -f 6) |\u003cspan class=\"built_in\"\u003ebase64\u003c/span\u003e -d \u0026gt; /tmp/client.pem\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"built_in\"\u003eecho\u003c/span\u003e $(kubectl config view --raw -oyaml | grep client-key-data  |\u003cspan class=\"built_in\"\u003ecut\u003c/span\u003e -d \u003cspan class=\"string\"\u003e\u0026#39; \u0026#39;\u003c/span\u003e -f 6 ) |\u003cspan class=\"built_in\"\u003ebase64\u003c/span\u003e -d \u0026gt; /tmp/client-key.pem\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"built_in\"\u003eecho\u003c/span\u003e $(kubectl config view --raw -oyaml | grep certificate-authority-data  |\u003cspan class=\"built_in\"\u003ecut\u003c/span\u003e -d \u003cspan class=\"string\"\u003e\u0026#39; \u0026#39;\u003c/span\u003e -f 6  ) |\u003cspan class=\"built_in\"\u003ebase64\u003c/span\u003e -d \u0026gt; /tmp/ca.pem\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"comment\"\u003e# 解决namespace Terminating，根据实际情况修改\u0026lt;namespaces\u0026gt;\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003ecurl --cert /tmp/client.pem --key /tmp/client-key.pem --cacert /tmp/ca.pem -H \u003cspan class=\"string\"\u003e\u0026#34;Content-Type: application/json\u0026#34;\u003c/span\u003e -X PUT --data-binary @/tmp/temp.json https://xxx.xxx.xxx.xxx:6443/api/v1/namespaces/\u0026lt;namespaces\u0026gt;/finalize\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003ch3 id=\"docker-启动时提示no-sockets-found-via-socket-activation\"\u003e\u003ca href=\"#docker-启动时提示no-sockets-found-via-socket-activation\" class=\"headerlink\" title=\"docker 启动时提示no sockets found via socket activation\"\u003e\u003c/a\u003edocker 启动时提示no sockets found via socket activation\u003c/h3\u003e\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/20210307212429.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e解决: 在start docker前先执行\u003ccode\u003esystemctl unmask docker.socket\u003c/code\u003e即可\u003c/p\u003e\n\u003ch3 id=\"Prometheus-opening-storage-failed-invalid-block-sequence\"\u003e\u003ca href=\"#Prometheus-opening-storage-failed-invalid-block-sequence\" class=\"headerlink\" title=\"Prometheus opening storage failed: invalid block sequence\"\u003e\u003c/a\u003ePrometheus opening storage failed: invalid block sequence\u003c/h3\u003e\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/image-20210302200255132.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e原因: 这个需要排查prometheus持久化目录中是否存在时间超出设置阈值的时间段的文件，删掉后重启即可\u003c/p\u003e\n\u003ch3 id=\"kubelet提示-The-node-was-low-on-resource-ephemeral-storage\"\u003e\u003ca href=\"#kubelet提示-The-node-was-low-on-resource-ephemeral-storage\" class=\"headerlink\" title=\"kubelet提示: The node was low on resource: ephemeral-storage\"\u003e\u003c/a\u003ekubelet提示: The node was low on resource: ephemeral-storage\u003c/h3\u003e\u003cp\u003e原因: 节点上kubelet的配置路径超过阈值会触发驱逐，默认情况下阈值是85%\u003c/p\u003e\n\u003cp\u003e解决: 或者清理磁盘释放资源，或者通过可修改kubelet的配置参数\u003ccode\u003eimagefs.available\u003c/code\u003e来提高阈值,然后重启kubelet.\u003c/p\u003e\n\u003cp\u003e参考: \u003ca target=\"_blank\" rel=\"noopener\" href=\"https://cloud.tencent.com/developer/article/1456389\"\u003ehttps://cloud.tencent.com/developer/article/1456389\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"kubectl查看日志时提示-Error-from-server-Get-https-xxx-10250-containerLogs-spring-prod-xxx-0-xxx-dial-tcp-xxx-10250-i-x2F-o-timeout\"\u003e\u003ca href=\"#kubectl查看日志时提示-Error-from-server-Get-https-xxx-10250-containerLogs-spring-prod-xxx-0-xxx-dial-tcp-xxx-10250-i-x2F-o-timeout\" class=\"headerlink\" title=\"kubectl查看日志时提示: Error from server: Get https://xxx:10250/containerLogs/spring-prod/xxx-0/xxx: dial tcp xxx:10250: i/o timeout\"\u003e\u003c/a\u003ekubectl查看日志时提示: Error from server: Get \u003ca target=\"_blank\" rel=\"noopener\" href=\"https://xxx:10250/containerLogs/spring-prod/xxx-0/xxx\"\u003ehttps://xxx:10250/containerLogs/spring-prod/xxx-0/xxx\u003c/a\u003e: dial tcp xxx:10250: i/o timeout\u003c/h3\u003e\u003cp\u003e原因: 目地机器的iptables对10250这个端口进行了drop，如下图\u003c/p\u003e\n\u003cfigure class=\"highlight bash\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003eiptables-save -L INPUT –-line-numbers\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/20210106185555.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e解决: 删除对应的规则 \u003c/p\u003e\n\u003cfigure class=\"highlight bash\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003eiptables -D INPUT 10\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003ch3 id=\"Service解析提示-Temporary-failure-in-name-resolution\"\u003e\u003ca href=\"#Service解析提示-Temporary-failure-in-name-resolution\" class=\"headerlink\" title=\"Service解析提示 Temporary failure in name resolution\"\u003e\u003c/a\u003eService解析提示 Temporary failure in name resolution\u003c/h3\u003e\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/20201223232538.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e原因: 出现这种情况很奇怪，现象显示就是域名无法解析，全格式的域名能够解析是因为在pod的/etc/hosts中有全域名的记录,那么问题就出在于corddns解析上，coredns从日志来看，没有任何报错，但是从pod的状态来看，虽然处于Running状态，但是0/1可以看出coredns并未处于ready状态.\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/20201223233150.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e可以查看ep记录，会发现endpoint那一栏是空的，这也就证实了k8s把coredns的状态分为了notready状态，所以ep才没有记录，经过与其它环境比较后发现跟配置有关，最终定位在coredns的配置文件上,在插件上需要加上ready\u003c/p\u003e\n\u003cp\u003e解决: 在cm的配置上添加read插件，如下图\u003c/p\u003e\n\u003cfigure class=\"highlight bash\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e2\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e3\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e4\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e5\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e6\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e7\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e8\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e9\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e10\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e11\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e12\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e13\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"comment\"\u003e# ... 省略\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003edata:\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  Corefile: |\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e    .:53 {\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        errors\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        health\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        ready  \u003cspan class=\"comment\"\u003e# 加上该行后问题解决\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        kubernetes cluster.local in-addr.arpa ip6.arpa {\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e          pods insecure\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e          upstream /etc/resolv.conf\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e          fallthrough in-addr.arpa ip6.arpa\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        }\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e       \u003cspan class=\"comment\"\u003e# ... 省略\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003cp\u003e关于coredns的ready插件的使用,可以参考\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://coredns.io/plugins/ready/\"\u003e这里\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e总结起来就是使用ready来表明当前已准备好可以接收请求，从codedns的yaml文件也可以看到有\u003ccode\u003elivenessProbe\u003c/code\u003e\u003c/p\u003e\n\u003ch3 id=\"使用Kubectl命令行时提示-Unable-to-connect-to-the-server-x509-certificate-relies-on-legacy-Common-Name-field-use-SANs-or-temporarily-enable-Common-Name-matching-with-GODEBUG-x3D-x509ignoreCN-x3D-0\"\u003e\u003ca href=\"#使用Kubectl命令行时提示-Unable-to-connect-to-the-server-x509-certificate-relies-on-legacy-Common-Name-field-use-SANs-or-temporarily-enable-Common-Name-matching-with-GODEBUG-x3D-x509ignoreCN-x3D-0\" class=\"headerlink\" title=\"使用Kubectl命令行时提示: Unable to connect to the server: x509: certificate relies on legacy Common Name field, use SANs or temporarily enable Common Name matching with GODEBUG=x509ignoreCN=0\"\u003e\u003c/a\u003e使用Kubectl命令行时提示: Unable to connect to the server: x509: certificate relies on legacy Common Name field, use SANs or temporarily enable Common Name matching with GODEBUG=x509ignoreCN=0\u003c/h3\u003e\u003cp\u003e原因: 这个跟本地的go环境有关\u003c/p\u003e\n\u003cp\u003e解决: 在使用kubectl前使用命令\u003ccode\u003eexport GODEBUG=x509ignoreCN=0\u003c/code\u003e即可\u003c/p\u003e\n\u003ch3 id=\"namespaces-quot-kube-system-quot-is-forbidden-this-namespace-may-not-be-deleted\"\u003e\u003ca href=\"#namespaces-quot-kube-system-quot-is-forbidden-this-namespace-may-not-be-deleted\" class=\"headerlink\" title=\"namespaces \u0026#34;kube-system\u0026#34; is forbidden: this namespace may not be deleted\"\u003e\u003c/a\u003enamespaces \u0026#34;kube-system\u0026#34; is forbidden: this namespace may not be deleted\u003c/h3\u003e\u003cp\u003e原因: kube-system是集群中受保护的ns, 被禁止删除，主要是防止误操作，如果需要删除的话，可以使用–force\u003c/p\u003e\n\u003cp\u003e参考: \u003ca target=\"_blank\" rel=\"noopener\" href=\"https://github.com/kubernetes/kubernetes/pull/62167/files\"\u003ehttps://github.com/kubernetes/kubernetes/pull/62167/files\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"unknown-field-volumeClaimTemplates\"\u003e\u003ca href=\"#unknown-field-volumeClaimTemplates\" class=\"headerlink\" title=\"unknown field volumeClaimTemplates\"\u003e\u003c/a\u003eunknown field volumeClaimTemplates\u003c/h3\u003e\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/20201112171302.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e原因: 提示这个错误的原因是资源对象是deployment, 而deployment本就是无状态的， 所以也就没有使用pv这一说法了，可以参考api\u003c/p\u003e\n\u003cp\u003e参考: \u003ca target=\"_blank\" rel=\"noopener\" href=\"https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#deploymentspec-v1-apps\"\u003edeploymentspec-v1-apps\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"CoreDNS提示Loop-127-0-0-1-38827-gt-53-detected-for-zone-“-”\"\u003e\u003ca href=\"#CoreDNS提示Loop-127-0-0-1-38827-gt-53-detected-for-zone-“-”\" class=\"headerlink\" title=\"CoreDNS提示Loop (127.0.0.1:38827 -\u0026gt; :53) detected for zone “.”\"\u003e\u003c/a\u003eCoreDNS提示Loop (127.0.0.1:38827 -\u0026gt; :53) detected for zone “.”\u003c/h3\u003e\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/20201017221807.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e原因: CoreDNS所在的宿主机上\u003ccode\u003e/etc/resolv.conf\u003c/code\u003e中存在有127.0.xx的nameserver,这样会造成解析死循环.\u003c/p\u003e\n\u003cp\u003e解决: 修改宿主机\u003ccode\u003e/etc/resolv.conf\u003c/code\u003e或者将CoreDNS的configmap中的forward修改为一个可用的地址, 如\u003ccode\u003e8.8.8.8\u003c/code\u003e\u003c/p\u003e\n\u003ch3 id=\"hostPath-volumes-are-not-allowed-to-be-used\"\u003e\u003ca href=\"#hostPath-volumes-are-not-allowed-to-be-used\" class=\"headerlink\" title=\"hostPath volumes are not allowed to be used\"\u003e\u003c/a\u003ehostPath volumes are not allowed to be used\u003c/h3\u003e\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/20200909154834.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e原因: 集群中存在psp禁止pod直接挂载hostpath.\u003c/p\u003e\n\u003cp\u003e解决: 通过添加以下的psp规则来允许或者删除存在的psp都可\u003c/p\u003e\n\u003cfigure class=\"highlight yaml\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e2\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e3\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e4\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e5\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e6\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e7\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e8\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e9\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e10\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e11\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e12\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e13\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e14\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e15\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e16\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e17\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e18\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e19\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e20\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e21\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e22\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e23\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e24\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e25\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e26\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e27\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e28\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e29\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e30\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e31\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e32\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e33\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e34\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e35\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e36\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e37\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e38\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"attr\"\u003eapiVersion:\u003c/span\u003e \u003cspan class=\"string\"\u003eextensions/v1beta1\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"attr\"\u003ekind:\u003c/span\u003e \u003cspan class=\"string\"\u003ePodSecurityPolicy\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"attr\"\u003emetadata:\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  \u003cspan class=\"attr\"\u003ename:\u003c/span\u003e \u003cspan class=\"string\"\u003eauth-privilege-psp\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"attr\"\u003espec:\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  \u003cspan class=\"attr\"\u003eallowPrivilegeEscalation:\u003c/span\u003e \u003cspan class=\"literal\"\u003etrue\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  \u003cspan class=\"attr\"\u003eallowedHostPaths:\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  \u003cspan class=\"bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"attr\"\u003epathPrefix:\u003c/span\u003e \u003cspan class=\"string\"\u003e/\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  \u003cspan class=\"attr\"\u003efsGroup:\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e    \u003cspan class=\"attr\"\u003eranges:\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e    \u003cspan class=\"bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"attr\"\u003emax:\u003c/span\u003e \u003cspan class=\"number\"\u003e65535\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e      \u003cspan class=\"attr\"\u003emin:\u003c/span\u003e \u003cspan class=\"number\"\u003e1\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e    \u003cspan class=\"attr\"\u003erule:\u003c/span\u003e \u003cspan class=\"string\"\u003eRunAsAny\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  \u003cspan class=\"attr\"\u003ehostNetwork:\u003c/span\u003e \u003cspan class=\"literal\"\u003etrue\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  \u003cspan class=\"attr\"\u003ehostPID:\u003c/span\u003e \u003cspan class=\"literal\"\u003etrue\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  \u003cspan class=\"attr\"\u003ehostPorts:\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  \u003cspan class=\"bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"attr\"\u003emax:\u003c/span\u003e \u003cspan class=\"number\"\u003e9796\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e    \u003cspan class=\"attr\"\u003emin:\u003c/span\u003e \u003cspan class=\"number\"\u003e9796\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  \u003cspan class=\"attr\"\u003eprivileged:\u003c/span\u003e \u003cspan class=\"literal\"\u003etrue\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  \u003cspan class=\"attr\"\u003erequiredDropCapabilities:\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  \u003cspan class=\"bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"string\"\u003eALL\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  \u003cspan class=\"attr\"\u003erunAsUser:\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e    \u003cspan class=\"attr\"\u003erule:\u003c/span\u003e \u003cspan class=\"string\"\u003eRunAsAny\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  \u003cspan class=\"attr\"\u003eseLinux:\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e    \u003cspan class=\"attr\"\u003erule:\u003c/span\u003e \u003cspan class=\"string\"\u003eRunAsAny\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  \u003cspan class=\"attr\"\u003esupplementalGroups:\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e    \u003cspan class=\"attr\"\u003eranges:\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e    \u003cspan class=\"bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"attr\"\u003emax:\u003c/span\u003e \u003cspan class=\"number\"\u003e65535\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e      \u003cspan class=\"attr\"\u003emin:\u003c/span\u003e \u003cspan class=\"number\"\u003e1\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e    \u003cspan class=\"attr\"\u003erule:\u003c/span\u003e \u003cspan class=\"string\"\u003eRunAsAny\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  \u003cspan class=\"attr\"\u003evolumes:\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  \u003cspan class=\"bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"string\"\u003econfigMap\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  \u003cspan class=\"bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"string\"\u003eemptyDir\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  \u003cspan class=\"bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"string\"\u003eprojected\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  \u003cspan class=\"bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"string\"\u003esecret\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  \u003cspan class=\"bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"string\"\u003edownwardAPI\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  \u003cspan class=\"bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"string\"\u003epersistentVolumeClaim\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  \u003cspan class=\"bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"string\"\u003ehostPath\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003ch3 id=\"container-has-runAsNonRoot-and-image-has-non-numeric-user-grafana-cannot-verify-user-is-non-root\"\u003e\u003ca href=\"#container-has-runAsNonRoot-and-image-has-non-numeric-user-grafana-cannot-verify-user-is-non-root\" class=\"headerlink\" title=\"container has runAsNonRoot and image has non-numeric user (grafana), cannot verify user is non-root\"\u003e\u003c/a\u003econtainer has runAsNonRoot and image has non-numeric user (grafana), cannot verify user is non-root\u003c/h3\u003e\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/20200908211841.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e原因: 这是由于在deploy中设置了\u003ccode\u003esecurityContext: runAsNonRoot: true\u003c/code\u003e, 在这种情况下，当pod启动时，使用的默认用户,比如上面的grafana，k8s无法确定他是不是root用户\u003c/p\u003e\n\u003cp\u003e解决: 指定\u003ccode\u003esecurityContext:runAsUser: 1000\u003c/code\u003e, 随便一个id号即可, 只要不是0(0代表root)\u003c/p\u003e\n\u003cp\u003e参考: \u003ca target=\"_blank\" rel=\"noopener\" href=\"https://stackoverflow.com/questions/51544003/using-runasnonroot-in-kubernetes\"\u003ehttps://stackoverflow.com/questions/51544003/using-runasnonroot-in-kubernetes\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"OCI-runtime-create-failed-no-such-file-or-directory\"\u003e\u003ca href=\"#OCI-runtime-create-failed-no-such-file-or-directory\" class=\"headerlink\" title=\"OCI runtime create failed: no such file or directory\"\u003e\u003c/a\u003eOCI runtime create failed: no such file or directory\u003c/h3\u003e\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/20200902132758.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e原因: /var/lib/kubelet/pod下的数据目录已经损坏.\u003c/p\u003e\n\u003cp\u003e解决: 删除对应的目录即可\u003c/p\u003e\n\u003ch3 id=\"镜像拉取时出现ImageInspectError\"\u003e\u003ca href=\"#镜像拉取时出现ImageInspectError\" class=\"headerlink\" title=\"镜像拉取时出现ImageInspectError\"\u003e\u003c/a\u003e镜像拉取时出现ImageInspectError\u003c/h3\u003e\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/20200902123531.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e原因: 这种情况下一般都是镜像损坏了\u003c/p\u003e\n\u003cp\u003e解决: 把相关的镜像删除后重新拉取\u003c/p\u003e\n\u003ch3 id=\"kubelet日志提示-node-not-found\"\u003e\u003ca href=\"#kubelet日志提示-node-not-found\" class=\"headerlink\" title=\"kubelet日志提示: node not found\"\u003e\u003c/a\u003ekubelet日志提示: node not found\u003c/h3\u003e\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/20200901183122.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e原因: 这个报错只是中间过程，真正的原因在于apiserver没有启动成功，导致会一直出现这个错误\u003c/p\u003e\n\u003cp\u003e解决: 排查kubelet与apiserver的连通是否正常\u003c/p\u003e\n\u003ch3 id=\"OCI-runtime-create-failed-executable-file-not-found-in-PATH\"\u003e\u003ca href=\"#OCI-runtime-create-failed-executable-file-not-found-in-PATH\" class=\"headerlink\" title=\"OCI runtime create failed: executable file not found in PATH\"\u003e\u003c/a\u003eOCI runtime create failed: executable file not found in PATH\u003c/h3\u003e\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/20200902101139.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e原因: 在path中没有nvidia-container-runtime-hook这个二进制文件，可能跟本人删除nvidia显卡驱动有关.\u003c/p\u003e\n\u003cp\u003e解决: nvidia-container-runtime-hook是docker nvidia的runtime文件，重新安装即可.\u003c/p\u003e\n\u003ch3 id=\"Nginx-Ingress-Empty-address\"\u003e\u003ca href=\"#Nginx-Ingress-Empty-address\" class=\"headerlink\" title=\"Nginx Ingress Empty address\"\u003e\u003c/a\u003eNginx Ingress Empty address\u003c/h3\u003e\u003cfigure class=\"highlight bash\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e2\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e3\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"comment\"\u003e# kubectl get ingress\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003eNAME         HOSTS                                       ADDRESS   PORTS   AGE\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003eprometheus   prometheus.1box.com                                   80      31d\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003cp\u003e会发现address中的ip是空的，而查看生产环境时却是有ip列表的.\u003c/p\u003e\n\u003cp\u003e原因: 这个其实不是一个错误，也不影响使用，原因在于测试环境中是不存在LoadBalance类型的svc, 如果需要address中显示ip的话需要做些额外的设置\u003c/p\u003e\n\u003cp\u003e解决: \u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e在nginx controller的容器中指定启动参数\u003ccode\u003e-report-ingress-status\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e在nginx controller引用的configmap中添加\u003ccode\u003eexternal-status-address: \u0026#34;10.164.15.220\u0026#34;\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e这样的话,在address中变会显示\u003ccode\u003e10.164.15.220\u003c/code\u003e了\u003c/p\u003e\n\u003cp\u003e参考:\u003c/p\u003e\n\u003cp\u003e\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://github.com/nginxinc/kubernetes-ingress/issues/587\"\u003ehttps://github.com/nginxinc/kubernetes-ingress/issues/587\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://docs.nginx.com/nginx-ingress-controller/configuration/global-configuration/reporting-resources-status/\"\u003ehttps://docs.nginx.com/nginx-ingress-controller/configuration/global-configuration/reporting-resources-status/\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"kubelet-but-volume-paths-are-still-present-on-disk\"\u003e\u003ca href=\"#kubelet-but-volume-paths-are-still-present-on-disk\" class=\"headerlink\" title=\"kubelet: but volume paths are still present on disk\"\u003e\u003c/a\u003ekubelet: but volume paths are still present on disk\u003c/h3\u003e\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/20200827183609.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e原因: 这种pod已经被删除了，但是volume还存在于disk中\u003c/p\u003e\n\u003cp\u003e解决: 删除对应的目录\u003ccode\u003e/var/lib/kubelet/pods/3cd73...\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e参考: \u003ca target=\"_blank\" rel=\"noopener\" href=\"https://github.com/longhorn/longhorn/issues/485\"\u003ehttps://github.com/longhorn/longhorn/issues/485\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"PLEG-is-not-healthy\"\u003e\u003ca href=\"#PLEG-is-not-healthy\" class=\"headerlink\" title=\"PLEG is not healthy\"\u003e\u003c/a\u003ePLEG is not healthy\u003c/h3\u003e\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/20200827184435.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e原因: 宿主机上面跑的容器太多，导致pod无法在3m钟内完成生命周期检查\u003c/p\u003e\n\u003cp\u003e解决:  PLEG(Pod Lifecycle Event Generator)用于kublet同步pod生命周期，本想着如果是因为时间短导致的超时，那是不是可以直接调整这个时间呢? 查看kubelet的源码发现不太行，3m时间是写在代码里的因此无法修改，当然修改再编译肯定没问题，但成本太大，所以只得优化容器的调度情况.\u003c/p\u003e\n\u003cp\u003e参考: \u003ca target=\"_blank\" rel=\"noopener\" href=\"https://developers.redhat.com/blog/2019/11/13/pod-lifecycle-event-generator-understanding-the-pleg-is-not-healthy-issue-in-kubernetes/\"\u003ehttps://developers.redhat.com/blog/2019/11/13/pod-lifecycle-event-generator-understanding-the-pleg-is-not-healthy-issue-in-kubernetes/\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"metrics-server-10255-connection-refused\"\u003e\u003ca href=\"#metrics-server-10255-connection-refused\" class=\"headerlink\" title=\"metrics-server: 10255 connection refused\"\u003e\u003c/a\u003emetrics-server: 10255 connection refused\u003c/h3\u003e\u003cfigure class=\"highlight yaml\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"attr\"\u003eunable to fully collect metrics:\u003c/span\u003e [\u003cspan class=\"attr\"\u003eunable to fully scrape metrics from source kubelet_summary:k8s-node-49:\u003c/span\u003e \u003cspan class=\"string\"\u003eunable\u003c/span\u003e \u003cspan class=\"string\"\u003eto\u003c/span\u003e \u003cspan class=\"string\"\u003efetch\u003c/span\u003e \u003cspan class=\"string\"\u003emetrics\u003c/span\u003e \u003cspan class=\"string\"\u003efrom\u003c/span\u003e \u003cspan class=\"string\"\u003eKubelet\u003c/span\u003e \u003cspan class=\"string\"\u003ek8s-node-49\u003c/span\u003e \u003cspan class=\"string\"\u003e(xxx.xxx.xxx.49):\u003c/span\u003e \u003cspan class=\"string\"\u003eGet\u003c/span\u003e \u003cspan class=\"string\"\u003ehttp://xxx.xxx.xxx.49:10255/stats/summary?only_cpu_and_memory=true:\u003c/span\u003e \u003cspan class=\"attr\"\u003edial tcp xxx.xxx.xxx.49:10255: connect:\u003c/span\u003e \u003cspan class=\"string\"\u003econnection\u003c/span\u003e \u003cspan class=\"string\"\u003erefused\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003cp\u003e原因: 现在的k8s都默认禁用了kubelet的10255端口，出现这个错误是因此在kubelet启动命令中启用了该端口\u003c/p\u003e\n\u003cp\u003e解决: 将\u003ccode\u003e- --kubelet-port=10255\u003c/code\u003e注释\u003c/p\u003e\n\u003ch3 id=\"metrics-server-no-such-host\"\u003e\u003ca href=\"#metrics-server-no-such-host\" class=\"headerlink\" title=\"metrics-server: no such host\"\u003e\u003c/a\u003emetrics-server: no such host\u003c/h3\u003e\u003cfigure class=\"highlight yaml\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"string\"\u003eunable\u003c/span\u003e \u003cspan class=\"string\"\u003eto\u003c/span\u003e \u003cspan class=\"string\"\u003efetch\u003c/span\u003e \u003cspan class=\"string\"\u003emetrics\u003c/span\u003e \u003cspan class=\"string\"\u003efrom\u003c/span\u003e \u003cspan class=\"string\"\u003eKubelet\u003c/span\u003e \u003cspan class=\"string\"\u003ek8s-node-234\u003c/span\u003e \u003cspan class=\"string\"\u003e(k8s-node-234):\u003c/span\u003e \u003cspan class=\"string\"\u003eGet\u003c/span\u003e \u003cspan class=\"string\"\u003ehttps://k8s-node-234:10250/stats/summary?only_cpu_and_memory=true:\u003c/span\u003e \u003cspan class=\"attr\"\u003edial tcp: lookup k8s-node-234 on 10.96.0.10:53:\u003c/span\u003e \u003cspan class=\"literal\"\u003eno\u003c/span\u003e \u003cspan class=\"string\"\u003esuch\u003c/span\u003e \u003cspan class=\"string\"\u003ehost\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003cp\u003e解决: 使用\u003ccode\u003ekubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP\u003c/code\u003e参数\u003c/p\u003e\n\u003cp\u003e参考: \u003ca target=\"_blank\" rel=\"noopener\" href=\"https://github.com/kubernetes-sigs/metrics-server/blob/master/README.md\"\u003ehttps://github.com/kubernetes-sigs/metrics-server/blob/master/README.md\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"pod无法解析域名\"\u003e\u003ca href=\"#pod无法解析域名\" class=\"headerlink\" title=\"pod无法解析域名\"\u003e\u003c/a\u003epod无法解析域名\u003c/h3\u003e\u003cp\u003e集群中新增了几台机器用于部署clickhouse用于做大数据分析，为了不让这类占用大量资源的Pod影响其它Pod，因此选择给机器打taint的形式控制该类Pod的调度, 创建Pod后发现这些Pod都会出现DNS解析异常, \u003c/p\u003e\n\u003cp\u003e原因； 要注意容器网络，比如这里使用的是flannel是否容忍了这些机器的taint，不然的话，flannel是无法被调度到这些机器的,因此容器间的通信会出现问题，\u003cstrong\u003e可以将类似flannel这些的基础POD容忍所有的NoScheule与NoExecute\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e解决: flannel的ds yaml中添加以下toleration，这样适用任何的场景\u003c/p\u003e\n\u003cfigure class=\"highlight yaml\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e2\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e3\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e4\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e5\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"attr\"\u003etolerations:\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"attr\"\u003eeffect:\u003c/span\u003e \u003cspan class=\"string\"\u003eNoSchedule\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  \u003cspan class=\"attr\"\u003eoperator:\u003c/span\u003e \u003cspan class=\"string\"\u003eExists\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"attr\"\u003eeffect:\u003c/span\u003e \u003cspan class=\"string\"\u003eNoExecute\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  \u003cspan class=\"attr\"\u003eoperator:\u003c/span\u003e \u003cspan class=\"string\"\u003eExists\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003ch3 id=\"Are-you-tring-to-mount-a-directory-on-to-a-file\"\u003e\u003ca href=\"#Are-you-tring-to-mount-a-directory-on-to-a-file\" class=\"headerlink\" title=\"Are you tring to mount a directory on to a file\"\u003e\u003c/a\u003eAre you tring to mount a directory on to a file\u003c/h3\u003e\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/20200430132115.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e原因:  Yaml文件中使用了subPath, 但是mountPath指向了一个目录\u003c/p\u003e\n\u003cp\u003e解决: mountPath需要加上文件名\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/20200430132148.png\"/\u003e\u003c/p\u003e\n\u003ch3 id=\"Kubernetes启动后提示slice-no-such-file-ro-directory\"\u003e\u003ca href=\"#Kubernetes启动后提示slice-no-such-file-ro-directory\" class=\"headerlink\" title=\"Kubernetes启动后提示slice: no such file ro directory\"\u003e\u003c/a\u003eKubernetes启动后提示slice: no such file ro directory\u003c/h3\u003e\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/41B2684F-312C-41ED-AF56-D6014C6B74E6.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e原因: yum安装的kubelet默认的是cgroupfs，而docker一般默认的是systemd。但是kubernetes安装的时候建议使用systemd, kubelet跟docker的不一致, 要么修改kubelet的启动参数 , 要么修改dokcer启动参数\u003c/p\u003e\n\u003cp\u003e解决: \u003c/p\u003e\n\u003cp\u003edocker的启动参数文件为: \u003ccode\u003e/etc/docker/daemon.json: \u0026#34;exec-opts\u0026#34;: [\u0026#34;native.cgroupdriver=systemd”]\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003ekubelet的启动参数文件为: \u003ccode\u003e/var/lib/kubelet/config.yaml:  cgroupDriver: systemd\u003c/code\u003e\u003c/p\u003e\n\u003ch3 id=\"“cni0”-already-has-an-IP-address-different-from-xxx-xxxx-xxx-xxx\"\u003e\u003ca href=\"#“cni0”-already-has-an-IP-address-different-from-xxx-xxxx-xxx-xxx\" class=\"headerlink\" title=\"“cni0” already has an IP address different from xxx.xxxx.xxx.xxx\"\u003e\u003c/a\u003e“cni0” already has an IP address different from xxx.xxxx.xxx.xxx\u003c/h3\u003e\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/20200430145913.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e原因: 使用kubeadm reset 重复操作过, reset之后，之前flannel创建的bridge device cni0和网口设备flannel.1依然健在\u003c/p\u003e\n\u003cp\u003e 解决: 添加之前需要清除下网络\u003c/p\u003e\n\u003cfigure class=\"highlight bash\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e2\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e3\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e4\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e5\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e6\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e7\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e8\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e9\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e10\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e11\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e12\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e13\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003ekubeadm reset\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003esystemctl stop kubelet\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003esystemctl stop docker\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"built_in\"\u003erm\u003c/span\u003e -rf /var/lib/cni/\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"built_in\"\u003erm\u003c/span\u003e -rf /var/lib/kubelet/*\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"built_in\"\u003erm\u003c/span\u003e -rf /etc/cni/\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003eifconfig cni0 down\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003eifconfig flannel.1 down\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003eifconfig docker0 down\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003eip \u003cspan class=\"built_in\"\u003elink\u003c/span\u003e delete cni0\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003eip \u003cspan class=\"built_in\"\u003elink\u003c/span\u003e delete flannel.1\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003esystemctl start docker\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003esystemctl start kubelet\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003ch3 id=\"kubeadm初始化时提示-CPU小于2\"\u003e\u003ca href=\"#kubeadm初始化时提示-CPU小于2\" class=\"headerlink\" title=\"kubeadm初始化时提示 CPU小于2\"\u003e\u003c/a\u003ekubeadm初始化时提示 CPU小于2\u003c/h3\u003e\u003cfigure class=\"highlight bash\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e2\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e3\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e4\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e[preflight] Running pre-flight checks\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003eerror execution phase preflight: [preflight] Some fatal errors occurred:\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e    [ERROR NumCPU]: the number of available CPUs 1 is less than the required 2\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e[preflight] If you know what you are doing, you can make a check non-fatal with `--ignore-preflight-errors=...`\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003cp\u003e原因: kubeadm对资源一定的要求，如果是测试环境无所谓的话,可忽略\u003c/p\u003e\n\u003cp\u003e解决:\u003c/p\u003e\n\u003cfigure class=\"highlight bash\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e使用 --ignore-preflight-errors 忽略\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003ch3 id=\"Unable-to-update-cni-config-no-network-found\"\u003e\u003ca href=\"#Unable-to-update-cni-config-no-network-found\" class=\"headerlink\" title=\"Unable to update cni config: no network found\"\u003e\u003c/a\u003eUnable to update cni config: no network found\u003c/h3\u003e\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/63611F8A-F803-46F5-8792-67111E03DF91.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e原因: 还未部署网络插件容器，导致在/etc/cni下还没有文件\u003c/p\u003e\n\u003cp\u003e解决: 根据实际情况部署网络插件\u003c/p\u003e\n\u003ch3 id=\"while-reading-‘google-dockercfg’-metadata\"\u003e\u003ca href=\"#while-reading-‘google-dockercfg’-metadata\" class=\"headerlink\" title=\"while reading ‘google-dockercfg’ metadata\"\u003e\u003c/a\u003ewhile reading ‘google-dockercfg’ metadata\u003c/h3\u003e\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/FB66ABBE-FF79-48A4-8A8B-7FDC3AED6634.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e原因: 从其它机器访问上述这些url确实出现 404\u003c/p\u003e\n\u003cp\u003e解决: 由于是在RKE上部署k8s, 所以可能会去访问google相关的url, 不影响业务,可以忽略\u003c/p\u003e\n\u003ch3 id=\"no-providers-available-to-validate-pod-request\"\u003e\u003ca href=\"#no-providers-available-to-validate-pod-request\" class=\"headerlink\" title=\"no providers available to validate pod request\"\u003e\u003c/a\u003eno providers available to validate pod request\u003c/h3\u003e\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/ACEB6BE6-7E22-4A43-AB4A-A51E00CE9EFE.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e原因: 在api-server的启动参数enable-admission中设置了PodSecrityPolicy, 但是集群中又没有任何的podsecritypolicy，因此导致整个集群都无法新建出pod\u003c/p\u003e\n\u003cp\u003e解决: 删除相应的podsecritypolicy即可\u003c/p\u003e\n\u003ch3 id=\"unable-to-upgrade-connection-Unauthorized\"\u003e\u003ca href=\"#unable-to-upgrade-connection-Unauthorized\" class=\"headerlink\" title=\"unable to upgrade connection: Unauthorized\"\u003e\u003c/a\u003eunable to upgrade connection: Unauthorized\u003c/h3\u003e\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/5FECFF57-F204-4ADF-A9E0-5F1D9A917194.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e原因: kubelet的启动参数少了x509认证方式\u003c/p\u003e\n\u003cp\u003e解决: 配置证书的路径, 加上重启kubelet即可\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/447313AF-7DD6-4FB4-8F46-AE1DC468C7CA.png\"/\u003e\u003c/p\u003e\n\u003ch3 id=\"kubectl-get-cs-提示-lt-unknown-gt\"\u003e\u003ca href=\"#kubectl-get-cs-提示-lt-unknown-gt\" class=\"headerlink\" title=\"kubectl get cs 提示\u0026lt;unknown\u0026gt;\"\u003e\u003c/a\u003ekubectl get cs 提示\u0026lt;unknown\u0026gt;\u003c/h3\u003e\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/C73EB07F-14CD-43A7-86C0-49B4812F57A6.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e原因: 这是个kubectl的bug, 跟版本相关，kubernetes有意废除get cs命令\u003c/p\u003e\n\u003cp\u003e解决: 目前对集群的运行无影响, 可通过加-oyaml 查看状态\u003c/p\u003e\n\u003ch3 id=\"安装kubeadm时提示Depends错误\"\u003e\u003ca href=\"#安装kubeadm时提示Depends错误\" class=\"headerlink\" title=\"安装kubeadm时提示Depends错误\"\u003e\u003c/a\u003e安装kubeadm时提示Depends错误\u003c/h3\u003e\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/2AE46262-2624-446C-909E-1FA0E76A8AD7.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e原因:  跟kubeadm没多大关系, 系统安装的有问题\u003c/p\u003e\n\u003cp\u003e解决: 执行以下命令修复\u003c/p\u003e\n\u003cfigure class=\"highlight bash\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e2\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003eapt --fix-broken install \u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003eapt-get update\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003ch3 id=\"访问service时提示Connection-refused\"\u003e\u003ca href=\"#访问service时提示Connection-refused\" class=\"headerlink\" title=\"访问service时提示Connection refused\"\u003e\u003c/a\u003e访问service时提示Connection refused\u003c/h3\u003e\u003cp\u003e现象: 从另一环境中把yaml文件导入到新环境后有些service访问不通\u003c/p\u003e\n\u003cfigure class=\"highlight bash\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e2\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e3\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003etelnet mongodb-mst.external 27017\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003eTrying 10.97.135.242...\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003etelnet: Unable to connect to remote host: Connection refused\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003cp\u003e首先排除了域名、端口的配置问题。\u003c/p\u003e\n\u003cp\u003e会发现提示连接拒绝.可以确定的是集群内的DNS是正常的.\u003c/p\u003e\n\u003cp\u003e那么就是通过clusterIP无法到达realserver. 查看iptables规则\u003c/p\u003e\n\u003cp\u003e发现提示\u003ccode\u003edefault has no endpoints --reject-with icmp-port-unreachable\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/20200506115705.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e很奇怪, 提示没有endpoints, 但是使用\u003ccode\u003ekubectl get ep\u003c/code\u003e又能看到ep存在且配置没有问题\u003c/p\u003e\n\u003cp\u003e而且这个default是怎么来的.\u003c/p\u003e\n\u003cp\u003e为了方便部署, 很多配置是从别的环境导出的配置, 有些service访问是没问题的, 只有少部分\u003ccode\u003econnection refused\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e结比一下发现一个很有趣的问题，先来看下不正常的yaml文件:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/20200506115805.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e由于服务在集群外部署的, 因此这里使用了subset方式, 开始怀疑问题在这里, 但是后来知道这个不是重点\u003c/p\u003e\n\u003cp\u003e乍一看这个配置没什么问题, 部署也很正常, 但是对比正常的yaml文件，发现一个区别：\u003c/p\u003e\n\u003cp\u003e如果在services中的端口指定了名字, 那么在subsets中的端口也要带名字, 没有带名字的就会出现\u003ccode\u003econnection refused\u003c/code\u003e，这个确实之前从来没有关注过, 一个端口的情况下也不会指定名字\u003c/p\u003e\n\u003cp\u003e而且这面iptalbes中提示的default刚好就是这里的port name,虽然不敢相信，但是也只能试一试这个方法: 在subsets中也加了port name\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/20200506120151.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e重新部署一个，再次查看iptalbes规则 \u003c/p\u003e\n\u003cp\u003e\u003ccode\u003eiptables-save|grep mongodb-mst\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/20200506120040.png\"/\u003e\u003c/p\u003e\n\u003cp\u003eOMG, 居然可行, 再看下telnet的结果:\u003c/p\u003e\n\u003cfigure class=\"highlight bash\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e2\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e3\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003eTrying 10.105.116.92...\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003eConnected to mongodb-mst.external.svc.cluster.local.\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003eEscape character is \u003cspan class=\"string\"\u003e\u0026#39;^]\u0026#39;\u003c/span\u003e.\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003cp\u003e访问也是没问题, 那么原因就在于:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e在service中指定了port name时, 也需要在ep中指定port name\u003c/strong\u003e\u003c/p\u003e\n\u003ch3 id=\"error-converting-fieldPath-field-label-not-supported\"\u003e\u003ca href=\"#error-converting-fieldPath-field-label-not-supported\" class=\"headerlink\" title=\"error converting fieldPath: field label not supported\"\u003e\u003c/a\u003eerror converting fieldPath: field label not supported\u003c/h3\u003e\u003cp\u003e今天遇到一个部署deployment出错的问题, yaml文件如下:\u003c/p\u003e\n\u003cfigure class=\"highlight yaml\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e2\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e3\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e4\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e5\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e6\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e7\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e8\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e9\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e10\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e11\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e12\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e13\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e14\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e15\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e16\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e17\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e18\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e19\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e20\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e21\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e22\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e23\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e24\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e25\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e26\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e27\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e28\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e29\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e30\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e31\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e32\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e33\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e34\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e35\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e36\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e37\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"attr\"\u003eapiVersion:\u003c/span\u003e \u003cspan class=\"string\"\u003eapps/v1\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"attr\"\u003ekind:\u003c/span\u003e \u003cspan class=\"string\"\u003eDeployment\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"attr\"\u003emetadata:\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  \u003cspan class=\"attr\"\u003ename:\u003c/span\u003e \u003cspan class=\"string\"\u003edemo-deployment\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  \u003cspan class=\"attr\"\u003enamespace:\u003c/span\u003e \u003cspan class=\"string\"\u003e4test\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  \u003cspan class=\"attr\"\u003elabels:\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e    \u003cspan class=\"attr\"\u003eapp:\u003c/span\u003e \u003cspan class=\"string\"\u003econfig-demo-app\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"attr\"\u003espec:\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  \u003cspan class=\"attr\"\u003ereplicas:\u003c/span\u003e \u003cspan class=\"number\"\u003e1\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  \u003cspan class=\"attr\"\u003eselector:\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e    \u003cspan class=\"attr\"\u003ematchLabels:\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e      \u003cspan class=\"attr\"\u003eapp:\u003c/span\u003e \u003cspan class=\"string\"\u003econfig-demo-app\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  \u003cspan class=\"attr\"\u003etemplate:\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e    \u003cspan class=\"attr\"\u003emetadata:\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e      \u003cspan class=\"attr\"\u003elabels:\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        \u003cspan class=\"attr\"\u003eapp:\u003c/span\u003e \u003cspan class=\"string\"\u003econfig-demo-app\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e      \u003cspan class=\"attr\"\u003eannotations:\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        \u003cspan class=\"comment\"\u003e# The field we\u0026#39;ll use to couple our ConfigMap and Deployment\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        \u003cspan class=\"attr\"\u003econfigHash:\u003c/span\u003e \u003cspan class=\"string\"\u003e4431f6d28fdf60c8140d28c42cde331a76269ac7a0e6af01d0de0fa8392c1145\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e    \u003cspan class=\"attr\"\u003espec:\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e      \u003cspan class=\"attr\"\u003econtainers:\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e      \u003cspan class=\"bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"attr\"\u003ename:\u003c/span\u003e \u003cspan class=\"string\"\u003econfig-demo-app\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        \u003cspan class=\"attr\"\u003eimage:\u003c/span\u003e \u003cspan class=\"string\"\u003egcr.io/optimum-rock-145719/config-demo-app\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        \u003cspan class=\"attr\"\u003eports:\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        \u003cspan class=\"bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"attr\"\u003econtainerPort:\u003c/span\u003e \u003cspan class=\"number\"\u003e80\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        \u003cspan class=\"attr\"\u003eenvFrom:\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        \u003cspan class=\"comment\"\u003e# The ConfigMap we want to use\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        \u003cspan class=\"bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"attr\"\u003econfigMapRef:\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e            \u003cspan class=\"attr\"\u003ename:\u003c/span\u003e \u003cspan class=\"string\"\u003edemo-config\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        \u003cspan class=\"comment\"\u003e# Extra-curricular: We can make the hash of our ConfigMap available at a\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        \u003cspan class=\"comment\"\u003e# (e.g.) debug endpoint via a fieldRef\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        \u003cspan class=\"attr\"\u003eenv:\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        \u003cspan class=\"bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"attr\"\u003ename:\u003c/span\u003e \u003cspan class=\"string\"\u003eCONFIG_HASH\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e          \u003cspan class=\"comment\"\u003e#value: \u0026#34;4431f6d28fdf60c8140d28c42cde331a76269ac7a0e6af01d0de0fa8392c1145\u0026#34;\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e          \u003cspan class=\"attr\"\u003evalueFrom:\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e            \u003cspan class=\"attr\"\u003efieldRef:\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e              \u003cspan class=\"attr\"\u003efieldPath:\u003c/span\u003e \u003cspan class=\"string\"\u003espec.template.metadata.annotations.configHash\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003cp\u003e提示以下错误:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/20200511180743.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e会提示\u003ccode\u003eUnsupported value:spec.template.metadata.annotations.configHash\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e目的很简单: container中的环境变量中引用configHash变量, 这个值是当configmap变更时比对两个不同的sha值以此达到重启pod的目的, 但fieldPath显然不支持\u003ccode\u003espec.template.metadata.annotations.configHash\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e从报错提示来看, 支持列表有\u003ccode\u003emetadata.name, metadata.namespace, metadata.uid, spec.nodeName,spec.serviceAccountName, status.hostIp, status.PodIP, status.PodIPs\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e这些值用于容器中需要以下信息时可以不从k8s的apiserver中获取而是可以很方便地从这些变量直接获得\u003c/p\u003e\n\u003cp\u003e参考: \u003c/p\u003e\n\u003cp\u003e\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://www.magalix.com/blog/kubernetes-patterns-the-reflection-pattern\"\u003ehttps://www.magalix.com/blog/kubernetes-patterns-the-reflection-pattern\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/\"\u003ehttps://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"参考文章\"\u003e\u003ca href=\"#参考文章\" class=\"headerlink\" title=\"参考文章:\"\u003e\u003c/a\u003e\u003cstrong\u003e参考文章:\u003c/strong\u003e\u003c/h3\u003e\u003cblockquote\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://enix.io/fr/blog/kubernetes-tip-and-tricks-node-authorization-mode/\"\u003eKubernetes : Le Node Authorization Mode de l\u0026#39;API-Server\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://www.ibm.com/docs/en/cloud-private/3.2.0?topic=console-namespace-is-stuck-in-terminating-state\"\u003ehttps://www.ibm.com/docs/en/cloud-private/3.2.0?topic=console-namespace-is-stuck-in-terminating-state\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/\"\u003ehttps://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://github.com/kubernetes/kubernetes/issues/19317\"\u003ehttps://github.com/kubernetes/kubernetes/issues/19317\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca target=\"_blank\" rel=\"noopener\" href=\"http://www.xuyasong.com/?p=1725\"\u003ehttp://www.xuyasong.com/?p=1725\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://kubernetes.io/\"\u003ehttps://kubernetes.io/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://fuckcloudnative.io/\"\u003ehttps://fuckcloudnative.io/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://www.cnblogs.com/breezey/p/8810039.html\"\u003ehttps://www.cnblogs.com/breezey/p/8810039.html\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://ieevee.com/tech/2018/04/25/downwardapi.html\"\u003ehttps://ieevee.com/tech/2018/04/25/downwardapi.html\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://www.magalix.com/blog/kubernetes-patterns-the-reflection-pattern\"\u003ehttps://www.magalix.com/blog/kubernetes-patterns-the-reflection-pattern\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#deploymentspec-v1-apps\"\u003ehttps://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#deploymentspec-v1-apps\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/\"\u003ehttps://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://github.com/kubernetes/kubernetes/pull/62167/files\"\u003ehttps://github.com/kubernetes/kubernetes/pull/62167/files\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://github.com/kubernetes-sigs/metrics-server/blob/master/README.md\"\u003ehttps://github.com/kubernetes-sigs/metrics-server/blob/master/README.md\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://github.com/kubernetes/kubeadm/issues/1596\"\u003ehttps://github.com/kubernetes/kubeadm/issues/1596\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://izsk.me/2022/01/27/Kubernetes-pod-status-is-UnexpectedAdmissionError\"\u003ehttps://izsk.me/2022/01/27/Kubernetes-pod-status-is-UnexpectedAdmissionError\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/blockquote\u003e\n\u003ch3 id=\"转载请注明原作者-周淑科-https-izsk-me\"\u003e\u003ca href=\"#转载请注明原作者-周淑科-https-izsk-me\" class=\"headerlink\" title=\"转载请注明原作者: 周淑科(https://izsk.me)\"\u003e\u003c/a\u003e\u003cstrong\u003e转载请注明原作者: 周淑科(\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://izsk.me/\"\u003ehttps://izsk.me\u003c/a\u003e)\u003c/strong\u003e\u003c/h3\u003e\n      \n    \u003c/div\u003e",
  "Date": "2023-12-04T19:30:53+08:00",
  "Author": "Z.S.K."
}