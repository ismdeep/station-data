{
  "Source": "izsk.me",
  "Title": "Kubernetes学习(关于CoreDNS的5s超时问题)",
  "Link": "https://izsk.me/2020/06/10/Kubernetes-coredns-5s-timeout/",
  "Content": "\u003cdiv class=\"post-body\" itemprop=\"articleBody\"\u003e\n\n      \n      \n\n      \n        \u003cp\u003e今天同事反映了一个很奇怪的问题: 在k8s环境里的容器中curl另一个服务时会出现断断续续的超时, 问题现象很简单, 但问题根源很复杂\u003c/p\u003e\n\u003cspan id=\"more\"\u003e\u003c/span\u003e\n\n\n\n\u003ch3 id=\"环境说明\"\u003e\u003ca href=\"#环境说明\" class=\"headerlink\" title=\"环境说明\"\u003e\u003c/a\u003e环境说明\u003c/h3\u003e\u003cfigure class=\"highlight bash\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e2\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e3\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e4\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e5\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e6\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e7\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e8\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e9\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e10\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003eKubernetes-1.15.1\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003eserviceA: 10.244.3.45\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003eservieB:\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\tClusterIP: 10.105.224.130\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003eCoreDNS-1.3.1:\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\tClusterIP: 10.96.0.10\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\tinstanceA: 10.244.3.43\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\tinstanceB: 10.244.0.45\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\n\n\u003ch3 id=\"问题现象\"\u003e\u003ca href=\"#问题现象\" class=\"headerlink\" title=\"问题现象\"\u003e\u003c/a\u003e问题现象\u003c/h3\u003e\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/20200610105313.png\"/\u003e\u003c/p\u003e\n\u003ch3 id=\"排查过程\"\u003e\u003ca href=\"#排查过程\" class=\"headerlink\" title=\"排查过程\"\u003e\u003c/a\u003e排查过程\u003c/h3\u003e\u003ch4 id=\"Services\"\u003e\u003ca href=\"#Services\" class=\"headerlink\" title=\"Services\"\u003e\u003c/a\u003eServices\u003c/h4\u003e\u003cp\u003e首先很容易想到, 因为现象是时好时坏，是不是有可能服务中有某一个节点不正常, 排查一圈后，涉及的服务都未见异常.\u003c/p\u003e\n\u003ch4 id=\"ClusterIP\"\u003e\u003ca href=\"#ClusterIP\" class=\"headerlink\" title=\"ClusterIP\"\u003e\u003c/a\u003eClusterIP\u003c/h4\u003e\u003cp\u003e既然服务都正常, 那会不会是解析有问题, 因为coreDNS也是多实例的, 因此直接通过ClusterIP访问, 未见超时，因此可以判断是\u003cstrong\u003e解析环节出现问题\u003c/strong\u003e\u003c/p\u003e\n\u003ch4 id=\"CoreDNS\"\u003e\u003ca href=\"#CoreDNS\" class=\"headerlink\" title=\"CoreDNS\"\u003e\u003c/a\u003eCoreDNS\u003c/h4\u003e\u003cp\u003e在确认是解析环节的问题后, 就从\u003ccode\u003ecoreDNS\u003c/code\u003e入手了, \u003ccode\u003ecoreDNS\u003c/code\u003e的运行情况未见异常, 日志也不见错误输出, 打开\u003ccode\u003edebug\u003c/code\u003e日志后，再次观察\u003c/p\u003e\n\u003cp\u003e也未见任何错误日志，这就奇怪了, 没办法，只能\u003ccode\u003etcpdump\u003c/code\u003e了\u003c/p\u003e\n\u003ch4 id=\"Tcpdump\"\u003e\u003ca href=\"#Tcpdump\" class=\"headerlink\" title=\"Tcpdump\"\u003e\u003c/a\u003eTcpdump\u003c/h4\u003e\u003cp\u003e只需要看53端口的流量即可, 通过\u003ccode\u003etcpdump\u003c/code\u003e抓到包后,也并未发现在任何可疑的地方. 本来寄希望于神器, 却什么都没发现，这就尴尬了\u003c/p\u003e\n\u003ch4 id=\"Kube-proxy\"\u003e\u003ca href=\"#Kube-proxy\" class=\"headerlink\" title=\"Kube-proxy\"\u003e\u003c/a\u003eKube-proxy\u003c/h4\u003e\u003cp\u003e由于dns的实现是通过kube-proxy实现的, 因此kube-proxy的差异也会引起转发异常, 但是通过比对两台宿主机的\u003ccode\u003eiptables\u003c/code\u003e，也一样,  这就排除了\u003ccode\u003ekube-proxy\u003c/code\u003e的问题\u003c/p\u003e\n\u003ch3 id=\"Why-5s\"\u003e\u003ca href=\"#Why-5s\" class=\"headerlink\" title=\"Why 5s\"\u003e\u003c/a\u003eWhy 5s\u003c/h3\u003e\u003cp\u003e其实从现象来看, 我第一想到的是这个5s很熟悉, 因为之前优化过DNS服务器重试的问题, 记得默认的超时时间就是5s, 所以很容易想到了\u003c/p\u003e\n\u003cp\u003e这里需要简单地说一下k8s中的服务名字 –\u0026gt;实例IP的一个的流程:\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eServiceName –\u0026gt; coreDNS ClusterIP –\u0026gt; coreDNS instance –\u0026gt; ClusterIP –\u0026gt; Service instance\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e测试了从\u003ccode\u003ecoreDNS ClusterIP --\u0026gt; coreDNS instance\u003c/code\u003e的解析是没有问题的，因此问题出现\u003ccode\u003ecoreDNS instance --\u0026gt; ClusterIP\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e那么单独通过coreDNS的两个实例去解析, 会发现有一个实例确实会出现\u003ccode\u003e5s Timeout的情况\u003c/code\u003e,另一个实例没有问题.\u003c/p\u003e\n\u003cp\u003e对比了两个实例所在的宿主机, 跟dns相关的\u003ccode\u003e/etc/resolv.conf\u003c/code\u003e，\u003ccode\u003e/etc/sysctl.conf\u003c/code\u003e等有关系的文件，没有差别\u003c/p\u003e\n\u003cp\u003e这就很奇怪了, 几乎相关的组件都排查了，并没有发现可疑的地方, 没办法，只能网上查看是否有相关的\u003ccode\u003eissue\u003c/code\u003e.\u003c/p\u003e\n\u003ch3 id=\"Conntrack\"\u003e\u003ca href=\"#Conntrack\" class=\"headerlink\" title=\"Conntrack\"\u003e\u003c/a\u003eConntrack\u003c/h3\u003e\u003cp\u003e经过一番搜索, 还真发现是踩到了一个坑, 倒不是k8s的问题, 而是conntrack的一个bug.\u003c/p\u003e\n\u003cp\u003e关于\u003ccode\u003econntrack\u003c/code\u003e, 涉及到内核的一些知识，没办法搞的太深，太深了也看不太懂, 大家可自行了解\u003c/p\u003e\n\u003cp\u003e这里就参考\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://opengers.github.io/openstack/openstack-base-netfilter-framework-overview/\"\u003e这里\u003c/a\u003e总结一下:\u003c/p\u003e\n\u003cp\u003e当加载内核模块\u003ccode\u003enf_conntrack\u003c/code\u003e后，conntrack机制就开始工作，如上图，椭圆形方框\u003ccode\u003econntrack\u003c/code\u003e在内核中有两处位置(PREROUTING和OUTPUT之前)能够跟踪数据包。对于每个通过\u003ccode\u003econntrack\u003c/code\u003e的数据包，内核都为其生成一个conntrack条目用以跟踪此连接，对于后续通过的数据包，内核会判断若此数据包属于一个已有的连接，则更新所对应的conntrack条目的状态(比如更新为ESTABLISHED状态)，否则内核会为它新建一个conntrack条目。所有的conntrack条目都存放在一张表里，称为连接跟踪表\u003c/p\u003e\n\u003cp\u003e连接跟踪表存放于系统内存中，可以用\u003ccode\u003ecat /proc/net/nf_conntrack\u003c/code\u003e, ubuntu则是\u003ccode\u003econntrack\u003c/code\u003e命令查看当前跟踪的所有conntrack条目\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://raw.githubusercontent.com/zhoushuke/BlogPhoto/master/githuboss/20200612162959.png\"/\u003e\u003c/p\u003e\n\u003ch3 id=\"Prombles\"\u003e\u003ca href=\"#Prombles\" class=\"headerlink\" title=\"Prombles\"\u003e\u003c/a\u003ePrombles\u003c/h3\u003e\u003cp\u003e经过一番参考之后, 发现问题的根源在于:\u003c/p\u003e\n\u003cp\u003eDNS client (glibc 或 musl libc) 会并发请求 A (ipv4地址) 和 AAAA (ipv6地址)记录，跟 DNS Server 通信自然会先 connect (建立fd)，后面请求报文使用这个 fd 来发送，由于 UDP 是无状态协议， connect 时并不会创建 conntrack 表项, 而并发请求的 A 和 AAAA 记录默认使用同一个 fd 发包，这时它们源 Port 相同，当并发发包时，两个包都还没有被插入 conntrack 表项，所以 netfilter 会为它们\u003cstrong\u003e分别创建 conntrack 表项\u003c/strong\u003e，而集群内请求 kube-dns 或 coredns 都是访问的CLUSTER-IP，报文最终会被 DNAT 成一个 endpoint 的 POD IP，当两个包被 DNAT 成同一个 IP，最终它们的五元组就相同了，在最终插入的时候后面那个包就会被丢掉，如果 dns 的 pod 副本只有一个实例的情况就很容易发生，现象就是 dns 请求超时，client 默认策略是等待 5s 自动重试，如果重试成功，我们看到的现象就是 dns 请求有 5s 的延时， \u003c/p\u003e\n\u003cp\u003enetfilter conntrack 模块为每个连接创建 conntrack 表项时，表项的创建和最终插入之间还有一段逻辑，没有加锁，是一种乐观锁的过程。conntrack 表项并发刚创建时五元组不冲突的话可以创建成功，但中间经过 NAT 转换之后五元组就可能变成相同，第一个可以插入成功，后面的就会插入失败，因为已经有相同的表项存在。比如一个 SYN 已经做了 NAT 但是还没到最终插入的时候，另一个 SYN 也在做 NAT，因为之前那个 SYN 还没插入，这个 SYN 做 NAT 的时候就认为这个五元组没有被占用，那么它 NAT 之后的五元组就可能跟那个还没插入的包相同\u003c/p\u003e\n\u003cp\u003e所以总结来说，根本原因是内核 conntrack 模块的 bug，DNS client(在linux上一般就是resolver)会并发地请求A 和 AAAA 记录netfilter 做 NAT 时可能发生资源竞争导致部分报文丢弃\u003c/p\u003e\n\u003cp\u003e这篇post有非常详细的解释，建议大家都好好地读一读\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://dzone.com/articles/racy-conntrack-and-dns-lookup-timeouts\"\u003eracy conntrack and dns lookup timeouts\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003epost的相关结论：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e只有多个线程或进程，并发从同一个 socket 发送相同五元组的 UDP 报文时，才有一定概率会发生\u003c/li\u003e\n\u003cli\u003eglibc, musl(alpine linux的libc库)都使用 “parallel query”, 就是并发发出多个查询请求，因此很容易碰到这样的冲突，造成查询请求被丢弃\u003c/li\u003e\n\u003cli\u003e由于 ipvs 也使用了 conntrack, 使用 kube-proxy 的 ipvs 模式，并不能避免这个问题\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"Resolve\"\u003e\u003ca href=\"#Resolve\" class=\"headerlink\" title=\"Resolve\"\u003e\u003c/a\u003eResolve\u003c/h3\u003e\u003ch4 id=\"Use-TCP\"\u003e\u003ca href=\"#Use-TCP\" class=\"headerlink\" title=\"Use TCP\"\u003e\u003c/a\u003eUse TCP\u003c/h4\u003e\u003cp\u003e默认情况下, dns的请求一般都是使用UDP请求的, 因为力求效率，不需要三握四挥\u003c/p\u003e\n\u003cp\u003e由于TCP没有这个问题，有人提出可以在容器的resolv.conf中增加\u003ccode\u003eoptions use-vc\u003c/code\u003e, 强制glibc使用TCP协议发送DNS query。下面是这个man resolv.conf中关于这个选项的说明：\u003c/p\u003e\n\u003cfigure class=\"highlight bash\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e2\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e3\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003euse-vc (since glibc 2.14)\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e                     Sets RES_USEVC \u003cspan class=\"keyword\"\u003ein\u003c/span\u003e _res.options.  This option forces the\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e                     use of TCP \u003cspan class=\"keyword\"\u003efor\u003c/span\u003e DNS resolutions.\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003cp\u003e缺点是使用tcp会比udp稍慢\u003c/p\u003e\n\u003ch4 id=\"single-request-reopen-sing-request\"\u003e\u003ca href=\"#single-request-reopen-sing-request\" class=\"headerlink\" title=\"single-request-reopen/sing-request\"\u003e\u003c/a\u003e\u003ccode\u003esingle-request-reopen/sing-request\u003c/code\u003e\u003c/h4\u003e\u003cp\u003eresolv.conf还有另外两个相关的参数：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003esingle-request-reopen (since glibc 2.9)\u003c/li\u003e\n\u003cli\u003esingle-request (since glibc 2.10)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eman resolv.conf中解释如下：\u003c/p\u003e\n\u003cfigure class=\"highlight bash\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e2\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e3\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e4\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e5\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e6\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e7\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e8\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003esingle-request-reopen (since glibc 2.9)                     \u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003eSets RES_SNGLKUPREOP \u003cspan class=\"keyword\"\u003ein\u003c/span\u003e _res.options.  The resolver                     uses the same socket \u003cspan class=\"keyword\"\u003efor\u003c/span\u003e the A and AAAA requests.  Some                     hardware mistakenly sends back only one reply.  When                     that happens the client system will sit and \u003cspan class=\"built_in\"\u003ewait\u003c/span\u003e \u003cspan class=\"keyword\"\u003efor\u003c/span\u003e                     the second reply.  Turning this option on changes this                     behavior so that \u003cspan class=\"keyword\"\u003eif\u003c/span\u003e two r\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003eequests from the same port are                     not handled correctly it will close the socket and open                     a new one before sending the second request.\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003esingle-request (since glibc 2.10)                     \u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003eSets RES_SNGLKUP \u003cspan class=\"keyword\"\u003ein\u003c/span\u003e _res.options.  By default, glibc                     performs IPv4 and IPv6 lookups \u003cspan class=\"keyword\"\u003ein\u003c/span\u003e parallel since                     version 2.9.  Some appliance DNS servers cannot handle                     these queries properly and make the requests time out.                     This option disables the behavior and makes glibc                     perform the IPv6 and IPv4 requests sequentially (at the                     cost of some slowdown of the resolving process).\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003esingle-request-reopen\u003c/code\u003e: 发送 A 类型请求和 AAAA 类型请求使用不同的源端口，这样两个请求在 conntrack 表中不占用同一个表项，从而避免冲突\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003esingle-request\u003c/code\u003e: 避免并发，改为串行发送 A 类型和 AAAA 类型请求，没有了并发，从而也避免了冲突\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e所以需要将参数写入到容器的\u003ccode\u003e/etc/resolv.conf\u003c/code\u003e中, 那么也有几种办法\u003c/p\u003e\n\u003cp\u003e对于已经上线运行的容器可以直接修改yaml定义\u003c/p\u003e\n\u003cfigure class=\"highlight plaintext\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e2\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e3\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e4\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e5\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003etemplate:\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  spec:\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e    dnsConfig:\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e      options:\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        - name: single-request-reopen\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003cp\u003e也可以直接写到启动命令中\u003c/p\u003e\n\u003cfigure class=\"highlight yaml\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e2\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e3\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e4\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e5\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e6\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e7\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"attr\"\u003elifecycle:\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  \u003cspan class=\"attr\"\u003epostStart:\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e    \u003cspan class=\"attr\"\u003eexec:\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e      \u003cspan class=\"attr\"\u003ecommand:\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e      \u003cspan class=\"bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"string\"\u003e/bin/sh\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e      \u003cspan class=\"bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"string\"\u003e-c\u003c/span\u003e \u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e      \u003cspan class=\"bullet\"\u003e-\u003c/span\u003e \u003cspan class=\"string\"\u003e\u0026#34;/bin/echo \u0026#39;options single-request-reopen\u0026#39; \u0026gt;\u0026gt; /etc/resolv.conf\u0026#34;\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003cp\u003e再或者直接通过\u003ccode\u003eEntrypoint/CMD\u003c/code\u003e写入\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e/bin/echo \u0026#39;options single-request-reopen\u0026#39; \u0026gt;\u0026gt; /etc/resolv.conf\u003c/code\u003e\u003c/p\u003e\n\u003ch4 id=\"musl-libc\"\u003e\u003ca href=\"#musl-libc\" class=\"headerlink\" title=\"musl libc\"\u003e\u003c/a\u003emusl libc\u003c/h4\u003e\u003cp\u003e前几种方案是 glibc 支持的，而基于 alpine 的镜像底层库是 musl libc 不是 glibc，所以即使加了这些 options 也没用，因此如果是通过alpine构建的镜像，当然也有解决办法，网上推荐使用本地DNS的方式，也就是每个节点都部署一个本地的dns, 容器直接请求本的dns，不需要走 DNAT，也不会发生 conntrack 冲突。另外还有个好处，就是避免 DNS 服务成为性能瓶颈\u003c/p\u003e\n\u003cp\u003eNodeLocal DNScache（k8s v1.15中的beta），这已在k8s官方文档中进行了详细介绍。该解决方案旨在通过作为DaemonSet在每个节点上运行DNS缓存代理来提高群集上的总体DNS性能，以便Pod可以与在同一节点上运行的这些代理联系，从而减少仍然使用conntrack发生竞争的可能性\u003c/p\u003e\n\u003cp\u003e当然，这个需要集群版本在1.15以上，如果低于这个版本使用其它的dns加速工具如dnsmasq nscd等也可以实现本地dns缓存，但部署起来会比较复杂\u003c/p\u003e\n\u003cp\u003e当然也可以直接设置 POD.spec.dnsPolicy 为 “Default”， 意思是POD里面的 /etc/resolv.conf是从宿主机上的 /etc/resolv.conf继承而来，这样宿主机上的所有pod都会将宿主机上的/etc/resolv.conf直接挂载到容器内.这样也能解决问题\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e“\u003ccode\u003eDefault\u003c/code\u003e“: The Pod inherits the name resolution configuration from the node that the pods run on. See \u003ca target=\"_blank\" rel=\"noopener\" href=\"https://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/#inheriting-dns-from-the-node\"\u003erelated discussion\u003c/a\u003e for more details.\u003c/li\u003e\n\u003cli\u003e“\u003ccode\u003eClusterFirst\u003c/code\u003e“: Any DNS query that does not match the configured cluster domain suffix, such as “\u003ccode\u003ewww.kubernetes.io\u003c/code\u003e“, is forwarded to the upstream nameserver inherited from the node. Cluster administrators may have extra stub-domain and upstream DNS servers configured. See \u003ca target=\"_blank\" rel=\"noopener\" href=\"https://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/#effects-on-pods\"\u003erelated discussion\u003c/a\u003e for details on how DNS queries are handled in those cases.\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e如果想尽快解决问题的话，个人还是建议直接更换alpine镜像, 基于alpine构建的镜像还是有挺多坑的.\u003c/p\u003e\n\u003ch3 id=\"getaddrinfo-、gethostbyname\"\u003e\u003ca href=\"#getaddrinfo-、gethostbyname\" class=\"headerlink\" title=\"getaddrinfo()、gethostbyname()\"\u003e\u003c/a\u003egetaddrinfo()、gethostbyname()\u003c/h3\u003e\u003cp\u003e关于A与AAAA, 有两个很常用的函数, 之前也有用过, 但是从来没有关注过这两者的区别\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003egetaddrinfo()\u003c/code\u003e会返回A与AAAA的记录, 如果其中任一个没有返回，都将超时重试\u003c/p\u003e\n\u003cp\u003e而\u003ccode\u003egethostbyname()\u003c/code\u003e只会返回A记录\u003c/p\u003e\n\u003cp\u003e参考\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://unix.stackexchange.com/questions/141163/dns-lookups-sometimes-take-5-seconds\"\u003estackoverflow\u003c/a\u003e\u003c/p\u003e\n\u003ch3 id=\"参考文章\"\u003e\u003ca href=\"#参考文章\" class=\"headerlink\" title=\"参考文章:\"\u003e\u003c/a\u003e\u003cstrong\u003e参考文章:\u003c/strong\u003e\u003c/h3\u003e\u003cblockquote\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://tencentcloudcontainerteam.github.io/\"\u003ehttps://tencentcloudcontainerteam.github.io/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://dzone.com/articles/racy-conntrack-and-dns-lookup-timeouts\"\u003ehttps://dzone.com/articles/racy-conntrack-and-dns-lookup-timeouts\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://www.bookstack.cn/read/kubernetes-practice-guide/troubleshooting-cases-dns-lookup-5s-delay.md\"\u003ehttps://www.bookstack.cn/read/kubernetes-practice-guide/troubleshooting-cases-dns-lookup-5s-delay.md\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://tencentcloudcontainerteam.github.io/\"\u003ehttps://tencentcloudcontainerteam.github.io/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://unix.stackexchange.com/questions/141163/dns-lookups-sometimes-take-5-seconds\"\u003ehttps://unix.stackexchange.com/questions/141163/dns-lookups-sometimes-take-5-seconds\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://blog.codacy.com/dns-hell-in-kubernetes/\"\u003ehttps://blog.codacy.com/dns-hell-in-kubernetes/\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://opengers.github.io/openstack/openstack-base-netfilter-framework-overview/\"\u003ehttps://opengers.github.io/openstack/openstack-base-netfilter-framework-overview/\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/blockquote\u003e\n\u003ch3 id=\"转载请注明原作者-周淑科-https-izsk-me\"\u003e\u003ca href=\"#转载请注明原作者-周淑科-https-izsk-me\" class=\"headerlink\" title=\"转载请注明原作者: 周淑科(https://izsk.me)\"\u003e\u003c/a\u003e\u003cstrong\u003e转载请注明原作者: 周淑科(\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://izsk.me/\"\u003ehttps://izsk.me\u003c/a\u003e)\u003c/strong\u003e\u003c/h3\u003e\n      \n    \u003c/div\u003e",
  "Date": "2020-06-10T20:10:53+08:00",
  "Author": "Z.S.K."
}