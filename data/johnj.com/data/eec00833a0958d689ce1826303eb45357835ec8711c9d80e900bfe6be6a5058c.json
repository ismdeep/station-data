{
  "Source": "johnj.com",
  "Title": "Practices for Software Projects",
  "Link": "http://johnj.com/posts/practices/",
  "Content": "\u003cmain class=\"content\"\u003e\n\n\n\u003ch1\u003ePractices for Software Projects\u003c/h1\u003e\n\n\n\u003cp\u003e\n\n\u003ca class=\"tag\" href=\"/tags/code\"\u003e\u003cspan class=\"code-tag\"\u003ecode\u003c/span\u003e\u003c/a\u003e\n\n  ..... \u003cem\u003e\u003ctime class=\"postdate\" datetime=\"2022-09-06T00:00:00Z\"\u003e\n      September 6, 2022\n    \u003c/time\u003e\u003c/em\u003e\n\u003c/p\u003e\n\n\n\n\u003cp\u003e\n\n\nLater: \u003ca href=\"http://johnj.com/posts/2024-blogging-intentions/\"\u003e2024 Blogging Intentions\u003c/a\u003e\n\n\n\u003cbr/\u003e\n\n\nEarlier: \u003ca href=\"http://johnj.com/posts/tco/\"\u003eAdding Tail Call Optimization to A Lisp Written in Go\u003c/a\u003e\n\n\n\u003c/p\u003e\u003cp\u003e\n\n\n\n\u003c/p\u003e\u003cp\u003eWhat follows is a selection of practices which I find helpful when\nworking with software projects, whether in public or in private, in\nsolo or in collaborative work.  Many of these practices are quite\nstandard, but some are, at best, unevenly practiced by either\ncommercial or academic software teams. Although they are especially\nimportant for collaborative work, I like to follow these practices on\nmy own projects, both as a matter of professional habit, and because I\nfind them beneficial.  (This post is essentially an expanded version\nof \u003ca href=\"https://gist.github.com/eigenhombre/204400c786f9dba3a8d3193d3ce1542a\"\u003ea\ngist\u003c/a\u003e\nwhich I made a few months ago, and which I have been endeavoring to\nadhere to in my open source work ever since.)\u003c/p\u003e\n\u003cdiv\u003e\n    \u003ch2\u003eTable Of Contents\u003c/h2\u003e\n    \u003cnav id=\"TableOfContents\"\u003e\n  \u003cul\u003e\n    \u003cli\u003e\u003ca href=\"#checklist\"\u003eChecklist\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#automated-tests\"\u003eAutomated Tests\u003c/a\u003e\n      \u003cul\u003e\n        \u003cli\u003e\u003ca href=\"#test-driven-development\"\u003eTest-Driven Development\u003c/a\u003e\u003c/li\u003e\n        \u003cli\u003e\u003ca href=\"#probing-with-tests\"\u003eProbing With Tests\u003c/a\u003e\u003c/li\u003e\n        \u003cli\u003e\u003ca href=\"#tradeoffs\"\u003eTradeoffs\u003c/a\u003e\u003c/li\u003e\n        \u003cli\u003e\u003ca href=\"#speed\"\u003eSpeed\u003c/a\u003e\u003c/li\u003e\n        \u003cli\u003e\u003ca href=\"#clean-tests\"\u003eClean Tests\u003c/a\u003e\u003c/li\u003e\n        \u003cli\u003e\u003ca href=\"#test-frameworks\"\u003eTest Frameworks\u003c/a\u003e\u003c/li\u003e\n        \u003cli\u003e\u003ca href=\"#test-coverage\"\u003eTest Coverage\u003c/a\u003e\u003c/li\u003e\n      \u003c/ul\u003e\n    \u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#style-checks\"\u003eStyle Checks\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#automated-builds\"\u003eAutomated Builds\u003c/a\u003e\n      \u003cul\u003e\n        \u003cli\u003e\u003ca href=\"#repeatability\"\u003eRepeatability\u003c/a\u003e\u003c/li\u003e\n      \u003c/ul\u003e\n    \u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#readmes\"\u003eREADMEs\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#versions-and-releases\"\u003eVersions and Releases\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#issue-tracking\"\u003eIssue Tracking\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#other-helpful-practices\"\u003eOther Helpful Practices\u003c/a\u003e\n      \u003cul\u003e\n        \u003cli\u003e\u003ca href=\"#using-a-common-build-tool\"\u003eUsing a Common Build Tool\u003c/a\u003e\u003c/li\u003e\n        \u003cli\u003e\u003ca href=\"#getting-invisible-feedback\"\u003eGetting “Invisible Feedback”\u003c/a\u003e\u003c/li\u003e\n        \u003cli\u003e\u003ca href=\"#auto-generating-documentation\"\u003eAuto-Generating Documentation\u003c/a\u003e\u003c/li\u003e\n      \u003c/ul\u003e\n    \u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#summary\"\u003eSummary\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#acknowledgments\"\u003eAcknowledgments\u003c/a\u003e\u003c/li\u003e\n  \u003c/ul\u003e\n\u003c/nav\u003e\n\u003c/div\u003e\n\n\n\u003ch1 id=\"checklist\"\u003eChecklist\u003c/h1\u003e\n\u003cp\u003eI’ll start with a checklist I use, then go into details for each item.\nI try to check these items for each repository/project I work on.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eAre there unit tests?\n\u003cul\u003e\n\u003cli\u003eAre they maintained?\u003c/li\u003e\n\u003cli\u003eAre they run often by developers…\u003c/li\u003e\n\u003cli\u003e… and by builds that are triggered by every commit pushed to the\ngroup repository?\u003c/li\u003e\n\u003cli\u003eIs test coverage sufficient, to the extent agreed by the team?\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eIs there a README for the project?  Does it show:\n\u003cul\u003e\n\u003cli\u003eWhat the project is for?\u003c/li\u003e\n\u003cli\u003eHow to build it and run tests?\u003c/li\u003e\n\u003cli\u003eAn example snippet showing it in action?\u003c/li\u003e\n\u003cli\u003eLinks to further documentation, if any?\u003c/li\u003e\n\u003cli\u003eA license / copyright information?\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eAre builds generally repeatable?  Does firing the same build several\ntimes for a recent commit yield the same result each time?\u003c/li\u003e\n\u003cli\u003eIs an issue tracker being used to track bugs and planned work?\u003c/li\u003e\n\u003cli\u003eIs there any relevant documentation that can be auto-generated?\u003c/li\u003e\n\u003cli\u003eIs the software versioned?\n\u003cul\u003e\n\u003cli\u003eDoes your application / library know what version it is?\u003c/li\u003e\n\u003cli\u003eIs the process of updating the version and releasing the code automated?\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003eAre the following steps fully automated and executable via Make?\n\u003cul\u003e\n\u003cli\u003eRunning unit tests (\u003ccode\u003emake test\u003c/code\u003e … can be broken into \u003ccode\u003efast\u003c/code\u003e and \u003ccode\u003eslow\u003c/code\u003e targets)\u003c/li\u003e\n\u003cli\u003eRunning code style checks and/or automated reformatting (\u003ccode\u003emake lint\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003eReleasing the software, including building any needed artifacts (\u003ccode\u003emake release\u003c/code\u003e)\u003c/li\u003e\n\u003cli\u003eRunning all the tests in Docker, to ensure consistency with the automated build (\u003ccode\u003emake docker\u003c/code\u003e)\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"automated-tests\"\u003eAutomated Tests\u003c/h1\u003e\n\u003cp\u003eLet’s start with the most important item. Writing and continuously\nrunning automated tests is always my first line of defense against\ndefects.  To quote \u003ca href=\"https://www.r7krecon.com/legacy-code\"\u003eMichael\nFeathers\u003c/a\u003e,\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eCode without tests is bad code. It doesn’t matter how well written\nit is; it doesn’t matter how pretty or object-oriented or\nwell-encapsulated it is. With tests, we can change the behavior of\nour code quickly and verifiably. Without them, we don’t really know\nif our code is getting better or worse.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eIn what follows, “production code” means the code that actually does\nwhat your program is trying to do.\u003c/p\u003e\n\u003cp\u003eThere’s a lot to be said about how to write tests, about the\ndifference between unit tests and integration tests, about test\nframeworks, mocks, fakes, stubs, and so on.  These discussions shade\ninto ones about architectural patterns, object orientation,\norganization of software into layers, etc.  To my view, these are all\nsecondary points. The main point is, is our software safe to change?\nOnly the presence, or absence, of tests, and the extent to which our\ntests cover our production code determine the answer to this question.\u003c/p\u003e\n\u003ch2 id=\"test-driven-development\"\u003eTest-Driven Development\u003c/h2\u003e\n\u003cp\u003eWhen starting to write tests, a common question is, What should I\ntest? This question is usually easiest to answer \u003cem\u003ebefore you’ve\nwritten the code you want to test\u003c/em\u003e. In that case, the the answer is:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eWrite the tests that force you to write code that implements the\nfunctionality you want.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eIn other words, don’t write production code that a failing\ntest hasn’t forced you to write.  This is the key to test-driven\ndevelopment (TDD).\u003c/p\u003e\n\u003cp\u003eNote that I wrote “\u003cem\u003ea\u003c/em\u003e failing test,” above, rather than “failing tests”: the\nprocess is best done in tiny increments, as follows:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eAdd a small increment of test code, which makes the tests fail\n(“red”).\u003c/li\u003e\n\u003cli\u003eAdd a small increment of production code, which makes the tests\npass (“green”).\u003c/li\u003e\n\u003cli\u003eRefactor as needed to keep the design of the code clean and simple\n(repeat 3 in small steps, as needed, before going back to step 1).\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eIdeally, each step in the “red…green…refactor…” cycle is as\nsmall as possible.  It should be the “red” step which actually pushes\nyou towards the functionality you want – that failing test is a\nstatement about where you want the production code to go.\u003c/p\u003e\n\u003cp\u003eIf you find yourself writing multiple tests at once, or a more complex\ntest than is strictly needed at the moment, then stop and try to break\nthe problem into smaller steps.\u003c/p\u003e\n\u003cp\u003eIf you only write production code in response to a failing unit test,\nthen you know that code has both a reason to exist, and test coverage\nfor that code.  See Kent Beck’s \u003cem\u003eTest-Driven Development By Example\u003c/em\u003e\nfor a more detailed explanation of this process, or try it for\nyourself.  I found when I adapted this pattern, about 14 years ago, my\nproductivity, the reliability of my software, and my enjoyment of\nwriting it, all increased noticeably.\u003c/p\u003e\n\u003ch2 id=\"probing-with-tests\"\u003eProbing With Tests\u003c/h2\u003e\n\u003cp\u003eOn the other hand, maybe you’ve inherited some code from somebody\nelse. Or maybe you’ve written a bunch of untested code as a prototype,\nand you want to consider it production code. In these situations, I\nlike to poke at the code in question using tests. These exploratory\ntests can be a way to build understanding about how the code works,\nand to document your findings. While reading the code under study, I\nmay find an area that looks like a defect: in that case, I write\ntests to try to show the defect (or prove its absence).  Some of these\nmore speculative tests may not live on permanently in my test suite,\nbut they usually do.\u003c/p\u003e\n\u003cp\u003eImportantly, whenever a defect is found, I always try to write a test\nwhich detects that defect before I fix it, to guard against the defect\never recurring.\u003c/p\u003e\n\u003ch2 id=\"tradeoffs\"\u003eTradeoffs\u003c/h2\u003e\n\u003cp\u003eTDD affects development speed – for very exploratory work, such as\nbuilding prototypes, it can be slower, though I find it usually helps\nvelocity as projects grow in complexity.  It definitely, in my\nexperience, helps add reliability.\u003c/p\u003e\n\u003cp\u003eI like the analogy of rock climbing. If I’m scrambling over a short\nwall, I don’t need a rope. Summiting a tall mountain face definitely\ncalls for serious protection. Bouldering close to the ground, not so\nmuch, but be careful that your “bouldering” code doesn’t sneak into\nyour “summiting” code.\u003c/p\u003e\n\u003cp\u003eThere are times when strict TDD is less critical, and times when I\nfind it especially important. Obviously, for exploratory coding on a\nprototype, TDD can be overkill.  Lispers armed with REPLs tend to be\nespecially comfortable with exploratory coding, but it is important to\nshore up our REPL-driven snippets with tests, lest we lose the ability\nto make changes safely. (A favorite technique for this situation is to\ncomment out bits of code and see if your existing tests fail, then TDD\nyour way back to bringing the code back in.)\u003c/p\u003e\n\u003cp\u003eOne place where I find TDD especially helpful is when the programming\nis more difficult than usual.  With TDD, I handle the simple cases\nfirst, and gradually add sophistication to my production code as I add\ntests. As Robert Martin puts it: As the tests get more specific, the\nproduction code becomes more general.\u003c/p\u003e\n\u003cp\u003eIf your software is hard to test, step back and take a look at the\ndesign. A difficult testing story is often a symptom of an\noverly-coupled design.  Picking apart the ball of yarn can be\ndifficult, and demands experience, but there are established\ntechniques for doing so. The book \u003cem\u003eWorking Effectively with Legacy\nCode\u003c/em\u003e, by Michael Feathers, treats this challenge in detail.\u003c/p\u003e\n\u003cp\u003eWhile orienting your code around testability can help somewhat with\nthe design of that code (by encouraging less coupling, etc.), the\npractice of TDD by itself will not guarantee a good, understandable,\nmaintainable design, as Rich Hickey explains in his talk \u003ca href=\"https://www.infoq.com/presentations/Simple-Made-Easy/\"\u003eSimple Made\nEasy\u003c/a\u003e (which I highly recommend).  The wrong tests can actually\nslow your ability to make design changes – one needs to consider\ndesign throughout the process, and be willing to rework both the tests\nand the production code as needed if the design needs improving.\u003c/p\u003e\n\u003ch2 id=\"speed\"\u003eSpeed\u003c/h2\u003e\n\u003cp\u003eA key point about all these tests: they must be fast. Fast, as in\nrunning the entire test suite takes less than a couple seconds.  When\nyou start relying on tests as your safety net, you can only move as\nfast as that net lets you move. If the tests take 10 seconds, a\nminute, 10 minutes to run, progress will be slow. I often run my tests\nmultiple times a minute, literally every time I save a file in the\ndirectory I’m working in. Or, if I am writing in a dialect of Lisp\nsuch as Clojure or Common Lisp, I run the entire test suite with a\ncouple of keystrokes and expect instant visual feedback directly in my\neditor. The good news is that most of the tests you write can be very\nfast. Slow tests can be a design smell – or at least a sign that\nyou’re testing at too high a level, through too many dependencies.  It\ncan take time to do the needed refactoring to make your tests fast,\nbut it is worth the investment.  Organizing your software in layers\nand using mocking, dependency injection, or polymorphic dispatch\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e\nto implement simple test harnesses for individual layers can be\nhelpful here.\u003c/p\u003e\n\u003cp\u003eSome tests are intrinsically slower. Examples include tests through a\ndatabase, or generative / fuzzing tests that programmatically generate\nmany examples and make assertions about program behavior while running\nagainst those examples. These tests can run separately, such as after\nevery commit, or prior to a release. Invest in separating out slow\ntests when needed.\u003c/p\u003e\n\u003ch2 id=\"clean-tests\"\u003eClean Tests\u003c/h2\u003e\n\u003cp\u003eAll code in your repository has a maintenance cost, including tests. I\ntry to keep test code as clean as production code. Because tests\ncan be repetitive, often with subtle variation between test cases, it\nis important to use the abstractions available in your programming\nlanguage to make the test as clean and as readable as\npossible. Squeezing out repetitive idioms is one area where\nhigh-level languages such as Python, Clojure, or Lisps have an\nadvantage. But clean tests should be a goal regardless of language.\u003c/p\u003e\n\u003cp\u003eTests by example, also known as table-driven tests, are an excellent\nway to get clean tests.  While Clojure’s \u003ccode\u003eare\u003c/code\u003e macro (\u003ca href=\"http://johnj.com/posts/tests-by-example/\"\u003etrivially\nportable to other Lisps\u003c/a\u003e)\ngives the cleanest implementation I know of, table tests are also\ncommon in Go and probably in several other languages as well.  It is\nworth discovering if your language offers this construction, or, if\nnot, perhaps creating it, if possible.\u003c/p\u003e\n\u003ch2 id=\"test-frameworks\"\u003eTest Frameworks\u003c/h2\u003e\n\u003cp\u003eThough some communities (Java, Python) gravitate towards the\n\u003ca href=\"https://en.wikipedia.org/wiki/XUnit\"\u003exUnit\u003c/a\u003e style of test frameworks,\nI find them to be overkill for most small projects, and for some large\nones.  Test frameworks try to address common areas of repetition in\ntests, but the abstractions your project needs in order to write clean\ntests may or may not be a natural fit for an existing, heavyweight\nframework.  The job of your test framework is to make sure that every\nassertion in your test suite is run, and that the build fails if any\nof the assertions fail.  Good examples of lightweight test frameworks\ninclude\n\u003ca href=\"https://clojure.github.io/clojure/clojure.test-api.html\"\u003e\u003ccode\u003eclojure.test\u003c/code\u003e\u003c/a\u003e\nfor Clojure and \u003ca href=\"https://github.com/lmj/1am\"\u003e1AM\u003c/a\u003e for Common Lisp\n(perhaps with the addition of \u003ca href=\"/posts/tests-by-example/\"\u003emy \u003ccode\u003eare\u003c/code\u003e\nmacro\u003c/a\u003e).  Go’s \u003ccode\u003etesting\u003c/code\u003e package is another.\u003c/p\u003e\n\u003cp\u003eIn some cases, a few test functions, a trivial test runner, and\njudicious use of \u003ccode\u003eassert\u003c/code\u003e are all you need.\u003c/p\u003e\n\u003ch2 id=\"test-coverage\"\u003eTest Coverage\u003c/h2\u003e\n\u003cp\u003eTools are available for most languages which will generate test\ncoverage reports.  I am not a fan of strictly defined coverage targets\nwhich fail test suites if coverage is below some fixed amount.\nNevertheless, I think that inspecting code coverage from time to time\ncan be helpful in identifying areas that need better tests.\u003c/p\u003e\n\u003cp\u003eDiscuss with your team what fraction of coverage you want to shoot for\nin your project.  Some projects made up of mostly \u003ca href=\"https://en.wikipedia.org/wiki/Pure_function\"\u003epure\nfunctions\u003c/a\u003e can get to\n100% easily, whereas others, perhaps heavy on error handling, UI code,\nor integration with other services which are hard to mock, settle at\nsomething less, perhaps 70%-80%.\u003c/p\u003e\n\u003cp\u003eMonitor your repositories for slippage in coverage, and use coverage\nreports to inform your work as you write code.\u003c/p\u003e\n\u003ch1 id=\"style-checks\"\u003eStyle Checks\u003c/h1\u003e\n\u003cp\u003eIt is my experience working with a variety of junior and senior\nengineers that standardizing coding styles leads to more readable code\nand higher quality overall.  Therefore, I like to run “linters” at the\nsame time I run my tests.  Linters are tools which check code style\naccording to agreed-upon guidelines and which raise an error when the\nstandard isn’t met.  Examples of linters are \u003ccode\u003elint\u003c/code\u003e in C,\n\u003ccode\u003epycodestyle\u003c/code\u003e (formerly \u003ccode\u003epep8\u003c/code\u003e) in Python, and \u003ccode\u003ekibit\u003c/code\u003e and \u003ccode\u003ebikeshed\u003c/code\u003e\nin Clojure.\u003c/p\u003e\n\u003cp\u003eIn addition to or in place of a linter, an automated code formatter, such\nas \u003ccode\u003egofmt\u003c/code\u003e for the Go programming language, or \u003ccode\u003eclang-format\u003c/code\u003e for C,\ncan reformat source code to adhere to community style rules.  Since\nthey fix style violations automatically, they have the benefit of not\nslowing down your workflow or miring developers in endless\nformatting discussions during code review, etc. The Go story is\nespecially successful in this regard, with the result that formatting\ndiscussions are largely absent in the Go community.\u003c/p\u003e\n\u003cp\u003eMost code formatters and style checkers are configurable.  These\noptions should be stored in a configuration inside the repository.  As\nthe team’s preferences and needs evolve, the configuration options can\nbe discussed and updated via pull requests or according to your team’s\ncode review procedures.\u003c/p\u003e\n\u003ch1 id=\"automated-builds\"\u003eAutomated Builds\u003c/h1\u003e\n\u003cp\u003eOnce the testing / linting rhythm is well established, automated\nbuilds shore up the practice further. Every project should have a\nprimary build which is triggered on a build machine, known commonly as\na Continuous Integration (CI) server, whenever anybody pushes to the\nmaster branch.  This build runs the automated tests for the project,\nto ensure that no broken code gets shipped, or, if it does, that it\ncan be noticed and fixed as soon as possible.\u003c/p\u003e\n\u003cp\u003eRarely is the code I’m writing destined to run on the machine I’m\nwriting it on. Achieving fidelity between development and production\nenvironments is frequently a challenge, though it is easier for some\nlanguages than for others. Regardless of the language or target\nplatform, I’ll try to achieve as much parity as possible, generally\nusing Docker for builds.  The \u003ccode\u003eDockerfile\u003c/code\u003es for these projects tend to\nbe simple: a few lines to install any needed dependencies, a line to\ncopy the files into the container, and a line to run the build tasks\nusing \u003ccode\u003emake\u003c/code\u003e (more on that below).\u003c/p\u003e\n\u003cp\u003eSince I host my repositories on GitHub, I use GitHub Actions for my\nbuilds, generally running them within Docker except for highly\nstandardized builds such as for Go programs.\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\n\u003cp\u003eThe history of the build success or failure can be easily seen on the\nGitHub Actions page for that build.  If the history is not full of\nmostly green builds, your developers are probably not running their\ntests locally often enough, or there may a discrepancy between build and\ndevelopment environments that needs addressing.\u003c/p\u003e\n\u003cp\u003eI make sure that the README has a build badge which shows the most\nrecent build status.  These badges may seem frivolous, but I feel they\ndo give some insight into how well a project is maintained. Any build\nfailures also trigger an email to the committer, and optionally to any\npeople “watching” the repository on GitHub.\u003c/p\u003e\n\u003ch2 id=\"repeatability\"\u003eRepeatability\u003c/h2\u003e\n\u003cp\u003eBuilds should be repeatable. That is to say, if a build fails for\nparticular commit, it should always fail, and if it succeeds for a\nparticular commit, it should always succeed.  Any other behavior makes\ncollaboration and troubleshooting harder (“it worked when I tried it /\non my machine”) and adds noise which slows the team down.  Tests that\nfail only sometimes are usually a sign of race conditions,\nparticularly insidious bugs which tend to be hard to identify and\ntroubleshoot but which generally boil down to defects in the tests in\nquestion and/or in the production code.\u003c/p\u003e\n\u003cp\u003eTo reiterate: unpredictable builds should be viewed as bugs, and\nshould not linger unaddressed.\u003c/p\u003e\n\u003ch1 id=\"readmes\"\u003eREADMEs\u003c/h1\u003e\n\u003cp\u003eI like every repository to have a \u003ccode\u003eREADME\u003c/code\u003e file, written in\n\u003ca href=\"https://en.wikipedia.org/wiki/Markdown\"\u003eMarkdown\u003c/a\u003e\u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e. The file explains:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ewhat the project is for;\u003c/li\u003e\n\u003cli\u003ehow to acquire, build and test it;\u003c/li\u003e\n\u003cli\u003egives an example snippet showing the project in action;\u003c/li\u003e\n\u003cli\u003eprovides links to further documentation, and\u003c/li\u003e\n\u003cli\u003ehas a license, along with a possible legal disclaimer (important for\nopen source projects where you may receive less legal cover, and\nfinancial benefit, than for software written for the benefit of an\nemployer).\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eThe example snippet shown in the README is especially important. It\nnot only gives a brief snapshot of your software in action, but it\nalso gives a feel for what it is like to use it and what it actually\ndoes.  Examples can be found\n\u003ca href=\"https://github.com/eigenhombre/hbook#examples\"\u003ehere\u003c/a\u003e and\n\u003ca href=\"https://github.com/eigenhombre/smallscheme#repl\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eWhether for open source or closed/private work, stating license terms clearly\ncan be helpful. GitHub \u003ca href=\"https://docs.github.com/en/repositories/managing-your-repositorys-settings-and-features/customizing-your-repository/licensing-a-repository\"\u003emakes this especially\neasy\u003c/a\u003e\nnow.  For closed-source, restricted repos I add a simple reminder in\nplace of a license: “\u003ccode\u003e© \u0026lt;year\u0026gt; MyOrganization.  All rights reserved.\u003c/code\u003e”\nFor open source projects, I generally choose a fairly permissive\nlicense, such as \u003ca href=\"https://choosealicense.com/licenses/mit/\"\u003ethe one from\nMIT\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eOn a personal note, for my own projects, I try to give every\nrepository its own unique artwork or photograph which appears at the\ntop of the README, an image ideally at least tangentially related in\nto the project (this can be a challenge for a software project). I\nfind that it adds to my enjoyment while working on the software, and\nhelps me keep track of what repo I’m looking at at any given time. It\nalso ties my creative practices (painting, drawing, photography) with\nmy work in software development, areas of my life which otherwise\nrarely overlap.\u003c/p\u003e\n\u003ch1 id=\"versions-and-releases\"\u003eVersions and Releases\u003c/h1\u003e\n\u003cp\u003eYour project should have versioned releases.  \u003cstrong\u003eThe process of making\nreleases should be entirely automated\u003c/strong\u003e. Typically, I make a release by\ntagging the software in Git, and pushing the tags to GitHub. Depending\non the language and target platform, this could involve other steps,\nlike making a tarball, triggering a build on GitHub which makes the\nrelease files available for download, or starting a deployment process\nto a staging environment.\u003c/p\u003e\n\u003cp\u003eYour program, Web service, app, or library should know and be able to\nreport what version it currently is.\u003c/p\u003e\n\u003cp\u003eThe release process should be scripted and have the following steps:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eMake sure no code is uncommitted in the local directory\u003c/li\u003e\n\u003cli\u003eGet the most recent version, e.g. \u003ccode\u003ev0.0.99\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eDetermine the new version, e.g. \u003ccode\u003ev0.0.100\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eUpdate the local software so that it can report the new version\nwhen asked\u003c/li\u003e\n\u003cli\u003eCommit the updated local software, e.g. \u003ccode\u003egit commit -am \u0026#34;Release v0.0.100\u0026#34;\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eTag the new software, e.g. \u003ccode\u003egit tag v0.0.100\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003ePush the new tag, e.g. \u003ccode\u003egit push --tags\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eIncrement the local software version to indicate its “tainted”\n(off-version) state, based on the new tagged version,\ne.g. \u003ccode\u003ev0.0.100-dirty\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003eCommit the local software again, e.g. \u003ccode\u003egit commit -am \u0026#34;Taint post v0.0.100 release\u0026#34;\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eAn example release script can be seen\n\u003ca href=\"https://github.com/eigenhombre/l1/blob/master/bumpver\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eAny released artifacts (tarballs, jar files, etc.) should reflect the\nrelease number, so if you’re generating release artifacts locally, it\nshould be done after Step 4, and before Step 8, above.  Otherwise, do\nit on your CI server based on the tag you push in Step 7.\u003c/p\u003e\n\u003cp\u003eThis automation can take some effort up front to set up.  But it\nremoves a major source of resistance to doing releases, and allows for\ntighter feedback loops.  An important component to this practice is to\nkeep the software releasable at all times – use feature flags or\nother mechanism to “hide” new features under development from users or\notherwise indicate their preliminary nature.  If software is always\nreleasable, and the automated tests are trustworthy, it will be safer\nto quickly deploy bug fixes quickly when things go wrong, and\neasier to track down bugs in production if they ever occur.\u003c/p\u003e\n\u003cp\u003eIf you can package your software using the target platform’s packaging\nsystem (Homebrew, \u003ccode\u003eapt\u003c/code\u003e, Clojars, PyPI, etc.), it goes a long way to\nmaking installation easier, particularly when onboarding new\ndevelopers.  This is typically easier for open source projects, but\ncan be helpful for private repositories as well (an example is private\nMaven repositories for the Clojure and Java ecosystems).\u003c/p\u003e\n\u003cp\u003eA side note on semantic versioning: there are many passionate opinions\nabout how to do semver correctly. If you follow my open source\nrepositories, you’ll see mostly versions like “v0.0.123” rather than\n“v1.10.1”.  I personally believe that semver is overrated – in\nparticular, \u003cem\u003ebreaking changes\u003c/em\u003e (so-called MAJOR revisions) are both\nhostile to users and usually avoidable … and should therefore be\nextremely rare.  \u003ca href=\"https://www.youtube.com/watch?v=oyLBGkS5ICk\"\u003eThis talk by Rich\nHickey\u003c/a\u003e explains the\nviewpoint far better than I can.\u003c/p\u003e\n\u003cp\u003eMy advice is to avoid breaking changes whenever possible.  One\nstrategy for this is to version your APIs, so that older and newer\nversions coexist together. Another is to simply call an existing\nproject done, and releasing a completely new project with extensive\nchanges, rather than breaking the old one for existing users.\u003c/p\u003e\n\u003ch1 id=\"issue-tracking\"\u003eIssue Tracking\u003c/h1\u003e\n\u003cp\u003eYour organization probably already does issue tracking, either through\nJIRA or some similar system. If you don’t, consider using GitHub\nIssues to track bugs and future work. I use these for my open source\nprojects, both to encourage other people to file issues, and to give\nvisibility into my to-do list.  When I make a commit, I tag the issue\nnumber in the commit.  For example,\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003egit commit -am \u0026#34;Fix horrible race condition.  Fixes #66.\u0026#34;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eGitHub automatically cross links these commits with the issues\nthemselves, and in the case of the above example actually closes the\nticket automagically. JIRA can be configured to do the much same. This\nkind of tracking can make it easier to understand what changes were\nmade when, and for what reason.  In some kinds of environments (e.g.,\nhighly regulated ones), this kind of change-tracking is a firm\nrequirement.\u003c/p\u003e\n\u003ch1 id=\"other-helpful-practices\"\u003eOther Helpful Practices\u003c/h1\u003e\n\u003ch2 id=\"using-a-common-build-tool\"\u003eUsing a Common Build Tool\u003c/h2\u003e\n\u003cp\u003eI frequently work in Python, C, Clojure, Go, and Common Lisp.  Each of\nthese languages has its own toolchain, which may be more or less\nstandard for the language, but I find that using\n\u003ca href=\"https://en.wikipedia.org/wiki/Make_(software)\"\u003e\u003ccode\u003emake\u003c/code\u003e\u003c/a\u003e to automate most\ncommon tasks makes it very easy for me to remember how to carry them\nout.\u003c/p\u003e\n\u003cp\u003eHere are some make targets I write for most projects:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003emake test\u003c/code\u003e runs all the unit tests locally.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003emake lint\u003c/code\u003e does any style checks.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003emake\u003c/code\u003e on its own runs any tests and linting / style checking (this\ntypically matches what runs on the build server after every push).\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003emake release\u003c/code\u003e does everything needed to cause a release to be created.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003emake docker\u003c/code\u003e runs tests and linting steps inside a Docker container.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThough make is an old tool, with a strange, if powerful, syntax, it is\nvery fast, and relatively simple to use for the above use cases.  (I\nfind many modern build tools to be slower and more awkward to use than\nMake for common operational tasks.)  The practice of using Make in\nthis way can be helpful in larger organizations, where several\ndifferent languages may be in use, and where developers may switch\nfrom project to project with some regularity.  Some of my repos which\nuse this pattern are \u003ca href=\"https://github.com/eigenhombre/l1\"\u003e\u003ccode\u003el1\u003c/code\u003e (Go)\u003c/a\u003e,\n\u003ca href=\"https://github.com/eigenhombre/smallscheme\"\u003e\u003ccode\u003esmallscheme\u003c/code\u003e (Python)\u003c/a\u003e,\n\u003ca href=\"https://github.com/eigenhombre/oatmeal\"\u003e\u003ccode\u003eoatmeal\u003c/code\u003e (Clojure)\u003c/a\u003e,\n\u003ca href=\"https://github.com/eigenhombre/cl-oju\"\u003e\u003ccode\u003ecl-oju\u003c/code\u003e (Common Lisp)\u003c/a\u003e,\n\u003ca href=\"https://github.com/eigenhombre/lexutil\"\u003e\u003ccode\u003elexutil\u003c/code\u003e (Go)\u003c/a\u003e and\n\u003ca href=\"https://github.com/eigenhombre/hbook\"\u003e\u003ccode\u003ehbook\u003c/code\u003e (Common Lisp)\u003c/a\u003e.\u003c/p\u003e\n\u003ch2 id=\"getting-invisible-feedback\"\u003eGetting “Invisible Feedback”\u003c/h2\u003e\n\u003cp\u003eThis trick relates to the “fast feedback” requirement mentioned\nearlier and applies primarily to local development. If you can get\nyour computer to speak out loud, you can run your builds in another\nterminal window and not even look at that window most of the time.\nFor example, on my Mac, I have the following running much of the day:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003econttest \u0026#39;make \u0026amp;\u0026amp; say ok || say fail\u0026#39;\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eHere \u003ca href=\"https://github.com/eigenhombre/conttest\"\u003e\u003ccode\u003econttest\u003c/code\u003e\u003c/a\u003e is a\ncontinuous testing tool I wrote, which runs the supplied argument, a\nshell command, every time the current directory is updated.  When I\nsave a file I’m editing, I instantly hear the result.  If my tests are\ndoing what I think they should be doing in the moment, I can simply\nhear the results as I go, without looking away from the code.  Once\nagain, this little genie is especially helpful when my tests are fast.\u003c/p\u003e\n\u003ch2 id=\"auto-generating-documentation\"\u003eAuto-Generating Documentation\u003c/h2\u003e\n\u003cp\u003eLook for opportunities to automate your documentation. Examples\ninclude \u003ca href=\"https://github.com/eigenhombre/l1/blob/master/l1.md#api-index\"\u003eAPI documentation generated from the source\ncode\u003c/a\u003e,\n\u003ca href=\"https://en.wikipedia.org/wiki/Literate_programming\"\u003eliterate\nprograms\u003c/a\u003e, and\nautomating updates to READMEs based on program output (for example,\nsee \u003ca href=\"https://github.com/eigenhombre/l1/blob/master/updatereadme.py\"\u003ehow the L1 README is\nupdated\u003c/a\u003e).\u003c/p\u003e\n\u003ch1 id=\"summary\"\u003eSummary\u003c/h1\u003e\n\u003cp\u003eAlthough these practices require some up-front investment, I find that\nthey increase my efficiency overall, especially when I’m working with\nother collaborators.  Good, clean operational practices like these can\ncut back on common sources of pain when working on long-running\nsoftware projects.\u003c/p\u003e\n\u003ch1 id=\"acknowledgments\"\u003eAcknowledgments\u003c/h1\u003e\n\u003cp\u003eMy views have been influenced by discussions with former colleagues at\nOppFi and OpinionLab over the past eight years.  \u003ca href=\"https://github.com/timhc22\"\u003eTimothy\nColeman\u003c/a\u003e wrote the original version of the\n\u003ccode\u003ebumpver\u003c/code\u003e release script linked above.\u003c/p\u003e\n\u003cdiv class=\"footnotes\" role=\"doc-endnotes\"\u003e\n\u003chr/\u003e\n\u003col\u003e\n\u003cli id=\"fn:1\"\u003e\n\u003cp\u003eExamples include using protocols in Clojure, generics in Common\nLisp, and interfaces in Go. \u003ca href=\"#fnref:1\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:2\"\u003e\n\u003cp\u003eI prefer to run even Clojure builds in Docker, since I’ve been\nbitten by subtle differences between my local and the target\ndeployment environments). \u003ca href=\"#fnref:2\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:3\"\u003e\n\u003cp\u003eI actually prefer Org Mode for my own writing, but GitHub’s\nMarkdown support is better than its support for Org Mode. \u003ca href=\"#fnref:3\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/div\u003e\n\n\n\n\u003cp\u003e\n\n\nLater: \u003ca href=\"http://johnj.com/posts/2024-blogging-intentions/\"\u003e2024 Blogging Intentions\u003c/a\u003e\n\n\n\u003cbr/\u003e\n\n\nEarlier: \u003ca href=\"http://johnj.com/posts/tco/\"\u003eAdding Tail Call Optimization to A Lisp Written in Go\u003c/a\u003e\n\n\n\u003c/p\u003e\u003cp\u003e\n\n\n\n\u003c/p\u003e\u003c/main\u003e",
  "Date": "2022-09-06T00:00:00Z",
  "Author": "John Jacobsen"
}