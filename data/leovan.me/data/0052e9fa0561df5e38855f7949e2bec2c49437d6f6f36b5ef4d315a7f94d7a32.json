{
  "Source": "leovan.me",
  "Title": "序列到序列 (Seq2Seq) 和注意力机制 (Attention Machanism)",
  "Link": "https://leovan.me/cn/2018/10/seq2seq-and-attention-machanism/",
  "Content": "\u003carticle class=\"main\"\u003e\n    \u003cheader class=\"content-title\"\u003e\n    \n\u003ch1 class=\"title\"\u003e\n  \n  序列到序列 (Seq2Seq) 和注意力机制 (Attention Machanism)\n  \n\u003c/h1\u003e\n\n\n\n\n\n\n\n\u003ch2 class=\"author-date\"\u003e范叶亮 / \n2018-10-12\u003c/h2\u003e\n\n\n\n\u003ch3 class=\"post-meta\"\u003e\n\n\n\u003cstrong\u003e分类: \u003c/strong\u003e\n\u003ca href=\"/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0\"\u003e深度学习\u003c/a\u003e, \u003ca href=\"/categories/%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0\"\u003e表示学习\u003c/a\u003e, \u003ca href=\"/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86\"\u003e自然语言处理\u003c/a\u003e\n\n\n\n\n/\n\n\n\n\n\u003cstrong\u003e标签: \u003c/strong\u003e\n\u003cspan\u003e序列\u003c/span\u003e, \u003cspan\u003eseq2seq\u003c/span\u003e, \u003cspan\u003eSequence to Sequence\u003c/span\u003e, \u003cspan\u003eEncoder\u003c/span\u003e, \u003cspan\u003eDecoder\u003c/span\u003e, \u003cspan\u003eBeam Search\u003c/span\u003e, \u003cspan\u003e注意力机制\u003c/span\u003e, \u003cspan\u003eAttention Mechanism\u003c/span\u003e, \u003cspan\u003e自注意力\u003c/span\u003e, \u003cspan\u003eSelf-attention\u003c/span\u003e, \u003cspan\u003eIntra-attention\u003c/span\u003e\n\n\n\n\n/\n\n\n\u003cstrong\u003e字数: \u003c/strong\u003e\n8159\n\u003c/h3\u003e\n\n\n\n\u003chr/\u003e\n\n\n\n    \n    \n    \u003cins class=\"adsbygoogle\" style=\"display:block; text-align:center;\" data-ad-layout=\"in-article\" data-ad-format=\"fluid\" data-ad-client=\"ca-pub-2608165017777396\" data-ad-slot=\"1261604535\"\u003e\u003c/ins\u003e\n    \u003cscript\u003e\n    (adsbygoogle = window.adsbygoogle || []).push({});\n    \u003c/script\u003e\n    \n    \n    \u003c/header\u003e\n\n\n\n\n\u003ch1 id=\"encoder-decoder-seq2seq\"\u003eEncoder-Decoder \u0026amp; Seq2Seq\u003c/h1\u003e\n\u003cp\u003eEncoder-Decoder 是一种包含两个神经网络的模型，两个网络分别扮演编码器和解码器的角色。Cho 等人 \u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e 提出了一个基于 RNN 的 Encoder-Decoder 神经网络用于机器翻译。网络结构如下图所示：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-10-12-seq2seq-and-attention-machanism/rnn-encoder-decoder.png\" alt=\"RNN-Encoder-Decoder\"/\u003e\u003c/p\u003e\n\u003cp\u003e整个模型包含编码器 (Encoder) 和解码器 (Decoder) 两部分：Encoder 将一个可变长度的序列转换成为一个固定长度的向量表示，Decoder 再将这个固定长度的向量表示转换为一个可变长度的序列。这使得模型可以处理从一个可变长度序列到另一个可变长度序例的转换，即学习到对应的条件概率 \u003ccode\u003e$p \\left(y_1, \\dotsc, y_{T\u0026#39;} | x_1, \\dotsc, x_T\\right)$\u003c/code\u003e，其中 \u003ccode\u003e$T$\u003c/code\u003e 和 \u003ccode\u003e$T\u0026#39;$\u003c/code\u003e 可以为不同的值，也就是说输入和输出的序列的长度不一定相同。\u003c/p\u003e\n\u003cp\u003e在模型中，Encoder 为一个 RNN，逐次读入输入序列 \u003ccode\u003e$\\mathbf{x}$\u003c/code\u003e 中的每个元素，其中 RNN 隐状态的更新方式如下：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\mathbf{h}_{\\langle t \\rangle} = f \\left(\\mathbf{h}_{\\langle t-1 \\rangle}, x_t\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e在读入序列的最后一个元素后 (通常为一个结束标记)，RNN 的隐状态则为整个输入序列的概括信息 \u003ccode\u003e$\\mathbf{c}$\u003c/code\u003e。Decoder 为另一个 RNN，用于根据隐状态 \u003ccode\u003e$\\mathbf{h}\u0026#39;_{\\langle t \\rangle}$\u003c/code\u003e 预测下一个元素 \u003ccode\u003e$y_t$\u003c/code\u003e，从而生成整个输出序列。不同于 Encoder 中的 RNN，Decoder 中 RNN 的隐状态 \u003ccode\u003e$\\mathbf{h}\u0026#39;_{\\langle t \\rangle}$\u003c/code\u003e 除了依赖上一个隐含层的状态和之前的输出外，还依赖整个输入序列的概括信息 \u003ccode\u003e$\\mathbf{c}$\u003c/code\u003e，即：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\mathbf{h}\u0026#39;_{\\langle t \\rangle} = f \\left(\\mathbf{h}\u0026#39;_{\\langle t-1 \\rangle}, y_{t-1}, \\mathbf{c}\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e类似的，下一个输出元素的条件分布为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ P \\left(y_t | y_{t-1}, y_{t-2}, \\dotsc, y_1, \\mathbf{c}\\right) = g \\left(\\mathbf{h}_{\\langle t \\rangle}, y_{t-1}, \\mathbf{c}\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eRNN Encoder-Decoder 的两部分通过最大化如下的对数似然函数的联合训练进行优化：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\max_{\\theta} \\dfrac{1}{N} \\sum_{n=1}^{N}{\\log p_{\\theta} \\left(\\mathbf{y}_n | \\mathbf{x}_n\\right)} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$\\theta$\u003c/code\u003e 为模型的参数，\u003ccode\u003e$\\mathbf{x}_n$\u003c/code\u003e 和 \u003ccode\u003e$\\mathbf{y}_n$\u003c/code\u003e 分别为输入和输出序列的成对样本。当模型训练完毕后，我们可以利用模型根据给定的输入序列生成相应的输出序列，或是根据给定的输入和输出序列对计算概率得分 \u003ccode\u003e$p_{\\theta} \\left(\\mathbf{y} | \\mathbf{x}\\right)$\u003c/code\u003e。同时，作者还提出了一种新的 RNN 单元 GRU (Gated Recurrent Unit)，有关 GRU 的更多介绍请参见 \u003ca href=\"/cn/2018/09/rnn\"\u003e之前的博客\u003c/a\u003e。\u003c/p\u003e\n\u003cp\u003e序列到序列 (Sequence to Sequence, Seq2Seq) 模型从名称中不难看出来是一种用于处理序列数据到序列数据转换问题 (例如：机器翻译等) 的方法。Sutskever 等人 \u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e 提出了一种基于 Encoder-Decoder 网络结构的 Seq2Seq 模型用于机器翻译，网络结构细节同 RNN Encoder-Decoder 略有不同，如下图所示：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-10-12-seq2seq-and-attention-machanism/seq2seq.png\" alt=\"Seq2Seq\"/\u003e\u003c/p\u003e\n\u003cp\u003e模型的相关细节如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e对数据进行预处理，在每个句子的结尾添加特殊字符 \u003ccode\u003e\u0026lt;EOS\u0026gt;\u003c/code\u003e，如上图所示。首先计算 \u003ccode\u003eA, B, C, \u0026lt;EOS\u0026gt;\u003c/code\u003e 的表示，再利用该表示计算 \u003ccode\u003eW, X, Y, Z, \u0026lt;EOS\u0026gt;\u003c/code\u003e 的条件概率。\u003c/li\u003e\n\u003cli\u003e利用两个不同的 LSTM，一个用于输入序列，另一个用于输出序列。\u003c/li\u003e\n\u003cli\u003e选用一个较深的 LSTM 模型 (4 层) 提升模型效果。\u003c/li\u003e\n\u003cli\u003e对输入序列进行倒置处理，例如对于输入序列 \u003ccode\u003e$a, b, c$\u003c/code\u003e 和对应的输出序列 \u003ccode\u003e$\\alpha, \\beta, \\gamma$\u003c/code\u003e，LSTM 需要学习的映射关系为 \u003ccode\u003e$c, b, a \\to \\alpha, \\beta, \\gamma$\u003c/code\u003e。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e在模型的解码阶段，模型采用简单的从左到右的 Beam Search，该方法维护一个大小为 \u003ccode\u003e$B$\u003c/code\u003e 的集合保存最好的结果。下图展示了 \u003ccode\u003e$B = 2$\u003c/code\u003e 情况下 Beam Search 的具体工作方式：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-10-12-seq2seq-and-attention-machanism/beam-search.png\" alt=\"Beam-Search\"/\u003e\u003c/p\u003e\n\u003cp\u003e其中，红色的虚线箭头表示每一步可能的搜索方向，绿色的实线箭头表示每一步概率为 Top \u003ccode\u003e$B$\u003c/code\u003e 的方向。例如，从 S 开始搜索：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e第一步搜索的可能结果为 SA 和 SB，保留 Top 2，结果为 SA 和 SB。\u003c/li\u003e\n\u003cli\u003e第二步搜索的可能结果为 SAC，SAD，SBE 和 SBF，保留 Top 2，结果为 SAC 和 SBE。\u003c/li\u003e\n\u003cli\u003e第三步搜索的可能结果为 SACG，SACH，SBEK 和 SBEL，保留 Top 2，结果为 SACH 和 SBEK。至此，整个搜索结束。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eBahdanau 等人 \u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e 提出了一种基于双向 RNN (Bidirectional RNN, BiRNN) 结合注意力机制 (Attention Mechanism) 的网络结构用于机器翻译。网络结构如下：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-10-12-seq2seq-and-attention-machanism/seq2seq-birnn-attention.png\" alt=\"Seq2Seq-BiRNN-Attention\"/\u003e\u003c/p\u003e\n\u003cp\u003e模型的编码器使用了一个双向的 RNN，前向的 RNN \u003ccode\u003e$\\overrightarrow{f}$\u003c/code\u003e 以从 \u003ccode\u003e$x_1$\u003c/code\u003e 到 \u003ccode\u003e$x_T$\u003c/code\u003e 的顺序读取输入序列并计算前向隐状态 \u003ccode\u003e$\\left(\\overrightarrow{h}_1, \\dotsc, \\overrightarrow{h}_T\\right)$\u003c/code\u003e，后向的 RNN \u003ccode\u003e$\\overleftarrow{f}$\u003c/code\u003e 以从 \u003ccode\u003e$x_T$\u003c/code\u003e 到 \u003ccode\u003e$x_1$\u003c/code\u003e 的顺序读取输入序列并计算后向隐状态 \u003ccode\u003e$\\left(\\overleftarrow{h}_1, \\dotsc, \\overleftarrow{h}_T\\right)$\u003c/code\u003e。对于一个词 \u003ccode\u003e$x_j$\u003c/code\u003e，通过将对应的前向隐状态 \u003ccode\u003e$\\overrightarrow{h}_j$\u003c/code\u003e 和后向隐状态 \u003ccode\u003e$\\overleftarrow{h}_j$\u003c/code\u003e 进行拼接得到最终的隐状态 \u003ccode\u003e$h_j = \\left[\\overrightarrow{h}_j^{\\top}; \\overleftarrow{h}_j^{\\top}\\right]^{\\top}$\u003c/code\u003e。这样的操作使得隐状态 \u003ccode\u003e$h_j$\u003c/code\u003e 既包含了前面词的信息也包含了后面词的信息。\u003c/p\u003e\n\u003cp\u003e在模型的解码器中，对于一个给定的序例 \u003ccode\u003e$\\mathbf{x}$\u003c/code\u003e，每一个输出的条件概率为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ p \\left(y_i | y_1, \\dotsc, y_{i-1}, \\mathbf{x}\\right) = g \\left(y_{i-1}, s_i, c_i\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$s_i$\u003c/code\u003e 为 \u003ccode\u003e$i$\u003c/code\u003e 时刻 RNN 隐含层的状态，即：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ s_i = f \\left(s_{i-1}, y_{i-1}, c_i\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e这里需要注意的是不同于之前的 Encoder-Decoder 模型，此处每一个输出词 \u003ccode\u003e$y_i$\u003c/code\u003e 的条件概率均依赖于一个单独的上下文向量 \u003ccode\u003e$c_i$\u003c/code\u003e。该部分的改进即结合了注意力机制，有关注意力机制的详细内容将在下个小节中展开说明。\u003c/p\u003e\n\u003ch1 id=\"注意力机制-attention-mechanism\"\u003e注意力机制 (Attention Mechanism)\u003c/h1\u003e\n\u003cp\u003eBahdanau 等人在文中 \u003csup id=\"fnref1:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e 提出传统的 Encoder-Decoder 模型将输入序列压缩成一个固定长度的向量 \u003ccode\u003e$c$\u003c/code\u003e，但当输入的序例很长时，尤其是当比训练集中的语料还长时，模型的的效果会显著下降。针对这个问题，如上文所述，上下文向量 \u003ccode\u003e$c_i$\u003c/code\u003e 依赖于 \u003ccode\u003e$\\left(h_1, \\dotsc, h_T\\right)$\u003c/code\u003e。其中，每个 \u003ccode\u003e$h_i$\u003c/code\u003e 都包含了整个序列的信息，同时又会更多地关注第 \u003ccode\u003e$i$\u003c/code\u003e 个词附近的信息。对于 \u003ccode\u003e$c_i$\u003c/code\u003e，计算方式如下：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ c_i = \\sum_{j=1}^{T}{\\alpha_{ij} h_j} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e对于每个 \u003ccode\u003e$h_j$\u003c/code\u003e 的权重 \u003ccode\u003e$\\alpha_{ij}$\u003c/code\u003e，计算方式如下：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\alpha_{ij} = \\dfrac{\\exp \\left(e_{ij}\\right)}{\\sum_{k=1}^{T}{\\exp \\left(e_{ik}\\right)}} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$e_{ij} = a \\left(s_{i-1}, h_j\\right)$\u003c/code\u003e 为一个 Alignment 模型，用于评价对于输入的位置 \u003ccode\u003e$j$\u003c/code\u003e 附近的信息与输出的位置 \u003ccode\u003e$i$\u003c/code\u003e 附近的信息的匹配程度。Alignment 模型 \u003ccode\u003e$a$\u003c/code\u003e 为一个用于评分的前馈神经网络，与整个模型进行联合训练，计算方式如下：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ a \\left(s_{i-1}, h_j\\right) = v_a^{\\top} \\tanh \\left(W_a s_{i-1} + U_a h_j\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$W_a \\in \\mathbb{R}^{n \\times n}, U_a \\in \\mathbb{R}^{n \\times 2n}，v_a \\in \\mathbb{R}^n$\u003c/code\u003e 为网络的参数。\u003c/p\u003e\n\u003ch2 id=\"hard-soft-attention\"\u003eHard \u0026amp; Soft Attention\u003c/h2\u003e\n\u003cp\u003eXu 等人 \u003csup id=\"fnref:4\"\u003e\u003ca href=\"#fn:4\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e4\u003c/a\u003e\u003c/sup\u003e 在图像标题生成 (Image Caption Generation) 任务中引入了注意力机制。在文中作者提出了 Hard Attenttion 和 Soft Attention 两种不同的注意力机制。\u003c/p\u003e\n\u003cp\u003e对于 Hard Attention 而言，令 \u003ccode\u003e$s_t$\u003c/code\u003e 表示在生成第 \u003ccode\u003e$t$\u003c/code\u003e 个词时所关注的位置变量，\u003ccode\u003e$s_{t, i} = 1$\u003c/code\u003e 表示当第 \u003ccode\u003e$i$\u003c/code\u003e 个位置用于提取视觉特征。将注意力位置视为一个中间潜变量，可以以一个参数为 \u003ccode\u003e$\\left\\{\\alpha_i\\right\\}$\u003c/code\u003e 的多项式分布表示，同时将上下文向量 \u003ccode\u003e$\\hat{\\mathbf{z}}_t$\u003c/code\u003e 视为一个随机变量：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{equation} \\begin{split} \u0026amp; p \\left(s_{t, i} = 1 | s_{j \u0026lt; t}, \\mathbf{a}\\right) = \\alpha_{t, i} \\\\ \u0026amp; \\hat{\\mathbf{z}}_t = \\sum_{i}{s_{t, i} \\mathbf{a}_i} \\end{split} \\end{equation} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e因此 Hard Attention 可以依据概率值从隐状态中进行采样计算得到上下文向量，同时为了实现梯度的反向传播，需要利用蒙特卡罗采样的方法来估计梯度。\u003c/p\u003e\n\u003cp\u003e对于 Soft Attention 而言，则直接计算上下文向量 \u003ccode\u003e$\\hat{\\mathbf{z}}_t$\u003c/code\u003e 的期望，计算方式如下：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\mathbb{E}_{p \\left(s_t | a\\right)} \\left[\\hat{\\mathbf{z}}_t\\right] = \\sum_{i=1}^{L}{\\alpha_{t, i} \\mathbf{a}_i} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其余部分的计算方式同 Bahdanau 等人 \u003csup id=\"fnref2:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e 的论文类似。Soft Attention 模型可以利用标准的反向传播算法进行求解，直接嵌入到整个模型中一同训练，相对更加简单。\u003c/p\u003e\n\u003cp\u003e下图展示了一些图片标题生成结果的可视化示例，其中图片内 \u003cspan style=\"background-color:#000; color:#FFF; font-style:bold;\"\u003e白色\u003c/span\u003e 为关注的区域，\u003cspan style=\"border-bottom:2px solid;\"\u003e画线的文本\u003c/span\u003e 即为生成的标题中对应的词。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-10-12-seq2seq-and-attention-machanism/image-caption-generation-visual-attention.png\" alt=\"Image-Caption-Generation-Visual-Attention\"/\u003e\u003c/p\u003e\n\u003ch2 id=\"global-local-attention\"\u003eGlobal \u0026amp; Local Attention\u003c/h2\u003e\n\u003cp\u003eLuong 等人 \u003csup id=\"fnref:5\"\u003e\u003ca href=\"#fn:5\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e5\u003c/a\u003e\u003c/sup\u003e 提出了 Global Attention 和 Local Attention 两种不同的注意力机制用于机器翻译。Global Attention 的思想是在计算上下文向量 \u003ccode\u003e$c_t$\u003c/code\u003e 时将编码器的所有隐状态均考虑在内。对于对齐向量 \u003ccode\u003e$\\boldsymbol{a}_t$\u003c/code\u003e，通过比较当前目标的隐状态 \u003ccode\u003e$\\boldsymbol{h}_t$\u003c/code\u003e 与每一个输入的隐状态 \u003ccode\u003e$\\bar{\\boldsymbol{h}}_s$\u003c/code\u003e 得到，即：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{equation} \\begin{split} \\boldsymbol{a}_t \u0026amp;= \\text{align} \\left(\\boldsymbol{h}_t, \\bar{\\boldsymbol{h}}_s\\right) \\\\ \u0026amp;= \\dfrac{\\exp \\left(\\text{score} \\left(\\boldsymbol{h}_t, \\bar{\\boldsymbol{h}}_s\\right)\\right)}{\\sum_{s\u0026#39;}{\\exp \\left(\\text{score} \\left(\\boldsymbol{h}_t, \\bar{\\boldsymbol{h}}_{s\u0026#39;}\\right)\\right)}} \\end{split} \\end{equation} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中 \u003ccode\u003e$\\text{score}$\u003c/code\u003e 为一个基于内容 (content-based) 的函数，可选的考虑如下三种形式：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\text{score} \\left(\\boldsymbol{h}_t, \\bar{\\boldsymbol{h}}_s\\right) = \\begin{cases} \\boldsymbol{h}_t^{\\top} \\bar{\\boldsymbol{h}}_s \u0026amp; dot \\\\ \\boldsymbol{h}_t^{\\top} \\boldsymbol{W}_a \\bar{\\boldsymbol{h}}_s \u0026amp; general \\\\ \\boldsymbol{W}_a \\left[\\boldsymbol{h}_t; \\bar{\\boldsymbol{h}}_s\\right] \u0026amp; concat \\end{cases} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e我们利用一个基于位置 (location-based) 的函数构建注意力模型，其中对齐分数通过目标的隐状态计算得到：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\boldsymbol{a}_t = \\text{softmax} \\left(\\boldsymbol{W}_a \\boldsymbol{h}_t\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eGlobal Attention 模型的网络结构如下所示：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-10-12-seq2seq-and-attention-machanism/global-attention.png\" alt=\"Global-Attention\"/\u003e\u003c/p\u003e\n\u003cp\u003eGlobal Attention 的一个问题在于任意一个输出都需要考虑输入端的所有隐状态，这对于很长的文本 (例如：一个段落或一篇文章) 计算量太大。Local Attention 为了解决这个问题，首先在 \u003ccode\u003e$t$\u003c/code\u003e 时刻对于每个目标词生成一个对齐位置 \u003ccode\u003e$p_t$\u003c/code\u003e，其次上下文向量 \u003ccode\u003e$\\boldsymbol{c}_t$\u003c/code\u003e 则由以 \u003ccode\u003e$p_t$\u003c/code\u003e 为中心前后各 \u003ccode\u003e$D$\u003c/code\u003e 大小的窗口 \u003ccode\u003e$\\left[p_t - D, p_t + D\\right]$\u003c/code\u003e 内的输入的隐状态计算得到。不同于 Global Attention，Local Attention 的对齐向量 \u003ccode\u003e$\\boldsymbol{a}_t \\in \\mathbb{R}^{2D + 1}$\u003c/code\u003e 为固定维度。\u003c/p\u003e\n\u003cp\u003e一个比较简单的做法是令 \u003ccode\u003e$p_t = t$\u003c/code\u003e，也就是假设输入和输出序列是差不多是单调对齐的，我们称这种做法为 \u003cem\u003eMonotonic\u003c/em\u003e Alignment (\u003cstrong\u003elocal-m\u003c/strong\u003e)。另一种做法是预测 \u003ccode\u003e$p_t$\u003c/code\u003e，即：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ p_t = S \\cdot \\text{sigmoid} \\left(\\boldsymbol{v}_p^{\\top} \\tanh \\left(\\boldsymbol{W}_p \\boldsymbol{h}_t\\right)\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$\\boldsymbol{W}_p$\u003c/code\u003e 和 \u003ccode\u003e$\\boldsymbol{h}_t$\u003c/code\u003e 为预测位置模型的参数，\u003ccode\u003e$S$\u003c/code\u003e 为输入句子的长度。我们称这种做法为 \u003cem\u003ePredictive\u003c/em\u003e Alignment (\u003cstrong\u003elocal-p\u003c/strong\u003e)。作为 \u003ccode\u003e$\\text{sigmoid}$\u003c/code\u003e 函数的结果，\u003ccode\u003e$p_t \\in \\left[0, S\\right]$\u003c/code\u003e，则通过一个以 \u003ccode\u003e$p_t$\u003c/code\u003e 为中心的高斯分布定义对齐权重：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\boldsymbol{a}_t \\left(s\\right) = \\text{align} \\left(\\boldsymbol{h}_t, \\bar{\\boldsymbol{h}}_s\\right) \\exp \\left(- \\dfrac{\\left(s - p_t\\right)^2}{2 \\sigma^2}\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，根据经验设置 \u003ccode\u003e$\\sigma = \\dfrac{D}{2}$\u003c/code\u003e，\u003ccode\u003e$s$\u003c/code\u003e 为在窗口大小内的一个整数。\u003c/p\u003e\n\u003cp\u003eLocal Attention 模型的网络结构如下所示：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-10-12-seq2seq-and-attention-machanism/local-attention.png\" alt=\"Local-Attention\"/\u003e\u003c/p\u003e\n\u003ch2 id=\"self-attention\"\u003eSelf Attention\u003c/h2\u003e\n\u003cp\u003eVaswani 等人 \u003csup id=\"fnref:6\"\u003e\u003ca href=\"#fn:6\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e6\u003c/a\u003e\u003c/sup\u003e 提出了一种新的网络结构，称之为 Transformer，其中采用了自注意力 (Self-attention) 机制。自注意力是一种将同一个序列的不同位置进行自我关联从而计算一个句子表示的机制。Transformer 利用堆叠的 Self Attention 和全链接网络构建编码器 (下图左) 和解码器 (下图右)，整个网络架构如下图所示：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-10-12-seq2seq-and-attention-machanism/self-attention.png\" alt=\"Self-Attention\"/\u003e\u003c/p\u003e\n\u003ch3 id=\"编码器和解码器\"\u003e编码器和解码器\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e编码器\u003c/strong\u003e 是由 \u003ccode\u003e$N = 6$\u003c/code\u003e 个相同的网络层构成，每层中包含两个子层。第一层为一个 Multi-Head Self-Attention 层，第二层为一个 Position-Wise 全链接的前馈神经网络。每一层再应用一个残差连接 (Residual Connection) \u003csup id=\"fnref:7\"\u003e\u003ca href=\"#fn:7\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e7\u003c/a\u003e\u003c/sup\u003e 和一个层标准化 (Layer Normalization) \u003csup id=\"fnref:8\"\u003e\u003ca href=\"#fn:8\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e8\u003c/a\u003e\u003c/sup\u003e。则每一层的输出为 \u003ccode\u003e$\\text{LayerNorm} \\left(x + \\text{Sublayer} \\left(x\\right)\\right)$\u003c/code\u003e，其中 \u003ccode\u003e$\\text{Sublayer} \\left(x\\right)$\u003c/code\u003e 为子层本身的函数实现。为了实现残差连接，模型中所有的子层包括 Embedding 层的输出维度均为 \u003ccode\u003e$d_{\\text{model}} = 512$\u003c/code\u003e。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e解码器\u003c/strong\u003e 也是由 \u003ccode\u003e$N = 6$\u003c/code\u003e 个相同的网络层构成，但每层中包含三个子层，增加的第三层用于处理编码器的输出。同编码器一样，每一层应用一个残差连接和一个层标准化。除此之外，解码器对 Self-Attention 层进行了修改，确保对于位置 \u003ccode\u003e$i$\u003c/code\u003e 的预测仅依赖于位置在 \u003ccode\u003e$i$\u003c/code\u003e 之前的输出。\u003c/p\u003e\n\u003ch3 id=\"scaled-dot-product-multi-head-attention\"\u003eScaled Dot-Product \u0026amp; Multi-Head Attention\u003c/h3\u003e\n\u003cp\u003e一个 Attention 函数可以理解为从一个序列 (Query) 和一个键值对集合 (Key-Value Pairs Set) 到一个输出的映射。文中提出了一种名为 \u003cstrong\u003eScaled Dot-Product Attention\u003c/strong\u003e (如下图所示)，其中输入包括 queries，维度为 \u003ccode\u003e$d_k$\u003c/code\u003e 的 keys 和维度为 \u003ccode\u003e$d_v$\u003c/code\u003e 的 values。通过计算 queries 和所有 keys 的点积，除以 \u003ccode\u003e$\\sqrt{d_k}$\u003c/code\u003e，再应用一个 softmax 函数获取 values 的权重。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-10-12-seq2seq-and-attention-machanism/scaled-dot-product-attention.png\" alt=\"Scaled-Dot-Product-Attention\"/\u003e\u003c/p\u003e\n\u003cp\u003e实际中，我们会同时计算一个 Queries 集合中的 Attention，并将其整合成一个矩阵 \u003ccode\u003e$Q$\u003c/code\u003e。Keys 和 Values 也相应的整合成矩阵 \u003ccode\u003e$K$\u003c/code\u003e 和 \u003ccode\u003e$V$\u003c/code\u003e，则有：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\text{Attention} \\left(Q, K, V\\right) = \\text{softmax} \\left(\\dfrac{Q K^{\\top}}{\\sqrt{d_k}}\\right) V $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$Q \\in \\mathbb{R}^{n \\times d_k}$\u003c/code\u003e，\u003ccode\u003e$Q$\u003c/code\u003e 中的每一行为一个 query，\u003ccode\u003e$K \\in \\mathbb{R}^{n \\times d_k}, V \\in \\mathbb{R}^{n \\times d_v}$\u003c/code\u003e。\u003ccode\u003e$\\dfrac{1}{\\sqrt{d_k}}$\u003c/code\u003e 为一个归一化因子，避免点积的值过大导致 softmax 之后的梯度过小。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003eMulti-Head Attention\u003c/strong\u003e 的做法并不直接对原始的 keys，values 和 queries 应用注意力函数，而是学习一个三者各自的映射再应用 Atteneion，同时将这个过程重复 \u003ccode\u003e$h$\u003c/code\u003e 次。Multi-Head Attention 的网路结构如下图所示：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-10-12-seq2seq-and-attention-machanism/multi-head-attention.png\" alt=\"Multi-Head-Attention\"/\u003e\u003c/p\u003e\n\u003cp\u003eMulti-Head Attention 的计算过程如下所示：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{equation} \\begin{split} \\text{MultiHead} \\left(Q, K, V\\right) \u0026amp;= \\text{Concat} \\left(\\text{head}_1, \\dotsc, \\text{head}_h\\right) W^O \\\\ \\textbf{where } \\text{head}_i \u0026amp;= \\text{Attention} \\left(QW_i^Q, KW_i^K, VW_i^V\\right) \\end{split} \\end{equation} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$W_i^Q \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}, W_i^K \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}, W_i^V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_v}, W_i^O \\in \\mathbb{R}^{h d_v \\times d_{\\text{model}}}, $\u003c/code\u003e 为映射的参数，\u003ccode\u003e$h = 8$\u003c/code\u003e 为重复的次数，则有 \u003ccode\u003e$d_k = d_v = d_{\\text{model}} / h = 64$\u003c/code\u003e。\u003c/p\u003e\n\u003cp\u003e整个 Transformer 模型在三处使用了 Multi-Head Attention，分别是：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eEncoder-Decoder Attention Layers，其中 queries 来自于之前的 Decoder 层，keys 和 values 来自于 Encoder 的输出，该部分同其他 Seq2Seq 模型的 Attention 机制类似。\u003c/li\u003e\n\u003cli\u003eEncoder Self-Attention Layers，其中 queries，keys 和 values 均来自之前的 Encoder 层的输出，同时 Encoder 层中的每个位置都能够从之前层的所有位置获取到信息。\u003c/li\u003e\n\u003cli\u003eDecoder Self-Attention Layers，其中 queries，keys 和 values 均来自之前的 Decoder 层的输出，但 Decoder 层中的每个位置仅可以从之前网络层的包含当前位置之前的位置获取信息。\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"position-wise-feed-forward-networks\"\u003ePosition-wise Feed-Forward Networks\u003c/h3\u003e\n\u003cp\u003e在 Encoder 和 Decoder 中的每一层均包含一个全链接的前馈神经网络，其使用两层线性变换和一个 ReLU 激活函数实现：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\text{FFN} \\left(x\\right) = \\max \\left(0, x W_1 + b_1\\right) W_2 + b_2 $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e全链接层的输入和输出的维度 \u003ccode\u003e$d_{\\text{model}} = 512$\u003c/code\u003e，内层的维度 \u003ccode\u003e$d_{ff} = 2048$\u003c/code\u003e。\u003c/p\u003e\n\u003ch3 id=\"positional-encoding\"\u003ePositional Encoding\u003c/h3\u003e\n\u003cp\u003eTransformer 模型由于未使用任何循环和卷积组件，因此为了利用序列的位置信息则在模型的 Embedding 输入中添加了 \u003cstrong\u003ePosition Encoding\u003c/strong\u003e。Position Encoding 的维度同 Embedding 的维度相同，从而可以与 Embedding 进行加和，文中使用了如下两种形式：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{equation} \\begin{split} PE_{\\left(pos, 2i\\right)} \u0026amp;= \\sin \\left(pos / 10000^{2i / d_{\\text{model}}}\\right) \\\\ PE_{\\left(pos, 2i+1\\right)} \u0026amp;= \\cos \\left(pos / 10000^{2i / d_{\\text{model}}}\\right) \\end{split} \\end{equation} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$pos$\u003c/code\u003e 为位置，\u003ccode\u003e$i$\u003c/code\u003e 为对应的维度，选用这种表示形式的原因是对于一个固定的偏移 \u003ccode\u003e$k$\u003c/code\u003e，\u003ccode\u003e$PE_{pos + k}$\u003c/code\u003e 都可以利用 \u003ccode\u003e$PE_{pos}$\u003c/code\u003e 线性表示。这是因为对于正弦和余弦函数有：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{equation} \\begin{split} \\sin \\left(\\alpha + \\beta\\right) \u0026amp;= \\sin \\alpha \\cos \\beta + \\cos \\alpha \\sin \\beta \\\\ \\cos \\left(\\alpha + \\beta\\right) \u0026amp;= \\cos \\alpha \\sin \\beta - \\sin \\alpha \\sin \\beta \\end{split} \\end{equation} $$\u003c/code\u003e\u003c/p\u003e\n\u003ch3 id=\"why-self-attention\"\u003eWhy Self-Attention\u003c/h3\u003e\n\u003cp\u003e相比于循环和卷积层，Transformer 模型利用 Self-Attention 层用于一个序列 \u003ccode\u003e$\\left(x_1, \\dotsc, x_n\\right)$\u003c/code\u003e 到另一个等长序例 \u003ccode\u003e$\\left(z_1, \\dotsc, z_n\\right)$\u003c/code\u003e 的映射，其中 \u003ccode\u003e$x_i, z_i \\in \\mathbb{R}^d$\u003c/code\u003e。Self-Attention 与循环和卷积的对比如下表所示：\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e层类型\u003c/th\u003e\n\u003cth\u003e每层的复杂度\u003c/th\u003e\n\u003cth\u003e序列操作数\u003c/th\u003e\n\u003cth\u003e长距离依赖路径长度\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eSelf-Attention\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003e$O \\left(n^2 \\cdot d\\right)$\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003e$O \\left(1\\right)$\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003e$O \\left(1\\right)$\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eRecurrent\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003e$O \\left(n \\cdot d^2\\right)$\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003e$O \\left(n\\right)$\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003e$O \\left(n\\right)$\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eConvolutional\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003e$O \\left(k \\cdot n \\cdot d^2\\right)$\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003e$O \\left(1\\right)$\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003e$O \\left(\\log_k \\left(n\\right)\\right)$\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eSelf-Attention (restricted)\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003e$O \\left(r \\cdot n \\cdot d\\right)$\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003e$O \\left(1\\right)$\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003e$O \\left(n/r\\right)$\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003col\u003e\n\u003cli\u003e对于每层的复杂度，当序例的长度 \u003ccode\u003e$n$\u003c/code\u003e 比表示的维度 \u003ccode\u003e$d$\u003c/code\u003e 小时，Self-Attention 要比循环结构计算复杂度小。为了改进在长序列上 Self-Attention 的计算性能，Self-Attention 可以被限制成仅考虑与输出位置对应的输入序列位置附近 \u003ccode\u003e$r$\u003c/code\u003e 窗口大小内的信息。\u003c/li\u003e\n\u003cli\u003eRecurrent 层的最小序列操作数为 \u003ccode\u003e$O \\left(n\\right)$\u003c/code\u003e，其他情况为 \u003ccode\u003e$O \\left(1\\right)$\u003c/code\u003e，这使得 Recurrent 的并行能力较差，即上表中的 Self-Attention (restricted)。\u003c/li\u003e\n\u003cli\u003e学习到长距离依赖是很多序列任务的关键，影响该能力的一个重要因素就是前向和后向信号穿越整个网络的路径长度，这个路径长度越短，越容易学习到长距离依赖。\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"attention-visualizations\"\u003eAttention Visualizations\u003c/h3\u003e\n\u003cp\u003e第一张图展示了 Self-Attention 学到的句子内部的一个长距离依赖 \u003cstrong\u003e“making … more diffcult”\u003c/strong\u003e，图中不同的颜色表示不同 Head 的 Attention，颜色越深表示 Attention 的值越大。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-10-12-seq2seq-and-attention-machanism/self-attention-long-distance-dependencies.png\" alt=\"Self-Attention-Long-Distance-Dependencies\"/\u003e\u003c/p\u003e\n\u003cp\u003e第二张图展示了 Self-Attention 学到的一个指代消解关系 (Anaphora Resolution)，its 指代的为上文中的 law。下图 (上) 为 Head 5 的所有 Attention，下图 (下) 为 Head 5 和 6 关于词 its 的 Attention，不难看出模型学习到了 its 和 law 之间的依赖关系 (指代消解关系)。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-10-12-seq2seq-and-attention-machanism/self-attention-anaphora-resolution.png\" alt=\"Self-Attention-Long-Anaphora-Resolution\"/\u003e\u003c/p\u003e\n\u003ch2 id=\"hierarchical-attention\"\u003eHierarchical Attention\u003c/h2\u003e\n\u003cp\u003eYang 等人 \u003csup id=\"fnref:9\"\u003e\u003ca href=\"#fn:9\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e9\u003c/a\u003e\u003c/sup\u003e 提出了一种层级的注意力 (Hierarchical Attention) 网络用于文档分类。Hierarchical Attention 共包含 4 层：一个词编码器 (Word Encoder)，一个词级别的注意力层 (Word Attention)，一个句子编码器 (Sentence Encoder) 和一个句子级别的注意力层 (Sentence Attention)。网络架构如下图所示：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-10-12-seq2seq-and-attention-machanism/hierarchical-attention.png\" alt=\"Hierarchical-Attention\"/\u003e\u003c/p\u003e\n\u003ch3 id=\"word-encoder\"\u003eWord Encoder\u003c/h3\u003e\n\u003cp\u003e对于一个给定的句子 \u003ccode\u003e$w_{it}, t \\in \\left[0, T\\right]$\u003c/code\u003e，通过一个 Embedding 矩阵 \u003ccode\u003e$W_e$\u003c/code\u003e 得到每个词的向量表示，再应用一个双向的 GRU，即：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{equation} \\begin{split} x_{it} \u0026amp;= W_e w_{it}, t \\in \\left[1, T\\right] \\\\ \\overrightarrow{h}_{it} \u0026amp;= \\overrightarrow{\\text{GRU}} \\left(x_{it}\\right), t \\in \\left[1, T\\right] \\\\ \\overleftarrow{h}_{it} \u0026amp;= \\overleftarrow{\\text{GRU}} \\left(x_{it}\\right), t \\in \\left[T, 1\\right] \\end{split} \\end{equation} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e最后将前向的隐状态 \u003ccode\u003e$\\overrightarrow{h}_{it}$\u003c/code\u003e 和后向的隐状态 \u003ccode\u003e$\\overleftarrow{h}_{it}$\u003c/code\u003e 进行拼接，得到 \u003ccode\u003e$h_{ij} = \\left[\\overrightarrow{h}_{it}, \\overleftarrow{h}_{it}\\right]$\u003c/code\u003e 为整个句子在词 \u003ccode\u003e$w_{ij}$\u003c/code\u003e 附近的汇总信息。\u003c/p\u003e\n\u003ch3 id=\"word-attention\"\u003eWord Attention\u003c/h3\u003e\n\u003cp\u003eWord Attention 同一般的 Attention 机制类似，计算方式如下：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{equation} \\begin{split} u_{it} \u0026amp;= \\tanh \\left(W_w h_{it} + b_w\\right) \\\\ a_{it} \u0026amp;= \\dfrac{\\exp \\left(u_{it}^{\\top} u_w\\right)}{\\sum_{t}{\\exp \\left(u_{it}^{\\top} u_w\\right)}} \\\\ s_i \u0026amp;= \\sum_{t}{a_{it} h_{it}} \\end{split} \\end{equation} $$\u003c/code\u003e\u003c/p\u003e\n\u003ch3 id=\"sentence-encoder\"\u003eSentence Encoder\u003c/h3\u003e\n\u003cp\u003e在 Word Attention 之后，我们得到了一个句子的表示 \u003ccode\u003e$s_i$\u003c/code\u003e，类似的我们利用一个双向的 GRU 编码文档中的 \u003ccode\u003e$L$\u003c/code\u003e 个句子：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{equation} \\begin{split} \\overrightarrow{h}_i \u0026amp;= \\overrightarrow{\\text{GRU}} \\left(s_i\\right), i \\in \\left[1, L\\right] \\\\ \\overleftarrow{h}_i \u0026amp;= \\overleftarrow{\\text{GRU}} \\left(s_i\\right), i \\in \\left[L, 1\\right] \\end{split} \\end{equation} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e最后将前向的隐状态 \u003ccode\u003e$\\overrightarrow{h}_i$\u003c/code\u003e 和后向的隐状态 \u003ccode\u003e$\\overleftarrow{h}_i$\u003c/code\u003e 进行拼接，得到 \u003ccode\u003e$h_i = \\left[\\overrightarrow{h}_i, \\overleftarrow{h}_i\\right]$\u003c/code\u003e 为整个文档关于句子 \u003ccode\u003e$s_i$\u003c/code\u003e 的注意力汇总信息。\u003c/p\u003e\n\u003ch3 id=\"sentence-attention\"\u003eSentence Attention\u003c/h3\u003e\n\u003cp\u003e同理可得 Sentence Attention 的计算方式如下：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{equation} \\begin{split} u_i \u0026amp;= \\tanh \\left(W_s h_i + b_s\\right) \\\\ a_i \u0026amp;= \\dfrac{\\exp \\left(u_i^{\\top} u_s\\right)}{\\sum_{i}{\\exp \\left(u_i^{\\top} u_s\\right)}} \\\\ v \u0026amp;= \\sum_{i}{a_i h_i} \\end{split} \\end{equation} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e最终得到整个文档的向量表示 \u003ccode\u003e$v$\u003c/code\u003e。\u003c/p\u003e\n\u003ch2 id=\"attention-over-attention\"\u003eAttention-over-Attention\u003c/h2\u003e\n\u003cp\u003eCui 等人 \u003csup id=\"fnref:10\"\u003e\u003ca href=\"#fn:10\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e10\u003c/a\u003e\u003c/sup\u003e 提出了 Attention-over-Attention 的模型用于阅读理解 (Reading Comprehension)。网络结构如下图所示：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-10-12-seq2seq-and-attention-machanism/attention-over-attention.png\" alt=\"Attention-over-Attention\"/\u003e\u003c/p\u003e\n\u003cp\u003e对于一个给定的训练集 \u003ccode\u003e$\\langle \\mathcal{D}, \\mathcal{Q}, \\mathcal{A} \\rangle$\u003c/code\u003e，模型包含两个输入，一个文档 (Document) 和一个问题序列 (Query)。网络的工作流程如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e先获取 Document 和 Query 的 Embedding 结果，再应用一个双向的 GRU 得到对应的隐状态 \u003ccode\u003e$h_{doc}$\u003c/code\u003e 和 \u003ccode\u003e$h_{query}$\u003c/code\u003e。\u003c/li\u003e\n\u003cli\u003e计算一个 Document 和 Query 的匹配程度矩阵 \u003ccode\u003e$M \\in \\mathbb{R}^{\\lvert \\mathcal{D} \\rvert \\times \\lvert \\mathcal{Q} \\rvert}$\u003c/code\u003e，其中第 \u003ccode\u003e$i$\u003c/code\u003e 行第 \u003ccode\u003e$j$\u003c/code\u003e 列的值计算方式如下：\n\u003ccode\u003e$$ M \\left(i, j\\right) = h_{doc} \\left(i\\right)^{\\top} \\cdot h_{query} \\left(j\\right) $$\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e按照 \u003cstrong\u003e列\u003c/strong\u003e 的方向对矩阵 \u003ccode\u003e$M$\u003c/code\u003e 应用 softmax 函数，矩阵中的每一列为考虑一个 Query 中的词的 Document 级别的 Attention，因此定义 \u003ccode\u003e$\\alpha \\left(t\\right) \\in \\mathbb{R}^{\\lvert \\mathcal{D} \\rvert}$\u003c/code\u003e 为 \u003ccode\u003e$t$\u003c/code\u003e 时刻的 Document 级别 Attention (\u003cstrong\u003e\u003cem\u003equery-to-document\u003c/em\u003e attention\u003c/strong\u003e)。计算方式如下：\n\u003ccode\u003e$$ \\begin{equation} \\begin{split} \\alpha \\left(t\\right) \u0026amp;= \\text{softmax} \\left(M \\left(1, t\\right), \\dotsc, M \\left(\\lvert \\mathcal{D} \\rvert, t\\right)\\right) \\\\ \\alpha \u0026amp;= \\left[\\alpha \\left(1\\right), \\alpha \\left(2\\right), \\dotsc, \\alpha \\left(\\lvert \\mathcal{Q} \\rvert\\right)\\right] \\end{split} \\end{equation} $$\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e同理按照 \u003cstrong\u003e行\u003c/strong\u003e 的方向对矩阵 \u003ccode\u003e$M$\u003c/code\u003e 应用 softmax 函数，可以得到 \u003ccode\u003e$\\beta \\left(t\\right) \\in \\mathbb{R}^{\\lvert \\mathcal{Q} \\rvert}$\u003c/code\u003e 为 \u003ccode\u003e$t$\u003c/code\u003e 时刻的 Query 级别的 Attention (\u003cstrong\u003e\u003cem\u003edocument-to-query\u003c/em\u003e attention\u003c/strong\u003e)。计算方式如下：\n\u003ccode\u003e$$ \\beta \\left(t\\right) = \\text{softmax} \\left(M \\left(t, 1\\right), \\dotsc, M \\left(t, \\lvert \\mathcal{Q} \\rvert\\right)\\right) $$\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e对于 document-to-query attention，我们对结果进行平均得到：\n\u003ccode\u003e$$ \\beta = \\dfrac{1}{n} \\sum_{t=1}^{\\lvert \\mathcal{D} \\rvert}{\\beta \\left(t\\right)} $$\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e最终利用 \u003ccode\u003e$\\alpha$\u003c/code\u003e 和 \u003ccode\u003e$\\beta$\u003c/code\u003e 的点积 \u003ccode\u003e$s = \\alpha^{\\top} \\beta \\in \\mathbb{R}^{\\lvert \\mathcal{D} \\rvert}$\u003c/code\u003e 得到 attended document-level attention (即 \u003cstrong\u003e\u003cem\u003eattention-over-attention\u003c/em\u003e\u003c/strong\u003e)。\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"multi-step-attention\"\u003eMulti-step Attention\u003c/h2\u003e\n\u003cp\u003eGehring 等人 \u003csup id=\"fnref:11\"\u003e\u003ca href=\"#fn:11\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e11\u003c/a\u003e\u003c/sup\u003e 提出了基于 CNN 和 Multi-step Attention 的模型用于机器翻译。网络结构如下图所示：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-10-12-seq2seq-and-attention-machanism/multi-step-attention.png\" alt=\"Multi-step-Attention\"/\u003e\u003c/p\u003e\n\u003ch3 id=\"position-embeddings\"\u003ePosition Embeddings\u003c/h3\u003e\n\u003cp\u003e模型首先得到序列 \u003ccode\u003e$\\mathbf{x} = \\left(x_1, \\dotsc, x_m\\right)$\u003c/code\u003e 的 Embedding \u003ccode\u003e$\\mathbf{w} = \\left(w_1, \\dotsc , w_m\\right), w_j \\in \\mathbb{R}^f$\u003c/code\u003e。除此之外还将输入序列的位置信息映射为 \u003ccode\u003e$\\mathbf{p} = \\left(p_1, \\dotsc, p_m\\right), p_j \\in \\mathbb{R}^f$\u003c/code\u003e，最终将两者进行合并得到最终的输入 \u003ccode\u003e$\\mathbf{e} = \\left(w_1 + p_1, \\dotsc, w_m + p_m\\right)$\u003c/code\u003e。同时在解码器部分也采用类似的操作，将其与解码器网络的输出表示合并之后再喂入解码器网络 \u003ccode\u003e$\\mathbf{g} = \\left(g_1, \\dotsc, g_n\\right)$\u003c/code\u003e 中。\u003c/p\u003e\n\u003ch3 id=\"convolutional-block-structure\"\u003eConvolutional Block Structure\u003c/h3\u003e\n\u003cp\u003e编码器和解码器均由多个 Convolutional Block 构成，每个 Block 包含一个卷积计算和一个非线性计算。令 \u003ccode\u003e$\\mathbf{h}^l = \\left(h_1^l, \\dotsc, h_n^l\\right)$\u003c/code\u003e 表示解码器第 \u003ccode\u003e$l$\u003c/code\u003e 个 Block 的输出，\u003ccode\u003e$\\mathbf{z}^l = \\left(z_1^l, \\dotsc, z_m^l\\right)$\u003c/code\u003e 表示编码器第 \u003ccode\u003e$l$\u003c/code\u003e 个 Block 的输出。对于一个大小 \u003ccode\u003e$k = 5$\u003c/code\u003e 的卷积核，其结果的隐状态包含了这 5 个输入，则对于一个 6 层的堆叠结构，结果的隐状态则包含了输入中的 25 个元素。\u003c/p\u003e\n\u003cp\u003e在每一个 Convolutional Block 中，卷积核的参数为 \u003ccode\u003e$W \\in \\mathbb{R}^{2d \\times kd}, b_w \\in \\mathbb{R}^{2d}$\u003c/code\u003e，其中 \u003ccode\u003e$k$\u003c/code\u003e 为卷积核的大小，经过卷积后的输出为 \u003ccode\u003e$Y \\in \\mathbb{R}^{2d}$\u003c/code\u003e。之后的非线性层采用了 Dauphin 等人 \u003csup id=\"fnref:12\"\u003e\u003ca href=\"#fn:12\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e12\u003c/a\u003e\u003c/sup\u003e 提出的 Gated Linear Units (GLU)，对于卷积后的输出 \u003ccode\u003e$Y = \\left[A, B\\right]$\u003c/code\u003e 有：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ v \\left(\\left[A, B\\right]\\right) = A \\otimes \\sigma \\left(B\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$A, B \\in \\mathbb{R}^d$\u003c/code\u003e 为非线性单元的输入，\u003ccode\u003e$\\otimes$\u003c/code\u003e 为逐元素相乘，\u003ccode\u003e$\\sigma \\left(B\\right)$\u003c/code\u003e 为用于控制输入 \u003ccode\u003e$A$\u003c/code\u003e 与当前上下文相关度的门结构。\u003c/p\u003e\n\u003cp\u003e模型中还加入了残差连接，即：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ h_i^l = v \\left(W^l \\left[h_{i-k/2}^{l-1}, \\dotsc, h_{i+k/2}^{l-1}\\right] + b_w^l\\right) + h_i^{l-1} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e为了确保网络卷积层的输出同输入的长度相匹配，模型对输入数据的前后填补 \u003ccode\u003e$k - 1$\u003c/code\u003e 个零值，同时为了避免解码器使用当前预测位置之后的信息，模型删除了卷积输出尾部的 \u003ccode\u003e$k$\u003c/code\u003e 个元素。在将 Embedding 喂给编码器网络之前，在解码器输出应用 softmax 之前以及所有解码器层计算 Attention 分数之前，建立了一个从 Embedding 维度 \u003ccode\u003e$f$\u003c/code\u003e 到卷积输出大小 \u003ccode\u003e$2d$\u003c/code\u003e 的线性映射。最终，预测下一个词的概率计算方式如下：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ p \\left(y_{i+1} | y_1, \\dotsc, y_i, \\mathbf{x}\\right) = \\text{softmax} \\left(W_o h_i^L + b_o\\right) \\in \\mathbb{R}^T $$\u003c/code\u003e\u003c/p\u003e\n\u003ch3 id=\"multi-step-attention-1\"\u003eMulti-step Attention\u003c/h3\u003e\n\u003cp\u003e模型的解码器网络中引入了一个分离的注意力机制，在计算 Attention 时，将解码器当前的隐状态 \u003ccode\u003e$h_i^l$\u003c/code\u003e 同之前输出元素的 Embedding 进行合并：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ d_i^l = W_d^l h_i^l + b_d^l + g_i $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e对于解码器网络层 \u003ccode\u003e$l$\u003c/code\u003e 中状态 \u003ccode\u003e$i$\u003c/code\u003e 和输入元素 \u003ccode\u003e$j$\u003c/code\u003e 之间的的 Attention \u003ccode\u003e$a_{ij}^l$\u003c/code\u003e 通过解码器汇总状态 \u003ccode\u003e$d_i^l$\u003c/code\u003e 和最后一个解码器 Block \u003ccode\u003e$u$\u003c/code\u003e 的输出 \u003ccode\u003e$z_j^u$\u003c/code\u003e 进行点积运算得到：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ a_{ij}^l = \\dfrac{\\exp \\left(d_i^l \\cdot z_j^u\\right)}{\\sum_{t=1}^{m}{\\exp \\left(d_i^l \\cdot z_t^u\\right)}} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e条件输入 \u003ccode\u003e$c_i^l$\u003c/code\u003e 的计算方式如下：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ c_i^l = \\sum_{j=1}^{m}{a_{ij}^l \\left(z_j^u + e_j\\right)} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$e_j$\u003c/code\u003e 为输入元素的 Embedding。与传统的 Attention 不同，\u003ccode\u003e$e_j$\u003c/code\u003e 的加入提供了一个有助于预测的具体输入元素信息。\u003c/p\u003e\n\u003cp\u003e最终将 \u003ccode\u003e$c_i^l$\u003c/code\u003e 加到对应的解码器层的输出 \u003ccode\u003e$h_i^l$\u003c/code\u003e。这个过程与传统的单步 Attention 不同，被称之为 Multiple Hops \u003csup id=\"fnref:13\"\u003e\u003ca href=\"#fn:13\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e13\u003c/a\u003e\u003c/sup\u003e。这种方式使得模型在计算 Attention 时会考虑之前已经注意过的输入信息。\u003c/p\u003e\n\u003cdiv class=\"footnotes\" role=\"doc-endnotes\"\u003e\n\u003chr/\u003e\n\u003col\u003e\n\u003cli id=\"fn:1\"\u003e\n\u003cp\u003eCho, K., van Merrienboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., \u0026amp; Bengio, Y. (2014). Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation. In \u003cem\u003eProceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)\u003c/em\u003e (pp. 1724–1734). \u003ca href=\"#fnref:1\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:2\"\u003e\n\u003cp\u003eSutskever, I., Vinyals, O., \u0026amp; Le, Q. V. (2014). Sequence to Sequence Learning with Neural Networks. In Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, \u0026amp; K. Q. Weinberger (Eds.), \u003cem\u003eAdvances in Neural Information Processing Systems 27\u003c/em\u003e (pp. 3104–3112). \u003ca href=\"#fnref:2\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:3\"\u003e\n\u003cp\u003eBahdanau, D., Cho, K., \u0026amp; Bengio, Y. (2014). Neural Machine Translation by Jointly Learning to Align and Translate. \u003cem\u003earXiv preprint arXiv:1409.0473\u003c/em\u003e \u003ca href=\"#fnref:3\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e \u003ca href=\"#fnref1:3\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e \u003ca href=\"#fnref2:3\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:4\"\u003e\n\u003cp\u003eXu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., … Bengio, Y. (2015). Show, Attend and Tell: Neural Image Caption Generation with Visual Attention. In \u003cem\u003eInternational Conference on Machine Learning\u003c/em\u003e (pp. 2048–2057). \u003ca href=\"#fnref:4\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:5\"\u003e\n\u003cp\u003eLuong, T., Pham, H., \u0026amp; Manning, C. D. (2015). Effective Approaches to Attention-based Neural Machine Translation. In \u003cem\u003eProceedings of the 2015 Conference on Empirical Methods in Natural Language Processing\u003c/em\u003e (pp. 1412–1421). \u003ca href=\"#fnref:5\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:6\"\u003e\n\u003cp\u003eVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., … Polosukhin, I. (2017). Attention is All you Need. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, \u0026amp; R. Garnett (Eds.), \u003cem\u003eAdvances in Neural Information Processing Systems 30\u003c/em\u003e (pp. 5998–6008). \u003ca href=\"#fnref:6\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:7\"\u003e\n\u003cp\u003eHe, K., Zhang, X., Ren, S., \u0026amp; Sun, J. (2016). Deep Residual Learning for Image Recognition. In \u003cem\u003e2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\u003c/em\u003e (pp. 770–778). \u003ca href=\"#fnref:7\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:8\"\u003e\n\u003cp\u003eBa, J. L., Kiros, J. R., \u0026amp; Hinton, G. E. (2016). Layer Normalization. \u003cem\u003earXiv preprint arXiv:1607.06450\u003c/em\u003e \u003ca href=\"#fnref:8\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:9\"\u003e\n\u003cp\u003eYang, Z., Yang, D., Dyer, C., He, X., Smola, A., \u0026amp; Hovy, E. (2016). Hierarchical Attention Networks for Document Classification. In \u003cem\u003eProceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics\u003c/em\u003e: Human Language Technologies (pp. 1480–1489). \u003ca href=\"#fnref:9\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:10\"\u003e\n\u003cp\u003eCui, Y., Chen, Z., Wei, S., Wang, S., Liu, T., \u0026amp; Hu, G. (2017). Attention-over-Attention Neural Networks for Reading Comprehension. In \u003cem\u003eProceedings of the 55th Annual Meeting of the Association for Computational Linguistics\u003c/em\u003e (Volume 1: Long Papers) (pp. 593–602). \u003ca href=\"#fnref:10\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:11\"\u003e\n\u003cp\u003eGehring, J., Auli, M., Grangier, D., Yarats, D., \u0026amp; Dauphin, Y. N. (2017). Convolutional Sequence to Sequence Learning. In \u003cem\u003eInternational Conference on Machine Learning\u003c/em\u003e (pp. 1243–1252). \u003ca href=\"#fnref:11\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:12\"\u003e\n\u003cp\u003eDauphin, Y. N., Fan, A., Auli, M., \u0026amp; Grangier, D. (2016). Language Modeling with Gated Convolutional Networks. \u003cem\u003earXiv preprint arXiv:1612.08083\u003c/em\u003e \u003ca href=\"#fnref:12\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:13\"\u003e\n\u003cp\u003eSukhbaatar, S., szlam,  arthur, Weston, J., \u0026amp; Fergus, R. (2015). End-To-End Memory Networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, \u0026amp; R. Garnett (Eds.), \u003cem\u003eAdvances in Neural Information Processing Systems 28\u003c/em\u003e (pp. 2440–2448). \u003ca href=\"#fnref:13\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/div\u003e\n\n\n\n\n\n\u003cdiv class=\"donate\"\u003e\n  \u003cdiv class=\"donate-header\"\u003e\u003c/div\u003e\n  \u003cdiv class=\"donate-slug\" id=\"donate-slug\"\u003eseq2seq-and-attention-machanism\u003c/div\u003e\n  \u003cbutton class=\"donate-button\"\u003e赞 赏\u003c/button\u003e\n  \u003cdiv class=\"donate-footer\"\u003e「真诚赞赏，手留余香」\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"donate-modal-wrapper\"\u003e\n  \u003cdiv class=\"donate-modal\"\u003e\n    \u003cdiv class=\"donate-box\"\u003e\n      \u003cdiv class=\"donate-box-content\"\u003e\n        \u003cdiv class=\"donate-box-content-inner\"\u003e\n          \u003cdiv class=\"donate-box-header\"\u003e「真诚赞赏，手留余香」\u003c/div\u003e\n          \u003cdiv class=\"donate-box-body\"\u003e\n            \u003cdiv class=\"donate-box-money\"\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-2\" data-v=\"2\" data-unchecked=\"￥ 2\" data-checked=\"2 元\"\u003e￥ 2\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-5\" data-v=\"5\" data-unchecked=\"￥ 5\" data-checked=\"5 元\"\u003e￥ 5\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-10\" data-v=\"10\" data-unchecked=\"￥ 10\" data-checked=\"10 元\"\u003e￥ 10\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-50\" data-v=\"50\" data-unchecked=\"￥ 50\" data-checked=\"50 元\"\u003e￥ 50\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-100\" data-v=\"100\" data-unchecked=\"￥ 100\" data-checked=\"100 元\"\u003e￥ 100\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-custom\" data-v=\"custom\" data-unchecked=\"任意金额\" data-checked=\"任意金额\"\u003e任意金额\u003c/button\u003e\n            \u003c/div\u003e\n            \u003cdiv class=\"donate-box-pay\"\u003e\n              \u003cimg class=\"donate-box-pay-qrcode\" id=\"donate-box-pay-qrcode\" src=\"\"/\u003e\n            \u003c/div\u003e\n          \u003c/div\u003e\n          \u003cdiv class=\"donate-box-footer\"\u003e\n            \u003cdiv class=\"donate-box-pay-method donate-box-pay-method-checked\" data-v=\"wechat-pay\"\u003e\n              \u003cimg class=\"donate-box-pay-method-image\" id=\"donate-box-pay-method-image-wechat-pay\" src=\"\"/\u003e\n            \u003c/div\u003e\n            \u003cdiv class=\"donate-box-pay-method\" data-v=\"alipay\"\u003e\n              \u003cimg class=\"donate-box-pay-method-image\" id=\"donate-box-pay-method-image-alipay\" src=\"\"/\u003e\n            \u003c/div\u003e\n          \u003c/div\u003e\n        \u003c/div\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n    \u003cbutton type=\"button\" class=\"donate-box-close-button\"\u003e\n      \u003csvg class=\"donate-box-close-button-icon\" fill=\"#fff\" viewBox=\"0 0 24 24\" width=\"24\" height=\"24\"\u003e\u003cpath d=\"M13.486 12l5.208-5.207a1.048 1.048 0 0 0-.006-1.483 1.046 1.046 0 0 0-1.482-.005L12 10.514 6.793 5.305a1.048 1.048 0 0 0-1.483.005 1.046 1.046 0 0 0-.005 1.483L10.514 12l-5.208 5.207a1.048 1.048 0 0 0 .006 1.483 1.046 1.046 0 0 0 1.482.005L12 13.486l5.207 5.208a1.048 1.048 0 0 0 1.483-.006 1.046 1.046 0 0 0 .005-1.482L13.486 12z\" fill-rule=\"evenodd\"\u003e\u003c/path\u003e\u003c/svg\u003e\n    \u003c/button\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\u003cscript type=\"text/javascript\" src=\"/js/donate.js\"\u003e\u003c/script\u003e\n\n\n  \u003cfooter\u003e\n  \n\u003cnav class=\"post-nav\"\u003e\n  \u003cspan class=\"nav-prev\"\u003e← \u003ca href=\"/cn/2018/10/word-embeddings/\"\u003e词向量 (Word Embeddings)\u003c/a\u003e\u003c/span\u003e\n  \u003cspan class=\"nav-next\"\u003e\u003ca href=\"/cn/2018/10/serving-models-with-flask-and-gae/\"\u003e利用 Flask 和 Google App Engine 部署模型服务\u003c/a\u003e →\u003c/span\u003e\n\u003c/nav\u003e\n\n\n\n\n\u003cins class=\"adsbygoogle\" style=\"display:block; text-align:center;\" data-ad-layout=\"in-article\" data-ad-format=\"fluid\" data-ad-client=\"ca-pub-2608165017777396\" data-ad-slot=\"8302038603\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n  (adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/js-cookie@3.0.5/dist/js.cookie.min.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"/js/toggle-theme.js\"\u003e\u003c/script\u003e\n\n\n\u003cscript src=\"/js/no-highlight.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"/js/math-code.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"/js/heading-anchor.js\"\u003e\u003c/script\u003e\n\n\n\n\u003csection class=\"comments\"\u003e\n\u003cscript src=\"https://giscus.app/client.js\" data-repo=\"leovan/leovan.me\" data-repo-id=\"MDEwOlJlcG9zaXRvcnkxMTMxOTY0Mjc=\" data-category=\"Comments\" data-category-id=\"DIC_kwDOBr89i84CT-R7\" data-mapping=\"pathname\" data-strict=\"1\" data-reactions-enabled=\"1\" data-emit-metadata=\"0\" data-input-position=\"top\" data-theme=\"preferred_color_scheme\" data-lang=\"zh-CN\" data-loading=\"lazy\" crossorigin=\"anonymous\" defer=\"\"\u003e\n\u003c/script\u003e\n\u003c/section\u003e\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cscript async=\"\" src=\"/js/center-img.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/right-quote.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/external-link.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/alt-title.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/figure.js\"\u003e\u003c/script\u003e\n\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js\"\u003e\u003c/script\u003e\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/vanilla-back-to-top@latest/dist/vanilla-back-to-top.min.js\"\u003e\u003c/script\u003e\n\u003cscript\u003e\naddBackToTop({\n  diameter: 48\n});\n\u003c/script\u003e\n\n  \u003chr/\u003e\n  \u003cdiv class=\"copyright no-border-bottom\"\u003e\n    \u003cdiv class=\"copyright-author-year\"\u003e\n      \u003cspan\u003eCopyright © 2017-2024 \u003ca href=\"/\"\u003e范叶亮 | Leo Van\u003c/a\u003e\u003c/span\u003e\n    \u003c/div\u003e\n  \u003c/div\u003e\n  \u003c/footer\u003e\n  \u003c/article\u003e",
  "Date": "2018-10-12T00:00:00Z",
  "Author": "范叶亮"
}