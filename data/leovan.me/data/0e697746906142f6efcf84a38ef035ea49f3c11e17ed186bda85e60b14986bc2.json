{
  "Source": "leovan.me",
  "Title": "文本相似度 (Text Similarity)",
  "Link": "https://leovan.me/cn/2020/10/text-similarity/",
  "Content": "\u003carticle class=\"main\"\u003e\n    \u003cheader class=\"content-title\"\u003e\n    \n\u003ch1 class=\"title\"\u003e\n  \n  文本相似度 (Text Similarity)\n  \n\u003c/h1\u003e\n\n\n\n\n\n\n\n\u003ch2 class=\"author-date\"\u003e范叶亮 / \n2020-10-31\u003c/h2\u003e\n\n\n\n\u003ch3 class=\"post-meta\"\u003e\n\n\n\u003cstrong\u003e分类: \u003c/strong\u003e\n\u003ca href=\"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0\"\u003e机器学习\u003c/a\u003e, \u003ca href=\"/categories/%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0\"\u003e表示学习\u003c/a\u003e, \u003ca href=\"/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86\"\u003e自然语言处理\u003c/a\u003e\n\n\n\n\n/\n\n\n\n\n\u003cstrong\u003e标签: \u003c/strong\u003e\n\u003cspan\u003e文本相似度\u003c/span\u003e, \u003cspan\u003eText Similarity\u003c/span\u003e, \u003cspan\u003e文本切分\u003c/span\u003e, \u003cspan\u003eN 元语法\u003c/span\u003e, \u003cspan\u003eN-gram\u003c/span\u003e, \u003cspan\u003e分词\u003c/span\u003e, \u003cspan\u003eSegment\u003c/span\u003e, \u003cspan\u003eTokenise\u003c/span\u003e, \u003cspan\u003e主题模型\u003c/span\u003e, \u003cspan\u003eTopic Model\u003c/span\u003e, \u003cspan\u003eTF-IDF\u003c/span\u003e, \u003cspan\u003eTFIDF\u003c/span\u003e, \u003cspan\u003eTF\u003c/span\u003e, \u003cspan\u003e词频\u003c/span\u003e, \u003cspan\u003eIDF\u003c/span\u003e, \u003cspan\u003e逆文本频率\u003c/span\u003e, \u003cspan\u003eBM25\u003c/span\u003e, \u003cspan\u003eOkapi BM25\u003c/span\u003e, \u003cspan\u003eBest Match\u003c/span\u003e, \u003cspan\u003ePageRank\u003c/span\u003e, \u003cspan\u003eTextRank\u003c/span\u003e, \u003cspan\u003eLSA\u003c/span\u003e, \u003cspan\u003eLatent Semantic Analysis\u003c/span\u003e, \u003cspan\u003e潜在语义分析\u003c/span\u003e, \u003cspan\u003e潜语义分析\u003c/span\u003e, \u003cspan\u003eLSI\u003c/span\u003e, \u003cspan\u003eLatent Semantic Index\u003c/span\u003e, \u003cspan\u003ePLSA\u003c/span\u003e, \u003cspan\u003eProbabilistic Latent Semantic Analysis\u003c/span\u003e, \u003cspan\u003e概率潜在语义分析\u003c/span\u003e, \u003cspan\u003e概率潜语义分析\u003c/span\u003e, \u003cspan\u003ePLSI\u003c/span\u003e, \u003cspan\u003eProbabilistic Latent Semantic Index\u003c/span\u003e, \u003cspan\u003eLDA\u003c/span\u003e, \u003cspan\u003eLatent Dirichlet Allocation\u003c/span\u003e, \u003cspan\u003e隐含狄利克雷分布\u003c/span\u003e, \u003cspan\u003e隐狄利克雷分布\u003c/span\u003e, \u003cspan\u003eHDP\u003c/span\u003e, \u003cspan\u003eHierarchical Dirichlet Processes\u003c/span\u003e, \u003cspan\u003e层次狄利克雷过程\u003c/span\u003e, \u003cspan\u003e距离度量\u003c/span\u003e, \u003cspan\u003eDistance Measurement\u003c/span\u003e, \u003cspan\u003eJaccard 系数\u003c/span\u003e, \u003cspan\u003eJaccard Index\u003c/span\u003e, \u003cspan\u003eDice 系数\u003c/span\u003e, \u003cspan\u003eDice Index\u003c/span\u003e, \u003cspan\u003eTversky 系数\u003c/span\u003e, \u003cspan\u003eTversky Index\u003c/span\u003e, \u003cspan\u003eLevenshtein 距离\u003c/span\u003e, \u003cspan\u003eLevenshtein Distance\u003c/span\u003e, \u003cspan\u003e编辑距离\u003c/span\u003e, \u003cspan\u003eEditor Distance\u003c/span\u003e, \u003cspan\u003eJaro-Winkler 距离\u003c/span\u003e, \u003cspan\u003eJaro-Winkler Distance\u003c/span\u003e, \u003cspan\u003e汉明距离\u003c/span\u003e, \u003cspan\u003eHamming Distance\u003c/span\u003e, \u003cspan\u003e表示学习\u003c/span\u003e, \u003cspan\u003eRepresentation Learning\u003c/span\u003e, \u003cspan\u003e文本表示\u003c/span\u003e, \u003cspan\u003eText Representation\u003c/span\u003e, \u003cspan\u003e词法\u003c/span\u003e, \u003cspan\u003eLexical\u003c/span\u003e, \u003cspan\u003e词性\u003c/span\u003e, \u003cspan\u003e命名实体\u003c/span\u003e, \u003cspan\u003eNER\u003c/span\u003e, \u003cspan\u003eNamed Entity\u003c/span\u003e, \u003cspan\u003e句法\u003c/span\u003e, \u003cspan\u003eSyntax\u003c/span\u003e, \u003cspan\u003e依存句法分析\u003c/span\u003e, \u003cspan\u003eDependency Syntactic Parsing\u003c/span\u003e, \u003cspan\u003e句法分析树\u003c/span\u003e, \u003cspan\u003eSyntactic Parsing Tree\u003c/span\u003e, \u003cspan\u003eParsing Tree\u003c/span\u003e, \u003cspan\u003e语义\u003c/span\u003e, \u003cspan\u003eSemantics\u003c/span\u003e, \u003cspan\u003e短文本\u003c/span\u003e, \u003cspan\u003eShort Text\u003c/span\u003e, \u003cspan\u003e长文本\u003c/span\u003e, \u003cspan\u003eLong Text\u003c/span\u003e, \u003cspan\u003eSimHash\u003c/span\u003e\n\n\n\n\n/\n\n\n\u003cstrong\u003e字数: \u003c/strong\u003e\n6726\n\u003c/h3\u003e\n\n\n\n\u003chr/\u003e\n\n\n\n    \n    \n    \u003cins class=\"adsbygoogle\" style=\"display:block; text-align:center;\" data-ad-layout=\"in-article\" data-ad-format=\"fluid\" data-ad-client=\"ca-pub-2608165017777396\" data-ad-slot=\"1261604535\"\u003e\u003c/ins\u003e\n    \u003cscript\u003e\n    (adsbygoogle = window.adsbygoogle || []).push({});\n    \u003c/script\u003e\n    \n    \n    \u003c/header\u003e\n\n\n\n\u003cdiv class=\"toc-depth-3\"\u003e\u003cnav id=\"TableOfContents\"\u003e\n  \u003cul\u003e\n    \u003cli\u003e\u003ca href=\"#文本表示角度\"\u003e文本表示角度\u003c/a\u003e\n      \u003cul\u003e\n        \u003cli\u003e\u003ca href=\"#统计模型\"\u003e统计模型\u003c/a\u003e\n          \u003cul\u003e\n            \u003cli\u003e\u003ca href=\"#文本切分\"\u003e文本切分\u003c/a\u003e\u003c/li\u003e\n            \u003cli\u003e\u003ca href=\"#主题模型\"\u003e主题模型\u003c/a\u003e\u003c/li\u003e\n            \u003cli\u003e\u003ca href=\"#距离度量\"\u003e距离度量\u003c/a\u003e\u003c/li\u003e\n          \u003c/ul\u003e\n        \u003c/li\u003e\n        \u003cli\u003e\u003ca href=\"#表示学习\"\u003e表示学习\u003c/a\u003e\u003c/li\u003e\n      \u003c/ul\u003e\n    \u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#文本词法-句法和语义角度\"\u003e文本词法，句法和语义角度\u003c/a\u003e\n      \u003cul\u003e\n        \u003cli\u003e\u003ca href=\"#词法\"\u003e词法\u003c/a\u003e\u003c/li\u003e\n        \u003cli\u003e\u003ca href=\"#句法\"\u003e句法\u003c/a\u003e\u003c/li\u003e\n        \u003cli\u003e\u003ca href=\"#语义\"\u003e语义\u003c/a\u003e\u003c/li\u003e\n      \u003c/ul\u003e\n    \u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#文本长度角度\"\u003e文本长度角度\u003c/a\u003e\n      \u003cul\u003e\n        \u003cli\u003e\u003ca href=\"#短文本-v-s-短文本\"\u003e短文本 v.s. 短文本\u003c/a\u003e\u003c/li\u003e\n        \u003cli\u003e\u003ca href=\"#短文本-v-s-长文本\"\u003e短文本 v.s. 长文本\u003c/a\u003e\u003c/li\u003e\n        \u003cli\u003e\u003ca href=\"#长文本-v-s-长文本\"\u003e长文本 v.s. 长文本\u003c/a\u003e\u003c/li\u003e\n      \u003c/ul\u003e\n    \u003c/li\u003e\n  \u003c/ul\u003e\n\u003c/nav\u003e\u003c/div\u003e\n\n\n\u003cp\u003e文本相似度是指衡量两个文本的相似程度，相似程度的评价有很多角度：单纯的字面相似度（例如：我和他 v.s. 我和她），语义的相似度（例如：爸爸 v.s. 父亲）和风格的相似度（例如：我喜欢你 v.s. 我好喜欢你耶）等等。\u003c/p\u003e\n\u003ch1 id=\"文本表示角度\"\u003e文本表示角度\u003c/h1\u003e\n\u003ch2 id=\"统计模型\"\u003e统计模型\u003c/h2\u003e\n\u003ch3 id=\"文本切分\"\u003e文本切分\u003c/h3\u003e\n\u003cp\u003e在中文和拉丁语系中，文本的直观表示就存在一定的差异，拉丁语系中词与词之间存在天然的分隔符，而中文则没有。\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eI can eat glass, it doesn’t hurt me.\u003cbr/\u003e\n我能吞下玻璃而不伤身体。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e因此针对拉丁语系的文本切分相对中文容易许多。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eN 元语法\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eN-gram (N 元语法) 是一种文本表示方法，指文中连续出现的 $n$ 个词语。N-gram 模型是基于 $n-1$ 阶马尔科夫链的一种概率语言模型，可以通过前 $n-1$ 个词对第 $n$ 个词进行预测。以 \u003ccode\u003e南京市长江大桥\u003c/code\u003e 为例，N-gram 的表示如下：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e一元语法（unigram）：南/京/市/长/江/大/桥\n二元语法（bigram）：南京/京市/市长/长江/江大/大桥\n三元语法（trigram）：南京市/京市长/市长江/长江大/江大桥\n\u003c/code\u003e\u003c/pre\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport re\nfrom nltk.util import ngrams\n\ns = \u0026#39;南京市长江大桥\u0026#39;\ntokens = re.sub(r\u0026#39;\\s\u0026#39;, \u0026#39;\u0026#39;, s)\n\nlist(ngrams(tokens, 1))\n# [(\u0026#39;南\u0026#39;,), (\u0026#39;京\u0026#39;,), (\u0026#39;市\u0026#39;,), (\u0026#39;长\u0026#39;,), (\u0026#39;江\u0026#39;,), (\u0026#39;大\u0026#39;,), (\u0026#39;桥\u0026#39;,)]\n\nlist(ngrams(tokens, 2))\n# [(\u0026#39;南\u0026#39;, \u0026#39;京\u0026#39;), (\u0026#39;京\u0026#39;, \u0026#39;市\u0026#39;), (\u0026#39;市\u0026#39;, \u0026#39;长\u0026#39;),\n#  (\u0026#39;长\u0026#39;, \u0026#39;江\u0026#39;), (\u0026#39;江\u0026#39;, \u0026#39;大\u0026#39;), (\u0026#39;大\u0026#39;, \u0026#39;桥\u0026#39;)]\n\nlist(ngrams(tokens, 3, pad_left=True, pad_right=True, left_pad_symbol=\u0026#39;\u0026lt;s\u0026gt;\u0026#39;, right_pad_symbol=\u0026#39;\u0026lt;/s\u0026gt;\u0026#39;))\n# [(\u0026#39;\u0026lt;s\u0026gt;\u0026#39;, \u0026#39;\u0026lt;s\u0026gt;\u0026#39;, \u0026#39;南\u0026#39;),\n#  (\u0026#39;\u0026lt;s\u0026gt;\u0026#39;, \u0026#39;南\u0026#39;, \u0026#39;京\u0026#39;),\n#  (\u0026#39;南\u0026#39;, \u0026#39;京\u0026#39;, \u0026#39;市\u0026#39;),\n#  (\u0026#39;京\u0026#39;, \u0026#39;市\u0026#39;, \u0026#39;长\u0026#39;),\n#  (\u0026#39;市\u0026#39;, \u0026#39;长\u0026#39;, \u0026#39;江\u0026#39;),\n#  (\u0026#39;长\u0026#39;, \u0026#39;江\u0026#39;, \u0026#39;大\u0026#39;),\n#  (\u0026#39;江\u0026#39;, \u0026#39;大\u0026#39;, \u0026#39;桥\u0026#39;),\n#  (\u0026#39;大\u0026#39;, \u0026#39;桥\u0026#39;, \u0026#39;\u0026lt;/s\u0026gt;\u0026#39;),\n#  (\u0026#39;桥\u0026#39;, \u0026#39;\u0026lt;/s\u0026gt;\u0026#39;, \u0026#39;\u0026lt;/s\u0026gt;\u0026#39;)]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e分词\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e分词就是将连续的字序列按照一定的规范重新组合成词序列的过程。在英文的行文中，单词之间是以空格作为自然分界符的，而中文只是字、句和段能通过明显的分界符来简单划界，唯独词没有一个形式上的分界符，虽然英文也同样存在短语的划分问题，不过在词这一层上，中文比之英文要复杂得多、困难得多。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003es = \u0026#39;南京市长江大桥\u0026#39;\n\n# jieba\n# https://github.com/fxsjy/jieba\nimport jieba\n\nlist(jieba.cut(s, cut_all=False))\n# [\u0026#39;南京市\u0026#39;, \u0026#39;长江大桥\u0026#39;]\n\nlist(jieba.cut(s, cut_all=True))\n# [\u0026#39;南京\u0026#39;, \u0026#39;南京市\u0026#39;, \u0026#39;京市\u0026#39;, \u0026#39;市长\u0026#39;, \u0026#39;长江\u0026#39;, \u0026#39;长江大桥\u0026#39;, \u0026#39;大桥\u0026#39;]\n\nlist(jieba.cut_for_search(s))\n# [\u0026#39;南京\u0026#39;, \u0026#39;京市\u0026#39;, \u0026#39;南京市\u0026#39;, \u0026#39;长江\u0026#39;, \u0026#39;大桥\u0026#39;, \u0026#39;长江大桥\u0026#39;]\n\n# THULAC\n# https://github.com/thunlp/THULAC-Python\nimport thulac\n\nthulac_ins = thulac.thulac()\n\nthulac_ins.cut(s)\n# [[\u0026#39;南京市\u0026#39;, \u0026#39;ns\u0026#39;], [\u0026#39;长江\u0026#39;, \u0026#39;ns\u0026#39;], [\u0026#39;大桥\u0026#39;, \u0026#39;n\u0026#39;]]\n\n# PKUSEG\n# https://github.com/lancopku/PKUSeg-python\nimport pkuseg\n\nseg = pkuseg.pkuseg(postag=True)\n\nseg.cut(s)\n# [(\u0026#39;南京市\u0026#39;, \u0026#39;ns\u0026#39;), (\u0026#39;长江\u0026#39;, \u0026#39;ns\u0026#39;), (\u0026#39;大桥\u0026#39;, \u0026#39;n\u0026#39;)]\n\n# HanLP\n# https://github.com/hankcs/HanLP\nimport hanlp\n\ntokenizer = hanlp.load(\u0026#39;LARGE_ALBERT_BASE\u0026#39;)\n\ntokenizer(s)\n# [\u0026#39;南京市\u0026#39;, \u0026#39;长江\u0026#39;, \u0026#39;大桥\u0026#39;]\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch3 id=\"主题模型\"\u003e主题模型\u003c/h3\u003e\n\u003cp\u003e除了对文本进行切分将切分后结果全部用于表示文本外，还可以用部分字词表示一篇文档。主题模型（Topic Model）在机器学习和自然语言处理等领域是用来在一系列文档中发现抽象主题的一种统计模型。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2020-10-31-text-similarity/what-is-topic-model.png\" alt=\"What is Topic Model\"/\u003e\u003c/p\u003e\n\u003cp\u003e直观来讲，如果一篇文章有一个中心思想，那么一些特定词语会更频繁的出现。比方说，如果一篇文章是在讲狗的，那“狗”和“骨头”等词出现的频率会高些。如果一篇文章是在讲猫的，那“猫”和“鱼”等词出现的频率会高些。而有些词例如“这个”、“和”大概在两篇文章中出现的频率会大致相等。但真实的情况是，一篇文章通常包含多种主题，而且每个主题所占比例各不相同。因此，如果一篇文章 10% 和猫有关，90% 和狗有关，那么和狗相关的关键字出现的次数大概会是和猫相关的关键字出现次数的 9 倍。\u003c/p\u003e\n\u003cp\u003e一个主题模型试图用数学框架来体现文档的这种特点。主题模型自动分析每个文档，统计文档内的词语，根据统计的信息来断定当前文档含有哪些主题，以及每个主题所占的比例各为多少 \u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eTF-IDF\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTF-IDF 是 Term Frequency - Inverse Document Frequency 的缩写，即“词频-逆文本频率”。TF-IDF 可以用于评估一个字词在语料中的一篇文档中的重要程度，基本思想是如果某个字词在一篇文档中出现的频率较高，而在其他文档中出现频率较低，则认为这个字词更能够代表这篇文档。\u003c/p\u003e\n\u003cp\u003e形式化地，对于文档 $y$ 中的字词 $x$ 的 TF-IDF 重要程度可以表示为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ w_{x, y} = tf_{x, y} \\times \\log \\left(\\dfrac{N}{df_{x}}\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$tf_{x, y}$\u003c/code\u003e 表示字词 \u003ccode\u003e$x$\u003c/code\u003e 在文档 \u003ccode\u003e$y$\u003c/code\u003e 中出现的频率，\u003ccode\u003e$df_x$\u003c/code\u003e 为包含字词 \u003ccode\u003e$x$\u003c/code\u003e 的文档数量，\u003ccode\u003e$N$\u003c/code\u003e 为语料中文档的总数量。\u003c/p\u003e\n\u003cp\u003e以 \u003ca href=\"https://github.com/liuhuanyong/MusicLyricChatbot\"\u003e14 万歌词语料\u003c/a\u003e 为例，通过 TF-IDF 计算周杰伦的《简单爱》中最重要的 3 个词为 \u003ccode\u003e[\u0026#39;睡着\u0026#39;, \u0026#39;放开\u0026#39;, \u0026#39;棒球\u0026#39;]\u003c/code\u003e。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eBM25\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eBM25 算法的全称为 Okapi BM25，是一种搜索引擎用于评估查询和文档之间相关程度的排序算法，其中 BM 是 Best Match 的缩写。\u003c/p\u003e\n\u003cp\u003e对于一个给定的查询 \u003ccode\u003e$Q$\u003c/code\u003e，包含的关键词为 \u003ccode\u003e$q_1, \\cdots, q_n$\u003c/code\u003e，一个文档 \u003ccode\u003e$D$\u003c/code\u003e 的 BM25 值定义为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\operatorname{score}(D, Q)=\\sum_{i=1}^{n} \\operatorname{IDF}\\left(q_{i}\\right) \\cdot \\frac{f\\left(q_{i}, D\\right) \\cdot\\left(k_{1}+1\\right)}{f\\left(q_{i}, D\\right)+k_{1} \\cdot\\left(1-b+b \\cdot \\frac{|D|}{\\text { avgdl }}\\right)} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$f\\left(q_{i}, D\\right)$\u003c/code\u003e 表示 \u003ccode\u003e$q_i$\u003c/code\u003e 在文档 \u003ccode\u003e$D$\u003c/code\u003e 中的词频，\u003ccode\u003e$|D|$\u003c/code\u003e 表示文档 \u003ccode\u003e$D$\u003c/code\u003e 中的词数，\u003ccode\u003e$\\text{avgdl}$\u003c/code\u003e 表示语料中所有文档的平均长度。\u003ccode\u003e$k_1$\u003c/code\u003e 和 \u003ccode\u003e$b$\u003c/code\u003e 为自由参数，通常取值为 \u003ccode\u003e$k_1 \\in \\left[1.2, 2.0\\right], b = 0.75$\u003c/code\u003e \u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e。\u003ccode\u003e$\\operatorname{IDF} \\left(q_i\\right)$\u003c/code\u003e 表示词 \u003ccode\u003e$q_i$\u003c/code\u003e 的逆文档频率，通常计算方式如下：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\operatorname{IDF}\\left(q_{i}\\right)=\\ln \\left(\\frac{N-n\\left(q_{i}\\right)+0.5}{n\\left(q_{i}\\right)+0.5}+1\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$N$\u003c/code\u003e 为语料中文档的总数量，\u003ccode\u003e$n \\left(q_i\\right)$\u003c/code\u003e 表示包含 \u003ccode\u003e$q_i$\u003c/code\u003e 的文档数量。\u003c/p\u003e\n\u003cp\u003eBM25 算法是对 TF-IDF 算法的优化，在词频的计算上，BM25 限制了文档 \u003ccode\u003e$D$\u003c/code\u003e 中关键词 \u003ccode\u003e$q_i$\u003c/code\u003e 的词频对评分的影响。为了防止词频过大，BM25 将这个值的上限设置为 \u003ccode\u003e$k_1 + 1$\u003c/code\u003e。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2020-10-31-text-similarity/bm25-tf-1.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cp\u003e同时，BM25 还引入了平均文档长度 \u003ccode\u003e$\\text{avgdl}$\u003c/code\u003e，不同的平均文档长度 \u003ccode\u003e$\\text{avgdl}$\u003c/code\u003e 对 TF 分值的影响如下图所示：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2020-10-31-text-similarity/bm25-tf-2.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eTextRank\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eTextRank \u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e 是基于 PageRank \u003csup id=\"fnref:4\"\u003e\u003ca href=\"#fn:4\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e4\u003c/a\u003e\u003c/sup\u003e 算法的一种关键词提取算法。PageRank 最早是用于 Google 的网页排名，因此以公司创始人拉里·佩奇（Larry Page）的姓氏来命名。PageRank 的计算公式如下：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ S\\left(V_{i}\\right)=(1-d)+d * \\sum_{V_{j} \\in I n\\left(V_{i}\\right)} \\frac{1}{\\left|O u t\\left(V_{j}\\right)\\right|} S\\left(V_{j}\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$V_i$\u003c/code\u003e 表示任意一个网页，\u003ccode\u003e$V_j$\u003c/code\u003e 表示链接到网页 \u003ccode\u003e$V_i$\u003c/code\u003e 的网页，\u003ccode\u003e$S \\left(V_i\\right)$\u003c/code\u003e 表示网页 \u003ccode\u003e$V_i$\u003c/code\u003e 的 PageRank 值，\u003ccode\u003e$In \\left(V_i\\right)$\u003c/code\u003e 表示网页 \u003ccode\u003e$V_i$\u003c/code\u003e 所有的入链集合，\u003ccode\u003e$Out \\left(V_j\\right)$\u003c/code\u003e 表示网页 \u003ccode\u003e$V_j$\u003c/code\u003e 所有的出链集合，\u003ccode\u003e$|\\cdot|$\u003c/code\u003e 表示集合的大小，\u003ccode\u003e$d$\u003c/code\u003e 为阻尼系数，是为了确保每个网页的 PageRank 值都大于 0。\u003c/p\u003e\n\u003cp\u003eTextRank 由 PageRank 改进而来，计算公式如下：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ WS \\left(V_{i}\\right)=(1-d)+d * \\sum_{V_{j} \\in In\\left(V_{i}\\right)} \\frac{w_{j i}}{\\sum_{V_{k} \\in Out\\left(V_{j}\\right)} w_{j k}} WS \\left(V_{j}\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e相比于 PageRank 公式增加了权重项 \u003ccode\u003e$W_{ji}$\u003c/code\u003e，用来表示两个节点之间的边的权重。TextRank 提取关键词的算法流程如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e将文本进行切分得到 \u003ccode\u003e$S_i = \\left[t_{i1}, t_{i2}, \\cdots, t_{in}\\right]$\u003c/code\u003e。\u003c/li\u003e\n\u003cli\u003e将 \u003ccode\u003e$S_i$\u003c/code\u003e 中大小为 \u003ccode\u003e$k$\u003c/code\u003e 的滑动窗口中的词定义为共现关系，构建关键词图 \u003ccode\u003e$G = \\left(V, E\\right)$\u003c/code\u003e。\u003c/li\u003e\n\u003cli\u003e根据 TextRank 的计算公式对每个节点的值进行计算，直至收敛。\u003c/li\u003e\n\u003cli\u003e对节点的 TextRank 的值进行倒叙排序，获取前 \u003ccode\u003e$n$\u003c/code\u003e 个词作为关键词。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eLSA, PLSA, LDA \u0026amp; HDP\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e潜在语义分析（LSA, Latent Semantic Analysis）\u003c/strong\u003e\u003csup id=\"fnref:5\"\u003e\u003ca href=\"#fn:5\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e5\u003c/a\u003e\u003c/sup\u003e 的核心思想是将文本的高维词空间映射到一个低维的向量空间，我们称之为隐含语义空间。降维可以通过\u003ca href=\"/cn/2017/12/evd-svd-and-pca/\"\u003e奇异值分解（SVD）\u003c/a\u003e实现，令 \u003ccode\u003e$X$\u003c/code\u003e 表示语料矩阵，元素 \u003ccode\u003e$\\left(i, j\\right)$\u003c/code\u003e 表示词 \u003ccode\u003e$i$\u003c/code\u003e 和文档 \u003ccode\u003e$j$\u003c/code\u003e 的共现情况（例如：词频）：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ X = \\mathbf{d}_{j} \\cdot \\mathbf{t}_{i}^{T} = \\left[\\begin{array}{c} x_{1, j} \\\\ \\vdots \\\\ x_{i, j} \\\\ \\vdots \\\\ x_{m, j} \\end{array}\\right] \\cdot \\left[\\begin{array}{ccccc} x_{i, 1} \u0026amp; \\ldots \u0026amp; x_{i, j} \u0026amp; \\ldots \u0026amp; x_{i, n} \\end{array}\\right] = \\left[\\begin{array}{ccccc} x_{1,1} \u0026amp; \\ldots \u0026amp; x_{1, j} \u0026amp; \\ldots \u0026amp; x_{1, n} \\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ x_{i, 1} \u0026amp; \\ldots \u0026amp; x_{i, j} \u0026amp; \\ldots \u0026amp; x_{i, n} \\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ x_{m, 1} \u0026amp; \\ldots \u0026amp; x_{m, j} \u0026amp; \\ldots \u0026amp; x_{m, n} \\end{array}\\right] $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e利用奇异值分解：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ X = U \\Sigma V^{T} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e取最大的 \u003ccode\u003e$K$\u003c/code\u003e 个奇异值，则可以得到原始矩阵的近似矩阵：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\widetilde{X} =U \\widetilde{\\Sigma} V^{T} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e在处理一个新的文档时，可以利用下面的公式将原始的词空间映射到潜在语义空间：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\tilde{x} =\\tilde{\\Sigma} ^{-1} V^{T} x_{test} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eLSA 的优点：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e低维空间可以刻画同义词\u003c/li\u003e\n\u003cli\u003e无监督模型\u003c/li\u003e\n\u003cli\u003e降维可以减少噪声，使特征更加鲁棒\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eLSA 的缺点：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e未解决多义词问题\u003c/li\u003e\n\u003cli\u003e计算复杂度高，增加新文档时需要重新训练\u003c/li\u003e\n\u003cli\u003e没有明确的物理解释\u003c/li\u003e\n\u003cli\u003e高斯分布假设不符合文本特征（词频不为负）\u003c/li\u003e\n\u003cli\u003e维度的确定是 Ad hoc 的\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003e概率潜语义分析（Probabilistic Latent Semantic Analysis, PLSA）\u003c/strong\u003e\u003csup id=\"fnref:6\"\u003e\u003ca href=\"#fn:6\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e6\u003c/a\u003e\u003c/sup\u003e 相比于 LSA 增加了概率模型，每个变量以及相应的概率分布和条件概率分布都有明确的物理解释。\u003c/p\u003e\n\u003cp\u003ePLSA 认为一篇文档可以由多个主题混合而成，而每个主题都是词上的概率分布，文章中的每个词都是由一个固定的主题生成的，如下图所示：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2020-10-31-text-similarity/plsa.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cp\u003e针对第 \u003ccode\u003e$m$\u003c/code\u003e 篇文档 \u003ccode\u003e$d_m$\u003c/code\u003e 中的每个词的生成概率为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ p\\left(w \\mid d_{m}\\right)=\\sum_{z=1}^{K} p(w \\mid z) p\\left(z \\mid d_{m}\\right)=\\sum_{z=1}^{K} \\varphi_{z w} \\theta_{m z} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e因此整篇文档的生成概率为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ p\\left(\\vec{w} \\mid d_{m}\\right)=\\prod_{i=1}^{n} \\sum_{z=1}^{K} p\\left(w_{i} \\mid z\\right) p\\left(z \\mid d_{m}\\right)=\\prod_{i=1}^{n} \\sum_{z=1}^{K} \\varphi_{z w_{i}} \\theta_{d z} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003ePLSA 可以利用 EM 算法求得局部最优解。\u003c/p\u003e\n\u003cp\u003ePLSA 优点：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e定义了概率模型，有明确的物理解释\u003c/li\u003e\n\u003cli\u003e多项式分布假设更加符合文本特征\u003c/li\u003e\n\u003cli\u003e可以通过模型选择和复杂度控制来确定主题的维度\u003c/li\u003e\n\u003cli\u003e解决了同义词和多义词的问题\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003ePLSA 缺点：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e随着文本和词的增加，PLSA 模型参数也随之线性增加\u003c/li\u003e\n\u003cli\u003e可以生成语料中的文档的模型，但不能生成新文档的模型\u003c/li\u003e\n\u003cli\u003eEM 算法求解的计算量较大\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cstrong\u003e隐含狄利克雷分布（Latent Dirichlet Allocation, LDA）\u003c/strong\u003e\u003csup id=\"fnref:7\"\u003e\u003ca href=\"#fn:7\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e7\u003c/a\u003e\u003c/sup\u003e 在 PLSA 的基础上增加了参数的先验分布。在 PLSA 中，对于一个新文档，是无法获取 \u003ccode\u003e$p \\left(d\\right)$\u003c/code\u003e 的，因此这个概率模型是不完备的。LDA 对于 \u003ccode\u003e$\\vec{\\theta}_m$\u003c/code\u003e 和 \u003ccode\u003e$\\vec{\\phi}_k$\u003c/code\u003e 都增加了多项式分布的共轭分布狄利克雷分布作为先验，整个 LDA 模型如下图所示：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2020-10-31-text-similarity/lda.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cp\u003eLDA 的参数估计可以通过\u003ca href=\"/cn/2017/12/mcmc-and-gibbs-sampling/\"\u003e吉布斯采样\u003c/a\u003e实现。PLSA 和 LDA 的更多细节请参见《LDA 数学八卦》\u003csup id=\"fnref:8\"\u003e\u003ca href=\"#fn:8\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e8\u003c/a\u003e\u003c/sup\u003e。\u003c/p\u003e\n\u003cp\u003eLDA 在使用过程中仍需要指定主题的个数，而\u003cstrong\u003e层次狄利克雷过程（Hierarchical Dirichlet Processes, HDP）\u003c/strong\u003e\u003csup id=\"fnref:9\"\u003e\u003ca href=\"#fn:9\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e9\u003c/a\u003e\u003c/sup\u003e 通过过程的构造可以自动训练出主题的个数，更多实现细节请参考论文。\u003c/p\u003e\n\u003cp\u003eLSA，PLSA，LDA 和 HDP 之间的演化关系如下图所示：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2020-10-31-text-similarity/lsa-plsa-lda-hdp.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e本节相关代码详见 \u003ca href=\"https://github.com/leovan/leovan.me/tree/main/static/codes/cn/2020-10-31-text-similarity/topic-model.py\"\u003e这里\u003c/a\u003e。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch3 id=\"距离度量\"\u003e距离度量\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003e本节内容源自 \u003ca href=\"/cn/2019/01/similarity-and-distance-measurement/\"\u003e相似性和距离度量 (Similarity \u0026amp; Distance Measurement)\u003c/a\u003e。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e相似性度量 (Similarity Measurement) 用于衡量两个元素之间的相似性程度或两者之间的距离 (Distance)。距离衡量的是指元素之间的不相似性 (Dissimilarity)，通常情况下我们可以利用一个距离函数定义集合 \u003ccode\u003e$X$\u003c/code\u003e 上元素间的距离，即：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ d: X \\times X \\to \\mathbb{R} $$\u003c/code\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eJaccard 系数\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ccode\u003e$$ s = \\dfrac{\\left|X \\cap Y\\right|}{\\left| X \\cup Y \\right|} = \\dfrac{\\left|X \\cap Y\\right|}{\\left|X\\right| + \\left|Y\\right| - \\left|X \\cap Y\\right|} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eJaccard 系数的取值范围为：\u003ccode\u003e$\\left[0, 1\\right]$\u003c/code\u003e，0 表示两个集合没有重合，1 表示两个集合完全重合。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eDice 系数\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ccode\u003e$$ s = \\dfrac{2 \\left| X \\cap Y \\right|}{\\left|X\\right| + \\left|Y\\right|} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e与 Jaccard 系数相同，Dice 系数的取值范围为：\u003ccode\u003e$\\left[0, 1\\right]$\u003c/code\u003e，两者之间可以相互转换 \u003ccode\u003e$s_d = 2 s_j / \\left(1 + s_j\\right), s_j = s_d / \\left(2 - s_d\\right)$\u003c/code\u003e。不同于 Jaccard 系数，Dice 系数的差异函数 \u003ccode\u003e$d = 1 - s$\u003c/code\u003e 并不是一个合适的距离度量，因为其并不满足距离函数的三角不等式。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eTversky 系数\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ccode\u003e$$ s = \\dfrac{\\left| X \\cap Y \\right|}{\\left| X \\cap Y \\right| + \\alpha \\left| X \\setminus Y \\right| + \\beta \\left| Y \\setminus X \\right|} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$X \\setminus Y$\u003c/code\u003e 表示集合的相对补集。Tversky 系数可以理解为 Jaccard 系数和 Dice 系数的一般化，当 \u003ccode\u003e$\\alpha = \\beta = 1$\u003c/code\u003e 时为 Jaccard 系数，当 \u003ccode\u003e$\\alpha = \\beta = 0.5$\u003c/code\u003e 时为 Dice 系数。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eLevenshtein 距离\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLevenshtein 距离是 \u003cstrong\u003e编辑距离 (Editor Distance)\u003c/strong\u003e 的一种，指两个字串之间，由一个转成另一个所需的最少编辑操作次数。允许的编辑操作包括将一个字符替换成另一个字符，插入一个字符，删除一个字符。例如将 \u003cstrong\u003ekitten\u003c/strong\u003e 转成 \u003cstrong\u003esitting\u003c/strong\u003e，转换过程如下：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{equation*} \\begin{split} \\text{kitten} \\to \\text{sitten} \\left(k \\to s\\right) \\\\ \\text{sitten} \\to \\text{sittin} \\left(e \\to i\\right) \\\\ \\text{sittin} \\to \\text{sitting} \\left(\\  \\to g\\right) \\end{split} \\end{equation*} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e编辑距离的求解可以利用动态规划的思想优化计算的时间复杂度。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eJaro-Winkler 距离\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e对于给定的两个字符串 \u003ccode\u003e$s_1$\u003c/code\u003e 和 \u003ccode\u003e$s_2$\u003c/code\u003e，Jaro 相似度定义为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ sim = \\begin{cases} 0 \u0026amp; \\text{if} \\  m = 0 \\\\ \\dfrac{1}{3} \\left(\\dfrac{m}{\\left|s_1\\right|} + \\dfrac{m}{\\left|s_2\\right|} + \\dfrac{m-t}{m}\\right) \u0026amp; \\text{otherwise} \\end{cases} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$\\left|s_i\\right|$\u003c/code\u003e 为字符串 \u003ccode\u003e$s_i$\u003c/code\u003e 的长度，\u003ccode\u003e$m$\u003c/code\u003e 为匹配的字符的个数，\u003ccode\u003e$t$\u003c/code\u003e 换位数目的一半。如果字符串 \u003ccode\u003e$s_1$\u003c/code\u003e 和 \u003ccode\u003e$s_2$\u003c/code\u003e 相差不超过 \u003ccode\u003e$\\lfloor \\dfrac{\\max \\left(\\left|s_1\\right|, \\left|s_2\\right|\\right)}{2} \\rfloor - 1$\u003c/code\u003e，我们则认为两个字符串是匹配的。例如，对于字符串 \u003cstrong\u003eCRATE\u003c/strong\u003e 和 \u003cstrong\u003eTRACE\u003c/strong\u003e，仅 \u003cstrong\u003eR, A, E\u003c/strong\u003e 三个字符是匹配的，因此 \u003ccode\u003e$m = 3$\u003c/code\u003e，尽管 \u003cstrong\u003eC, T\u003c/strong\u003e 均出现在两个字符串中，但是他们的距离超过了 1 (即，\u003ccode\u003e$\\lfloor \\dfrac{5}{2} \\rfloor - 1$\u003c/code\u003e)，因此 \u003ccode\u003e$t = 0$\u003c/code\u003e。\u003c/p\u003e\n\u003cp\u003eJaro-Winkler 相似度给予了起始部分相同的字符串更高的分数，其定义为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ sim_w = sim_j + l p \\left(1 - sim_j\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$sim_j$\u003c/code\u003e 为字符串 \u003ccode\u003e$s_1$\u003c/code\u003e 和 \u003ccode\u003e$s_2$\u003c/code\u003e 的 Jaro 相似度，\u003ccode\u003e$l$\u003c/code\u003e 为共同前缀的长度 (规定不超过 \u003ccode\u003e$4$\u003c/code\u003e)，\u003ccode\u003e$p$\u003c/code\u003e 为调整系数 (规定不超过 \u003ccode\u003e$0.25$\u003c/code\u003e)，Winkler 将其设置为 \u003ccode\u003e$p = 0.1$\u003c/code\u003e。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e汉明距离\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e汉明距离为两个\u003cstrong\u003e等长字符串\u003c/strong\u003e对应位置的不同字符的个数，也就是将一个字符串变换成另外一个字符串所需要\u003cstrong\u003e替换\u003c/strong\u003e的字符个数。例如：\u003cstrong\u003e10\u003cspan style=\"color:#0000ff;\"\u003e1\u003c/span\u003e1\u003cspan style=\"color:#0000ff;\"\u003e1\u003c/span\u003e01\u003c/strong\u003e 与 \u003cstrong\u003e10\u003cspan style=\"color:#ff0000;\"\u003e0\u003c/span\u003e1\u003cspan style=\"color:#ff0000;\"\u003e0\u003c/span\u003e01\u003c/strong\u003e 之间的汉明距离是 2，\u003cstrong\u003e“\u003cspan style=\"color:#0000ff;\"\u003et\u003c/span\u003eo\u003cspan style=\"color:#0000ff;\"\u003en\u003c/span\u003ee\u003cspan style=\"color:#0000ff;\"\u003ed\u003c/span\u003e”\u003c/strong\u003e 与 \u003cstrong\u003e“\u003cspan style=\"color:#ff0000;\"\u003er\u003c/span\u003eo\u003cspan style=\"color:#ff0000;\"\u003es\u003c/span\u003ee\u003cspan style=\"color:#ff0000;\"\u003es\u003c/span\u003e”\u003c/strong\u003e 之间的汉明距离是 3。\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eimport textdistance as td\n\ns1 = \u0026#39;南京市长江大桥\u0026#39;\ns2 = \u0026#39;北京市三元桥\u0026#39;\n\ntd.jaccard(s1, s2)\n# 0.6666666666666666\n\ntd.sorensen_dice(s1, s2)\n# 0.46153846153846156\n\ntd.tversky(s1, s2)\n# 0.3\n\ntd.levenshtein(s1, s2)\n# 4\n\ntd.jaro(s1, s2)\n# 0.6428571428571429\n\ntd.hamming(s1, s2)\n# 5\n\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id=\"表示学习\"\u003e表示学习\u003c/h2\u003e\n\u003cp\u003e基于表示学习的文本相似度计算方法的思路如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e利用表示学习方法将不定长的文本表示为定长的实值向量。\u003c/li\u003e\n\u003cli\u003e计算转换后的实值向量相似度，用于表示两个文本的相似度。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e关于文本表示学习和实值向量相似度计算请参见之前博客：\u003ca href=\"/cn/2018/10/word-embeddings/\"\u003e词向量 (Word Embeddings)\u003c/a\u003e，\u003ca href=\"/cn/2019/01/similarity-and-distance-measurement/\"\u003e相似性和距离度量 (Similarity \u0026amp; Distance Measurement)\u003c/a\u003e，\u003ca href=\"/cn/2020/03/pre-trained-model-for-nlp/\"\u003e预训练自然语言模型 (Pre-trained Models for NLP)\u003c/a\u003e。\u003c/p\u003e\n\u003ch1 id=\"文本词法-句法和语义角度\"\u003e文本词法，句法和语义角度\u003c/h1\u003e\n\u003cblockquote\u003e\n\u003cp\u003e本节主要参考自《基于词法、句法和语义的句子相似度计算方法》\u003csup id=\"fnref:10\"\u003e\u003ca href=\"#fn:10\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e10\u003c/a\u003e\u003c/sup\u003e。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e一段文本的内容分析由浅及深可以分为词法，句法和语义三个层次。\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e词法，以词为对象，研究包括分词，词性和命名实体等。\u003c/li\u003e\n\u003cli\u003e句法，以句子为对象，研究包括句子成分和句子结构等。\u003c/li\u003e\n\u003cli\u003e语义，研究文字所表达的含义和蕴含的知识等。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e词法和句法可以统一成为语法，如下图所示：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2020-10-31-text-similarity/lexical-syntax.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003ch2 id=\"词法\"\u003e词法\u003c/h2\u003e\n\u003cp\u003e词法层以单个句子作为输入，其输出为已标记（词性，命名实体等）的词汇序列。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2020-10-31-text-similarity/lexical-demo.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cp\u003e词汇序列的相似度计算可以采用上文中的距离度量等方式实现。\u003c/p\u003e\n\u003ch2 id=\"句法\"\u003e句法\u003c/h2\u003e\n\u003cp\u003e句法层用于研究句子各个组成部分及其排列顺序，将文本分解为句法单位，以理解句法元素的排列方式。句法层接收词法层分析后的将其转化为依存图。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2020-10-31-text-similarity/syntax-demo.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cp\u003e对于依存图，我们可以利用三元组 \u003ccode\u003e$S = \\left(V_1, E, V_2\\right)$\u003c/code\u003e 表示任意一个依存关系，然后通过统计计算两个文本的依存图的三元组集合之间的相似度来评价句法层的相似度。此外，也可以从树结构的角度直接评价依存句法的相似度，更多细节可参考相关论文 \u003csup id=\"fnref:11\"\u003e\u003ca href=\"#fn:11\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e11\u003c/a\u003e\u003c/sup\u003e \u003csup id=\"fnref:12\"\u003e\u003ca href=\"#fn:12\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e12\u003c/a\u003e\u003c/sup\u003e。\u003c/p\u003e\n\u003ch2 id=\"语义\"\u003e语义\u003c/h2\u003e\n\u003cp\u003e语义层用于研究文本所蕴含的意义。例如“父亲”和“爸爸”在词法层完全不同，但在语义层却具有相同的含义。针对语义相似度的两种深度学习范式如下：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2020-10-31-text-similarity/deep-learning-paradigms-for-text-similarity.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cp\u003e第一种范式首先通过神经网络获取文本的向量表示，再通过向量之间的相似度来衡量文本的语义相似度。这种范式在提取特征时不考虑另一个文本的信息，更适合做大规模的语义相似召回，例如：DSSM \u003csup id=\"fnref:13\"\u003e\u003ca href=\"#fn:13\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e13\u003c/a\u003e\u003c/sup\u003e，ARC-I \u003csup id=\"fnref:14\"\u003e\u003ca href=\"#fn:14\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e14\u003c/a\u003e\u003c/sup\u003e，CNTN \u003csup id=\"fnref:15\"\u003e\u003ca href=\"#fn:15\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e15\u003c/a\u003e\u003c/sup\u003e，LSTM-RNN \u003csup id=\"fnref:16\"\u003e\u003ca href=\"#fn:16\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e16\u003c/a\u003e\u003c/sup\u003e 等。\u003c/p\u003e\n\u003cp\u003e第二种范式首先通过深度模型提取两个文本的交叉特征，得到匹配信号张量，再聚合为匹配分数。这种范式同时考虑两个文本的输入信息，更适合做小规模的语义相似精排，例如：ARC-II \u003csup id=\"fnref1:14\"\u003e\u003ca href=\"#fn:14\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e14\u003c/a\u003e\u003c/sup\u003e，MatchPyramid \u003csup id=\"fnref:17\"\u003e\u003ca href=\"#fn:17\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e17\u003c/a\u003e\u003c/sup\u003e，Match-SRNN \u003csup id=\"fnref:18\"\u003e\u003ca href=\"#fn:18\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e18\u003c/a\u003e\u003c/sup\u003e，Duet \u003csup id=\"fnref:19\"\u003e\u003ca href=\"#fn:19\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e19\u003c/a\u003e\u003c/sup\u003e 等。\u003c/p\u003e\n\u003ch1 id=\"文本长度角度\"\u003e文本长度角度\u003c/h1\u003e\n\u003cp\u003e从文本长度角度出发，我们可以粗略的将文本分类为\u003cstrong\u003e短文本\u003c/strong\u003e和\u003cstrong\u003e长文本\u003c/strong\u003e。\u003cstrong\u003e短文本\u003c/strong\u003e包括“字词”，“短语”，“句子”等相对比较短的文本形式，\u003cstrong\u003e长文本\u003c/strong\u003e包括“段落”，“篇章”等相对比较长的文本形式。\u003c/p\u003e\n\u003ch2 id=\"短文本-v-s-短文本\"\u003e短文本 v.s. 短文本\u003c/h2\u003e\n\u003cp\u003e短文本同短文本的常见比较形式有：关键词（字词）同文本标题（句子）的匹配，相似查询（句子）的匹配等。如果单纯的希望获取字符层面的差异，可以通过距离度量进行相似度比较。如果需要从语义的角度获取相似度，则可以利用表示学习对需要比对的文本进行表示，在通过语义向量之间的相似程度来衡量原始文本之间的相似度，详情可参见上文。\u003c/p\u003e\n\u003ch2 id=\"短文本-v-s-长文本\"\u003e短文本 v.s. 长文本\u003c/h2\u003e\n\u003cp\u003e短文本同长文本的比较多见于文档的搜索，即给定相关的查询（字词），给出最相关的文档（段落和篇章）。对于这类问题常见的解决方式是对长文本利用 TF-IDF，BM25等方法或进行主题建模后，再同查询的关键词进行匹配计算相似度度。\u003c/p\u003e\n\u003ch2 id=\"长文本-v-s-长文本\"\u003e长文本 v.s. 长文本\u003c/h2\u003e\n\u003cp\u003e长文本同长文本的比较多见于文档的匹配和去重，对于这类问题常见的解决方式是利用关键词提取获取长文本的特征向量，然后利用特征向量之间的相似度衡量对应文本的相似程度。在针对海量文本的去重，还以应用 \u003ca href=\"/cn/2020/08/nearest-neighbor-search/\"\u003eSimHash\u003c/a\u003e 等技术对文本生成一个指纹，从而实现快速去重。\u003c/p\u003e\n\u003cdiv class=\"footnotes\" role=\"doc-endnotes\"\u003e\n\u003chr/\u003e\n\u003col\u003e\n\u003cli id=\"fn:1\"\u003e\n\u003cp\u003e\u003ca href=\"https://zh.wikipedia.org/wiki/%E4%B8%BB%E9%A2%98%E6%A8%A1%E5%9E%8B\"\u003ehttps://zh.wikipedia.org/wiki/主题模型\u003c/a\u003e \u003ca href=\"#fnref:1\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:2\"\u003e\n\u003cp\u003eManning, C. D., Schütze, H., \u0026amp; Raghavan, P. (2008). \u003cem\u003eIntroduction to information retrieval\u003c/em\u003e. Cambridge university press. \u003ca href=\"#fnref:2\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:3\"\u003e\n\u003cp\u003eMihalcea, R., \u0026amp; Tarau, P. (2004, July). Textrank: Bringing order into text. In \u003cem\u003eProceedings of the 2004 conference on empirical methods in natural language processing\u003c/em\u003e (pp. 404-411). \u003ca href=\"#fnref:3\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:4\"\u003e\n\u003cp\u003ePage, L., Brin, S., Motwani, R., \u0026amp; Winograd, T. (1999). \u003cem\u003eThe PageRank citation ranking: Bringing order to the web\u003c/em\u003e. Stanford InfoLab. \u003ca href=\"#fnref:4\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:5\"\u003e\n\u003cp\u003eDeerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., \u0026amp; Harshman, R. (1990). Indexing by latent semantic analysis. \u003cem\u003eJournal of the American society for information science\u003c/em\u003e, 41(6), 391-407. \u003ca href=\"#fnref:5\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:6\"\u003e\n\u003cp\u003eHofmann, T. (1999, August). Probabilistic latent semantic indexing. In \u003cem\u003eProceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval\u003c/em\u003e (pp. 50-57). \u003ca href=\"#fnref:6\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:7\"\u003e\n\u003cp\u003eBlei, D. M., Ng, A. Y., \u0026amp; Jordan, M. I. (2003). Latent dirichlet allocation. \u003cem\u003eJournal of machine Learning research\u003c/em\u003e, 3(Jan), 993-1022. \u003ca href=\"#fnref:7\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:8\"\u003e\n\u003cp\u003eRickjin(靳志辉). 2013. LDA数学八卦 \u003ca href=\"#fnref:8\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:9\"\u003e\n\u003cp\u003eTeh, Y. W., Jordan, M. I., Beal, M. J., \u0026amp; Blei, D. M. (2006). Hierarchical dirichlet processes. \u003cem\u003eJournal of the american statistical association\u003c/em\u003e, 101(476), 1566-1581. \u003ca href=\"#fnref:9\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:10\"\u003e\n\u003cp\u003e翟社平, 李兆兆, 段宏宇, 李婧, \u0026amp; 董迪迪. (2019). 基于词法, 句法和语义的句子相似度计算方法. \u003cem\u003e东南大学学报: 自然科学版\u003c/em\u003e, 49(6), 1094-1100. \u003ca href=\"#fnref:10\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:11\"\u003e\n\u003cp\u003eZhang, K., \u0026amp; Shasha, D. (1989). Simple fast algorithms for the editing distance between trees and related problems. \u003cem\u003eSIAM journal on computing\u003c/em\u003e, 18(6), 1245-1262. \u003ca href=\"#fnref:11\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:12\"\u003e\n\u003cp\u003eMeila, M., \u0026amp; Jordan, M. I. (2000). Learning with mixtures of trees. \u003cem\u003eJournal of Machine Learning Research\u003c/em\u003e, 1(Oct), 1-48. \u003ca href=\"#fnref:12\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:13\"\u003e\n\u003cp\u003eHuang, P. S., He, X., Gao, J., Deng, L., Acero, A., \u0026amp; Heck, L. (2013, October). Learning deep structured semantic models for web search using clickthrough data. In \u003cem\u003eProceedings of the 22nd ACM international conference on Information \u0026amp; Knowledge Management\u003c/em\u003e (pp. 2333-2338). \u003ca href=\"#fnref:13\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:14\"\u003e\n\u003cp\u003eHu, B., Lu, Z., Li, H., \u0026amp; Chen, Q. (2014). Convolutional neural network architectures for matching natural language sentences. In \u003cem\u003eAdvances in neural information processing systems\u003c/em\u003e (pp. 2042-2050). \u003ca href=\"#fnref:14\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e \u003ca href=\"#fnref1:14\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:15\"\u003e\n\u003cp\u003eQiu, X., \u0026amp; Huang, X. (2015, June). Convolutional neural tensor network architecture for community-based question answering. In \u003cem\u003eTwenty-Fourth international joint conference on artificial intelligence\u003c/em\u003e. \u003ca href=\"#fnref:15\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:16\"\u003e\n\u003cp\u003ePalangi, H., Deng, L., Shen, Y., Gao, J., He, X., Chen, J., … \u0026amp; Ward, R. (2016). Deep sentence embedding using long short-term memory networks: Analysis and application to information retrieval. \u003cem\u003eIEEE/ACM Transactions on Audio, Speech, and Language Processing\u003c/em\u003e, 24(4), 694-707. \u003ca href=\"#fnref:16\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:17\"\u003e\n\u003cp\u003ePang, L., Lan, Y., Guo, J., Xu, J., Wan, S., \u0026amp; Cheng, X. (2016). Text matching as image recognition. In \u003cem\u003eProceedings of the Thirtieth AAAI Conference on Artificial Intelligence (AAAI\u0026#39;16)\u003c/em\u003e. (pp. 2793–2799). \u003ca href=\"#fnref:17\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:18\"\u003e\n\u003cp\u003eWan, S., Lan, Y., Xu, J., Guo, J., Pang, L., \u0026amp; Cheng, X. (2016, July). Match-SRNN: modeling the recursive matching structure with spatial RNN. In \u003cem\u003eProceedings of the Twenty-Fifth International Joint Conference on Artificial Intelligence\u003c/em\u003e (pp. 2922-2928). \u003ca href=\"#fnref:18\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:19\"\u003e\n\u003cp\u003eMitra, B., Diaz, F., \u0026amp; Craswell, N. (2017, April). Learning to match using local and distributed representations of text for web search. In \u003cem\u003eProceedings of the 26th International Conference on World Wide Web\u003c/em\u003e (pp. 1291-1299). \u003ca href=\"#fnref:19\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/div\u003e\n\n\n\n\n\n\u003cdiv class=\"donate\"\u003e\n  \u003cdiv class=\"donate-header\"\u003e\u003c/div\u003e\n  \u003cdiv class=\"donate-slug\" id=\"donate-slug\"\u003etext-similarity\u003c/div\u003e\n  \u003cbutton class=\"donate-button\"\u003e赞 赏\u003c/button\u003e\n  \u003cdiv class=\"donate-footer\"\u003e「真诚赞赏，手留余香」\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"donate-modal-wrapper\"\u003e\n  \u003cdiv class=\"donate-modal\"\u003e\n    \u003cdiv class=\"donate-box\"\u003e\n      \u003cdiv class=\"donate-box-content\"\u003e\n        \u003cdiv class=\"donate-box-content-inner\"\u003e\n          \u003cdiv class=\"donate-box-header\"\u003e「真诚赞赏，手留余香」\u003c/div\u003e\n          \u003cdiv class=\"donate-box-body\"\u003e\n            \u003cdiv class=\"donate-box-money\"\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-2\" data-v=\"2\" data-unchecked=\"￥ 2\" data-checked=\"2 元\"\u003e￥ 2\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-5\" data-v=\"5\" data-unchecked=\"￥ 5\" data-checked=\"5 元\"\u003e￥ 5\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-10\" data-v=\"10\" data-unchecked=\"￥ 10\" data-checked=\"10 元\"\u003e￥ 10\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-50\" data-v=\"50\" data-unchecked=\"￥ 50\" data-checked=\"50 元\"\u003e￥ 50\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-100\" data-v=\"100\" data-unchecked=\"￥ 100\" data-checked=\"100 元\"\u003e￥ 100\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-custom\" data-v=\"custom\" data-unchecked=\"任意金额\" data-checked=\"任意金额\"\u003e任意金额\u003c/button\u003e\n            \u003c/div\u003e\n            \u003cdiv class=\"donate-box-pay\"\u003e\n              \u003cimg class=\"donate-box-pay-qrcode\" id=\"donate-box-pay-qrcode\" src=\"\"/\u003e\n            \u003c/div\u003e\n          \u003c/div\u003e\n          \u003cdiv class=\"donate-box-footer\"\u003e\n            \u003cdiv class=\"donate-box-pay-method donate-box-pay-method-checked\" data-v=\"wechat-pay\"\u003e\n              \u003cimg class=\"donate-box-pay-method-image\" id=\"donate-box-pay-method-image-wechat-pay\" src=\"\"/\u003e\n            \u003c/div\u003e\n            \u003cdiv class=\"donate-box-pay-method\" data-v=\"alipay\"\u003e\n              \u003cimg class=\"donate-box-pay-method-image\" id=\"donate-box-pay-method-image-alipay\" src=\"\"/\u003e\n            \u003c/div\u003e\n          \u003c/div\u003e\n        \u003c/div\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n    \u003cbutton type=\"button\" class=\"donate-box-close-button\"\u003e\n      \u003csvg class=\"donate-box-close-button-icon\" fill=\"#fff\" viewBox=\"0 0 24 24\" width=\"24\" height=\"24\"\u003e\u003cpath d=\"M13.486 12l5.208-5.207a1.048 1.048 0 0 0-.006-1.483 1.046 1.046 0 0 0-1.482-.005L12 10.514 6.793 5.305a1.048 1.048 0 0 0-1.483.005 1.046 1.046 0 0 0-.005 1.483L10.514 12l-5.208 5.207a1.048 1.048 0 0 0 .006 1.483 1.046 1.046 0 0 0 1.482.005L12 13.486l5.207 5.208a1.048 1.048 0 0 0 1.483-.006 1.046 1.046 0 0 0 .005-1.482L13.486 12z\" fill-rule=\"evenodd\"\u003e\u003c/path\u003e\u003c/svg\u003e\n    \u003c/button\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\u003cscript type=\"text/javascript\" src=\"/js/donate.js\"\u003e\u003c/script\u003e\n\n\n  \u003cfooter\u003e\n  \n\u003cnav class=\"post-nav\"\u003e\n  \u003cspan class=\"nav-prev\"\u003e← \u003ca href=\"/cn/2020/08/life-before-30/\"\u003e而立之前 (Life before 30)\u003c/a\u003e\u003c/span\u003e\n  \u003cspan class=\"nav-next\"\u003e\u003ca href=\"/cn/2020/11/network-representation-measures-and-metrics/\"\u003e网络表示，测度和度量 (Network Representation, Measures \u0026amp; Metrics)\u003c/a\u003e →\u003c/span\u003e\n\u003c/nav\u003e\n\n\n\n\n\u003cins class=\"adsbygoogle\" style=\"display:block; text-align:center;\" data-ad-layout=\"in-article\" data-ad-format=\"fluid\" data-ad-client=\"ca-pub-2608165017777396\" data-ad-slot=\"8302038603\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n  (adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/js-cookie@3.0.5/dist/js.cookie.min.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"/js/toggle-theme.js\"\u003e\u003c/script\u003e\n\n\n\u003cscript src=\"/js/no-highlight.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"/js/math-code.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"/js/heading-anchor.js\"\u003e\u003c/script\u003e\n\n\n\n\u003csection class=\"comments\"\u003e\n\u003cscript src=\"https://giscus.app/client.js\" data-repo=\"leovan/leovan.me\" data-repo-id=\"MDEwOlJlcG9zaXRvcnkxMTMxOTY0Mjc=\" data-category=\"Comments\" data-category-id=\"DIC_kwDOBr89i84CT-R7\" data-mapping=\"pathname\" data-strict=\"1\" data-reactions-enabled=\"1\" data-emit-metadata=\"0\" data-input-position=\"top\" data-theme=\"preferred_color_scheme\" data-lang=\"zh-CN\" data-loading=\"lazy\" crossorigin=\"anonymous\" defer=\"\"\u003e\n\u003c/script\u003e\n\u003c/section\u003e\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"//cdn.jsdelivr.net/npm/prismjs@1.29.0/prism.min.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"//cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"//cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/toolbar/prism-toolbar.min.js\"\u003e\u003c/script\u003e\n\u003cscript\u003e\n  (function() {\n    if (!self.Prism) {\n      return;\n    }\n\n    \n    Prism.languages.dos = Prism.languages.powershell;\n    Prism.languages.gremlin = Prism.languages.groovy;\n\n    let languages = {\n      'r': 'R', 'python': 'Python', 'xml': 'XML', 'html': 'HTML',\n      'yaml': 'YAML', 'latex': 'LaTeX', 'tex': 'TeX',\n      'powershell': 'PowerShell', 'javascript': 'JavaScript',\n      'dos': 'DOS', 'qml': 'QML', 'json': 'JSON', 'bash': 'Bash',\n      'text': 'Text', 'txt': 'Text', 'sparql': 'SPARQL',\n      'gremlin': 'Gremlin', 'cypher': 'Cypher', 'ngql': 'nGQL',\n      'shell': 'Shell', 'sql': 'SQL', 'apacheconf': 'Apache Configuration', 'c': 'C', 'css': 'CSS'\n    };\n\n    Prism.hooks.add('before-highlight', function(env) {\n      if (env.language !== 'plain') {\n        let language = languages[env.language] || env.language;\n        env.element.setAttribute('data-language', language);\n      }\n    });\n\n    \n    let ClipboardJS = window.ClipboardJS || undefined;\n\n    Prism.plugins.toolbar.registerButton('copy-to-clipboard', function(env) {\n      let linkCopy = document.createElement('button');\n      linkCopy.classList.add('prism-button-copy');\n\n      registerClipboard();\n\n      return linkCopy;\n\n      function registerClipboard() {\n        let clip = new ClipboardJS(linkCopy, {\n          'text': function () {\n            return env.code;\n          }\n        });\n\n        clip.on('success', function() {\n          linkCopy.classList.add('prism-button-copy-success');\n          resetText();\n        });\n        clip.on('error', function () {\n          linkCopy.classList.add('prism-button-copy-error');\n          resetText();\n        });\n      }\n\n      function resetText() {\n        setTimeout(function () {\n          linkCopy.classList.remove('prism-button-copy-success');\n          linkCopy.classList.remove('prism-button-copy-error');\n        }, 1600);\n      }\n    });\n  })();\n\u003c/script\u003e\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cscript async=\"\" src=\"/js/center-img.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/right-quote.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/external-link.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/alt-title.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/figure.js\"\u003e\u003c/script\u003e\n\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js\"\u003e\u003c/script\u003e\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/vanilla-back-to-top@latest/dist/vanilla-back-to-top.min.js\"\u003e\u003c/script\u003e\n\u003cscript\u003e\naddBackToTop({\n  diameter: 48\n});\n\u003c/script\u003e\n\n  \u003chr/\u003e\n  \u003cdiv class=\"copyright no-border-bottom\"\u003e\n    \u003cdiv class=\"copyright-author-year\"\u003e\n      \u003cspan\u003eCopyright © 2017-2024 \u003ca href=\"/\"\u003e范叶亮 | Leo Van\u003c/a\u003e\u003c/span\u003e\n    \u003c/div\u003e\n  \u003c/div\u003e\n  \u003c/footer\u003e\n  \u003c/article\u003e",
  "Date": "2020-10-31T00:00:00Z",
  "Author": "范叶亮"
}