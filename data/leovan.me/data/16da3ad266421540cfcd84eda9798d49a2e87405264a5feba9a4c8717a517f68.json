{
  "Source": "leovan.me",
  "Title": "相似性和距离度量 (Similarity \u0026 Distance Measurement)",
  "Link": "https://leovan.me/cn/2019/01/similarity-and-distance-measurement/",
  "Content": "\u003carticle class=\"main\"\u003e\n    \u003cheader class=\"content-title\"\u003e\n    \n\u003ch1 class=\"title\"\u003e\n  \n  相似性和距离度量 (Similarity \u0026amp; Distance Measurement)\n  \n\u003c/h1\u003e\n\n\n\n\n\n\n\n\u003ch2 class=\"author-date\"\u003e范叶亮 / \n2019-01-01\u003c/h2\u003e\n\n\n\n\u003ch3 class=\"post-meta\"\u003e\n\n\n\u003cstrong\u003e分类: \u003c/strong\u003e\n\u003ca href=\"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0\"\u003e机器学习\u003c/a\u003e\n\n\n\n\n/\n\n\n\n\n\u003cstrong\u003e标签: \u003c/strong\u003e\n\u003cspan\u003e相似性度量\u003c/span\u003e, \u003cspan\u003eSimilarity Measurement\u003c/span\u003e, \u003cspan\u003e距离度量\u003c/span\u003e, \u003cspan\u003eDistance Measurement\u003c/span\u003e, \u003cspan\u003e明可夫斯基距离\u003c/span\u003e, \u003cspan\u003e明氏距离\u003c/span\u003e, \u003cspan\u003eMinkowski Distance\u003c/span\u003e, \u003cspan\u003e欧式距离\u003c/span\u003e, \u003cspan\u003eEuclidean Metric\u003c/span\u003e, \u003cspan\u003e曼哈顿距离\u003c/span\u003e, \u003cspan\u003e出租车距离\u003c/span\u003e, \u003cspan\u003eManhattan Distance\u003c/span\u003e, \u003cspan\u003e切比雪夫距离\u003c/span\u003e, \u003cspan\u003eChebyshev Distance\u003c/span\u003e, \u003cspan\u003e马哈拉诺比斯距离\u003c/span\u003e, \u003cspan\u003e马氏距离\u003c/span\u003e, \u003cspan\u003eMahalanobis Distance\u003c/span\u003e, \u003cspan\u003e向量内积\u003c/span\u003e, \u003cspan\u003eInner Product of Vectors\u003c/span\u003e, \u003cspan\u003e余弦距离\u003c/span\u003e, \u003cspan\u003eCosine Distance\u003c/span\u003e, \u003cspan\u003e余弦相似度\u003c/span\u003e, \u003cspan\u003eCosine Similarity\u003c/span\u003e, \u003cspan\u003e相关系数\u003c/span\u003e, \u003cspan\u003eCorrelation\u003c/span\u003e, \u003cspan\u003e皮尔逊相关系数\u003c/span\u003e, \u003cspan\u003ePearson Correlation\u003c/span\u003e, \u003cspan\u003eJaccard 系数\u003c/span\u003e, \u003cspan\u003eDice 系数\u003c/span\u003e, \u003cspan\u003eTversky 系数\u003c/span\u003e, \u003cspan\u003e编辑距离\u003c/span\u003e, \u003cspan\u003eEdit Distance\u003c/span\u003e, \u003cspan\u003e莱文斯坦距离\u003c/span\u003e, \u003cspan\u003eLevenshtein Distance\u003c/span\u003e, \u003cspan\u003e汉明距离\u003c/span\u003e, \u003cspan\u003eHamming Distance\u003c/span\u003e, \u003cspan\u003e熵\u003c/span\u003e, \u003cspan\u003eEntropy\u003c/span\u003e, \u003cspan\u003e条件熵\u003c/span\u003e, \u003cspan\u003eConditional Entropy\u003c/span\u003e, \u003cspan\u003e联合熵\u003c/span\u003e, \u003cspan\u003eJoint Entropy\u003c/span\u003e, \u003cspan\u003e互信息\u003c/span\u003e, \u003cspan\u003eMutual Information\u003c/span\u003e, \u003cspan\u003e相对熵\u003c/span\u003e, \u003cspan\u003eRelative Entropy\u003c/span\u003e, \u003cspan\u003eKL 散度\u003c/span\u003e, \u003cspan\u003eKullback-Leibler Divergence\u003c/span\u003e, \u003cspan\u003eJS 散度\u003c/span\u003e, \u003cspan\u003eJensen-Shannon Divergence\u003c/span\u003e, \u003cspan\u003e推土机距离\u003c/span\u003e, \u003cspan\u003eEarth Mover Distance\u003c/span\u003e, \u003cspan\u003eWasserstein Distance\u003c/span\u003e, \u003cspan\u003eDTW 距离\u003c/span\u003e, \u003cspan\u003eDynamic Time Warping Distance\u003c/span\u003e, \u003cspan\u003e流形距离\u003c/span\u003e, \u003cspan\u003eDisntace of Manifold\u003c/span\u003e\n\n\n\n\n/\n\n\n\u003cstrong\u003e字数: \u003c/strong\u003e\n4201\n\u003c/h3\u003e\n\n\n\n\u003chr/\u003e\n\n\n\n    \n    \n    \u003cins class=\"adsbygoogle\" style=\"display:block; text-align:center;\" data-ad-layout=\"in-article\" data-ad-format=\"fluid\" data-ad-client=\"ca-pub-2608165017777396\" data-ad-slot=\"1261604535\"\u003e\u003c/ins\u003e\n    \u003cscript\u003e\n    (adsbygoogle = window.adsbygoogle || []).push({});\n    \u003c/script\u003e\n    \n    \n    \u003c/header\u003e\n\n\n\n\u003cdiv class=\"toc-depth-2\"\u003e\u003cnav id=\"TableOfContents\"\u003e\n  \u003cul\u003e\n    \u003cli\u003e\u003ca href=\"#明可夫斯基距离-明氏距离-minkowski-distance\"\u003e明可夫斯基距离 (明氏距离, Minkowski Distance)\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#马哈拉诺比斯距离-马氏距离-mahalanobis-distance\"\u003e马哈拉诺比斯距离 (马氏距离, Mahalanobis Distance)\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#向量内积-inner-product-of-vectors\"\u003e向量内积 (Inner Product of Vectors)\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#集合距离-distance-of-sets\"\u003e集合距离 (Distance of Sets)\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#字符串距离-distance-of-strings\"\u003e字符串距离 (Distance of Strings)\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#信息论距离-information-theory-distance\"\u003e信息论距离 (Information Theory Distance)\u003c/a\u003e\u003c/li\u003e\n    \u003cli\u003e\u003ca href=\"#其他距离-other-distance\"\u003e其他距离 (Other Distance)\u003c/a\u003e\u003c/li\u003e\n  \u003c/ul\u003e\n\u003c/nav\u003e\u003c/div\u003e\n\n\n\u003cp\u003e相似性度量 (Similarity Measurement) 用于衡量两个元素之间的相似性程度或两者之间的距离 (Distance)。距离衡量的是指元素之间的不相似性 (Dissimilarity)，通常情况下我们可以利用一个距离函数定义集合 \u003ccode\u003e$X$\u003c/code\u003e 上元素间的距离，即：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ d: X \\times X \\to \\mathbb{R} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e同时，对于集合 \u003ccode\u003e$X$\u003c/code\u003e 内的元素 \u003ccode\u003e$x, y, z$\u003c/code\u003e，距离函数一般满足如下条件：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003e$d \\left(x, y\\right) \\geq 0$\u003c/code\u003e (非负性)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e$d \\left(x, y\\right) = 0, \\text{当且仅当} \\ x = y$\u003c/code\u003e (同一性)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e$d \\left(x, y\\right) = d \\left(y, x\\right)$\u003c/code\u003e (对称性)\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e$d \\left(x, z\\right) \\leq d \\left(x, y\\right) + d \\left(y, z\\right)$\u003c/code\u003e (三角不等式)\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch1 id=\"明可夫斯基距离-明氏距离-minkowski-distance\"\u003e明可夫斯基距离 (明氏距离, Minkowski Distance)\u003c/h1\u003e\n\u003cp\u003e对于点 \u003ccode\u003e$x = \\left(x_1, x_2, ..., x_n\\right)$\u003c/code\u003e 和点 \u003ccode\u003e$y = \\left(y_1, y_2, ..., y_n\\right)$\u003c/code\u003e，\u003ccode\u003e$p$\u003c/code\u003e \u003cstrong\u003e阶明可夫斯基距离\u003c/strong\u003e 定义为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ d \\left(x, y\\right) = \\left(\\sum_{i=1}^{n} |x_i - y_i|^p\\right)^{\\frac{1}{p}} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e当 \u003ccode\u003e$p = 1$\u003c/code\u003e 时，称之为 \u003cstrong\u003e曼哈顿距离 (Manhattan Distance)\u003c/strong\u003e 或 \u003cstrong\u003e出租车距离\u003c/strong\u003e：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ d \\left(x, y\\right) = \\sum_{i=1}^{n} |x_i - y_i| $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e当 \u003ccode\u003e$p = 2$\u003c/code\u003e 时，称之为 \u003cstrong\u003e欧式距离 (Euclidean Distance)\u003c/strong\u003e ：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ d \\left(x, y\\right) = \\sqrt{\\sum_{i=1}^{n} \\left(x_i - y_i\\right)^2} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2019-01-01-similarity-and-distance-measurement/manhattan-distance.svg\" alt=\"Manhattan Distance\"/\u003e\u003c/p\u003e\n\u003cp\u003e上图中 \u003cspan style=\"color:#00d100;\"\u003e\u003cstrong\u003e绿色\u003c/strong\u003e\u003c/span\u003e 的直线为两点间的欧式距离，\u003cspan style=\"color:#ff0000;\"\u003e\u003cstrong\u003e红色\u003c/strong\u003e\u003c/span\u003e \u003cspan style=\"color:#ffd600;\"\u003e\u003cstrong\u003e黄色\u003c/strong\u003e\u003c/span\u003e \u003cspan style=\"color:#0000ff;\"\u003e\u003cstrong\u003e蓝色\u003c/strong\u003e\u003c/span\u003e 的折线均为两点间的曼哈顿距离，不难看出 3 条折线的长度是相同的。\u003c/p\u003e\n\u003cp\u003e当 \u003ccode\u003e$p \\to \\infty$\u003c/code\u003e 时，称之为 \u003cstrong\u003e切比雪夫距离 (Chebyshev Distance)\u003c/strong\u003e ：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ d \\left(x, y\\right) = \\lim_{p \\to \\infty} \\left(\\sum_{i=1}^{n} |x_i - y_i|^p\\right)^{\\frac{1}{p}} = \\max_{i=1}^{n} |x_i - y_i| $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e下图展示了不同的 \u003ccode\u003e$p$\u003c/code\u003e 值下单位圆，即 \u003ccode\u003e$x^p + y^p = 1$\u003c/code\u003e，便于大家理解不同 \u003ccode\u003e$p$\u003c/code\u003e 值下的明可夫斯基距离：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2019-01-01-similarity-and-distance-measurement/2D-unit-balls.png\" alt=\"2D Unit Balls\"/\u003e\u003c/p\u003e\n\u003ch1 id=\"马哈拉诺比斯距离-马氏距离-mahalanobis-distance\"\u003e马哈拉诺比斯距离 (马氏距离, Mahalanobis Distance)\u003c/h1\u003e\n\u003cp\u003e马哈拉诺比斯距离表示数据的 \u003cstrong\u003e协方差距离\u003c/strong\u003e，与欧式距离不同其考虑到各种特性之间的联系是 \u003cstrong\u003e尺度无关 (Scale Invariant)\u003c/strong\u003e 的。对于一个协方差矩阵为 \u003ccode\u003e$\\sum$\u003c/code\u003e 的变量 \u003ccode\u003e$x$\u003c/code\u003e 和 \u003ccode\u003e$y$\u003c/code\u003e，马氏距离定义为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ d \\left(x, y\\right) = \\sqrt{\\left(x - y\\right)^{\\top} {\\sum}^{-1} \\left(x - y\\right)} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e马氏距离的最大优势就是其不受不同维度之间量纲的影响，同时引入的问题便是扩大了变化量较小的变量的影响。以下图为例 (源码详见 \u003ca href=\"https://github.com/leovan/leovan.me/tree/main/static/codes/cn/2019-01-01-similarity-and-distance-measurement/mahalanobis-distance.R\"\u003e这里\u003c/a\u003e)：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2019-01-01-similarity-and-distance-measurement/mahalanobis-distance.png\" alt=\"Mahalanobis Distance\"/\u003e\u003c/p\u003e\n\u003cp\u003e左侧图中根据欧式距离计算，\u003cspan style=\"color:#F07769;\"\u003e\u003cstrong\u003e红色\u003c/strong\u003e\u003c/span\u003e 的点距离 \u003cspan style=\"color:#34BA27;\"\u003e\u003cstrong\u003e绿色\u003c/strong\u003e\u003c/span\u003e 的点更近一些，右侧图是根据马氏距离进行座标变换后的示意图，不难看出此时 \u003cspan style=\"color:#F07769;\"\u003e\u003cstrong\u003e红色\u003c/strong\u003e\u003c/span\u003e 的点距离 \u003cspan style=\"color:#6C9BFF;\"\u003e\u003cstrong\u003e蓝色\u003c/strong\u003e\u003c/span\u003e 的点更近一些。\u003c/p\u003e\n\u003ch1 id=\"向量内积-inner-product-of-vectors\"\u003e向量内积 (Inner Product of Vectors)\u003c/h1\u003e\n\u003cp\u003e在欧几里得几何中，两个笛卡尔坐标向量的点积常称为内积，向量内积是两个向量的长度与它们夹角余弦的积，定义为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ x \\cdot y = \\sum_{i=1}^{n}{x_i y_i} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e从代数角度看，先对两个数字序列中的每组对应元素求积，再对所有积求和，结果即为点积。从几何角度看，点积则是两个向量的长度与它们夹角余弦的积。在欧几里得空间中，点积可以直观地定义为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ x \\cdot y = \\left| x \\right| \\left| y \\right| \\cos \\theta $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e余弦相似度 (Cosine Similarity)\u003c/strong\u003e 可以利用两个向量夹角的 cos 值定义，即：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ s \\left(x, y\\right) = \\cos \\left(\\theta\\right) = \\dfrac{x \\cdot y}{\\left| x \\right| \\left| y \\right|} = \\dfrac{\\sum_{i=1}^{n}{x_i y_i}}{\\sqrt{\\sum_{i=1}^{n}{x_i^2}} \\sqrt{\\sum_{i=1}^{n}{y_i^2}}} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e余弦相似度的取值范围为：\u003ccode\u003e$\\left[-1, 1\\right]$\u003c/code\u003e，1 表示两者完全正相关，-1 表示两者完全负相关，0 表示两者之间独立。余弦相似度与向量的长度无关，只与向量的方向有关，但余弦相似度会受到向量平移的影响。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e皮尔逊相关系数 (Pearson Correlation)\u003c/strong\u003e 解决了余弦相似度会收到向量平移影响的问题，其定义为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\rho \\left(x, y\\right) = \\dfrac{\\text{cov} \\left(x, y\\right)}{\\sigma_x \\sigma_y} = \\dfrac{E \\left[\\left(x - \\mu_x\\right) \\left(y - \\mu_y\\right)\\right]}{\\sigma_x \\sigma_y} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$\\text{cov}$\u003c/code\u003e 表示协方差，\u003ccode\u003e$E$\u003c/code\u003e 表示期望，\u003ccode\u003e$\\mu$\u003c/code\u003e 表示均值，\u003ccode\u003e$\\sigma$\u003c/code\u003e 表示标准差。对于样本的皮尔逊相关系数，可以通过如下方式计算：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{equation} \\begin{split} r \u0026amp;= \\dfrac{\\sum_{i=1}^{n}{\\left(x_i - \\bar{x}\\right) \\left(y_i - \\bar{y}\\right)}}{\\sqrt{\\sum_{i=1}^{n}{\\left(x_i - \\bar{x}\\right)^2}} \\sqrt{\\sum_{i=1}^{n}{\\left(y_i - \\bar{y}\\right)^2}}} \\\\ \u0026amp;= \\dfrac{1}{n-1} \\sum_{i=1}^{n}{\\left(\\dfrac{x_i - \\bar{x}}{\\sigma_x}\\right) \\left(\\dfrac{y_i - \\bar{y}}{\\sigma_y}\\right)} \\end{split} \\end{equation} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e皮尔逊相关系数的取值范围为：\u003ccode\u003e$\\left[-1, 1\\right]$\u003c/code\u003e，值的含义与余弦相似度相同。皮尔逊相关系数有一个重要的数学特性是：变量位置和尺度的变化并不会引起相关系数的改变。下图给出了不同的 \u003ccode\u003e$\\left(x, y\\right)$\u003c/code\u003e 之间的皮尔逊相关系数。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2019-01-01-similarity-and-distance-measurement/correlation-examples.png\" alt=\"Correlation Examples\"/\u003e\u003c/p\u003e\n\u003ch1 id=\"集合距离-distance-of-sets\"\u003e集合距离 (Distance of Sets)\u003c/h1\u003e\n\u003cp\u003e对于两个集合之间的相似性度量，主要有如下几种方法：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eJaccard 系数\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ccode\u003e$$ s = \\dfrac{\\left|X \\cap Y\\right|}{\\left| X \\cup Y \\right|} = \\dfrac{\\left|X \\cap Y\\right|}{\\left|X\\right| + \\left|Y\\right| - \\left|X \\cap Y\\right|} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eJaccard 系数的取值范围为：\u003ccode\u003e$\\left[0, 1\\right]$\u003c/code\u003e，0 表示两个集合没有重合，1 表示两个集合完全重合。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eDice 系数\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ccode\u003e$$ s = \\dfrac{2 \\left| X \\cap Y \\right|}{\\left|X\\right| + \\left|Y\\right|} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e与 Jaccard 系数相同，Dice 系数的取值范围为：\u003ccode\u003e$\\left[0, 1\\right]$\u003c/code\u003e，两者之间可以相互转换 \u003ccode\u003e$s_d = 2 s_j / \\left(1 + s_j\\right), s_j = s_d / \\left(2 - s_d\\right)$\u003c/code\u003e。不同于 Jaccard 系数，Dice 系数的差异函数 \u003ccode\u003e$d = 1 - s$\u003c/code\u003e 并不是一个合适的距离度量，因为其并不满足距离函数的三角不等式。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eTversky 系数\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ccode\u003e$$ s = \\dfrac{\\left| X \\cap Y \\right|}{\\left| X \\cap Y \\right| + \\alpha \\left| X \\setminus Y \\right| + \\beta \\left| Y \\setminus X \\right|} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$X \\setminus Y$\u003c/code\u003e 表示集合的相对补集。Tversky 系数可以理解为 Jaccard 系数和 Dice 系数的一般化，当 \u003ccode\u003e$\\alpha = \\beta = 1$\u003c/code\u003e 时为 Jaccard 系数，当 \u003ccode\u003e$\\alpha = \\beta = 0.5$\u003c/code\u003e 时为 Dice 系数。\u003c/p\u003e\n\u003ch1 id=\"字符串距离-distance-of-strings\"\u003e字符串距离 (Distance of Strings)\u003c/h1\u003e\n\u003cp\u003e对于两个字符串之间的相似性度量，主要有如下几种方法：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eLevenshtein 距离\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLevenshtein 距离是 \u003cstrong\u003e编辑距离 (Editor Distance)\u003c/strong\u003e 的一种，指两个字串之间，由一个转成另一个所需的最少编辑操作次数。允许的编辑操作包括将一个字符替换成另一个字符，插入一个字符，删除一个字符。例如将 \u003cstrong\u003ekitten\u003c/strong\u003e 转成 \u003cstrong\u003esitting\u003c/strong\u003e，转换过程如下：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{equation*} \\begin{split} \\text{kitten} \\to \\text{sitten} \\left(k \\to s\\right) \\\\ \\text{sitten} \\to \\text{sittin} \\left(e \\to i\\right) \\\\ \\text{sittin} \\to \\text{sitting} \\left(\\  \\to g\\right) \\end{split} \\end{equation*} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e编辑距离的求解可以利用动态规划的思想优化计算的时间复杂度。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eJaro-Winkler 距离\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e对于给定的两个字符串 \u003ccode\u003e$s_1$\u003c/code\u003e 和 \u003ccode\u003e$s_2$\u003c/code\u003e，Jaro 相似度定义为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ sim = \\begin{cases} 0 \u0026amp; \\text{if} \\  m = 0 \\\\ \\dfrac{1}{3} \\left(\\dfrac{m}{\\left|s_1\\right|} + \\dfrac{m}{\\left|s_2\\right|} + \\dfrac{m-t}{m}\\right) \u0026amp; \\text{otherwise} \\end{cases} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$\\left|s_i\\right|$\u003c/code\u003e 为字符串 \u003ccode\u003e$s_i$\u003c/code\u003e 的长度，\u003ccode\u003e$m$\u003c/code\u003e 为匹配的字符的个数，\u003ccode\u003e$t$\u003c/code\u003e 换位数目的一半。如果字符串 \u003ccode\u003e$s_1$\u003c/code\u003e 和 \u003ccode\u003e$s_2$\u003c/code\u003e 相差不超过 \u003ccode\u003e$\\lfloor \\dfrac{\\max \\left(\\left|s_1\\right|, \\left|s_2\\right|\\right)}{2} \\rfloor - 1$\u003c/code\u003e，我们则认为两个字符串是匹配的。例如，对于字符串 \u003cstrong\u003eCRATE\u003c/strong\u003e 和 \u003cstrong\u003eTRACE\u003c/strong\u003e，仅 \u003cstrong\u003eR, A, E\u003c/strong\u003e 三个字符是匹配的，因此 \u003ccode\u003e$m = 3$\u003c/code\u003e，尽管 \u003cstrong\u003eC, T\u003c/strong\u003e 均出现在两个字符串中，但是他们的距离超过了 1 (即，\u003ccode\u003e$\\lfloor \\dfrac{5}{2} \\rfloor - 1$\u003c/code\u003e)，因此 \u003ccode\u003e$t = 0$\u003c/code\u003e。\u003c/p\u003e\n\u003cp\u003eJaro-Winkler 相似度给予了起始部分相同的字符串更高的分数，其定义为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ sim_w = sim_j + l p \\left(1 - sim_j\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$sim_j$\u003c/code\u003e 为字符串 \u003ccode\u003e$s_1$\u003c/code\u003e 和 \u003ccode\u003e$s_2$\u003c/code\u003e 的 Jaro 相似度，\u003ccode\u003e$l$\u003c/code\u003e 为共同前缀的长度 (规定不超过 \u003ccode\u003e$4$\u003c/code\u003e)，\u003ccode\u003e$p$\u003c/code\u003e 为调整系数 (规定不超过 \u003ccode\u003e$0.25$\u003c/code\u003e)，Winkler 将其设置为 \u003ccode\u003e$p = 0.1$\u003c/code\u003e。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e汉明距离\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e汉明距离为两个\u003cstrong\u003e等长字符串\u003c/strong\u003e对应位置的不同字符的个数，也就是将一个字符串变换成另外一个字符串所需要\u003cstrong\u003e替换\u003c/strong\u003e的字符个数。例如：\u003cstrong\u003e10\u003cspan style=\"color:#0000ff;\"\u003e1\u003c/span\u003e1\u003cspan style=\"color:#0000ff;\"\u003e1\u003c/span\u003e01\u003c/strong\u003e 与 \u003cstrong\u003e10\u003cspan style=\"color:#ff0000;\"\u003e0\u003c/span\u003e1\u003cspan style=\"color:#ff0000;\"\u003e0\u003c/span\u003e01\u003c/strong\u003e 之间的汉明距离是 2，\u003cstrong\u003e“\u003cspan style=\"color:#0000ff;\"\u003et\u003c/span\u003eo\u003cspan style=\"color:#0000ff;\"\u003en\u003c/span\u003ee\u003cspan style=\"color:#0000ff;\"\u003ed\u003c/span\u003e”\u003c/strong\u003e 与 \u003cstrong\u003e“\u003cspan style=\"color:#ff0000;\"\u003er\u003c/span\u003eo\u003cspan style=\"color:#ff0000;\"\u003es\u003c/span\u003ee\u003cspan style=\"color:#ff0000;\"\u003es\u003c/span\u003e”\u003c/strong\u003e 之间的汉明距离是 3。\u003c/p\u003e\n\u003ch1 id=\"信息论距离-information-theory-distance\"\u003e信息论距离 (Information Theory Distance)\u003c/h1\u003e\n\u003cp\u003e首先我们需要理解什么是 \u003cstrong\u003e熵 (Entropy)\u003c/strong\u003e？熵最早是用来表示物理学中一个热力系统无序的程度，后来依据香农的信息论，熵用来衡量一个随机变量的不确定性程度。对于一个随机变量 \u003ccode\u003e$X$\u003c/code\u003e，其概率分布为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ P \\left(X = x_i\\right) = p_i, \\quad i = 1, 2, ..., n $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e则随机变量 \u003ccode\u003e$X$\u003c/code\u003e 的熵定义如下：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ H \\left(X\\right) = - \\sum_{i=1}^{n} P \\left(x_i\\right) \\log P \\left(x_i\\right) \\label{eq:entropy} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e例如抛一枚硬币，假设硬币正面向上 \u003ccode\u003e$X = 1$\u003c/code\u003e 的概率为 \u003ccode\u003e$p$\u003c/code\u003e，硬币反面向上 \u003ccode\u003e$X = 0$\u003c/code\u003e 的概率为 \u003ccode\u003e$1 - p$\u003c/code\u003e。则对于抛一枚硬币那个面朝上这个随机变量 \u003ccode\u003e$X$\u003c/code\u003e 的熵为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ H \\left(X\\right) = - p \\log p - \\left(1-p\\right) \\log \\left(1-p\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e随概率 \u003ccode\u003e$p$\u003c/code\u003e 变化如下图所示：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2019-01-01-similarity-and-distance-measurement/entropy-demo.png\" alt=\"Entropy Demo\"/\u003e\u003c/p\u003e\n\u003cp\u003e从图可以看出，当 \u003ccode\u003e$p = 0.5$\u003c/code\u003e 时熵最大，也就是说抛一枚硬币，当正反两面朝上的概率相同时，熵最大，系统最复杂。对于公式 \u003ccode\u003e$\\ref{eq:entropy}$\u003c/code\u003e，当取以 2 为底的对数时，熵的单位为比特 (bit)，当取自然对数时，熵的单位为纳特 (nat)，当取以 10 为底的对数时，熵的单位为哈特 (hart)。\u003c/p\u003e\n\u003cp\u003e对于随机变量 \u003ccode\u003e$\\left(X, Y\\right)$\u003c/code\u003e，其联合概率分布为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ P \\left(X = x_i, Y = y_i\\right) = p_{i, j}, \\quad i = 1,2,...,n; \\quad j = 1,2,...,m $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e条件熵 (Conditional Entropy)\u003c/strong\u003e 表示在已知 \u003ccode\u003e$X$\u003c/code\u003e 的条件下 \u003ccode\u003e$Y$\u003c/code\u003e 的不确定性，定义为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{equation} \\begin{split} H \\left(Y | X\\right) \u0026amp;= \\sum_{i=i}^{n} P \\left(x_i\\right) H \\left(Y | X = x_i\\right) \\\\ \u0026amp;= \\sum_{i=1}^{n}{\\sum_{j=1}^{m}{P \\left(x_i, y_j\\right) \\log \\dfrac{P \\left(x_i\\right)}{P \\left(x_i, y_j\\right)}}} \\end{split} \\end{equation} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e联合熵 (Joint Entropy)\u003c/strong\u003e 用于衡量多个随机变量的随机系统的信息量，定义为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ H \\left(X, Y\\right) = \\sum_{i=1}^{n}{\\sum_{j=1}^{m}{P \\left(x_i, y_j\\right) \\log P \\left(x_i, y_j\\right)}} $$\u003c/code\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e互信息 (Mutual Information)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e互信息用于衡量两个变量之间的关联程度，定义为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ I \\left(X; Y\\right) = \\sum_{i=1}^{n}{\\sum_{j=1}^{m}{P \\left(x_i, y_j\\right) \\log \\dfrac{P \\left(x_i, y_i\\right)}{P \\left(x_i\\right) P \\left(y_j\\right)}}} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e直观上，互信息度量 \u003ccode\u003e$X$\u003c/code\u003e 和 \u003ccode\u003e$Y$\u003c/code\u003e 共享的信息，它度量知道这两个变量其中一个，对另一个不确定度减少的程度。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e相对熵 (Relative Entropy)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e相对熵又称之为 \u003cstrong\u003eKL 散度 (Kullback-Leibler Divergence)\u003c/strong\u003e，用于衡量两个分布之间的差异，定义为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ D_{KL} \\left(P \\| Q\\right) = \\sum_{i}{P \\left(i\\right) \\ln \\dfrac{P \\left(i\\right)}{Q \\left(i\\right)}} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eKL 散度为非负数 \u003ccode\u003e$D_{KL} \\left(P \\| Q\\right) \\geq 0$\u003c/code\u003e，同时其不具有对称性 \u003ccode\u003e$D_{KL} \\left(P \\| Q\\right) \\neq D_{KL} \\left(Q \\| P\\right)$\u003c/code\u003e，也不满足距离函数的三角不等式。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e交叉熵 (Corss Entropy)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e交叉熵定义为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{equation} \\begin{split} H \\left(P, Q\\right) \u0026amp;= H \\left(P\\right) + D_{KL} \\left(P \\| Q\\right) \\\\ \u0026amp;= - \\sum_{i}{P \\left(i\\right) \\log Q \\left(i\\right)} \\end{split} \\end{equation} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e交叉熵常作为机器学习中的损失函数，用于衡量模型分布和训练数据分布之间的差异性。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eJS 散度 (Jensen-Shannon Divergence)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eJS 散度解决了 KL 散度不对称的问题，定义为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ D_{JS} \\left(P \\| Q\\right) = \\dfrac{1}{2} D_{KL} \\left(P \\| \\dfrac{P + Q}{2}\\right) + \\dfrac{1}{2} D_{KL} \\left(Q \\| \\dfrac{P + Q}{2}\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e当取以 2 为底的对数时，JS 散度的取值范围为：\u003ccode\u003e$\\left[0, 1\\right]$\u003c/code\u003e。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e推土机距离 (Earth Mover Distance, Wasserstein Distance)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e推土机距离用于描述两个多维分布之间相似性，之所以称为推土机距离是因为我们将分布看做空间中的泥土，两个分布之间的距离则是通过泥土的搬运将一个分布改变到另一个分布所消耗的最小能量 (即运送距离和运送重量的乘积)。\u003c/p\u003e\n\u003cp\u003e对于给定的分布 \u003ccode\u003e$P = \\left\\{\\left(p_1, w_{p1}\\right), \\left(p_2, w_{p2}\\right), \\cdots, \\left(p_m, w_{pm}\\right)\\right\\}$\u003c/code\u003e 和 \u003ccode\u003e$Q = \\left\\{\\left(q_1, w_{q1}\\right), \\left(q_2, w_{q2}\\right), \\cdots, \\left(q_n, w_{qn}\\right)\\right\\}$\u003c/code\u003e，定义从 \u003ccode\u003e$p_i$\u003c/code\u003e 到 \u003ccode\u003e$q_j$\u003c/code\u003e 之间的距离为 \u003ccode\u003e$d_{i, j}$\u003c/code\u003e，所需运送的重量为 \u003ccode\u003e$f_{i, j}$\u003c/code\u003e。对于 \u003ccode\u003e$f_{i, j}$\u003c/code\u003e 有如下 4 个约束：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e运送需从 \u003ccode\u003e$p_i$\u003c/code\u003e 到 \u003ccode\u003e$q_j$\u003c/code\u003e，不能反向，即 \u003ccode\u003e$f_{i, j} \\geq 0, 1 \\leq i \\leq m, 1 \\leq j \\leq n$\u003c/code\u003e。\u003c/li\u003e\n\u003cli\u003e从 \u003ccode\u003e$p_i$\u003c/code\u003e 运送出的总重量不超过原始的总重量 \u003ccode\u003e$w_{pi}$\u003c/code\u003e，即 \u003ccode\u003e$\\sum_{j=1}^{n}{f_{i, j}} \\leq w_{pi}, 1 \\leq i \\leq m$\u003c/code\u003e。\u003c/li\u003e\n\u003cli\u003e运送到 \u003ccode\u003e$q_j$\u003c/code\u003e 的总重量不超过其总容量 \u003ccode\u003e$w_{qj}$\u003c/code\u003e，即 \u003ccode\u003e$\\sum_{i=1}^{m}{f_{i, j}} \\leq w_{qj}, 1 \\leq j \\leq n$\u003c/code\u003e。\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e$\\sum_{i=1}^{m}{\\sum_{j=1}^{n}{f_{i, j}}} = \\min \\left\\{\\sum_{i=1}^{m}{w_{pi}}, \\sum_{j=1}^{n}{w_{qj}}\\right\\}$\u003c/code\u003e。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e在此约束下，通过最小化损失函数：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\min \\sum_{i=1}^{m}{\\sum_{j=1}^{n}{d_{i, j} f_{i, j}}} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e得到最优解 \u003ccode\u003e$f_{i, j}^*$\u003c/code\u003e，则推土机距离定义为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ D_{W} \\left(P, Q\\right) = \\dfrac{\\sum_{i=1}^{m}{\\sum_{j=1}^{n}{d_{i, j} f_{i, j}^*}}}{\\sum_{i=1}^{m}{\\sum_{j=1}^{n}{f_{i, j}^*}}} $$\u003c/code\u003e\u003c/p\u003e\n\u003ch1 id=\"其他距离-other-distance\"\u003e其他距离 (Other Distance)\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003eDTW (Dynamic Time Warping) 距离\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eDTW 距离用于衡量两个序列之间的相似性，序列的长度可能相等也可能不相等。对于两个给定的序列 \u003ccode\u003e$X = \\left(x_1, x_2, \\cdots, x_m\\right)$\u003c/code\u003e 和 \u003ccode\u003e$Y = \\left(y_1, y_2, \\cdots, y_n\\right)$\u003c/code\u003e，我们可以利用动态规划的方法求解 DTW 距离。首先我们构造一个 \u003ccode\u003e$m \\times n$\u003c/code\u003e 的矩阵，矩阵中的元素 \u003ccode\u003e$d_{i, j}$\u003c/code\u003e 表示 \u003ccode\u003e$x_i$\u003c/code\u003e 和 \u003ccode\u003e$y_j$\u003c/code\u003e 之间的距离。我们需要找到一条通过该矩阵的路径 \u003ccode\u003e$W = \\left(w_1, w_2, \\cdots, w_l\\right)$\u003c/code\u003e, \u003ccode\u003e$\\max\\left(m, n\\right) \\leq l \u0026lt; m + n + 1$\u003c/code\u003e，假设 \u003ccode\u003e$w_k$\u003c/code\u003e 对应的矩阵元素为 \u003ccode\u003e$\\left(i, j\\right)$\u003c/code\u003e，对应的距离为 \u003ccode\u003e$d_k$\u003c/code\u003e，则 DTW 的优化目标为 \u003ccode\u003e$\\min \\sum_{k=1}^{l}{d_k}$\u003c/code\u003e。如下图右上角部分所示：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2019-01-01-similarity-and-distance-measurement/dtw-threeway.png\" alt=\"DTW Three-Way\"/\u003e\u003c/p\u003e\n\u003cp\u003e对于路径 \u003ccode\u003e$W$\u003c/code\u003e，需要满足如下 3 个条件：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e边界条件\u003c/strong\u003e：\u003ccode\u003e$w_1 = \\left(1, 1\\right), w_k = \\left(m, n\\right)$\u003c/code\u003e，即路径须从左下角出发，在右上角终止。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e连续性\u003c/strong\u003e：对于 \u003ccode\u003e$w_{l-1} = \\left(i\u0026#39;, j\u0026#39;\\right), w_l = \\left(i, j\\right)$\u003c/code\u003e，需满足 \u003ccode\u003e$i - i\u0026#39; \\leq 1, j - j\u0026#39; \\leq 1$\u003c/code\u003e，即路径不能跨过任何一点进行匹配。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e单调性\u003c/strong\u003e：对于 \u003ccode\u003e$w_{l-1} = \\left(i\u0026#39;, j\u0026#39;\\right), w_l = \\left(i, j\\right)$\u003c/code\u003e，需满足 \u003ccode\u003e$0 \\leq i - i\u0026#39;, 0 \\leq j - j\u0026#39;$\u003c/code\u003e，即路径上的点需单调递增，不能回退进行匹配。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e利用动态规划求解 DTW 的状态转移方程为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ dtw_{i, j} = \\begin{cases} 0 \u0026amp; \\text{if} \\  i = j = 0 \\\\ \\infty \u0026amp; \\text{if} \\  i = 0 \\  \\text{or} \\  j = 0 \\\\ d_{i, j} + \\min \\left(dtw_{i-1, j}, dtw_{i-1, j-1}, dtw_{i, j-1}\\right) \u0026amp; \\text{otherwise} \\end{cases} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$dtw_{m, n}$\u003c/code\u003e 则为最终的 DTW 距离。在 DTW 求解的过程中还可以使用不同的 Local Warping Step 和窗口类型，更多详细信息可看见 R 中 \u003ca href=\"https://cran.r-project.org/web/packages/dtw/index.html\"\u003edtw 包\u003c/a\u003e。下图展示了利用 DTW 求解后不同点之间的对应关系：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2019-01-01-similarity-and-distance-measurement/dtw-twoway.png\" alt=\"DTW Two-Way\"/\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e流形距离 (Distance of Manifold)\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e关于流形距离请参见之前的博客：\u003ca href=\"/cn/2018/03/manifold-learning\"\u003e流形学习 (Manifold Learning)\u003c/a\u003e。\u003c/p\u003e\n\u003ch2\u003e:tada::tada::tada: Happe New Year! :tada::tada::tada:\u003c/h2\u003e\n\n\n\n\n\n\u003cdiv class=\"donate\"\u003e\n  \u003cdiv class=\"donate-header\"\u003e\u003c/div\u003e\n  \u003cdiv class=\"donate-slug\" id=\"donate-slug\"\u003esimilarity-and-distance-measurement\u003c/div\u003e\n  \u003cbutton class=\"donate-button\"\u003e赞 赏\u003c/button\u003e\n  \u003cdiv class=\"donate-footer\"\u003e「真诚赞赏，手留余香」\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"donate-modal-wrapper\"\u003e\n  \u003cdiv class=\"donate-modal\"\u003e\n    \u003cdiv class=\"donate-box\"\u003e\n      \u003cdiv class=\"donate-box-content\"\u003e\n        \u003cdiv class=\"donate-box-content-inner\"\u003e\n          \u003cdiv class=\"donate-box-header\"\u003e「真诚赞赏，手留余香」\u003c/div\u003e\n          \u003cdiv class=\"donate-box-body\"\u003e\n            \u003cdiv class=\"donate-box-money\"\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-2\" data-v=\"2\" data-unchecked=\"￥ 2\" data-checked=\"2 元\"\u003e￥ 2\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-5\" data-v=\"5\" data-unchecked=\"￥ 5\" data-checked=\"5 元\"\u003e￥ 5\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-10\" data-v=\"10\" data-unchecked=\"￥ 10\" data-checked=\"10 元\"\u003e￥ 10\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-50\" data-v=\"50\" data-unchecked=\"￥ 50\" data-checked=\"50 元\"\u003e￥ 50\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-100\" data-v=\"100\" data-unchecked=\"￥ 100\" data-checked=\"100 元\"\u003e￥ 100\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-custom\" data-v=\"custom\" data-unchecked=\"任意金额\" data-checked=\"任意金额\"\u003e任意金额\u003c/button\u003e\n            \u003c/div\u003e\n            \u003cdiv class=\"donate-box-pay\"\u003e\n              \u003cimg class=\"donate-box-pay-qrcode\" id=\"donate-box-pay-qrcode\" src=\"\"/\u003e\n            \u003c/div\u003e\n          \u003c/div\u003e\n          \u003cdiv class=\"donate-box-footer\"\u003e\n            \u003cdiv class=\"donate-box-pay-method donate-box-pay-method-checked\" data-v=\"wechat-pay\"\u003e\n              \u003cimg class=\"donate-box-pay-method-image\" id=\"donate-box-pay-method-image-wechat-pay\" src=\"\"/\u003e\n            \u003c/div\u003e\n            \u003cdiv class=\"donate-box-pay-method\" data-v=\"alipay\"\u003e\n              \u003cimg class=\"donate-box-pay-method-image\" id=\"donate-box-pay-method-image-alipay\" src=\"\"/\u003e\n            \u003c/div\u003e\n          \u003c/div\u003e\n        \u003c/div\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n    \u003cbutton type=\"button\" class=\"donate-box-close-button\"\u003e\n      \u003csvg class=\"donate-box-close-button-icon\" fill=\"#fff\" viewBox=\"0 0 24 24\" width=\"24\" height=\"24\"\u003e\u003cpath d=\"M13.486 12l5.208-5.207a1.048 1.048 0 0 0-.006-1.483 1.046 1.046 0 0 0-1.482-.005L12 10.514 6.793 5.305a1.048 1.048 0 0 0-1.483.005 1.046 1.046 0 0 0-.005 1.483L10.514 12l-5.208 5.207a1.048 1.048 0 0 0 .006 1.483 1.046 1.046 0 0 0 1.482.005L12 13.486l5.207 5.208a1.048 1.048 0 0 0 1.483-.006 1.046 1.046 0 0 0 .005-1.482L13.486 12z\" fill-rule=\"evenodd\"\u003e\u003c/path\u003e\u003c/svg\u003e\n    \u003c/button\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\u003cscript type=\"text/javascript\" src=\"/js/donate.js\"\u003e\u003c/script\u003e\n\n\n  \u003cfooter\u003e\n  \n\u003cnav class=\"post-nav\"\u003e\n  \u003cspan class=\"nav-prev\"\u003e← \u003ca href=\"/cn/2018/12/ensemble-learning/\"\u003e集成学习算法 (Ensemble Learning)\u003c/a\u003e\u003c/span\u003e\n  \u003cspan class=\"nav-next\"\u003e\u003ca href=\"/cn/2019/02/how-to-read-a-book/\"\u003e如何阅读一本书 (How to Read a Book)\u003c/a\u003e →\u003c/span\u003e\n\u003c/nav\u003e\n\n\n\n\n\u003cins class=\"adsbygoogle\" style=\"display:block; text-align:center;\" data-ad-layout=\"in-article\" data-ad-format=\"fluid\" data-ad-client=\"ca-pub-2608165017777396\" data-ad-slot=\"8302038603\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n  (adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/js-cookie@3.0.5/dist/js.cookie.min.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"/js/toggle-theme.js\"\u003e\u003c/script\u003e\n\n\n\u003cscript src=\"/js/no-highlight.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"/js/math-code.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"/js/heading-anchor.js\"\u003e\u003c/script\u003e\n\n\n\n\u003csection class=\"comments\"\u003e\n\u003cscript src=\"https://giscus.app/client.js\" data-repo=\"leovan/leovan.me\" data-repo-id=\"MDEwOlJlcG9zaXRvcnkxMTMxOTY0Mjc=\" data-category=\"Comments\" data-category-id=\"DIC_kwDOBr89i84CT-R7\" data-mapping=\"pathname\" data-strict=\"1\" data-reactions-enabled=\"1\" data-emit-metadata=\"0\" data-input-position=\"top\" data-theme=\"preferred_color_scheme\" data-lang=\"zh-CN\" data-loading=\"lazy\" crossorigin=\"anonymous\" defer=\"\"\u003e\n\u003c/script\u003e\n\u003c/section\u003e\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cscript async=\"\" src=\"/js/center-img.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/right-quote.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/external-link.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/alt-title.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/figure.js\"\u003e\u003c/script\u003e\n\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js\"\u003e\u003c/script\u003e\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/vanilla-back-to-top@latest/dist/vanilla-back-to-top.min.js\"\u003e\u003c/script\u003e\n\u003cscript\u003e\naddBackToTop({\n  diameter: 48\n});\n\u003c/script\u003e\n\n  \u003chr/\u003e\n  \u003cdiv class=\"copyright no-border-bottom\"\u003e\n    \u003cdiv class=\"copyright-author-year\"\u003e\n      \u003cspan\u003eCopyright © 2017-2024 \u003ca href=\"/\"\u003e范叶亮 | Leo Van\u003c/a\u003e\u003c/span\u003e\n    \u003c/div\u003e\n  \u003c/div\u003e\n  \u003c/footer\u003e\n  \u003c/article\u003e",
  "Date": "2019-01-01T00:00:00Z",
  "Author": "范叶亮"
}