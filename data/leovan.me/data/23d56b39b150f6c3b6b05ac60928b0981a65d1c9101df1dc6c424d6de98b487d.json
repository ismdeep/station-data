{
  "Source": "leovan.me",
  "Title": "循环神经网络 (Recurrent Neural Network, RNN)",
  "Link": "https://leovan.me/cn/2018/09/rnn/",
  "Content": "\u003carticle class=\"main\"\u003e\n    \u003cheader class=\"content-title\"\u003e\n    \n\u003ch1 class=\"title\"\u003e\n  \n  循环神经网络 (Recurrent Neural Network, RNN)\n  \n\u003c/h1\u003e\n\n\n\n\n\n\n\n\u003ch2 class=\"author-date\"\u003e范叶亮 / \n2018-09-21\u003c/h2\u003e\n\n\n\n\u003ch3 class=\"post-meta\"\u003e\n\n\n\u003cstrong\u003e分类: \u003c/strong\u003e\n\u003ca href=\"/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0\"\u003e深度学习\u003c/a\u003e\n\n\n\n\n/\n\n\n\n\n\u003cstrong\u003e标签: \u003c/strong\u003e\n\u003cspan\u003e循环神经网络\u003c/span\u003e, \u003cspan\u003eRecurrent Neural Network\u003c/span\u003e, \u003cspan\u003eRNN\u003c/span\u003e, \u003cspan\u003e长短时记忆网络\u003c/span\u003e, \u003cspan\u003eLong Short Term Memory\u003c/span\u003e, \u003cspan\u003eLSTM\u003c/span\u003e, \u003cspan\u003eGated Recurrent Unit\u003c/span\u003e, \u003cspan\u003eGRU\u003c/span\u003e\n\n\n\n\n/\n\n\n\u003cstrong\u003e字数: \u003c/strong\u003e\n4650\n\u003c/h3\u003e\n\n\n\n\u003chr/\u003e\n\n\n\n    \n    \n    \u003cins class=\"adsbygoogle\" style=\"display:block; text-align:center;\" data-ad-layout=\"in-article\" data-ad-format=\"fluid\" data-ad-client=\"ca-pub-2608165017777396\" data-ad-slot=\"1261604535\"\u003e\u003c/ins\u003e\n    \u003cscript\u003e\n    (adsbygoogle = window.adsbygoogle || []).push({});\n    \u003c/script\u003e\n    \n    \n    \u003c/header\u003e\n\n\n\n\n\u003cblockquote\u003e\n\u003cp\u003e文章部分内容参考了 Christopher 的博客 \u003ca href=\"http://colah.github.io/posts/2015-08-Understanding-LSTMs/\"\u003eUnderstanding LSTM Networks\u003c/a\u003e，内容翻译和图片重绘已得到原作者同意，重绘后的图片源文件请参见 \u003ca href=\"https://cdn.leovan.me/images/blog/cn/2018-09-21-rnn/rnn.graffle\"\u003e这里\u003c/a\u003e。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch1 id=\"发展史\"\u003e发展史\u003c/h1\u003e\n\u003cp\u003e循环神经网络 (Recurrent Neural Network, RNN) 一般是指时间递归神经网络而非结构递归神经网络 (Recursive Neural Network)，其主要用于对序列数据进行建模。Salehinejad 等人 \u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e 的一篇综述文章列举了 RNN 发展过程中的一些重大改进，如下表所示：\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003eYear\u003c/th\u003e\n\u003cth\u003e1st Author\u003c/th\u003e\n\u003cth\u003eContribution\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e1990\u003c/td\u003e\n\u003ctd\u003eElman\u003c/td\u003e\n\u003ctd\u003ePopularized simple RNNs (Elman network)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e1993\u003c/td\u003e\n\u003ctd\u003eDoya\u003c/td\u003e\n\u003ctd\u003eTeacher forcing for gradient descent (GD)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e1994\u003c/td\u003e\n\u003ctd\u003eBengio\u003c/td\u003e\n\u003ctd\u003eDifficulty in learning long term dependencies with gradient descend\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e1997\u003c/td\u003e\n\u003ctd\u003eHochreiter\u003c/td\u003e\n\u003ctd\u003eLSTM: long-short term memory for vanishing gradients problem\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e1997\u003c/td\u003e\n\u003ctd\u003eSchuster\u003c/td\u003e\n\u003ctd\u003eBRNN: Bidirectional recurrent neural networks\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e1998\u003c/td\u003e\n\u003ctd\u003eLeCun\u003c/td\u003e\n\u003ctd\u003eHessian matrix approach for vanishing gradients problem\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e2000\u003c/td\u003e\n\u003ctd\u003eGers\u003c/td\u003e\n\u003ctd\u003eExtended LSTM with forget gates\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e2001\u003c/td\u003e\n\u003ctd\u003eGoodman\u003c/td\u003e\n\u003ctd\u003eClasses for fast Maximum entropy training\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e2005\u003c/td\u003e\n\u003ctd\u003eMorin\u003c/td\u003e\n\u003ctd\u003eA hierarchical softmax function for language modeling using RNNs\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e2005\u003c/td\u003e\n\u003ctd\u003eGraves\u003c/td\u003e\n\u003ctd\u003eBLSTM: Bidirectional LSTM\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e2007\u003c/td\u003e\n\u003ctd\u003eJaeger\u003c/td\u003e\n\u003ctd\u003eLeaky integration neurons\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e2007\u003c/td\u003e\n\u003ctd\u003eGraves\u003c/td\u003e\n\u003ctd\u003eMDRNN: Multi-dimensional RNNs\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e2009\u003c/td\u003e\n\u003ctd\u003eGraves\u003c/td\u003e\n\u003ctd\u003eLSTM for hand-writing recognition\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e2010\u003c/td\u003e\n\u003ctd\u003eMikolov\u003c/td\u003e\n\u003ctd\u003eRNN based language model\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e2010\u003c/td\u003e\n\u003ctd\u003eNeir\u003c/td\u003e\n\u003ctd\u003eRectified linear unit (ReLU) for vanishing gradient problem\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e2011\u003c/td\u003e\n\u003ctd\u003eMartens\u003c/td\u003e\n\u003ctd\u003eLearning RNN with Hessian-free optimization\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e2011\u003c/td\u003e\n\u003ctd\u003eMikolov\u003c/td\u003e\n\u003ctd\u003eRNN by back-propagation through time (BPTT) for statistical language modeling\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e2011\u003c/td\u003e\n\u003ctd\u003eSutskever\u003c/td\u003e\n\u003ctd\u003eHessian-free optimization with structural damping\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e2011\u003c/td\u003e\n\u003ctd\u003eDuchi\u003c/td\u003e\n\u003ctd\u003eAdaptive learning rates for each weight\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e2012\u003c/td\u003e\n\u003ctd\u003eGutmann\u003c/td\u003e\n\u003ctd\u003eNoise-contrastive estimation (NCE)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e2012\u003c/td\u003e\n\u003ctd\u003eMnih\u003c/td\u003e\n\u003ctd\u003eNCE for training neural probabilistic language models (NPLMs)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e2012\u003c/td\u003e\n\u003ctd\u003ePascanu\u003c/td\u003e\n\u003ctd\u003eAvoiding exploding gradient problem by gradient clipping\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e2013\u003c/td\u003e\n\u003ctd\u003eMikolov\u003c/td\u003e\n\u003ctd\u003eNegative sampling instead of hierarchical softmax\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e2013\u003c/td\u003e\n\u003ctd\u003eSutskever\u003c/td\u003e\n\u003ctd\u003eStochastic gradient descent (SGD) with momentum\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e2013\u003c/td\u003e\n\u003ctd\u003eGraves\u003c/td\u003e\n\u003ctd\u003eDeep LSTM RNNs (Stacked LSTM)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e2014\u003c/td\u003e\n\u003ctd\u003eCho\u003c/td\u003e\n\u003ctd\u003eGated recurrent units\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e2015\u003c/td\u003e\n\u003ctd\u003eZaremba\u003c/td\u003e\n\u003ctd\u003eDropout for reducing Overfitting\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e2015\u003c/td\u003e\n\u003ctd\u003eMikolov\u003c/td\u003e\n\u003ctd\u003eStructurally constrained recurrent network (SCRN) to enhance learning longer memory for vanishing gradient problem\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e2015\u003c/td\u003e\n\u003ctd\u003eVisin\u003c/td\u003e\n\u003ctd\u003eReNet: A RNN-based alternative to convolutional neural networks\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e2015\u003c/td\u003e\n\u003ctd\u003eGregor\u003c/td\u003e\n\u003ctd\u003eDRAW: Deep recurrent attentive writer\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e2015\u003c/td\u003e\n\u003ctd\u003eKalchbrenner\u003c/td\u003e\n\u003ctd\u003eGrid long-short term memory\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e2015\u003c/td\u003e\n\u003ctd\u003eSrivastava\u003c/td\u003e\n\u003ctd\u003eHighway network\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e2017\u003c/td\u003e\n\u003ctd\u003eJing\u003c/td\u003e\n\u003ctd\u003eGated orthogonal recurrent units\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003ch1 id=\"rnn\"\u003eRNN\u003c/h1\u003e\n\u003ch2 id=\"网络结构\"\u003e网络结构\u003c/h2\u003e\n\u003cp\u003e不同于传统的前馈神经网络接受特定的输入得到输出，RNN 由人工神经元和一个或多个反馈循环构成，如下图所示：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-09-21-rnn/rnn-loops.png\" alt=\"RNN-Loops\"/\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$\\boldsymbol{x}_t$\u003c/code\u003e 为输入层，\u003ccode\u003e$\\boldsymbol{h}_t$\u003c/code\u003e 为带有循环的隐含层，\u003ccode\u003e$\\boldsymbol{y}_t$\u003c/code\u003e 为输出层。其中隐含层包含一个循环，为了便于理解我们将循环进行展开，展开后的网络结构如下图所示：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-09-21-rnn/rnn-loops-unrolled.png\" alt=\"RNN-Loops-Unrolled\"/\u003e\u003c/p\u003e\n\u003cp\u003e对于展开后的网络结构，其输入为一个时间序列 \u003ccode\u003e$\\left\\{\\dotsc, \\boldsymbol{x}_{t-1}, \\boldsymbol{x}_t, \\boldsymbol{x}_{t+1}, \\dotsc\\right\\}$\u003c/code\u003e，其中 \u003ccode\u003e$\\boldsymbol{x}_t \\in \\mathbb{R}^n$\u003c/code\u003e，\u003ccode\u003e$n$\u003c/code\u003e 为输入层神经元个数。相应的隐含层为 \u003ccode\u003e$\\left\\{\\dotsc, \\boldsymbol{h}_{t-1}, \\boldsymbol{h}_t, \\boldsymbol{h}_{t+1}, \\dotsc\\right\\}$\u003c/code\u003e，其中 \u003ccode\u003e$\\boldsymbol{h}_t \\in \\mathbb{R}^m$\u003c/code\u003e，\u003ccode\u003e$m$\u003c/code\u003e 为隐含层神经元个数。隐含层节点使用较小的非零数据进行初始化可以提升整体的性能和网络的稳定性 \u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e。隐含层定义了整个系统的状态空间 (state space)，或称之为 memory \u003csup id=\"fnref1:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\boldsymbol{h}_t = f_H \\left(\\boldsymbol{o}_t\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\boldsymbol{o}_t = \\boldsymbol{W}_{IH} \\boldsymbol{x}_t + \\boldsymbol{W}_{HH} \\boldsymbol{h}_{t-1} + \\boldsymbol{b}_h $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$f_H \\left(\\cdot\\right)$\u003c/code\u003e 为隐含层的激活函数，\u003ccode\u003e$\\boldsymbol{b}_h$\u003c/code\u003e 为隐含层的偏置向量。对应的输出层为 \u003ccode\u003e$\\left\\{\\dotsc, \\boldsymbol{y}_{t-1}, \\boldsymbol{y}_t, \\boldsymbol{y}_{t+1}, \\dotsc\\right\\}$\u003c/code\u003e，其中 \u003ccode\u003e$\\boldsymbol{y}_t \\in \\mathbb{R}^p$\u003c/code\u003e，\u003ccode\u003e$p$\u003c/code\u003e 为输出层神经元个数。则：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\boldsymbol{y}_t = f_O \\left(\\boldsymbol{W}_{HO} \\boldsymbol{h}_t + \\boldsymbol{b}_o\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中 \u003ccode\u003e$f_O \\left(\\cdot\\right)$\u003c/code\u003e 为隐含层的激活函数，\u003ccode\u003e$\\boldsymbol{b}_o$\u003c/code\u003e 为隐含层的偏置向量。\u003c/p\u003e\n\u003cp\u003e在 RNN 中常用的激活函数为双曲正切函数：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\tanh \\left(x\\right) = \\dfrac{e^{2x} - 1}{e^{2x} + 1} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eTanh 函数实际上是 Sigmoid 函数的缩放：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\sigma \\left(x\\right) = \\dfrac{1}{1 + e^{-x}} = \\dfrac{\\tanh \\left(x / 2\\right) + 1}{2} $$\u003c/code\u003e\u003c/p\u003e\n\u003ch2 id=\"梯度弥散和梯度爆炸\"\u003e梯度弥散和梯度爆炸\u003c/h2\u003e\n\u003cp\u003e原始 RNN 存在的严重的问题就是\u003cstrong\u003e梯度弥散 (Vanishing Gradients)\u003c/strong\u003e 和\u003cstrong\u003e梯度爆炸 (Exploding Gradients)\u003c/strong\u003e。我们以时间序列中的 3 个时间点 \u003ccode\u003e$t = 1, 2, 3$\u003c/code\u003e 为例进行说明，首先假设神经元在前向传导过程中没有激活函数，则有：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{equation} \\begin{split} \u0026amp;\\boldsymbol{h}_1 = \\boldsymbol{W}_{IH} \\boldsymbol{x}_1 + \\boldsymbol{W}_{HH} \\boldsymbol{h}_0 + \\boldsymbol{b}_h, \u0026amp;\\boldsymbol{y}_1 = \\boldsymbol{W}_{HO} \\boldsymbol{h}_1 + \\boldsymbol{b}_o \\\\ \u0026amp;\\boldsymbol{h}_2 = \\boldsymbol{W}_{IH} \\boldsymbol{x}_2 + \\boldsymbol{W}_{HH} \\boldsymbol{h}_1 + \\boldsymbol{b}_h, \u0026amp;\\boldsymbol{y}_2 = \\boldsymbol{W}_{HO} \\boldsymbol{h}_2 + \\boldsymbol{b}_o \\\\ \u0026amp;\\boldsymbol{h}_3 = \\boldsymbol{W}_{IH} \\boldsymbol{x}_3 + \\boldsymbol{W}_{HH} \\boldsymbol{h}_2 + \\boldsymbol{b}_h, \u0026amp;\\boldsymbol{y}_3 = \\boldsymbol{W}_{HO} \\boldsymbol{h}_3 + \\boldsymbol{b}_o \\end{split} \\end{equation} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e在对于一个序列训练的损失函数为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\mathcal{L} \\left(\\boldsymbol{y}, \\boldsymbol{\\hat{y}}\\right) = \\sum_{t=0}^{T}{\\mathcal{L}_t \\left(\\boldsymbol{y_t}, \\boldsymbol{\\hat{y}_t}\\right)} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中 \u003ccode\u003e$\\mathcal{L}_t \\left(\\boldsymbol{y_t}, \\boldsymbol{\\hat{y}_t}\\right)$\u003c/code\u003e 为 \u003ccode\u003e$t$\u003c/code\u003e 时刻的损失。我们利用 \u003ccode\u003e$t = 3$\u003c/code\u003e 时刻的损失对 \u003ccode\u003e$\\boldsymbol{W}_{IH}, \\boldsymbol{W}_{HH}, \\boldsymbol{W}_{HO}$\u003c/code\u003e 求偏导，有：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{equation} \\begin{split} \\dfrac{\\partial \\mathcal{L}_3}{\\partial \\boldsymbol{W}_{HO}} \u0026amp;= \\dfrac{\\partial \\mathcal{L}_3}{\\partial \\boldsymbol{y}_3} \\dfrac{\\partial \\boldsymbol{y}_3}{\\partial \\boldsymbol{W}_{HO}} \\\\ \\dfrac{\\partial \\mathcal{L}_3}{\\partial \\boldsymbol{W}_{IH}} \u0026amp;= \\dfrac{\\partial \\mathcal{L}_3}{\\partial \\boldsymbol{y}_3} \\dfrac{\\partial \\boldsymbol{y}_3}{\\partial \\boldsymbol{h}_3} \\dfrac{\\partial \\boldsymbol{h}_3}{\\partial \\boldsymbol{W}_{IH}} + \\dfrac{\\partial \\mathcal{L}_3}{\\partial \\boldsymbol{y}_3} \\dfrac{\\partial \\boldsymbol{y}_3}{\\partial \\boldsymbol{h}_3} \\dfrac{\\partial \\boldsymbol{h}_3}{\\partial \\boldsymbol{h}_2} \\dfrac{\\partial \\boldsymbol{h}_2}{\\partial \\boldsymbol{W}_{IH}} + \\dfrac{\\partial \\mathcal{L}_3}{\\partial \\boldsymbol{y}_3} \\dfrac{\\partial \\boldsymbol{y}_3}{\\partial \\boldsymbol{h}_3} \\dfrac{\\partial \\boldsymbol{h}_3}{\\partial \\boldsymbol{h}_2} \\dfrac{\\partial \\boldsymbol{h}_2}{\\partial \\boldsymbol{h}_1} \\dfrac{\\partial \\boldsymbol{h}_1}{\\partial \\boldsymbol{W}_{IH}} \\\\ \\dfrac{\\partial \\mathcal{L}_3}{\\partial \\boldsymbol{W}_{HH}} \u0026amp;= \\dfrac{\\partial \\mathcal{L}_3}{\\partial \\boldsymbol{y}_3} \\dfrac{\\partial \\boldsymbol{y}_3}{\\partial \\boldsymbol{h}_3} \\dfrac{\\partial \\boldsymbol{h}_3}{\\partial \\boldsymbol{W}_{HH}} + \\dfrac{\\partial \\mathcal{L}_3}{\\partial \\boldsymbol{y}_3} \\dfrac{\\partial \\boldsymbol{y}_3}{\\partial \\boldsymbol{h}_3} \\dfrac{\\partial \\boldsymbol{h}_3}{\\partial \\boldsymbol{h}_2} \\dfrac{\\partial \\boldsymbol{h}_2}{\\partial \\boldsymbol{W}_{HH}} + \\dfrac{\\partial \\mathcal{L}_3}{\\partial \\boldsymbol{y}_3} \\dfrac{\\partial \\boldsymbol{y}_3}{\\partial \\boldsymbol{h}_3} \\dfrac{\\partial \\boldsymbol{h}_3}{\\partial \\boldsymbol{h}_2} \\dfrac{\\partial \\boldsymbol{h}_2}{\\partial \\boldsymbol{h}_1} \\dfrac{\\partial \\boldsymbol{h}_1}{\\partial \\boldsymbol{W}_{HH}} \\end{split} \\end{equation} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e因此，不难得出对于任意时刻 \u003ccode\u003e$t$\u003c/code\u003e，\u003ccode\u003e$\\boldsymbol{W}_{IH}, \\boldsymbol{W}_{HH}$\u003c/code\u003e 的偏导为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\dfrac{\\partial \\mathcal{L}_t}{\\partial \\boldsymbol{W}_{IH}} = \\sum_{k=0}^{t}{\\dfrac{\\partial \\mathcal{L}_t}{\\partial \\boldsymbol{y}_t} \\dfrac{\\partial \\boldsymbol{y}_t}{\\partial \\boldsymbol{h}_t} \\left(\\prod_{j=k+1}^{t}{\\dfrac{\\partial \\boldsymbol{h}_j}{\\partial \\boldsymbol{h}_{j-1}}}\\right) \\dfrac{\\partial \\boldsymbol{h}_k}{\\partial \\boldsymbol{W}_{IH}}} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$\\dfrac{\\partial \\mathcal{L}_t}{\\partial \\boldsymbol{W}_{HH}}$\u003c/code\u003e 同理可得。对于 \u003ccode\u003e$\\dfrac{\\partial \\mathcal{L}_t}{\\partial \\boldsymbol{W}_{HH}}$\u003c/code\u003e，在存在激活函数的情况下，有：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\prod_{j=k+1}^{t}{\\dfrac{\\partial \\boldsymbol{h}_j}{\\partial \\boldsymbol{h}_{j-1}}} = \\prod_{j=k+1}^{t}{f\u0026#39;_H \\left(h_{j-1}\\right) \\boldsymbol{W}_{HH}} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e假设激活函数为 \u003ccode\u003e$\\tanh$\u003c/code\u003e，下图刻画了 \u003ccode\u003e$\\tanh$\u003c/code\u003e 函数及其导数的函数取值范围：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-09-21-rnn/tanh-function.png\" alt=\"Tanh-Function\"/\u003e\u003c/p\u003e\n\u003cp\u003e可得，\u003ccode\u003e$0 \\leq \\tanh\u0026#39; \\leq 1$\u003c/code\u003e，同时当且仅当 \u003ccode\u003e$x = 0$\u003c/code\u003e 时，\u003ccode\u003e$\\tanh\u0026#39; \\left(x\\right) = 1$\u003c/code\u003e。因此：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e当 \u003ccode\u003e$t$\u003c/code\u003e 较大时，\u003ccode\u003e$\\prod_{j=k+1}^{t}{f\u0026#39;_H \\left(h_{j-1}\\right) \\boldsymbol{W}_{HH}}$\u003c/code\u003e 趋近于 0，则会产生\u003cstrong\u003e梯度弥散\u003c/strong\u003e问题。\u003c/li\u003e\n\u003cli\u003e当 \u003ccode\u003e$\\boldsymbol{W}_{HH}$\u003c/code\u003e 较大时，\u003ccode\u003e$\\prod_{j=k+1}^{t}{f\u0026#39;_H \\left(h_{j-1}\\right) \\boldsymbol{W}_{HH}}$\u003c/code\u003e 趋近于无穷，则会产生\u003cstrong\u003e梯度爆炸\u003c/strong\u003e问题。\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"长期依赖问题\"\u003e长期依赖问题\u003c/h2\u003e\n\u003cp\u003eRNN 隐藏节点以循环结构形成记忆，每一时刻的隐藏层的状态取决于它的过去状态，这种结构使得 RNN 可以保存、记住和处理长时期的过去复杂信号。但有的时候，我们仅需利用最近的信息来处理当前的任务。例如：考虑一个用于利用之前的文字预测后续文字的语言模型，如果我们想预测 “the clouds are in the \u003cstrong\u003esky\u003c/strong\u003e” 中的最后一个词，我们不需要太远的上下信息，很显然这个词就应该是 \u003cstrong\u003esky\u003c/strong\u003e。在这个情况下，待预测位置与相关的信息之间的间隔较小，RNN 可以有效的利用过去的信息。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-09-21-rnn/rnn-long-term-dependencies-short.png\" alt=\"RNN-Long-Term-Dependencies-Short\"/\u003e\u003c/p\u003e\n\u003cp\u003e但也有很多的情况需要更多的上下文信息，考虑需要预测的文本为 “I grew up in France … I speak fluent \u003cstrong\u003eFrench\u003c/strong\u003e”。较近的信息表明待预测的位置应该是一种语言，但想确定具体是哪种语言需要更远位置的“在法国长大”的背景信息。理论上 RNN 有能力处理这种\u003cstrong\u003e长期依赖\u003c/strong\u003e，但在实践中 RNN 却很难解决这个问题 \u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-09-21-rnn/rnn-long-term-dependencies-long.png\" alt=\"RNN-Long-Term-Dependencies-Long\"/\u003e\u003c/p\u003e\n\u003ch1 id=\"lstm\"\u003eLSTM\u003c/h1\u003e\n\u003ch2 id=\"lstm-网络结构\"\u003eLSTM 网络结构\u003c/h2\u003e\n\u003cp\u003e长短时记忆网络 (Long Short Term Memroy, LSTM) 是由 Hochreiter 和 Schmidhuber \u003csup id=\"fnref:4\"\u003e\u003ca href=\"#fn:4\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e4\u003c/a\u003e\u003c/sup\u003e 提出一种特殊的 RNN。LSTM 的目的就是为了解决长期依赖问题，记住长时间的信息是 LSTM 的基本功能。\u003c/p\u003e\n\u003cp\u003e所有的循环神经网络都是由重复的模块构成的一个链条。在标准的 RNN 中，这个重复的模块的结构比较简单，仅包含一个激活函数为 \u003ccode\u003e$\\tanh$\u003c/code\u003e 的隐含层，如下图所示：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-09-21-rnn/rnn.png\" alt=\"RNN\"/\u003e\u003c/p\u003e\n\u003cp\u003eLSTM 也是类似的链条状结构，但其重复的模块的内部结构不同。模块内部并不是一个隐含层，而是四个，并且以一种特殊的方式进行交互，如下图所示：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-09-21-rnn/lstm.png\" alt=\"LSTM\"/\u003e\u003c/p\u003e\n\u003cp\u003e下面我们将一步一步的介绍 LSTM 单元 (cell) 的具体工作原理，在之前我们先对使用到的符号进行简单的说明，如下图所示：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-09-21-rnn/lstm-operations-symbols.png\" alt=\"LSTM-Operations-Symbols\"/\u003e\u003c/p\u003e\n\u003cp\u003e其中，每条线都包含一个从输出节点到其他节点的整个向量，粉红色的圆圈表示逐元素的操作，黄色的矩形为学习到的神经网络层，线条的合并表示连接，线条的分叉表示内容的复制并转移到不同位置。\u003c/p\u003e\n\u003ch2 id=\"lstm-单元状态和门控机制\"\u003eLSTM 单元状态和门控机制\u003c/h2\u003e\n\u003cp\u003eLSTM 的关键为单元的状态 (cell state)，即下图中顶部水平穿过单元的直线。单元的状态像是一条传送带，其直接运行在整个链条上，同时仅包含少量的线性操作。因此，信息可以很容易得传递下去并保持不变。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-09-21-rnn/lstm-cell-state.png\" alt=\"LSTM-Cell-State\"/\u003e\u003c/p\u003e\n\u003cp\u003eLSTM 具有向单元状态添加或删除信息的能力，这种能力被由一种称之为“门” (gates) 的结构所控制。门是一种可选择性的让信息通过的组件，其由一层以 Sigmoid 为激活函数的网络层和一个逐元素相乘操作构成的，如下图所示：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-09-21-rnn/lstm-pointwise-operation.png\" alt=\"LSTM-Pointwise-Operation\"/\u003e\u003c/p\u003e\n\u003cp\u003eSigmoid 层的输出值介于 0 和 1 之间，代表了所允许通过的数据量。0 表示不允许任何数据通过，1 表示允许所有数据通过。一个 LSTM 单元包含 3 个门用于控制单元的状态。\u003c/p\u003e\n\u003ch2 id=\"lstm-工作步骤\"\u003eLSTM 工作步骤\u003c/h2\u003e\n\u003cp\u003eLSTM 的第一步是要决定从单元状态中所\u003cstrong\u003e忘记\u003c/strong\u003e的信息，这一步是通过一个称之为“\u003cstrong\u003e遗忘门 (forget gate)\u003c/strong\u003e”的 Sigmoid 网络层控制。该层以上一时刻隐含层的输出 \u003ccode\u003e$h_{t-1}$\u003c/code\u003e 和当前这个时刻的输入 \u003ccode\u003e$x_t$\u003c/code\u003e 作为输入，输出为一个介于 0 和 1 之间的值，1 代表全部保留，0 代表全部丢弃。回到之前的语言模型，单元状态需要包含主语的性别信息以便选择正确的代词。但当遇见一个新的主语后，则需要忘记之前主语的性别信息。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-09-21-rnn/lstm-cell-forget-gate.png\" alt=\"LSTM-Cell-Forget-Gate\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ f_t = \\sigma \\left(W_f \\cdot \\left[h_{t-1}, x_t\\right] + b_f\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e第二步我们需要决定要在单元状态中存储什么样的新信息，这包含两个部分。第一部分为一个称之为“\u003cstrong\u003e输入门 (input gate)\u003c/strong\u003e” 的 Sigmoid 网络层，其决定更新那些数据。第二部分为一个 Tanh 网络层，其将产生一个新的候选值向量 \u003ccode\u003e$\\tilde{C}_t$\u003c/code\u003e 并用于添加到单元状态中。之后会将两者进行整合，并对单元状态进行更新。在我们的语言模型中，我们希望将新主语的性别信息添加到单元状态中并替代需要忘记的旧主语的性别信息。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-09-21-rnn/lstm-cell-input-gate.png\" alt=\"LSTM-Cell-Input-Gate\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{equation} \\begin{split} i_t \u0026amp;= \\sigma \\left(W_i \\cdot \\left[h_{t-1}, x_t\\right] + b_i\\right) \\\\ \\tilde{C}_t \u0026amp;= \\tanh \\left(W_C \\cdot \\left[h_{t-1}, x_t\\right] + b_C\\right) \\end{split} \\end{equation} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e接下来需要将旧的单元状态 \u003ccode\u003e$C_{t-1}$\u003c/code\u003e 更新为 \u003ccode\u003e$C_t$\u003c/code\u003e。我们将旧的单元状态乘以 \u003ccode\u003e$f_t$\u003c/code\u003e 以控制需要忘记多少之前旧的信息，再加上 \u003ccode\u003e$i_t \\odot \\tilde{C}_t$\u003c/code\u003e 用于控制单元状态的更新。在我们的语言模型中，该操作真正实现了我们对与之前主语性别信息的遗忘和对新信息的增加。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-09-21-rnn/lstm-cell-state-update.png\" alt=\"LSTM-Cell-State-Update\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e最后我们需要确定单元的输出，该输出将基于单元的状态，但为一个过滤版本。首先我们利用一个 Sigmoid 网络层来确定单元状态的输出，其次我们对单元状态进行 \u003ccode\u003e$\\tanh$\u003c/code\u003e 操作 (将其值缩放到 -1 和 1 之间) 并与之前 Sigmoid 层的输出相乘，最终得到需要输出的信息。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-09-21-rnn/lstm-cell-output-gate.png\" alt=\"LSTM-Cell-Output-Gate\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{equation} \\begin{split} o_t \u0026amp;= \\sigma \\left(W_o \\cdot \\left[h_{t-1}, x_t\\right] + b_o\\right) \\\\ h_t \u0026amp;= o_t \\odot \\tanh \\left(C_t\\right) \\end{split} \\end{equation} $$\u003c/code\u003e\u003c/p\u003e\n\u003ch2 id=\"lstm-变种\"\u003eLSTM 变种\u003c/h2\u003e\n\u003cp\u003e上文中介绍的基础的 LSTM 模型，事实上不同学者对 LSTM 的结构进行了或多或少的改变，其中一个比较有名的变种是由 Gers 和 Schmidhuber 提出的 \u003csup id=\"fnref:5\"\u003e\u003ca href=\"#fn:5\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e5\u003c/a\u003e\u003c/sup\u003e。其添加了一种“窥视孔连接 (peephole connections)”，这使得每一个门结构都能够窥视到单元的状态。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-09-21-rnn/peephole-cell.png\" alt=\"Peephole-Cell\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{equation} \\begin{split} f_t \u0026amp;= \\sigma \\left(W_f \\cdot \\left[\\boldsymbol{C_{t-1}}, h_{t-1}, x_t\\right] + b_f\\right) \\\\ i_t \u0026amp;= \\sigma \\left(W_i \\cdot \\left[\\boldsymbol{C_{t-1}}, h_{t-1}, x_t\\right] + b_i\\right) \\\\ o_t \u0026amp;= \\sigma \\left(W_o \\cdot \\left[\\boldsymbol{C_t}, h_{t-1}, x_t\\right] + b_o\\right) \\end{split} \\end{equation} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e另一个变种是使用了成对的遗忘门和输入门。不同于一般的 LSTM 中分别确定需要遗忘和新添加的信息，成对的遗忘门和输入门仅在需要添加新输入是才会忘记部分信息，同理仅在需要忘记信息时才会添加新的输入。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-09-21-rnn/cfig-cell.png\" alt=\"CFIG-Cell\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ C_t = f_t \\odot C_{t-1} + \\boldsymbol{\\left(1 - f_t\\right)} \\odot \\tilde{C}_t $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e另外一个比较有名的变种为 Cho 等人提出的 Gated Recurrent Unit (GRU) \u003csup id=\"fnref:6\"\u003e\u003ca href=\"#fn:6\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e6\u003c/a\u003e\u003c/sup\u003e，单元结构如下：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-09-21-rnn/gru-cell.png\" alt=\"GRU-Cell\"/\u003e\u003c/p\u003e\n\u003cp\u003eGRU 将遗忘门和输入门整个成一层，称之为“\u003cstrong\u003e更新门 (update gate)\u003c/strong\u003e”，同时配以一个“\u003cstrong\u003e重置门 (reset gate)\u003c/strong\u003e”。具体的计算过程如下：\u003c/p\u003e\n\u003cp\u003e首先计算更新门 \u003ccode\u003e$z_t$\u003c/code\u003e 和重置门 \u003ccode\u003e$r_t$\u003c/code\u003e：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{equation} \\begin{split} z_t \u0026amp;= \\sigma \\left(W_z \\cdot \\left[h_{t-1}, x_t\\right]\\right) \\\\ r_t \u0026amp;= \\sigma \\left(W_r \\cdot \\left[h_{t-1}, x_t\\right]\\right) \\end{split} \\end{equation} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其次计算候选隐含层 (candidate hidden layer) \u003ccode\u003e$\\tilde{h}_t$\u003c/code\u003e，与 LSTM 中计算 \u003ccode\u003e$\\tilde{C}_t$\u003c/code\u003e 类似，其中 \u003ccode\u003e$r_t$\u003c/code\u003e 用于控制保留多少之前的信息：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\tilde{h}_t = \\tanh \\left(W \\cdot \\left[r_t \\odot h_{t-1}, x_t\\right]\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e最后计算需要从之前的隐含层 \u003ccode\u003e$h_{t-1}$\u003c/code\u003e 遗忘多少信息，同时加入多少新的信息 \u003ccode\u003e$\\tilde{h}_t$\u003c/code\u003e，\u003ccode\u003e$z_t$\u003c/code\u003e 用于控制这个比例：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ h_t = \\left(1 - z_t\\right) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e因此，对于短距离依赖的单元重置门的值较大，对于长距离依赖的单元更新门的值较大。如果 \u003ccode\u003e$r_t = 1$\u003c/code\u003e 并且 \u003ccode\u003e$z_t = 0$\u003c/code\u003e，则 GRU 退化为一个标准的 RNN。\u003c/p\u003e\n\u003cp\u003e除此之外还有大量的 LSTM 变种，Greff 等人 \u003csup id=\"fnref:7\"\u003e\u003ca href=\"#fn:7\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e7\u003c/a\u003e\u003c/sup\u003e 对一些常见的变种进行了比较，Jozefowicz 等人 \u003csup id=\"fnref:8\"\u003e\u003ca href=\"#fn:8\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e8\u003c/a\u003e\u003c/sup\u003e 测试了大量的 RNN 结构在不同任务上的表现。\u003c/p\u003e\n\u003ch1 id=\"扩展与应用\"\u003e扩展与应用\u003c/h1\u003e\n\u003cp\u003e循环神经网络在序列建模上有着天然的优势，其在自然语言处理，包括：语言建模，语音识别，机器翻译，对话与QA，文本生成等；计算视觉，包括：目标识别，视觉追踪，图像生成等；以及一些综合场景，包括：图像标题生成，视频字幕生成等，多个领域均有不错的表现，有代表性的论文请参见 \u003ca href=\"https://github.com/kjw0612/awesome-rnn\"\u003eawesome-rnn\u003c/a\u003e。\u003c/p\u003e\n\u003cp\u003eGoogle 的 \u003ca href=\"https://magenta.tensorflow.org/\"\u003eMagenta\u003c/a\u003e 是一项利用机器学习创作艺术和音乐的研究，其中也包含了大量利用 RNN 相关模型构建的有趣项目。\u003ca href=\"https://magenta.tensorflow.org/sketch-rnn-demo\"\u003eSketchRNN\u003c/a\u003e 是由 Ha 等人 \u003csup id=\"fnref:9\"\u003e\u003ca href=\"#fn:9\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e9\u003c/a\u003e\u003c/sup\u003e 提出了一种能够根据用户描绘的一些简单图形自动完成后续绘画的 RNN 网络。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-09-21-rnn/sketch-rnn-demo.gif\" alt=\"SketchRNN-Demo\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://magenta.tensorflow.org/performance-rnn-browser\"\u003ePerformance RNN\u003c/a\u003e 是由 Ian\n等人 \u003csup id=\"fnref:10\"\u003e\u003ca href=\"#fn:10\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e10\u003c/a\u003e\u003c/sup\u003e 提出了一种基于时间和动态因素生成复合音乐的 LSTM 网络。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-09-21-rnn/performance-rnn-demo.gif\" alt=\"Performance-RNN-Demo\"/\u003e\u003c/p\u003e\n\u003cp\u003e更多有趣的作品请参见 Megenta 的 \u003ca href=\"https://magenta.tensorflow.org/demos\"\u003eDemos\u003c/a\u003e 页面。\u003c/p\u003e\n\u003cdiv class=\"footnotes\" role=\"doc-endnotes\"\u003e\n\u003chr/\u003e\n\u003col\u003e\n\u003cli id=\"fn:1\"\u003e\n\u003cp\u003eSalehinejad, H., Sankar, S., Barfett, J., Colak, E., \u0026amp; Valaee, S. (2017). Recent Advances in Recurrent Neural Networks. \u003cem\u003earXiv preprint arXiv:1801.01078.\u003c/em\u003e \u003ca href=\"#fnref:1\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e \u003ca href=\"#fnref1:1\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:2\"\u003e\n\u003cp\u003eSutskever, I., Martens, J., Dahl, G., \u0026amp; Hinton, G. (2013). On the importance of initialization and momentum in deep learning. In \u003cem\u003eInternational Conference on Machine Learning\u003c/em\u003e (pp. 1139–1147). \u003ca href=\"#fnref:2\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:3\"\u003e\n\u003cp\u003eBengio, Y., Simard, P., \u0026amp; Frasconi, P. (1994). Learning long-term dependencies with gradient descent is difficult. \u003cem\u003eIEEE Transactions on Neural Networks, 5(2)\u003c/em\u003e, 157–166. \u003ca href=\"#fnref:3\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:4\"\u003e\n\u003cp\u003eHochreiter, S., \u0026amp; Schmidhuber, J. (1997). Long short-term memory. Neural Computation, 9(8), 1735–1780. \u003ca href=\"#fnref:4\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:5\"\u003e\n\u003cp\u003eGers, F. A., \u0026amp; Schmidhuber, J. (2000). Recurrent nets that time and count. In \u003cem\u003eProceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium\u003c/em\u003e (Vol. 3, pp. 189–194 vol.3). \u003ca href=\"#fnref:5\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:6\"\u003e\n\u003cp\u003eCho, K., van Merrienboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., \u0026amp; Bengio, Y. (2014). Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation. In \u003cem\u003eProceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)\u003c/em\u003e (pp. 1724–1734). \u003ca href=\"#fnref:6\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:7\"\u003e\n\u003cp\u003eGreff, K., Srivastava, R. K., Koutník, J., Steunebrink, B. R., \u0026amp; Schmidhuber, J. (2017). LSTM: A Search Space Odyssey. \u003cem\u003eIEEE Transactions on Neural Networks and Learning Systems, 28(10)\u003c/em\u003e, 2222–2232. \u003ca href=\"#fnref:7\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:8\"\u003e\n\u003cp\u003eJozefowicz, R., Zaremba, W., \u0026amp; Sutskever, I. (2015). An Empirical Exploration of Recurrent Network Architectures. In \u003cem\u003eProceedings of the 32Nd International Conference on International Conference on Machine Learning\u003c/em\u003e - Volume 37 (pp. 2342–2350). \u003ca href=\"#fnref:8\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:9\"\u003e\n\u003cp\u003eHa, D., \u0026amp; Eck, D. (2017). A Neural Representation of Sketch Drawings. \u003cem\u003earXiv preprint arXiv:1704.03477\u003c/em\u003e \u003ca href=\"#fnref:9\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:10\"\u003e\n\u003cp\u003eIan S., \u0026amp; Sageev O. Performance RNN: Generating Music with Expressive Timing and Dynamics. Magenta Blog, 2017. \u003ca href=\"https://magenta.tensorflow.org/performance-rnn\"\u003ehttps://magenta.tensorflow.org/performance-rnn\u003c/a\u003e \u003ca href=\"#fnref:10\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/div\u003e\n\n\n\n\n\n\u003cdiv class=\"donate\"\u003e\n  \u003cdiv class=\"donate-header\"\u003e\u003c/div\u003e\n  \u003cdiv class=\"donate-slug\" id=\"donate-slug\"\u003ernn\u003c/div\u003e\n  \u003cbutton class=\"donate-button\"\u003e赞 赏\u003c/button\u003e\n  \u003cdiv class=\"donate-footer\"\u003e「真诚赞赏，手留余香」\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"donate-modal-wrapper\"\u003e\n  \u003cdiv class=\"donate-modal\"\u003e\n    \u003cdiv class=\"donate-box\"\u003e\n      \u003cdiv class=\"donate-box-content\"\u003e\n        \u003cdiv class=\"donate-box-content-inner\"\u003e\n          \u003cdiv class=\"donate-box-header\"\u003e「真诚赞赏，手留余香」\u003c/div\u003e\n          \u003cdiv class=\"donate-box-body\"\u003e\n            \u003cdiv class=\"donate-box-money\"\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-2\" data-v=\"2\" data-unchecked=\"￥ 2\" data-checked=\"2 元\"\u003e￥ 2\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-5\" data-v=\"5\" data-unchecked=\"￥ 5\" data-checked=\"5 元\"\u003e￥ 5\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-10\" data-v=\"10\" data-unchecked=\"￥ 10\" data-checked=\"10 元\"\u003e￥ 10\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-50\" data-v=\"50\" data-unchecked=\"￥ 50\" data-checked=\"50 元\"\u003e￥ 50\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-100\" data-v=\"100\" data-unchecked=\"￥ 100\" data-checked=\"100 元\"\u003e￥ 100\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-custom\" data-v=\"custom\" data-unchecked=\"任意金额\" data-checked=\"任意金额\"\u003e任意金额\u003c/button\u003e\n            \u003c/div\u003e\n            \u003cdiv class=\"donate-box-pay\"\u003e\n              \u003cimg class=\"donate-box-pay-qrcode\" id=\"donate-box-pay-qrcode\" src=\"\"/\u003e\n            \u003c/div\u003e\n          \u003c/div\u003e\n          \u003cdiv class=\"donate-box-footer\"\u003e\n            \u003cdiv class=\"donate-box-pay-method donate-box-pay-method-checked\" data-v=\"wechat-pay\"\u003e\n              \u003cimg class=\"donate-box-pay-method-image\" id=\"donate-box-pay-method-image-wechat-pay\" src=\"\"/\u003e\n            \u003c/div\u003e\n            \u003cdiv class=\"donate-box-pay-method\" data-v=\"alipay\"\u003e\n              \u003cimg class=\"donate-box-pay-method-image\" id=\"donate-box-pay-method-image-alipay\" src=\"\"/\u003e\n            \u003c/div\u003e\n          \u003c/div\u003e\n        \u003c/div\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n    \u003cbutton type=\"button\" class=\"donate-box-close-button\"\u003e\n      \u003csvg class=\"donate-box-close-button-icon\" fill=\"#fff\" viewBox=\"0 0 24 24\" width=\"24\" height=\"24\"\u003e\u003cpath d=\"M13.486 12l5.208-5.207a1.048 1.048 0 0 0-.006-1.483 1.046 1.046 0 0 0-1.482-.005L12 10.514 6.793 5.305a1.048 1.048 0 0 0-1.483.005 1.046 1.046 0 0 0-.005 1.483L10.514 12l-5.208 5.207a1.048 1.048 0 0 0 .006 1.483 1.046 1.046 0 0 0 1.482.005L12 13.486l5.207 5.208a1.048 1.048 0 0 0 1.483-.006 1.046 1.046 0 0 0 .005-1.482L13.486 12z\" fill-rule=\"evenodd\"\u003e\u003c/path\u003e\u003c/svg\u003e\n    \u003c/button\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\u003cscript type=\"text/javascript\" src=\"/js/donate.js\"\u003e\u003c/script\u003e\n\n\n  \u003cfooter\u003e\n  \n\u003cnav class=\"post-nav\"\u003e\n  \u003cspan class=\"nav-prev\"\u003e← \u003ca href=\"/cn/2018/09/tour-of-thailand/\"\u003e泰国之行 (Tour of Thailand)\u003c/a\u003e\u003c/span\u003e\n  \u003cspan class=\"nav-next\"\u003e\u003ca href=\"/cn/2018/10/word-embeddings/\"\u003e词向量 (Word Embeddings)\u003c/a\u003e →\u003c/span\u003e\n\u003c/nav\u003e\n\n\n\n\n\u003cins class=\"adsbygoogle\" style=\"display:block; text-align:center;\" data-ad-layout=\"in-article\" data-ad-format=\"fluid\" data-ad-client=\"ca-pub-2608165017777396\" data-ad-slot=\"8302038603\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n  (adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/js-cookie@3.0.5/dist/js.cookie.min.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"/js/toggle-theme.js\"\u003e\u003c/script\u003e\n\n\n\u003cscript src=\"/js/no-highlight.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"/js/math-code.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"/js/heading-anchor.js\"\u003e\u003c/script\u003e\n\n\n\n\u003csection class=\"comments\"\u003e\n\u003cscript src=\"https://giscus.app/client.js\" data-repo=\"leovan/leovan.me\" data-repo-id=\"MDEwOlJlcG9zaXRvcnkxMTMxOTY0Mjc=\" data-category=\"Comments\" data-category-id=\"DIC_kwDOBr89i84CT-R7\" data-mapping=\"pathname\" data-strict=\"1\" data-reactions-enabled=\"1\" data-emit-metadata=\"0\" data-input-position=\"top\" data-theme=\"preferred_color_scheme\" data-lang=\"zh-CN\" data-loading=\"lazy\" crossorigin=\"anonymous\" defer=\"\"\u003e\n\u003c/script\u003e\n\u003c/section\u003e\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cscript async=\"\" src=\"/js/center-img.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/right-quote.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/external-link.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/alt-title.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/figure.js\"\u003e\u003c/script\u003e\n\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js\"\u003e\u003c/script\u003e\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/vanilla-back-to-top@latest/dist/vanilla-back-to-top.min.js\"\u003e\u003c/script\u003e\n\u003cscript\u003e\naddBackToTop({\n  diameter: 48\n});\n\u003c/script\u003e\n\n  \u003chr/\u003e\n  \u003cdiv class=\"copyright no-border-bottom\"\u003e\n    \u003cdiv class=\"copyright-author-year\"\u003e\n      \u003cspan\u003eCopyright © 2017-2024 \u003ca href=\"/\"\u003e范叶亮 | Leo Van\u003c/a\u003e\u003c/span\u003e\n    \u003c/div\u003e\n  \u003c/div\u003e\n  \u003c/footer\u003e\n  \u003c/article\u003e",
  "Date": "2018-09-21T00:00:00Z",
  "Author": "范叶亮"
}