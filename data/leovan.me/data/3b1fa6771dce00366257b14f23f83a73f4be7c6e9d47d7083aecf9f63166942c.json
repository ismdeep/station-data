{
  "Source": "leovan.me",
  "Title": "生成对抗网络简介 (GAN Introduction)",
  "Link": "https://leovan.me/cn/2018/02/gan-introduction/",
  "Content": "\u003carticle class=\"main\"\u003e\n    \u003cheader class=\"content-title\"\u003e\n    \n\u003ch1 class=\"title\"\u003e\n  \n  生成对抗网络简介 (GAN Introduction)\n  \n\u003c/h1\u003e\n\n\n\n\n\n\n\n\u003ch2 class=\"author-date\"\u003e范叶亮 / \n2018-02-03\u003c/h2\u003e\n\n\n\n\u003ch3 class=\"post-meta\"\u003e\n\n\n\u003cstrong\u003e分类: \u003c/strong\u003e\n\u003ca href=\"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0\"\u003e机器学习\u003c/a\u003e, \u003ca href=\"/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0\"\u003e深度学习\u003c/a\u003e\n\n\n\n\n/\n\n\n\n\n\u003cstrong\u003e标签: \u003c/strong\u003e\n\u003cspan\u003eGAN\u003c/span\u003e, \u003cspan\u003eDCGAN\u003c/span\u003e, \u003cspan\u003e生成对抗网络\u003c/span\u003e\n\n\n\n\n/\n\n\n\u003cstrong\u003e字数: \u003c/strong\u003e\n3995\n\u003c/h3\u003e\n\n\n\n\u003chr/\u003e\n\n\n\n    \n    \n    \u003cins class=\"adsbygoogle\" style=\"display:block; text-align:center;\" data-ad-layout=\"in-article\" data-ad-format=\"fluid\" data-ad-client=\"ca-pub-2608165017777396\" data-ad-slot=\"1261604535\"\u003e\u003c/ins\u003e\n    \u003cscript\u003e\n    (adsbygoogle = window.adsbygoogle || []).push({});\n    \u003c/script\u003e\n    \n    \n    \u003c/header\u003e\n\n\n\n\n\u003ch1 id=\"generative-adversarial-networks-gan\"\u003eGenerative Adversarial Networks (GAN)\u003c/h1\u003e\n\u003cp\u003e\u003cstrong\u003e生成对抗网络\u003c/strong\u003e (\u003cstrong\u003eGenerative Adversarial Network, GAN\u003c/strong\u003e) 是由 Goodfellow \u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e 于 2014 年提出的一种对抗网络。这个网络框架包含两个部分，一个生成模型 (generative model) 和一个判别模型 (discriminative model)。其中，生成模型可以理解为一个伪造者，试图通过构造假的数据骗过判别模型的甄别；判别模型可以理解为一个警察，尽可能甄别数据是来自于真实样本还是伪造者构造的假数据。两个模型都通过不断的学习提高自己的能力，即生成模型希望生成更真的假数据骗过判别模型，而判别模型希望能学习如何更准确的识别生成模型的假数据。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-02-03-gan-introduction/zhoubotong.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003ch2 id=\"网络框架\"\u003e网络框架\u003c/h2\u003e\n\u003cp\u003eGAN 由两部分构成，一个\u003cstrong\u003e生成器\u003c/strong\u003e (\u003cstrong\u003eGenerator\u003c/strong\u003e) 和一个\u003cstrong\u003e判别器\u003c/strong\u003e (\u003cstrong\u003eDiscriminator\u003c/strong\u003e)。对于生成器，我们需要学习关于数据 \u003ccode\u003e$\\boldsymbol{x}$\u003c/code\u003e 的一个分布 \u003ccode\u003e$p_g$\u003c/code\u003e，首先定义一个输入数据的先验分布 \u003ccode\u003e$p_{\\boldsymbol{z}} \\left(\\boldsymbol{z}\\right)$\u003c/code\u003e，其次定义一个映射 \u003ccode\u003e$G \\left(\\boldsymbol{z}; \\theta_g\\right): \\boldsymbol{z} \\to \\boldsymbol{x}$\u003c/code\u003e。对于判别器，我们则需要定义一个映射 \u003ccode\u003e$D \\left(\\boldsymbol{x}; \\theta_d\\right)$\u003c/code\u003e 用于表示数据 \u003ccode\u003e$\\boldsymbol{x}$\u003c/code\u003e 是来自于真实数据，还是来自于 \u003ccode\u003e$p_g$\u003c/code\u003e。GAN 的网络框架如下图所示 \u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-02-03-gan-introduction/gan-framework.svg\" alt=\"\"/\u003e\u003c/p\u003e\n\u003ch2 id=\"模型训练\"\u003e模型训练\u003c/h2\u003e\n\u003cp\u003eGoodfellow 在文献中给出了一个重要的公式用于求解最优的生成器\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\min_{G} \\max_{D} V\\left(D, G\\right) = \\mathbb{E}_{\\boldsymbol{x} \\sim p_{data}{\\left(\\boldsymbol{x}\\right)}}{\\left[\\log D\\left(\\boldsymbol{x}\\right)\\right]} + \\mathbb{E}_{\\boldsymbol{z} \\sim p_{\\boldsymbol{z}}\\left(\\boldsymbol{z}\\right)}{\\left[\\log \\left(1 - D\\left(G\\left(\\boldsymbol{z}\\right)\\right)\\right)\\right]} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e上式中，在给定的 \u003ccode\u003e$G$\u003c/code\u003e 的情况下，\u003ccode\u003e$\\max_{D} V\\left(G, D\\right)$\u003c/code\u003e衡量的是 \u003ccode\u003e$p_{data}$\u003c/code\u003e 和 \u003ccode\u003e$p_g$\u003c/code\u003e 之间的“区别”，因此我们最终的优化目标就是找到最优的 \u003ccode\u003e$G^*$\u003c/code\u003e 使得 \u003ccode\u003e$p_{data}$\u003c/code\u003e 和 \u003ccode\u003e$p_g$\u003c/code\u003e 之间的“区别”最小。\u003c/p\u003e\n\u003cp\u003e首先，在给定 \u003ccode\u003e$G$\u003c/code\u003e 的时候，我们可以通过最大化 \u003ccode\u003e$V \\left(G, D\\right)$\u003c/code\u003e 得到最优 \u003ccode\u003e$D^*$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{equation} \\begin{split} V \\left(G, D\\right) \u0026amp;= \\mathbb{E}_{\\boldsymbol{x} \\sim p_{data}{\\left(\\boldsymbol{x}\\right)}}{\\left[\\log D\\left(\\boldsymbol{x}\\right)\\right]} + \\mathbb{E}_{\\boldsymbol{z} \\sim p_{\\boldsymbol{z}}\\left(\\boldsymbol{z}\\right)}{\\left[\\log \\left(1 - D\\left(G\\left(\\boldsymbol{z}\\right)\\right)\\right)\\right]} \\\\ \u0026amp;= \\int_{\\boldsymbol{x}}{p_{data}\\left(\\boldsymbol{x}\\right) \\log D\\left(\\boldsymbol{x}\\right) dx} + \\int_{\\boldsymbol{z}}{p_{\\boldsymbol{z}} \\left(\\boldsymbol{z}\\right) \\log \\left(1 - D\\left(g\\left(\\boldsymbol{z}\\right)\\right)\\right) dz} \\\\ \u0026amp;= \\int_{\\boldsymbol{x}}{p_{data}\\left(\\boldsymbol{x}\\right) \\log D\\left(\\boldsymbol{x}\\right) + p_g\\left(\\boldsymbol{x}\\right) \\log \\left(1 - D\\left(\\boldsymbol{x}\\right)\\right) dx} \\end{split} \\end{equation} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e对于给定的任意 \u003ccode\u003e$a, b \\in \\mathbb{R}^2 \\setminus \\{0, 0\\}$\u003c/code\u003e，\u003ccode\u003e$a \\log\\left(x\\right) + b \\log\\left(1 - x\\right)$\u003c/code\u003e在 \u003ccode\u003e$x = \\dfrac{a}{a+b}$\u003c/code\u003e 处取得最大值，\u003ccode\u003e$D$\u003c/code\u003e 的最优值为\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ D_{G}^{*} = \\dfrac{p_{data} \\left(\\boldsymbol{x}\\right)}{p_{data} \\left(\\boldsymbol{x}\\right) + p_g \\left(\\boldsymbol{x}\\right)} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e因此，\u003ccode\u003e$\\max_{D} V \\left(G, D\\right)$\u003c/code\u003e 可重写为\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{equation} \\begin{split} \u0026amp;C\\left(G\\right) \\\\ =\u0026amp; \\max_{D} V \\left(G, D\\right) = V \\left(G, D^*\\right) \\\\ =\u0026amp; \\mathbb{E}_{\\boldsymbol{x} \\sim p_{data}{\\left(\\boldsymbol{x}\\right)}}{\\left[\\log D_{G}^{*}\\left(\\boldsymbol{x}\\right)\\right]} + \\mathbb{E}_{\\boldsymbol{z} \\sim p_{\\boldsymbol{z}}\\left(\\boldsymbol{z}\\right)}{\\left[\\log \\left(1 - D_{G}^{*}\\left(G\\left(\\boldsymbol{z}\\right)\\right)\\right)\\right]} \\\\ =\u0026amp; \\mathbb{E}_{\\boldsymbol{x} \\sim p_{data}{\\left(\\boldsymbol{x}\\right)}}{\\left[\\log D_{G}^{*}\\left(\\boldsymbol{x}\\right)\\right]} + \\mathbb{E}_{\\boldsymbol{x} \\sim p_g\\left(\\boldsymbol{x}\\right)}{\\left[\\log \\left(1 - D_{G}^{*}\\left(\\boldsymbol{x}\\right)\\right)\\right]} \\\\ =\u0026amp; \\mathbb{E}_{\\boldsymbol{x} \\sim p_{data}{\\left(\\boldsymbol{x}\\right)}}{\\left[\\log \\dfrac{p_{data} \\left(\\boldsymbol{x}\\right)}{p_{data} \\left(\\boldsymbol{x}\\right) + p_g \\left(\\boldsymbol{x}\\right)} \\right]} + \\mathbb{E}_{\\boldsymbol{x} \\sim p_g\\left(\\boldsymbol{x}\\right)}{\\left[\\log  \\dfrac{p_g \\left(\\boldsymbol{x}\\right)}{p_{data} \\left(\\boldsymbol{x}\\right) + p_g \\left(\\boldsymbol{x}\\right)}\\right]} \\\\ =\u0026amp; \\int_{x}{p_{data} \\left(\\boldsymbol{x}\\right) \\log \\dfrac{\\dfrac{1}{2} p_{data} \\left(\\boldsymbol{x}\\right)}{\\dfrac{p_{data} \\left(\\boldsymbol{x}\\right) + p_g \\left(\\boldsymbol{x}\\right)}{2}} dx} + \\int_{x}{p_g \\left(\\boldsymbol{x}\\right) \\log  \\dfrac{\\dfrac{1}{2} p_g \\left(\\boldsymbol{x}\\right)}{\\dfrac{p_{data} \\left(\\boldsymbol{x}\\right) + p_g \\left(\\boldsymbol{x}\\right)}{2}} dx} \\\\ =\u0026amp; \\int_{x}{p_{data} \\left(\\boldsymbol{x}\\right) \\log \\dfrac{p_{data} \\left(\\boldsymbol{x}\\right)}{\\dfrac{p_{data} \\left(\\boldsymbol{x}\\right) + p_g \\left(\\boldsymbol{x}\\right)}{2}} dx} + \\int_{x}{p_g \\left(\\boldsymbol{x}\\right) \\log  \\dfrac{p_g \\left(\\boldsymbol{x}\\right)}{\\dfrac{p_{data} \\left(\\boldsymbol{x}\\right) + p_g \\left(\\boldsymbol{x}\\right)}{2}} dx} + 2 \\log \\dfrac{1}{2} \\\\ =\u0026amp; KL \\left(p_{data} \\left(\\boldsymbol{x}\\right) \\Vert \\dfrac{p_{data} \\left(\\boldsymbol{x}\\right) + p_g \\left(\\boldsymbol{x}\\right)}{2}\\right) + KL \\left(p_g \\left(\\boldsymbol{x}\\right) \\Vert \\dfrac{p_{data} \\left(\\boldsymbol{x}\\right) + p_g \\left(\\boldsymbol{x}\\right)}{2}\\right) - 2 \\log 2 \\\\ =\u0026amp; 2 JS \\left(p_{data} \\left(\\boldsymbol{x}\\right) \\Vert p_g \\left(\\boldsymbol{x}\\right) \\right) - 2 \\log 2 \\end{split} \\end{equation} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中 \u003ccode\u003e$KL$\u003c/code\u003e 表示 KL 散度 \u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e，\u003ccode\u003e$JS$\u003c/code\u003e 表示 JS 散度 \u003csup id=\"fnref:4\"\u003e\u003ca href=\"#fn:4\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e4\u003c/a\u003e\u003c/sup\u003e，因此在全局最优情况下 \u003ccode\u003e$p_g = p_{data}$\u003c/code\u003e。\u003c/p\u003e\n\u003cp\u003e整个 GAN 的训练过程如下所示：\u003c/p\u003e\n\n\n\u003clink rel=\"stylesheet\" type=\"text/css\" href=\"//cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css\"/\u003e\n\n\n\u003cdiv\u003e\u003cpre class=\"pseudocode\"\u003e\\begin{algorithm}\n\\caption{Minibatch SGD for GAN 算法}\n\\begin{algorithmic}\n\\REQUIRE $iter, k, m$\n\\ENSURE $\\theta_d, \\theta_g$\n\\FOR{$i = 1, 2, ..., iter$}\n    \\FOR{$j = 1, 2, ..., k$}\n        \\STATE Sample minibatch of $m$ noise samples $\\{z^{\\left(1\\right)}, ..., z^{\\left(m\\right)}\\}$ from $p_g \\left(\\boldsymbol{z}\\right)$\n        \\STATE Sample minibatch of $m$ examples $\\{x^{\\left(1\\right)}, ..., x^{\\left(m\\right)}\\}$ from $p_{data} \\left(\\boldsymbol{z}\\right)$\n        \\STATE $\\theta_d \\gets \\theta_d \\textcolor{red}{+} \\nabla_{\\theta_d} \\dfrac{1}{m} \\sum_{i=1}^{m}{\\left[\\log D \\left(x^{\\left(i\\right)}\\right) + \\log \\left(1 - D \\left(G \\left(z^{\\left(i\\right)}\\right)\\right)\\right)\\right]}$\n    \\ENDFOR\n    \\STATE Sample minibatch of $m$ noise samples $\\{z^{\\left(1\\right)}, ..., z^{\\left(m\\right)}\\}$ from $p_g \\left(\\boldsymbol{z}\\right)$\n    \\STATE $\\theta_g \\gets \\theta_g \\textcolor{red}{-} \\nabla_{\\theta_g} \\dfrac{1}{m} \\sum_{i=1}^{m}{\\log \\left(1 - D \\left(G \\left(z^{\\left(i\\right)}\\right)\\right)\\right)}$\n\\ENDFOR\n\\end{algorithmic}\n\\end{algorithm}\n\u003c/pre\u003e\u003c/div\u003e\n\n\u003cp\u003e在实际的训练过程中，我们通常不会直接训练 \u003ccode\u003e$G$\u003c/code\u003e \u003cstrong\u003e最小化\u003c/strong\u003e \u003ccode\u003e$\\log \\left(1 - D \\left(G \\left(\\boldsymbol{z}\\right)\\right)\\right)$\u003c/code\u003e，因为其在学习过程中的早起处于饱和状态，因此我们通常会通过\u003cstrong\u003e最大化\u003c/strong\u003e \u003ccode\u003e$\\log \\left(D \\left(G \\left(z\\right)\\right)\\right)$\u003c/code\u003e。\u003c/p\u003e\n\u003ch2 id=\"存在的问题\"\u003e存在的问题\u003c/h2\u003e\n\u003cp\u003e针对 GAN，包括 Goodfellow 自己在内也提出了其中包含的很多问题 \u003csup id=\"fnref1:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e，因此后人也提出了大量的改进，衍生出了大量的 GAN 变种。本章节仅对原始的 GAN 中存在的问题进行简略介绍，相关的改进请参见后续的具体改进算法。\u003c/p\u003e\n\u003ch3 id=\"js-散度问题\"\u003eJS 散度问题\u003c/h3\u003e\n\u003cp\u003e我们在训练判别器的时候，其目标是最大化 JS 散度，但 JS 散度真的能够很好的帮助我们训练判别器吗？ Wasserstein GAN 一文 \u003csup id=\"fnref:5\"\u003e\u003ca href=\"#fn:5\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e5\u003c/a\u003e\u003c/sup\u003e给出了不同生成器情况下 JS 散度的变化情况。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-02-03-gan-introduction/different-generator-jsd.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cp\u003e上图中，左边为一个基于 MLP 的生成器，右边为一个 DCGAN \u003csup id=\"fnref:6\"\u003e\u003ca href=\"#fn:6\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e6\u003c/a\u003e\u003c/sup\u003e 生成器，两者均有一个 DCGAN 的判别器。根据上文我们可以知道判别器的目标是最大化\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{equation} \\begin{split} L \\left(D, \\theta_g\\right) \u0026amp;= \\mathbb{E}_{\\boldsymbol{x} \\sim p_{data}{\\left(\\boldsymbol{x}\\right)}}{\\left[\\log D_{G}^{*}\\left(\\boldsymbol{x}\\right)\\right]} + \\mathbb{E}_{\\boldsymbol{x} \\sim p_g\\left(\\boldsymbol{x}\\right)}{\\left[\\log \\left(1 - D_{G}^{*}\\left(\\boldsymbol{x}\\right)\\right)\\right]} \\\\ \u0026amp;= 2 JS \\left(p_{data} \\left(\\boldsymbol{x}\\right) \\Vert p_g \\left(\\boldsymbol{x}\\right) \\right) - 2 \\log 2 \\end{split} \\end{equation} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e上图中 Y 轴绘制的为 \u003ccode\u003e$\\dfrac{1}{2} L \\left(D, \\theta_g\\right) + \\log 2$\u003c/code\u003e，因为 \u003ccode\u003e$-2 \\log 2 \\leq L \\left(D, \\theta_g\\right) \\leq 0$\u003c/code\u003e，因此我们可得 \u003ccode\u003e$0 \\leq \\dfrac{1}{2} L \\left(D, \\theta_g\\right) + \\log 2 \\leq \\log 2$\u003c/code\u003e。从图中我们可以看出，针对两种不同的情况，其值均很快的逼近最大值 \u003ccode\u003e$\\log 2 \\approx 0.69$\u003c/code\u003e，当接近最大值的时候，判别器将具有接近于零的损失，此时我们可以发现，尽管 JS 散度很快趋于饱和，但 DCGAN 生成器的效果却仍在不断的变好，因此，使用 JS 散度作为判别其的目标就显得不是很合适。\u003c/p\u003e\n\u003ch3 id=\"多样性问题-mode-collapse\"\u003e多样性问题 Mode Collapse\u003c/h3\u003e\n\u003cp\u003eMode Collapse 问题是指生成器更多的是生成了大量相同模式的数据，导致的结果就是生成的数据缺乏多样性，如下图所示 \u003csup id=\"fnref:7\"\u003e\u003ca href=\"#fn:7\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e7\u003c/a\u003e\u003c/sup\u003e:\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-02-03-gan-introduction/mode-collapse-demo.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cp\u003e不难看出，其中红色方框圈出来的图像十分相似，这样的问题我们就称之为 Mode Collapse。Goolfellow 曾经从不同的 KL 散度的角度解释引起 Mode Collapse 的问题，但最后发现其并非由散度的不同所导致。对于 KL 散度，其并非是对称的，即 \u003ccode\u003e$D_{KL} \\left(p_{data} \\Vert p_{model}\\right)$\u003c/code\u003e 与 \u003ccode\u003e$D_{KL} \\left(p_{model} \\Vert p_{data}\\right)$\u003c/code\u003e 是不同的。在最大化似然估计的时候使用的是前者，而在最小化 JS 散度的时候使用的更类似于后者。如下图所示\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-02-03-gan-introduction/difference-of-kl-distributation.svg\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cp\u003e假设我们的模型 \u003ccode\u003e$q$\u003c/code\u003e 并没有足够能能力去拟合真实数据分布 \u003ccode\u003e$p$\u003c/code\u003e，假设真实数据由两个二维的高斯分布构成，而模型需要使用一个一维的高斯分布去拟合。在左图中，模型更倾向于覆盖两个高斯分布，也就是说其更倾向与在有真实数据的地方得到更大的概率。在右图中，模型更倾向于覆盖其中一个高斯分布，也就是说其更倾向于在没有真实数据的地方取得更小的概率。这样，如果我们用 JS 散度训练模型的时候就容易出现模式缺失的问题，但尽管我们利用前者去优化模型，但结果中仍然出现了 Mode Collapse 的问题，这也就说明并非 JS 散度问题导致的 Mode Collapse。\u003c/p\u003e\n\u003cp\u003e针对 Mode Collapse 的问题，出现了大量不同角度的优化\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e基于正则化的优化 \u003csup id=\"fnref:8\"\u003e\u003ca href=\"#fn:8\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e8\u003c/a\u003e\u003c/sup\u003e\u003c/li\u003e\n\u003cli\u003e基于 Minibatch 的优化 \u003csup id=\"fnref:9\"\u003e\u003ca href=\"#fn:9\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e9\u003c/a\u003e\u003c/sup\u003e\u003c/li\u003e\n\u003cli\u003e基于 Unrolled Optimization 的优化 \u003csup id=\"fnref:10\"\u003e\u003ca href=\"#fn:10\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e10\u003c/a\u003e\u003c/sup\u003e\u003c/li\u003e\n\u003cli\u003e基于集成算法的优化 \u003csup id=\"fnref:11\"\u003e\u003ca href=\"#fn:11\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e11\u003c/a\u003e\u003c/sup\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"mnist-示例\"\u003eMNIST 示例\u003c/h2\u003e\n\u003cp\u003e我们利用 MNIST 数据集测试原始的 GAN 模型的效果，代码主要参考了 \u003ca href=\"https://github.com/eriklindernoren/Keras-GAN\"\u003e\u003ccode\u003eKeras-GAN\u003c/code\u003e\u003c/a\u003e，最终实现代码详见 \u003ca href=\"https://github.com/leovan/leovan.me/tree/main/static/codes/cn/2018-02-03-gan-introduction/image_gan_keras.py\"\u003e\u003ccode\u003eimage_gan_keras.py\u003c/code\u003e\u003c/a\u003e，我们简单对其核心部分进行说明。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e生成器\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef build_generator(self):\n    model = Sequential()\n\n    model.add(Dense(int(self._hidden_dim / 4),\n                        input_shape=self._noise_shape))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Dense(int(self._hidden_dim / 2)))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Dense(self._hidden_dim))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Dense(np.prod(self._input_shape), activation=\u0026#39;tanh\u0026#39;))\n    model.add(Reshape(self._input_shape))\n\n    print(\u0026#39;Generator Summary: \u0026#39;)\n    model.summary()\n\n    noise = Input(shape=self._noise_shape)\n    image = model(noise)\n\n    return Model(noise, image)\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e在生成器中，我们使用了一个包含3个隐含层的全链接网络，其中 \u003ccode\u003eself._hidden_dim\u003c/code\u003e 是我们定义的隐含节点最多一层的节点数；\u003ccode\u003eself._noise_shape\u003c/code\u003e 为用于生成器的噪音数据的形状；\u003ccode\u003eself._input_shape\u003c/code\u003e 为输入数据形状，即图片数据的形状，中间层次采用的激活函数为 \u003ccode\u003eLeakyReLU\u003c/code\u003e，最后一层采用的激活函数为 \u003ccode\u003etanh\u003c/code\u003e。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e判别器\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef build_discriminator(self):\n    model = Sequential()\n\n    model.add(Flatten(input_shape=self._input_shape))\n    model.add(Dense(int(self._hidden_dim / 2)))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(Dense(int(self._hidden_dim / 4)))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(Dense(1, activation=\u0026#39;sigmoid\u0026#39;))\n\n    print(\u0026#39;Discriminator Summary: \u0026#39;)\n    model.summary()\n\n    image = Input(shape=self._input_shape)\n    label = model(image)\n\n    return Model(image, label)\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e在判别器中，我们使用了一个包含2个隐含层的全链接网络，中间层次采用的激活函数为 \u003ccode\u003eLeakyReLU\u003c/code\u003e，最后一层采用的激活函数为 \u003ccode\u003esigmoid\u003c/code\u003e。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e对抗网络\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003eclass ImageBasicGAN():\n    def __init__(self, width, height, channels,\n                 a_optimizer=Adam(1e-4, beta_1=0.5),\n                 g_optimizer=Adam(1e-4, beta_1=0.5),\n                 d_optimizer=Adam(1e-4, beta_1=0.5),\n                 noise_dim=100, hidden_dim=1024):\n        \u0026#39;\u0026#39;\u0026#39;\n\n        Args:\n            width: 图像宽度\n            height: 图像高度\n            channels: 图像颜色通道数\n            a_optimizer: 对抗网络优化器\n            g_optimizer: 生成器优化器\n            d_optimizer: 判别器优化器\n            noise_dim: 噪音数据维度\n            hidden_dim: 隐含层最大维度\n        \u0026#39;\u0026#39;\u0026#39;\n\n        # 省略一大坨代码\n\n        # 构建和编译判别器\n        self._discriminator = self.build_discriminator()\n        self._discriminator.compile(loss=\u0026#39;binary_crossentropy\u0026#39;,\n                                    optimizer=d_optimizer,\n                                    metrics=[\u0026#39;accuracy\u0026#39;])\n\n        # 构建和编译生成器\n        self._generator = self.build_generator()\n        self._generator.compile(loss=\u0026#39;binary_crossentropy\u0026#39;,\n                                optimizer=g_optimizer)\n\n        # 生成器利用噪声数据作为输入\n        noise = Input(shape=self._noise_shape)\n        generated_image = self._generator(noise)\n\n        # 当训练整个对抗网络时，仅训练生成器\n        self._discriminator.trainable = False\n\n        # 判别器将生成的图像作为输入\n        label = self._discriminator(generated_image)\n\n        # 构建和编译整个对抗网络\n        self._adversarial = Model(noise, label)\n        self._adversarial.compile(loss=\u0026#39;binary_crossentropy\u0026#39;,\n                                  optimizer=a_optimizer)\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e在构造整个对抗网络的时候，需要注意我们训练完判别器后，通过训练整个对抗网络进而训练生成器的时候是固定住训练好的判别器的，因此在训练整个对抗网络的时候我们应该将判别器置为无需训练的状态。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e训练过程\u003c/p\u003e\n\u003cpre\u003e\u003ccode class=\"language-python\"\u003edef train(self, x_train, output_dir, iters,\n          batch_size=32, k=1, save_interval=200):\n    \u0026#39;\u0026#39;\u0026#39; 训练模型\n\n    Args:\n        x_train: 训练数据\n        output_dir: 相关输出路径\n        iters: 迭代次数\n        batch_size: 批大小\n        k: K\n        save_interval: 结果保存间隔\n    \u0026#39;\u0026#39;\u0026#39;\n\n    # 省略一大坨代码\n\n    for iter in range(iters):\n        # 训练判别器\n        for _ in range(k):\n            train_indices = np.random.randint(0, x_train.shape[0],\n                                              batch_size)\n            train_images = x_train[train_indices]\n\n            noises = np.random.normal(0, 1, (batch_size, self._noise_dim))\n            generated_images = self._generator.predict(noises)\n\n            self._discriminator.train_on_batch(train_images,\n                                               np.ones((batch_size, 1)))\n            self._discriminator.train_on_batch(generated_images,\n                                               np.zeros((batch_size, 1)))\n\n        # 训练生成器\n        noises = np.random.normal(0, 1, (batch_size, self._noise_dim))\n        labels = np.ones(batch_size)\n\n        self._adversarial.train_on_batch(noises, labels)\n\n    # 再省略一大坨代码\n\u003c/code\u003e\u003c/pre\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e在训练整个对抗网络的时候，我们对于一个给定的生成器，我们将生成器生成的数据作为负样本，将从真实数据中采样的数据作为正样本训练判别器。Goodfellow 在描述 GAN 训练的过程中，对于给定的生成器，训练判别器 \u003ccode\u003e$k$\u003c/code\u003e 次，不过通常取 \u003ccode\u003e$k = 1$\u003c/code\u003e。训练好判别器后，再随机生成噪音数据用于训练生成器，周而复始直至达到最大迭代次数。\u003c/p\u003e\n\u003cp\u003e在整个训练过程中，我们分别记录了判别器和生成器的损失的变化，以及判别器的准确率的变化，如下图所示：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-02-03-gan-introduction/mnist-gan-keras-train-history.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cp\u003e从上图中我们可以看出，在训练开始阶段，判别器能够相对容易的识别出哪些数据是来自于真实数据的采样，哪些数据是来自于生成器的伪造数据。随着训练的不断进行，判别器的准确率逐渐下降，并稳定在 60% 左右，也就是说生成器伪造的数据越来越像真实的数据，判别器越来越难进行甄别。\u003c/p\u003e\n\u003cp\u003e下图中我们展示了利用 MNIST 数据集，进行 30000 次的迭代，每 1000 次截取 100 张生成器利用相同噪音数据伪造的图像，最后合成的一张生成图片的变化动图。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-02-03-gan-introduction/mnist-gan-generated-images.gif\" alt=\"\"/\u003e\u003c/p\u003e\n\u003ch1 id=\"deep-convolutional-gan\"\u003eDeep Convolutional GAN\u003c/h1\u003e\n\u003cp\u003eDCGAN (Deep Convolutional GAN) 是由 Radford \u003csup id=\"fnref1:6\"\u003e\u003ca href=\"#fn:6\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e6\u003c/a\u003e\u003c/sup\u003e 等人提出的一种对原始 GAN 的变种，其基本的思想就是将原始 GAN 中的全链接层用卷积神经网络代替。在文中，Radford 等人给出构建一个稳定的 DCGAN 的建议，如下：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e在网络中不使用 pooling 层，而是使用多步长的卷积层 (判别器) 和多步长的反卷积层 (生成器)。\u003c/li\u003e\n\u003cli\u003e在生成器和判别器中均使用批标准化。\u003c/li\u003e\n\u003cli\u003e对于深层的框架，去掉全链接层。\u003c/li\u003e\n\u003cli\u003e在生成器中使用 ReLU 激活函数，最后一层使用 Tanh 激活函数。\u003c/li\u003e\n\u003cli\u003e在判别器中使用 LeakyReLU 激活函数。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e我们利用 MNIST 数据集测试 DCGAN 模型的效果，最终实现代码详见 \u003ca href=\"https://github.com/leovan/leovan.me/tree/main/static/codes/cn/2018-02-03-gan-introduction/image_dcgan_keras.py\"\u003e\u003ccode\u003eimage_dcgan_keras.py\u003c/code\u003e\u003c/a\u003e。训练过程中判别器和生成器的损失的变化，以及判别器的准确率的变化，如下图所示：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-02-03-gan-introduction/mnist-dcgan-keras-train-history.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cp\u003e下图中我们展示了利用 MNIST 数据集，进行 30000 次的迭代，每 1000 次截取 100 张生成器利用相同噪音数据伪造的图像，最后合成的一张生成图片的变化动图。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-02-03-gan-introduction/mnist-dcgan-generated-images.gif\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cp\u003e从生成的结果中可以看出，DCGAN 生成的图片的质量还是优于原始的 GAN 的，在原始的 GAN 中我们能够明显的看出其中仍旧包含大量的噪音点，而在 DCGAN 中这种情况几乎不存在了。\u003c/p\u003e\n\u003cdiv class=\"footnotes\" role=\"doc-endnotes\"\u003e\n\u003chr/\u003e\n\u003col\u003e\n\u003cli id=\"fn:1\"\u003e\n\u003cp\u003eGoodfellow, Ian, et al. “Generative adversarial nets.” \u003cem\u003eAdvances in neural information processing systems\u003c/em\u003e. 2014. \u003ca href=\"#fnref:1\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:2\"\u003e\n\u003cp\u003eGoodfellow, Ian. “NIPS 2016 tutorial: Generative adversarial networks.” \u003cem\u003earXiv preprint arXiv:1701.00160\u003c/em\u003e (2016). \u003ca href=\"#fnref:2\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e \u003ca href=\"#fnref1:2\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:3\"\u003e\n\u003cp\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Kullback\"\u003ehttps://en.wikipedia.org/wiki/Kullback\u003c/a\u003e–Leibler_divergence \u003ca href=\"#fnref:3\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:4\"\u003e\n\u003cp\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Jensen\"\u003ehttps://en.wikipedia.org/wiki/Jensen\u003c/a\u003e–Shannon_divergence \u003ca href=\"#fnref:4\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:5\"\u003e\n\u003cp\u003eArjovsky, Martin, Soumith Chintala, and Léon Bottou. “Wasserstein gan.” \u003cem\u003earXiv preprint arXiv:1701.07875\u003c/em\u003e (2017). \u003ca href=\"#fnref:5\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:6\"\u003e\n\u003cp\u003eRadford, Alec, Luke Metz, and Soumith Chintala. “Unsupervised representation learning with deep convolutional generative adversarial networks.” \u003cem\u003earXiv preprint arXiv:1511.06434\u003c/em\u003e (2015). \u003ca href=\"#fnref:6\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e \u003ca href=\"#fnref1:6\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:7\"\u003e\n\u003cp\u003e\u003ca href=\"http://speech.ee.ntu.edu.tw/~tlkagk/courses/MLDS_2017/Lecture/GAN%20(v11).pdf\"\u003ehttp://speech.ee.ntu.edu.tw/~tlkagk/courses/MLDS_2017/Lecture/GAN%20(v11).pdf\u003c/a\u003e \u003ca href=\"#fnref:7\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:8\"\u003e\n\u003cp\u003eChe, Tong, et al. “Mode regularized generative adversarial networks.” \u003cem\u003earXiv preprint arXiv:1612.02136\u003c/em\u003e (2016). \u003ca href=\"#fnref:8\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:9\"\u003e\n\u003cp\u003eSalimans, Tim, et al. “Improved techniques for training gans.” \u003cem\u003eAdvances in Neural Information Processing Systems.\u003c/em\u003e 2016. \u003ca href=\"#fnref:9\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:10\"\u003e\n\u003cp\u003eMetz, Luke, et al. “Unrolled generative adversarial networks.” \u003cem\u003earXiv preprint arXiv:1611.02163\u003c/em\u003e (2016). \u003ca href=\"#fnref:10\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:11\"\u003e\n\u003cp\u003eTolstikhin, Ilya O., et al. “Adagan: Boosting generative models.” \u003cem\u003eAdvances in Neural Information Processing Systems.\u003c/em\u003e 2017. \u003ca href=\"#fnref:11\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/div\u003e\n\n\n\n\n\n\u003cdiv class=\"donate\"\u003e\n  \u003cdiv class=\"donate-header\"\u003e\u003c/div\u003e\n  \u003cdiv class=\"donate-slug\" id=\"donate-slug\"\u003egan-introduction\u003c/div\u003e\n  \u003cbutton class=\"donate-button\"\u003e赞 赏\u003c/button\u003e\n  \u003cdiv class=\"donate-footer\"\u003e「真诚赞赏，手留余香」\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"donate-modal-wrapper\"\u003e\n  \u003cdiv class=\"donate-modal\"\u003e\n    \u003cdiv class=\"donate-box\"\u003e\n      \u003cdiv class=\"donate-box-content\"\u003e\n        \u003cdiv class=\"donate-box-content-inner\"\u003e\n          \u003cdiv class=\"donate-box-header\"\u003e「真诚赞赏，手留余香」\u003c/div\u003e\n          \u003cdiv class=\"donate-box-body\"\u003e\n            \u003cdiv class=\"donate-box-money\"\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-2\" data-v=\"2\" data-unchecked=\"￥ 2\" data-checked=\"2 元\"\u003e￥ 2\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-5\" data-v=\"5\" data-unchecked=\"￥ 5\" data-checked=\"5 元\"\u003e￥ 5\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-10\" data-v=\"10\" data-unchecked=\"￥ 10\" data-checked=\"10 元\"\u003e￥ 10\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-50\" data-v=\"50\" data-unchecked=\"￥ 50\" data-checked=\"50 元\"\u003e￥ 50\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-100\" data-v=\"100\" data-unchecked=\"￥ 100\" data-checked=\"100 元\"\u003e￥ 100\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-custom\" data-v=\"custom\" data-unchecked=\"任意金额\" data-checked=\"任意金额\"\u003e任意金额\u003c/button\u003e\n            \u003c/div\u003e\n            \u003cdiv class=\"donate-box-pay\"\u003e\n              \u003cimg class=\"donate-box-pay-qrcode\" id=\"donate-box-pay-qrcode\" src=\"\"/\u003e\n            \u003c/div\u003e\n          \u003c/div\u003e\n          \u003cdiv class=\"donate-box-footer\"\u003e\n            \u003cdiv class=\"donate-box-pay-method donate-box-pay-method-checked\" data-v=\"wechat-pay\"\u003e\n              \u003cimg class=\"donate-box-pay-method-image\" id=\"donate-box-pay-method-image-wechat-pay\" src=\"\"/\u003e\n            \u003c/div\u003e\n            \u003cdiv class=\"donate-box-pay-method\" data-v=\"alipay\"\u003e\n              \u003cimg class=\"donate-box-pay-method-image\" id=\"donate-box-pay-method-image-alipay\" src=\"\"/\u003e\n            \u003c/div\u003e\n          \u003c/div\u003e\n        \u003c/div\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n    \u003cbutton type=\"button\" class=\"donate-box-close-button\"\u003e\n      \u003csvg class=\"donate-box-close-button-icon\" fill=\"#fff\" viewBox=\"0 0 24 24\" width=\"24\" height=\"24\"\u003e\u003cpath d=\"M13.486 12l5.208-5.207a1.048 1.048 0 0 0-.006-1.483 1.046 1.046 0 0 0-1.482-.005L12 10.514 6.793 5.305a1.048 1.048 0 0 0-1.483.005 1.046 1.046 0 0 0-.005 1.483L10.514 12l-5.208 5.207a1.048 1.048 0 0 0 .006 1.483 1.046 1.046 0 0 0 1.482.005L12 13.486l5.207 5.208a1.048 1.048 0 0 0 1.483-.006 1.046 1.046 0 0 0 .005-1.482L13.486 12z\" fill-rule=\"evenodd\"\u003e\u003c/path\u003e\u003c/svg\u003e\n    \u003c/button\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\u003cscript type=\"text/javascript\" src=\"/js/donate.js\"\u003e\u003c/script\u003e\n\n\n  \u003cfooter\u003e\n  \n\u003cnav class=\"post-nav\"\u003e\n  \u003cspan class=\"nav-prev\"\u003e← \u003ca href=\"/cn/2018/01/ising-hopfield-and-rbm/\"\u003eIsing 模型，Hopfield 网络和受限的玻尔兹曼机 (Ising, Hopfield and RBM)\u003c/a\u003e\u003c/span\u003e\n  \u003cspan class=\"nav-next\"\u003e\u003ca href=\"/cn/2018/02/optimization-methods-for-deeplearning/\"\u003e深度学习优化算法 (Optimization Methods for Deeplearning)\u003c/a\u003e →\u003c/span\u003e\n\u003c/nav\u003e\n\n\n\n\n\u003cins class=\"adsbygoogle\" style=\"display:block; text-align:center;\" data-ad-layout=\"in-article\" data-ad-format=\"fluid\" data-ad-client=\"ca-pub-2608165017777396\" data-ad-slot=\"8302038603\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n  (adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/js-cookie@3.0.5/dist/js.cookie.min.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"/js/toggle-theme.js\"\u003e\u003c/script\u003e\n\n\n\u003cscript src=\"/js/no-highlight.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"/js/math-code.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"/js/heading-anchor.js\"\u003e\u003c/script\u003e\n\n\n\n\u003csection class=\"comments\"\u003e\n\u003cscript src=\"https://giscus.app/client.js\" data-repo=\"leovan/leovan.me\" data-repo-id=\"MDEwOlJlcG9zaXRvcnkxMTMxOTY0Mjc=\" data-category=\"Comments\" data-category-id=\"DIC_kwDOBr89i84CT-R7\" data-mapping=\"pathname\" data-strict=\"1\" data-reactions-enabled=\"1\" data-emit-metadata=\"0\" data-input-position=\"top\" data-theme=\"preferred_color_scheme\" data-lang=\"zh-CN\" data-loading=\"lazy\" crossorigin=\"anonymous\" defer=\"\"\u003e\n\u003c/script\u003e\n\u003c/section\u003e\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"//cdn.jsdelivr.net/npm/prismjs@1.29.0/prism.min.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"//cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"//cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/toolbar/prism-toolbar.min.js\"\u003e\u003c/script\u003e\n\u003cscript\u003e\n  (function() {\n    if (!self.Prism) {\n      return;\n    }\n\n    \n    Prism.languages.dos = Prism.languages.powershell;\n    Prism.languages.gremlin = Prism.languages.groovy;\n\n    let languages = {\n      'r': 'R', 'python': 'Python', 'xml': 'XML', 'html': 'HTML',\n      'yaml': 'YAML', 'latex': 'LaTeX', 'tex': 'TeX',\n      'powershell': 'PowerShell', 'javascript': 'JavaScript',\n      'dos': 'DOS', 'qml': 'QML', 'json': 'JSON', 'bash': 'Bash',\n      'text': 'Text', 'txt': 'Text', 'sparql': 'SPARQL',\n      'gremlin': 'Gremlin', 'cypher': 'Cypher', 'ngql': 'nGQL',\n      'shell': 'Shell', 'sql': 'SQL', 'apacheconf': 'Apache Configuration', 'c': 'C', 'css': 'CSS'\n    };\n\n    Prism.hooks.add('before-highlight', function(env) {\n      if (env.language !== 'plain') {\n        let language = languages[env.language] || env.language;\n        env.element.setAttribute('data-language', language);\n      }\n    });\n\n    \n    let ClipboardJS = window.ClipboardJS || undefined;\n\n    Prism.plugins.toolbar.registerButton('copy-to-clipboard', function(env) {\n      let linkCopy = document.createElement('button');\n      linkCopy.classList.add('prism-button-copy');\n\n      registerClipboard();\n\n      return linkCopy;\n\n      function registerClipboard() {\n        let clip = new ClipboardJS(linkCopy, {\n          'text': function () {\n            return env.code;\n          }\n        });\n\n        clip.on('success', function() {\n          linkCopy.classList.add('prism-button-copy-success');\n          resetText();\n        });\n        clip.on('error', function () {\n          linkCopy.classList.add('prism-button-copy-error');\n          resetText();\n        });\n      }\n\n      function resetText() {\n        setTimeout(function () {\n          linkCopy.classList.remove('prism-button-copy-success');\n          linkCopy.classList.remove('prism-button-copy-error');\n        }, 1600);\n      }\n    });\n  })();\n\u003c/script\u003e\n\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js\"\u003e\u003c/script\u003e\n\u003cscript type=\"text/javascript\"\u003e\nlet pseudocodeCaptionCount = 0;\n(function(d) {\n  d.querySelectorAll(\".pseudocode\").forEach(function(elem) {\n    let pseudocode_options = {\n      indentSize: '1.2em',\n      commentDelimiter: '\\/\\/',\n      lineNumber:  true ,\n      lineNumberPunc: ':',\n      noEnd:  false \n    };\n    pseudocode_options.captionCount = pseudocodeCaptionCount;\n    pseudocodeCaptionCount += 1;\n    pseudocode.renderElement(elem, pseudocode_options);\n  });\n})(document);\n\u003c/script\u003e\n\n\n\n\n\n\n\n\n\n\n\n\u003cscript async=\"\" src=\"/js/center-img.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/right-quote.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/external-link.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/alt-title.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/figure.js\"\u003e\u003c/script\u003e\n\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js\"\u003e\u003c/script\u003e\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/vanilla-back-to-top@latest/dist/vanilla-back-to-top.min.js\"\u003e\u003c/script\u003e\n\u003cscript\u003e\naddBackToTop({\n  diameter: 48\n});\n\u003c/script\u003e\n\n  \u003chr/\u003e\n  \u003cdiv class=\"copyright no-border-bottom\"\u003e\n    \u003cdiv class=\"copyright-author-year\"\u003e\n      \u003cspan\u003eCopyright © 2017-2024 \u003ca href=\"/\"\u003e范叶亮 | Leo Van\u003c/a\u003e\u003c/span\u003e\n    \u003c/div\u003e\n  \u003c/div\u003e\n  \u003c/footer\u003e\n  \u003c/article\u003e",
  "Date": "2018-02-03T00:00:00Z",
  "Author": "范叶亮"
}