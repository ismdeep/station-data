{
  "Source": "leovan.me",
  "Title": "多臂赌博机 (Multi-armed Bandit)",
  "Link": "https://leovan.me/cn/2020/05/multi-armed-bandit/",
  "Content": "\u003carticle class=\"main\"\u003e\n    \u003cheader class=\"content-title\"\u003e\n    \n\u003ch1 class=\"title\"\u003e\n  \n  多臂赌博机 (Multi-armed Bandit)\n  \n\u003c/h1\u003e\n\u003ch2 class=\"subtitle\"\u003e强化学习系列\u003c/h2\u003e\n\n\n\n\n\n\n\u003ch2 class=\"author-date\"\u003e范叶亮 / \n2020-05-16\u003c/h2\u003e\n\n\n\n\u003ch3 class=\"post-meta\"\u003e\n\n\n\u003cstrong\u003e分类: \u003c/strong\u003e\n\u003ca href=\"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0\"\u003e机器学习\u003c/a\u003e, \u003ca href=\"/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0\"\u003e强化学习\u003c/a\u003e\n\n\n\n\n/\n\n\n\n\n\u003cstrong\u003e标签: \u003c/strong\u003e\n\u003cspan\u003e强化学习\u003c/span\u003e, \u003cspan\u003eReinforcement Learning\u003c/span\u003e, \u003cspan\u003e多臂赌博机\u003c/span\u003e, \u003cspan\u003eMulti-armed Bandit\u003c/span\u003e, \u003cspan\u003eEpsilon Greedy\u003c/span\u003e, \u003cspan\u003eUpper Confidence Bound\u003c/span\u003e, \u003cspan\u003eUCB\u003c/span\u003e, \u003cspan\u003eGradient Bandit\u003c/span\u003e\n\n\n\n\n/\n\n\n\u003cstrong\u003e字数: \u003c/strong\u003e\n2784\n\u003c/h3\u003e\n\n\n\n\u003chr/\u003e\n\n\n\n    \n    \n    \u003cins class=\"adsbygoogle\" style=\"display:block; text-align:center;\" data-ad-layout=\"in-article\" data-ad-format=\"fluid\" data-ad-client=\"ca-pub-2608165017777396\" data-ad-slot=\"1261604535\"\u003e\u003c/ins\u003e\n    \u003cscript\u003e\n    (adsbygoogle = window.adsbygoogle || []).push({});\n    \u003c/script\u003e\n    \n    \n    \u003c/header\u003e\n\n\n\n\n\u003cblockquote\u003e\n\u003cp\u003e本文为\u003ca href=\"/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/\"\u003e《强化学习系列》\u003c/a\u003e文章\u003cbr/\u003e\n本文内容主要参考自《强化学习》\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch1 id=\"多臂赌博机问题\"\u003e多臂赌博机问题\u003c/h1\u003e\n\u003cp\u003e一个赌徒，要去摇老虎机，走进赌场一看，一排老虎机，外表一模一样，但是每个老虎机吐钱的概率可不一样，他不知道每个老虎机吐钱的概率分布是什么，那么每次该选择哪个老虎机可以做到最大化收益呢？这就是\u003cstrong\u003e多臂赌博机问题 (Multi-armed bandit problem, K- or N-armed bandit problem, MAB)\u003c/strong\u003e \u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e。\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-05-16-multi-armed-bandit/compulsive-gambling.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n  \u003cfigcaption class=\"kai\"\u003e图片来源：http://hagencartoons.com/cartoons_166_170.html\u003c/figcaption\u003e\n  \n\u003c/figure\u003e\n\u003cp\u003e\u003ccode\u003e$k$\u003c/code\u003e 臂赌博机问题中，\u003ccode\u003e$k$\u003c/code\u003e 个动作的每一个在被选择时都有一个期望或者平均收益，称之为这个动作的**“价值”**。令 \u003ccode\u003e$t$\u003c/code\u003e 时刻选择的动作为 \u003ccode\u003e$A_t$\u003c/code\u003e，对应的收益为 \u003ccode\u003e$R_t$\u003c/code\u003e，任一动作 \u003ccode\u003e$a$\u003c/code\u003e 对应的价值为 \u003ccode\u003e$q_* \\left(a\\right)$\u003c/code\u003e，即给定动作 \u003ccode\u003e$a$\u003c/code\u003e 时收益的期望：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ q_* \\left(a\\right) = \\mathbb{E} \\left[R_t | A_t = a\\right] $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e我们将对动作 \u003ccode\u003e$a$\u003c/code\u003e 在时刻 \u003ccode\u003e$t$\u003c/code\u003e 的价值的估计记做 \u003ccode\u003e$Q_t \\left(a\\right)$\u003c/code\u003e，我们希望它接近 \u003ccode\u003e$q_* \\left(a\\right)$\u003c/code\u003e。\u003c/p\u003e\n\u003cp\u003e如果持续对动作的价值进行估计，那么在任一时刻都会至少有一个动作的估计价值是最高的，将这些对应最高估计价值的动作成为\u003cstrong\u003e贪心\u003c/strong\u003e的动作。当从这些动作中选择时，称此为\u003cstrong\u003e开发\u003c/strong\u003e当前所知道的关于动作的价值的知识。如果不是如此，而是选择非贪心的动作，称此为\u003cstrong\u003e试探\u003c/strong\u003e，因为这可以让你改善对非贪心动作的价值的估计。“开发”对于最大化当前这一时刻的期望收益是正确的做法，但是“试探”从长远来看可能会带来总体收益的最大化。到底选择“试探”还是“开发”一种复杂的方式依赖于我们得到的函数估计、不确定性和剩余时刻的精确数值。\u003c/p\u003e\n\u003ch1 id=\"动作价值估计方法\"\u003e动作价值估计方法\u003c/h1\u003e\n\u003cp\u003e我们以一种自然的方式，就是通过计算实际收益的平均值来估计动作的价值：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{aligned} Q_t \\left(a\\right) \u0026amp;= \\dfrac{t \\text{ 时刻前执行动作 } a \\text{ 得到的收益总和 }}{t \\text{ 时刻前执行动作 } a \\text{ 的次数}} \\\\ \u0026amp;= \\dfrac{\\sum_{i=1}^{t-1}{R_i \\cdot \\mathbb{1}_{A_i = a}}}{\\sum_{i=1}^{t-1}{\\mathbb{1}_{A_i = a}}} \\end{aligned} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$\\mathbb{1}_{\\text{predicate}}$\u003c/code\u003e 表示随机变量，当 predicate 为真时其值为 1，反之为 0。当分母为 0 时，\u003ccode\u003e$Q_t \\left(a\\right) = 0$\u003c/code\u003e，当分母趋向无穷大时，根据大数定律，\u003ccode\u003e$Q_t \\left(a\\right)$\u003c/code\u003e 会收敛到 \u003ccode\u003e$q_* \\left(a\\right)$\u003c/code\u003e。这种估计动作价值的方法称为\u003cstrong\u003e采样平均方法\u003c/strong\u003e，因为每一次估计都是对相关收益样本的平均。\u003c/p\u003e\n\u003cp\u003e当然，这只是估计动作价值的一种方法，而且不一定是最好的方法。例如，我们也可以利用累积遗憾（Regret）来评估动作的价值：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\rho = T \\mu^* - \\sum_{t=1}^{T} \\hat{r}_t $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$\\mu^* = \\mathop{\\max}_{k} \\left\\{\\mu_k\\right\\}$\u003c/code\u003e 为最大的回报，\u003ccode\u003e$\\hat{r}_t$\u003c/code\u003e 为 \u003ccode\u003e$t$\u003c/code\u003e 时刻的回报。\u003c/p\u003e\n\u003ch1 id=\"多臂赌博机算法\"\u003e多臂赌博机算法\u003c/h1\u003e\n\u003cp\u003e以 10 臂赌博机为例，动作的收益分布如下图所示：\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-05-16-multi-armed-bandit/action-reward-distribution.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n\u003c/figure\u003e\n\u003cp\u003e动作的真实价值 \u003ccode\u003e$q_* \\left(a\\right), a = 1, \\cdots, 10$\u003c/code\u003e 为从一个均值为 0 方差为 1 的标准正态分布中选择。当对于该问题的学习方法在 \u003ccode\u003e$t$\u003c/code\u003e 时刻选择 \u003ccode\u003e$A_t$\u003c/code\u003e 时，实际的收益 \u003ccode\u003e$R_t$\u003c/code\u003e 则由一个均值为 \u003ccode\u003e$q_* \\left(A_t\\right)$\u003c/code\u003e 方差为 1 的正态分布决定。\u003c/p\u003e\n\u003ch2 id=\"epsilon-greedy\"\u003e\u003ccode\u003e$\\epsilon$\u003c/code\u003e-Greedy\u003c/h2\u003e\n\u003cp\u003e\u003ccode\u003e$\\epsilon$\u003c/code\u003e-Greedy 采用的动作选择逻辑如下：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e确定一个 \u003ccode\u003e$\\epsilon \\in \\left(0, 1\\right)$\u003c/code\u003e。\u003c/li\u003e\n\u003cli\u003e每次以 \u003ccode\u003e$\\epsilon$\u003c/code\u003e 的概率随机选择一个臂，以 \u003ccode\u003e$1 - \\epsilon$\u003c/code\u003e 选择平均收益最大的那个臂。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e下图分别展示了 \u003ccode\u003e$\\epsilon = 0$\u003c/code\u003e（贪婪），\u003ccode\u003e$\\epsilon = 0.01$\u003c/code\u003e 和 \u003ccode\u003e$\\epsilon = 0.1$\u003c/code\u003e 三种情况下的平均收益和最优动作占比随训练步数的变化情况。\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-05-16-multi-armed-bandit/epsilon-greedy-step-average-reward.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n\u003c/figure\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-05-16-multi-armed-bandit/epsilon-greedy-step-best-action-ratio.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n\u003c/figure\u003e\n\u003cp\u003e\u003ccode\u003e$\\epsilon$\u003c/code\u003e-Greedy 相比于 \u003ccode\u003e$\\epsilon = 0$\u003c/code\u003e（贪婪）算法的优势如下：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e对于更大方差的收益，找到最优的动作需要更多次的试探。\u003c/li\u003e\n\u003cli\u003e对于非平稳的任务，即动作的真实价值会随着时间而改变，这种情况下即使有确定性的情况下，也需要进行试探。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e令 \u003ccode\u003e$R_i$\u003c/code\u003e 表示一个动作被选择 \u003ccode\u003e$i$\u003c/code\u003e 次后获得的收益，\u003ccode\u003e$Q_n$\u003c/code\u003e 表示被选择 \u003ccode\u003e$n - 1$\u003c/code\u003e 次后它的估计的动作价值，其可以表示为增量计算的形式：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{aligned} Q_{n+1} \u0026amp;= \\dfrac{1}{n} \\sum_{i=1}^{n}{R_i} \\\\ \u0026amp;= \\dfrac{1}{n} \\left(R_n + \\sum_{i=1}^{n-1}{R_i}\\right) \\\\ \u0026amp;= \\dfrac{1}{n} \\left(R_n + \\left(n - 1\\right) \\dfrac{1}{n-1} \\sum_{i=1}^{n-1}{R_i}\\right) \\\\ \u0026amp;= \\dfrac{1}{n} \\left(R_n + \\left(n - 1\\right) Q_n\\right) \\\\ \u0026amp;= \\dfrac{1}{n} \\left(R_n + n Q_n - Q_n\\right) \\\\ \u0026amp;= Q_n + \\dfrac{1}{n} \\left[R_n - Q_n\\right] \\end{aligned} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e上述我们讨论的都是平稳的问题，即收益的概率分布不随着时间变化的赌博机问题。对于非平稳的问题，给近期的收益赋予比过去更高的权值是一个合理的处理方式。则收益均值 \u003ccode\u003e$Q_n$\u003c/code\u003e 的增量更新规则为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{aligned} Q_{n+1} \u0026amp;= Q_n + \\alpha \\left[R_n - Q_n\\right] \\\\ \u0026amp;= \\left(1 - \\alpha\\right)^n Q_1 + \\sum_{i=1}^{n} \\alpha \\left(1 - \\alpha\\right)^{n-i} R_i \\end{aligned} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e赋给收益 \u003ccode\u003e$R_i$\u003c/code\u003e 的权值 \u003ccode\u003e$\\alpha \\left(1 - \\alpha\\right)^{n-i}$\u003c/code\u003e 依赖于它被观测到的具体时刻和当前时刻的差，权值以指数形式递减，因此这个方法也称之为\u003cstrong\u003e指数近因加权平均\u003c/strong\u003e。\u003c/p\u003e\n\u003cp\u003e上述讨论中所有方法都在一定程度上依赖于初始动作值 \u003ccode\u003e$Q_1 \\left(a\\right)$\u003c/code\u003e 的选择。从统计学角度，初始估计值是有偏的，对于平均采样来说，当所有动作都至少被选择一次时，偏差会消失；对于步长为常数的情况，偏差会随时间而减小。\u003c/p\u003e\n\u003cp\u003e下图展示了不同初始动作估计值下最优动作占比随训练步数的变化情况：\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-05-16-multi-armed-bandit/epsilon-greedy-different-parameters-best-action-ratio.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n\u003c/figure\u003e\n\u003cp\u003e设置较大初始动作估计值会鼓励进行试探，这种方法称之为\u003cstrong\u003e乐观初始价值\u003c/strong\u003e，该方法在平稳问题中非常有效。\u003c/p\u003e\n\u003ch2 id=\"ucb\"\u003eUCB\u003c/h2\u003e\n\u003cp\u003e\u003ccode\u003e$\\epsilon$\u003c/code\u003e-Greedy 在进行尝试时是盲目地选择，因为它不大会选择接近贪心或者不确定性特别大的动作。在非贪心动作中，最好是根据它们的潜力来选择可能事实上是最优的动作，这要考虑它们的估计有多接近最大值，以及这些估计的不确定性。\u003c/p\u003e\n\u003cp\u003e一种基于\u003cstrong\u003e置信度上界\u003c/strong\u003e（Upper Confidence Bound，UCB）思想的选择动作依据如下：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ A_t = \\mathop{\\arg\\max}_{a} \\left[Q_t \\left(a\\right) + c \\sqrt{\\dfrac{\\ln t}{N_t \\left(a\\right)}}\\right] $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$N_t \\left(a\\right)$\u003c/code\u003e 表示在时刻 \u003ccode\u003e$t$\u003c/code\u003e 之前动作 \u003ccode\u003e$a$\u003c/code\u003e 被选择的次数，\u003ccode\u003e$c \u0026gt; 0$\u003c/code\u003e 用于控制试探的程度。平方根项是对 \u003ccode\u003e$a$\u003c/code\u003e 动作值估计的不确定性或方差的度量，最大值的大小是动作 \u003ccode\u003e$a$\u003c/code\u003e 的可能真实值的上限，参数 \u003ccode\u003e$c$\u003c/code\u003e 决定了置信水平。\u003c/p\u003e\n\u003cp\u003e下图展示了 UCB 算法和 \u003ccode\u003e$\\epsilon$\u003c/code\u003e-Greedy 算法平均收益随着训练步数的变化：\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-05-16-multi-armed-bandit/epsilon-greedy-ucb-step-average-reward.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n\u003c/figure\u003e\n\u003ch2 id=\"梯度赌博机算法\"\u003e梯度赌博机算法\u003c/h2\u003e\n\u003cp\u003e针对每个动作 \u003ccode\u003e$a$\u003c/code\u003e，考虑学习一个数值化的偏好函数 \u003ccode\u003e$H_t \\left(a\\right)$\u003c/code\u003e，偏好函数越大，动作就约频繁地被选择，但偏好函数的概念并不是从“收益”的意义上提出的。基于随机梯度上升的思想，在每个步骤中，在选择动作 \u003ccode\u003e$A_t$\u003c/code\u003e 并获得收益 \u003ccode\u003e$R_t$\u003c/code\u003e 之后，偏好函数会按如下方式更新：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{aligned} H_{t+1} \\left(A_t\\right) \u0026amp;eq H_t \\left(A_t\\right) + \\alpha \\left(R_t - \\bar{R}_t\\right) \\left(1 - \\pi_t \\left(A_t\\right)\\right) \\\\ H_{t+1} \\left(a\\right) \u0026amp;eq H_t \\left(a\\right) - \\alpha \\left(R_t - \\bar{R}_t\\right) \\pi_t \\left(a\\right) \\end{aligned} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$\\alpha \u0026gt; 0$\u003c/code\u003e 表示步长，\u003ccode\u003e$\\bar{R}_t \\in \\mathbb{R}$\u003c/code\u003e 表示时刻 \u003ccode\u003e$t$\u003c/code\u003e 内所有收益的平均值。\u003ccode\u003e$\\bar{R}_t$\u003c/code\u003e 项作为比较收益的一个基准项，如果收益高于它，那么在未来选择动作 \u003ccode\u003e$A_t$\u003c/code\u003e 的概率就会增加，反之概率就会降低，未选择的动作被选择的概率会上升。\u003c/p\u003e\n\u003cp\u003e下图展示了在 10 臂测试平台问题的变体上采用梯度赌博机算法的结果，在这个问题中，它们真实的期望收益是按照平均值为 4 而不是 0 的正态分布来选择的。所有收益的这种变化对梯度赌博机算法没有任何影响，因为收益基准项让它可以马上适应新的收益水平，如果没有基准项，那么性能将显著降低。\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-05-16-multi-armed-bandit/gradient-different-parameters-best-action-ratios.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n\u003c/figure\u003e\n\u003ch1 id=\"算法性能比较\"\u003e算法性能比较\u003c/h1\u003e\n\u003cp\u003e\u003ccode\u003e$\\epsilon$\u003c/code\u003e-Greedy 方法在一段时间内进行随机的动作选择；UCB 方法虽然采用确定的动作选择，但可以通过每个时刻对具有较少样本的动作进行优先选择来实现试探；梯度赌博机算法则不估计动作价值，而是利用偏好函数，使用 softmax 分布来以一种分级的、概率式的方式选择更优的动作；简单地将收益的初值进行乐观的设置，可以让贪心方法也能进行显示试探。\u003c/p\u003e\n\u003cp\u003e下图展示了上述算法在不同参数下的平均收益，每条算法性能曲线都看作一个自己参数的函数。\u003ccode\u003e$x$\u003c/code\u003e 轴上参数值的变化是 2 的倍数，并以对数坐标轴进行表示。\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-05-16-multi-armed-bandit/different-methods-performance.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n\u003c/figure\u003e\n\u003cp\u003e在评估方法时，不仅要关注它在最佳参数设置上的表现，还要注意它对参数值的敏感性。总的来说，在本文的问题上，UCB 表现最好。\u003c/p\u003e\n\u003cdiv class=\"footnotes\" role=\"doc-endnotes\"\u003e\n\u003chr/\u003e\n\u003col\u003e\n\u003cli id=\"fn:1\"\u003e\n\u003cp\u003eSutton, R. S., \u0026amp; Barto, A. G. (2018). \u003cem\u003eReinforcement learning: An introduction\u003c/em\u003e. MIT press. \u003ca href=\"#fnref:1\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:2\"\u003e\n\u003cp\u003e\u003ca href=\"https://cosx.org/2017/05/bandit-and-recommender-systems\"\u003ehttps://cosx.org/2017/05/bandit-and-recommender-systems\u003c/a\u003e \u003ca href=\"#fnref:2\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/div\u003e\n\n\n\n\n\n\u003cdiv class=\"donate\"\u003e\n  \u003cdiv class=\"donate-header\"\u003e\u003c/div\u003e\n  \u003cdiv class=\"donate-slug\" id=\"donate-slug\"\u003emulti-armed-bandit\u003c/div\u003e\n  \u003cbutton class=\"donate-button\"\u003e赞 赏\u003c/button\u003e\n  \u003cdiv class=\"donate-footer\"\u003e「真诚赞赏，手留余香」\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"donate-modal-wrapper\"\u003e\n  \u003cdiv class=\"donate-modal\"\u003e\n    \u003cdiv class=\"donate-box\"\u003e\n      \u003cdiv class=\"donate-box-content\"\u003e\n        \u003cdiv class=\"donate-box-content-inner\"\u003e\n          \u003cdiv class=\"donate-box-header\"\u003e「真诚赞赏，手留余香」\u003c/div\u003e\n          \u003cdiv class=\"donate-box-body\"\u003e\n            \u003cdiv class=\"donate-box-money\"\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-2\" data-v=\"2\" data-unchecked=\"￥ 2\" data-checked=\"2 元\"\u003e￥ 2\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-5\" data-v=\"5\" data-unchecked=\"￥ 5\" data-checked=\"5 元\"\u003e￥ 5\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-10\" data-v=\"10\" data-unchecked=\"￥ 10\" data-checked=\"10 元\"\u003e￥ 10\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-50\" data-v=\"50\" data-unchecked=\"￥ 50\" data-checked=\"50 元\"\u003e￥ 50\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-100\" data-v=\"100\" data-unchecked=\"￥ 100\" data-checked=\"100 元\"\u003e￥ 100\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-custom\" data-v=\"custom\" data-unchecked=\"任意金额\" data-checked=\"任意金额\"\u003e任意金额\u003c/button\u003e\n            \u003c/div\u003e\n            \u003cdiv class=\"donate-box-pay\"\u003e\n              \u003cimg class=\"donate-box-pay-qrcode\" id=\"donate-box-pay-qrcode\" src=\"\"/\u003e\n            \u003c/div\u003e\n          \u003c/div\u003e\n          \u003cdiv class=\"donate-box-footer\"\u003e\n            \u003cdiv class=\"donate-box-pay-method donate-box-pay-method-checked\" data-v=\"wechat-pay\"\u003e\n              \u003cimg class=\"donate-box-pay-method-image\" id=\"donate-box-pay-method-image-wechat-pay\" src=\"\"/\u003e\n            \u003c/div\u003e\n            \u003cdiv class=\"donate-box-pay-method\" data-v=\"alipay\"\u003e\n              \u003cimg class=\"donate-box-pay-method-image\" id=\"donate-box-pay-method-image-alipay\" src=\"\"/\u003e\n            \u003c/div\u003e\n          \u003c/div\u003e\n        \u003c/div\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n    \u003cbutton type=\"button\" class=\"donate-box-close-button\"\u003e\n      \u003csvg class=\"donate-box-close-button-icon\" fill=\"#fff\" viewBox=\"0 0 24 24\" width=\"24\" height=\"24\"\u003e\u003cpath d=\"M13.486 12l5.208-5.207a1.048 1.048 0 0 0-.006-1.483 1.046 1.046 0 0 0-1.482-.005L12 10.514 6.793 5.305a1.048 1.048 0 0 0-1.483.005 1.046 1.046 0 0 0-.005 1.483L10.514 12l-5.208 5.207a1.048 1.048 0 0 0 .006 1.483 1.046 1.046 0 0 0 1.482.005L12 13.486l5.207 5.208a1.048 1.048 0 0 0 1.483-.006 1.046 1.046 0 0 0 .005-1.482L13.486 12z\" fill-rule=\"evenodd\"\u003e\u003c/path\u003e\u003c/svg\u003e\n    \u003c/button\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\u003cscript type=\"text/javascript\" src=\"/js/donate.js\"\u003e\u003c/script\u003e\n\n\n  \u003cfooter\u003e\n  \n\u003cnav class=\"post-nav\"\u003e\n  \u003cspan class=\"nav-prev\"\u003e← \u003ca href=\"/cn/2020/05/introduction-of-reinforcement-learning/\"\u003e强化学习简介 (Introduction of Reinforcement Learning)\u003c/a\u003e\u003c/span\u003e\n  \u003cspan class=\"nav-next\"\u003e\u003ca href=\"/cn/2020/05/markov-decision-process/\"\u003e马尔可夫决策过程 (Markov Decision Process)\u003c/a\u003e →\u003c/span\u003e\n\u003c/nav\u003e\n\n\n\n\n\u003cins class=\"adsbygoogle\" style=\"display:block; text-align:center;\" data-ad-layout=\"in-article\" data-ad-format=\"fluid\" data-ad-client=\"ca-pub-2608165017777396\" data-ad-slot=\"8302038603\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n  (adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/js-cookie@3.0.5/dist/js.cookie.min.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"/js/toggle-theme.js\"\u003e\u003c/script\u003e\n\n\n\u003cscript src=\"/js/no-highlight.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"/js/math-code.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"/js/heading-anchor.js\"\u003e\u003c/script\u003e\n\n\n\n\u003csection class=\"comments\"\u003e\n\u003cscript src=\"https://giscus.app/client.js\" data-repo=\"leovan/leovan.me\" data-repo-id=\"MDEwOlJlcG9zaXRvcnkxMTMxOTY0Mjc=\" data-category=\"Comments\" data-category-id=\"DIC_kwDOBr89i84CT-R7\" data-mapping=\"pathname\" data-strict=\"1\" data-reactions-enabled=\"1\" data-emit-metadata=\"0\" data-input-position=\"top\" data-theme=\"preferred_color_scheme\" data-lang=\"zh-CN\" data-loading=\"lazy\" crossorigin=\"anonymous\" defer=\"\"\u003e\n\u003c/script\u003e\n\u003c/section\u003e\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cscript async=\"\" src=\"/js/center-img.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/right-quote.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/external-link.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/alt-title.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/figure.js\"\u003e\u003c/script\u003e\n\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js\"\u003e\u003c/script\u003e\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/vanilla-back-to-top@latest/dist/vanilla-back-to-top.min.js\"\u003e\u003c/script\u003e\n\u003cscript\u003e\naddBackToTop({\n  diameter: 48\n});\n\u003c/script\u003e\n\n  \u003chr/\u003e\n  \u003cdiv class=\"copyright no-border-bottom\"\u003e\n    \u003cdiv class=\"copyright-author-year\"\u003e\n      \u003cspan\u003eCopyright © 2017-2024 \u003ca href=\"/\"\u003e范叶亮 | Leo Van\u003c/a\u003e\u003c/span\u003e\n    \u003c/div\u003e\n  \u003c/div\u003e\n  \u003c/footer\u003e\n  \u003c/article\u003e",
  "Date": "2020-05-16T00:00:00Z",
  "Author": "范叶亮"
}