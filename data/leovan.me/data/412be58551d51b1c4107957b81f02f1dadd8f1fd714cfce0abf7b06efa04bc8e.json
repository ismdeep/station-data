{
  "Source": "leovan.me",
  "Title": "词向量 (Word Embeddings)",
  "Link": "https://leovan.me/cn/2018/10/word-embeddings/",
  "Content": "\u003carticle class=\"main\"\u003e\n    \u003cheader class=\"content-title\"\u003e\n    \n\u003ch1 class=\"title\"\u003e\n  \n  词向量 (Word Embeddings)\n  \n\u003c/h1\u003e\n\n\n\n\n\n\n\n\u003ch2 class=\"author-date\"\u003e范叶亮 / \n2018-10-01\u003c/h2\u003e\n\n\n\n\u003ch3 class=\"post-meta\"\u003e\n\n\n\u003cstrong\u003e分类: \u003c/strong\u003e\n\u003ca href=\"/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0\"\u003e深度学习\u003c/a\u003e, \u003ca href=\"/categories/%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0\"\u003e表示学习\u003c/a\u003e, \u003ca href=\"/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86\"\u003e自然语言处理\u003c/a\u003e\n\n\n\n\n/\n\n\n\n\n\u003cstrong\u003e标签: \u003c/strong\u003e\n\u003cspan\u003e词向量\u003c/span\u003e, \u003cspan\u003e维数灾难\u003c/span\u003e, \u003cspan\u003eCurse of Dimensionality\u003c/span\u003e, \u003cspan\u003eWord Embedding\u003c/span\u003e, \u003cspan\u003eN-gram\u003c/span\u003e, \u003cspan\u003eWord2Vec\u003c/span\u003e, \u003cspan\u003eCBOW\u003c/span\u003e, \u003cspan\u003eSkip-gram\u003c/span\u003e, \u003cspan\u003eGloVe\u003c/span\u003e, \u003cspan\u003efastText\u003c/span\u003e, \u003cspan\u003eWordRank\u003c/span\u003e, \u003cspan\u003ecw2vec\u003c/span\u003e\n\n\n\n\n/\n\n\n\u003cstrong\u003e字数: \u003c/strong\u003e\n9711\n\u003c/h3\u003e\n\n\n\n\u003chr/\u003e\n\n\n\n    \n    \n    \u003cins class=\"adsbygoogle\" style=\"display:block; text-align:center;\" data-ad-layout=\"in-article\" data-ad-format=\"fluid\" data-ad-client=\"ca-pub-2608165017777396\" data-ad-slot=\"1261604535\"\u003e\u003c/ins\u003e\n    \u003cscript\u003e\n    (adsbygoogle = window.adsbygoogle || []).push({});\n    \u003c/script\u003e\n    \n    \n    \u003c/header\u003e\n\n\n\n\n\u003ch1 id=\"文本表示\"\u003e文本表示\u003c/h1\u003e\n\u003cp\u003e文本表示是计算机处理自然语言的核心，我们希望计算机能够同人类一样对自然语言能够实现语义层面的理解，但这并非易事。在中文和拉丁语系中，文本的直观表示就存在一定的差异，拉丁语系中词与词之间存在天然的分隔符，而中文则没有。\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eI can eat glass, it doesn’t hurt me.\u003cbr/\u003e\n我能吞下玻璃而不伤身体。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e所以，在处理中文之前我们往往需要对原始文本进行分词，在此我们不谈这部分工作，假设我们已经得到了分词完的文本，即我们后续需要处理的“\u003cstrong\u003e词\u003c/strong\u003e”。早期的词表示方法多采用独热编码 (One-Hot Encoding)，对于每一个不同的词都使用一个单独的向量进行表示。对于一个包含 \u003ccode\u003e$n$\u003c/code\u003e 个词的语料而言，一个词的向量表示 \u003ccode\u003e$\\text{word}_i \\in \\left\\{0, 1\\right\\}^n$\u003c/code\u003e 仅在第 \u003ccode\u003e$i$\u003c/code\u003e 的位置值为 1，其他位置的值均为 0。例如，我们可以将“父亲”表示为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\left[1, 0, 0, 0, 0, 0, ...\\right] \\nonumber $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eOne-Hot Encoding 的表示方法十分简洁，但也存在着一些问题。\u003c/p\u003e\n\u003ch2 id=\"维数灾难-the-curse-of-dimensionality\"\u003e维数灾难 (The Curse of Dimensionality)\u003c/h2\u003e\n\u003cp\u003e在很多现实问题中，我们仅用少数的特征是很难利用一个线性模型将数据区分开来的，也就是线性不可分问题。一个有效的方法是利用核函数实现一个非线性变换，将非线性问题转化成线性问题，通过求解变换后的线性问题进而求解原来的非线性问题。\u003c/p\u003e\n\u003cp\u003e假设 \u003ccode\u003e$\\mathcal{X}$\u003c/code\u003e 是输入空间（欧式空间 \u003ccode\u003e$\\mathbb{R}^n$\u003c/code\u003e 的子集或离散结合），\u003ccode\u003e$\\mathcal{H}$\u003c/code\u003e 为特征空间（希尔伯特空间），若存在一个从 \u003ccode\u003e$\\mathcal{X}$\u003c/code\u003e 到 \u003ccode\u003e$ \\mathcal{H}$\u003c/code\u003e 的映射：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$\\phi \\left(x\\right): \\mathcal{X} \\rightarrow \\mathcal{H}$$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e使得对所有 \u003ccode\u003e$x, z \\in \\mathcal{X}$\u003c/code\u003e ，函数 \u003ccode\u003e$K\\left(x, z\\right)$\u003c/code\u003e 满足条件：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$K\\left(x, z\\right) = \\phi \\left(x\\right) \\cdot \\phi \\left(z\\right)$$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e则 \u003ccode\u003e$K\\left(x, z\\right)$\u003c/code\u003e 为核函数， \u003ccode\u003e$\\phi \\left(x\\right)$\u003c/code\u003e 为映射函数，其中 \u003ccode\u003e$\\phi \\left(x\\right) \\cdot \\phi \\left(z\\right)$\u003c/code\u003e 为 \u003ccode\u003e$\\phi \\left(x\\right)$\u003c/code\u003e 和 \u003ccode\u003e$\\phi \\left(z\\right)$\u003c/code\u003e 的内积。\u003c/p\u003e\n\u003cp\u003e例如，对于一个下图所示的二维数据，显然是线性不可分的。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-10-01-word-embeddings/2d-points.png\" alt=\"2d-Points\"/\u003e\u003c/p\u003e\n\u003cp\u003e构建一个映射 \u003ccode\u003e$\\phi: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^3$\u003c/code\u003e 经 \u003ccode\u003e$X$\u003c/code\u003e 映射为： \u003ccode\u003e$x = x^2, y = y^2, z = y$\u003c/code\u003e ，则通过变换后的数据通过可视化可以明显地看出，数据是可以通过一个超平面来分开的。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-10-01-word-embeddings/3d-points.png\" alt=\"3d-Points\"/\u003e\u003c/p\u003e\n\u003cp\u003e可以说随着维度的增加，我们更有可能找到一个超平面（线性模型）将数据划分开来。尽管看起来，随着维度的增加似乎有助于我们构建模型，但是同时数据在高维空间的分布变得越来越\u003cstrong\u003e稀疏\u003c/strong\u003e。因此，在构建机器学习模型时，当我们需要更好的覆盖数据的分布时，我们需要的数据量就更大，这也就会导致需要更多的时间去训练模型。例如，假设所有特征均为0到1之间连续分布的数据，针对1维的情况，当覆盖50%的数据时，仅需全体50%的样本即可；针对2维的情况，当覆盖50%的数据时，则需全体71% ( \u003ccode\u003e$0.71^2 \\approx 0.5$\u003c/code\u003e ) 的样本；针对3维的情况，当覆盖50%的数据时，则需全体79% ( \u003ccode\u003e$0.79^3 \\approx 0.5$\u003c/code\u003e )，这就是我们所说的维数灾难。\u003c/p\u003e\n\u003ch2 id=\"分散式表示-distributed-representations\"\u003e分散式表示 (Distributed Representations)\u003c/h2\u003e\n\u003cp\u003e分散式表示（Distributed Representations）\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e 最早由 Hiton 提出，对比于传统的 One-Hot Representation ，Distributed Representations 可以将数据表示为低维，稠密，连续的向量，也就是说将原始空间中的潜在信息分散的表示在低维空间的不同维度上。\u003c/p\u003e\n\u003cp\u003e传统的 One-Hot Representation 会将数据表示成一个很长的向量，例如，在 NLP 中，利用 One-Hot Representation 表示一个单词：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e父亲: [1, 0, 0, 0, 0, 0, ...]\n爸爸: [0, 1, 0, 0, 0, 0, ...]\n母亲: [0, 0, 1, 0, 0, 0, ...]\n妈妈: [0, 0, 0, 1, 0, 0, ...]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e这种表示形式很简介，但也很稀疏，相当于语料库中有多少个词，则表示空间的维度就需要多少。那么，对于传统的聚类算法，高斯混合模型，最邻近算法，决策树或高斯 SVM 需要 \u003ccode\u003e$O\\left(N\\right)$\u003c/code\u003e 个参数 (或 \u003ccode\u003e$O\\left(N\\right)$\u003c/code\u003e 个样本) 将能够将 \u003ccode\u003e$O\\left(N\\right)$\u003c/code\u003e 的输入区分开来。而像 RBMs ，稀疏编码，Auto-Encoder 或多层神经网络则可以利用 \u003ccode\u003e$O\\left(N\\right)$\u003c/code\u003e 个参数表示 \u003ccode\u003e$O\\left(2^k\\right)$\u003c/code\u003e 的输入，其中 \u003ccode\u003e$k \\leq N$\u003c/code\u003e 为稀疏表示中非零元素的个数 \u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e。\u003c/p\u003e\n\u003cp\u003e采用 Distributed Representation，则可以将单词表示为：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003e父亲: [0.12, 0.34, 0.65, ...]\n爸爸: [0.11, 0.33, 0.58, ...]\n母亲: [0.34, 0.98, 0.67, ...]\n妈妈: [0.29, 0.92, 0.66, ...]\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e利用这种表示，我们不仅可以将稀疏的高维空间转换为稠密的低维空间，同时我们还能学习出文本间的语义相似性来，例如实例中的 \u003ccode\u003e父亲\u003c/code\u003e 和 \u003ccode\u003e爸爸\u003c/code\u003e，从语义上看其均表示 \u003ccode\u003e父亲\u003c/code\u003e 的含义，但是如果利用 One-Hot Representation 编码则 \u003ccode\u003e父亲\u003c/code\u003e 与 \u003ccode\u003e爸爸\u003c/code\u003e 的距离同其与 \u003ccode\u003e母亲\u003c/code\u003e 或 \u003ccode\u003e妈妈\u003c/code\u003e 的距离时相同的，而利用 Distributed Representation 编码，则 \u003ccode\u003e父亲\u003c/code\u003e 同 \u003ccode\u003e爸爸\u003c/code\u003e 之间的距离要远小于其同 \u003ccode\u003e母亲\u003c/code\u003e 或 \u003ccode\u003e妈妈\u003c/code\u003e 之间的距离。\u003c/p\u003e\n\u003ch1 id=\"word-embedding-之路\"\u003eWord Embedding 之路\u003c/h1\u003e\n\u003ch2 id=\"n-gram-模型\"\u003eN-gram 模型\u003c/h2\u003e\n\u003cp\u003eN-gram (N 元语法) 是一种文本表示方法，指文中连续出现的 \u003ccode\u003e$n$\u003c/code\u003e 个词语。N-gram 模型是基于 \u003ccode\u003e$n-1$\u003c/code\u003e 阶马尔科夫链的一种概率语言模型，可以通过前 \u003ccode\u003e$n-1$\u003c/code\u003e 个词对第 \u003ccode\u003e$n$\u003c/code\u003e 个词进行预测。Bengio 等人 \u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e 提出了一个三层的神经网络的概率语言模型，其网络结构如下图所示：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-10-01-word-embeddings/nplm-network.png\" alt=\"NPLM-Network\"/\u003e\u003c/p\u003e\n\u003cp\u003e模型的最下面为前 \u003ccode\u003e$n-1$\u003c/code\u003e 个词 \u003ccode\u003e$w_{t-n+1}, ..., w_{t-2}, w_{t-1}$\u003c/code\u003e，每个词 \u003ccode\u003e$w_i$\u003c/code\u003e 通过查表的方式同输入层对应的词向量 \u003ccode\u003e$C \\left(w_i\\right)$\u003c/code\u003e 相连。词表 \u003ccode\u003e$C$\u003c/code\u003e 为一个 \u003ccode\u003e$\\lvert V\\rvert \\times m$\u003c/code\u003e 大小的矩阵，其中 \u003ccode\u003e$\\lvert V\\rvert$\u003c/code\u003e 表示语料中词的数量，\u003ccode\u003e$m$\u003c/code\u003e 表示词向量的维度。输入层则为前 \u003ccode\u003e$n-1$\u003c/code\u003e 个词向量拼接成的向量 \u003ccode\u003e$x$\u003c/code\u003e，其维度为 \u003ccode\u003e$m \\left(n-1\\right) \\times 1$\u003c/code\u003e。隐含层直接利用 \u003ccode\u003e$d + Hx$\u003c/code\u003e 计算得到，其中 \u003ccode\u003e$H$\u003c/code\u003e 为隐含层的权重，\u003ccode\u003e$d$\u003c/code\u003e 为隐含层的偏置。输出层共包含 \u003ccode\u003e$\\lvert V\\rvert$\u003c/code\u003e 个神经元，每个神经元 \u003ccode\u003e$y_i$\u003c/code\u003e 表示下一个词为第 \u003ccode\u003e$i$\u003c/code\u003e 个词的未归一化的 log 概率，即：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ y = b + Wx + U \\tanh \\left(d + Hx\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e对于该问题，我们的优化目标为最大化如下的 log 似然函数：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ L = \\dfrac{1}{T} \\sum_{t}{f \\left(w_t, w_{t-1}, ..., w_{t-n+1}\\right) + R \\left(\\theta\\right)} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$f \\left(w_t, w_{t-1}, ..., w_{t-n+1}\\right)$\u003c/code\u003e 为利用前 \u003ccode\u003e$n-1$\u003c/code\u003e 个词预测当前词 \u003ccode\u003e$w_t$\u003c/code\u003e 的条件概率，\u003ccode\u003e$R \\left(\\theta\\right)$\u003c/code\u003e 为参数的正则项，\u003ccode\u003e$\\theta = \\left(b, d, W, U, H, C\\right)$\u003c/code\u003e。\u003ccode\u003e$C$\u003c/code\u003e 作为模型的参数之一，随着模型的训练不断优化，在模型训练完毕后，\u003ccode\u003e$C$\u003c/code\u003e 中保存的即为词向量。\u003c/p\u003e\n\u003ch2 id=\"continuous-bag-of-words-cbow-和-skip-gram-模型\"\u003eContinuous Bag-of-Words (CBOW) 和 Skip-gram 模型\u003c/h2\u003e\n\u003cp\u003eCBOW 和 Skip-gram 均考虑一个词的上下文信息，两种模型的结构如下图所示：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-10-01-word-embeddings/cbow-skipgram.png\" alt=\"CBOW-Skipgram\"/\u003e\u003c/p\u003e\n\u003cp\u003e两者在给定的上下文信息中 (即前后各 \u003ccode\u003e$m$\u003c/code\u003e 个词) 忽略了上下文环境的序列信息，CBOW (上图左) 是利用上下文环境中的词预测当前的词，而 Skip-gram (上图右) 则是用当前词预测上下文中的词。\u003c/p\u003e\n\u003cp\u003e对于 CBOW，\u003ccode\u003e$x_{1k}, x_{2k}, ..., x_{Ck}$\u003c/code\u003e 为上下文词的 One-Hot 表示，\u003ccode\u003e$\\mathbf{W}_{V \\times N}$\u003c/code\u003e 为所有词向量构成的矩阵 (词汇表)，\u003ccode\u003e$y_j$\u003c/code\u003e 为利用上下文信息预测得到的当前词的 One-Hot 表示输出，其中 \u003ccode\u003e$C$\u003c/code\u003e 为上下文词汇的数量，\u003ccode\u003e$V$\u003c/code\u003e 为词汇表中词的总数量，\u003ccode\u003e$N$\u003c/code\u003e 为词向量的维度。从输入层到隐含层，我们对输入层词对应的词向量进行简单的加和，即：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ h_i = \\sum_{c=1}^{C}{x_{ck} \\mathbf{W}_{V \\times N}} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e对于 Skip-gram，\u003ccode\u003e$x_k$\u003c/code\u003e 为当前词的 One-Hot 表示，\u003ccode\u003e$\\mathbf{W}_{V \\times N}$\u003c/code\u003e 为所有词向量构成的矩阵 (词汇表)，\u003ccode\u003e$y_{1j}, y_{2j}, ..., y_{Cj}$\u003c/code\u003e 为预测的上次文词汇的 One-Hot 表示输出。从输入层到隐含层，直接将 One-Hot 的输入向量转换为词向量表示即可。\u003c/p\u003e\n\u003cp\u003e除此之外两者还有一些其他的区别：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eCBOW 要比 Skip-gram 模型训练快。从模型中我们不难发现：从隐含层到输出层，CBOW 仅需要计算一个损失，而 Skip-gram 则需要计算 \u003ccode\u003e$C$\u003c/code\u003e 个损失再进行平均进行参数优化。\u003c/li\u003e\n\u003cli\u003eSkip-gram 在小数量的数据集上效果更好，同时对于生僻词的表示效果更好。CBOW 在从输入层到隐含层时，对输入的词向量进行了平均 (可以理解为进行了平滑处理)，因此对于生僻词，平滑后则容易被模型所忽视。\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"word2vec\"\u003eWord2Vec\u003c/h2\u003e\n\u003cp\u003eMikolov 等人 \u003csup id=\"fnref:4\"\u003e\u003ca href=\"#fn:4\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e4\u003c/a\u003e\u003c/sup\u003e 利用上面介绍的 CBOW 和 Skip-gram 两种模型提出了经典的 Word2Vec 算法。Word2Vec 中针对 CBOW 和 Skip-gram 又提出了两种具体的实现方案 Hierarchical Softmax (层次 Softmax) 和 Negative Sampling (负采样)，因此共有 4 种不同的模型。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e基于 Hierarchical Softmax 的模型\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e基于 Hierarchical Softmax 的 CBOW 模型如下\u003c/strong\u003e：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-10-01-word-embeddings/hierarchical-softmax-cbow.png\" alt=\"Hierarchical-Softmax-CBOW\"/\u003e\u003c/p\u003e\n\u003cp\u003e其中：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003cstrong\u003e输入层\u003c/strong\u003e：包含了 \u003ccode\u003e$C$\u003c/code\u003e 个词的词向量，\u003ccode\u003e$\\mathbf{v} \\left(w_1\\right), \\mathbf{v} \\left(w_2\\right), ..., \\mathbf{v} \\left(w_C\\right) \\in \\mathbb{R}^N$\u003c/code\u003e，\u003ccode\u003e$N$\u003c/code\u003e 为词向量的维度。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e投影层\u003c/strong\u003e：将输入层的向量进行加和，即：\u003ccode\u003e$\\mathbf{x}_w = \\sum_{i=1}^{C}{\\mathbf{v} \\left(w_i\\right)} \\in \\mathbb{R}^N$\u003c/code\u003e。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e输出层\u003c/strong\u003e：输出为一颗二叉树，是根据语料构建出来的 Huffman 树 \u003csup id=\"fnref:5\"\u003e\u003ca href=\"#fn:5\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e5\u003c/a\u003e\u003c/sup\u003e，其中每个叶子节点为词汇表中的一个词。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eHierarchical Softmax 是解决概率语言模型中计算效率的关键，CBOW 模型去掉了隐含层，同时将输出层改为了 Huffman 树。对于该模型的优化求解，我们首先引入一些符号，对于 Huffman 树的一个叶子节点 (即词汇表中的词 \u003ccode\u003e$w$\u003c/code\u003e)，记：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003e$p^w$\u003c/code\u003e：从根节点出发到达 \u003ccode\u003e$w$\u003c/code\u003e 对应的叶子节点的路径。\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e$l^w$\u003c/code\u003e：路径 \u003ccode\u003e$p^w$\u003c/code\u003e 包含的节点的个数。\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e$p_1^w, p_1^w, ..., p_{l^w}^w$\u003c/code\u003e：路径 \u003ccode\u003e$p^w$\u003c/code\u003e 中的 \u003ccode\u003e$l^w$\u003c/code\u003e 个节点，其中 \u003ccode\u003e$p_1^w$\u003c/code\u003e 表示根节点，\u003ccode\u003e$p_{l^w}^w$\u003c/code\u003e 表示词 \u003ccode\u003e$w$\u003c/code\u003e 对应的叶子节点。\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e$d_2^w, d_3^w, ..., d_{l^w}^w \\in \\{0, 1\\}$\u003c/code\u003e：词 \u003ccode\u003e$w$\u003c/code\u003e 的 Huffman 编码，由 \u003ccode\u003e$l^w - 1$\u003c/code\u003e 位编码构成，\u003ccode\u003e$d_j^w$\u003c/code\u003e 表示路径 \u003ccode\u003e$p^w$\u003c/code\u003e 中第 \u003ccode\u003e$j$\u003c/code\u003e 个结点对应的编码。\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e$\\theta_1^w, \\theta_1^w, ..., \\theta_{l^w - 1}^w \\in \\mathbb{R}^N$\u003c/code\u003e：路径 \u003ccode\u003e$p^w$\u003c/code\u003e 中非叶子节点对应的向量，\u003ccode\u003e$\\theta_j^w$\u003c/code\u003e 表示路径 \u003ccode\u003e$p^w$\u003c/code\u003e 中第 \u003ccode\u003e$j$\u003c/code\u003e 个非叶子节点对应的向量。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e首先我们需要根据向量 \u003ccode\u003e$\\mathbf{x}_w$\u003c/code\u003e 和 Huffman 树定义条件概率 \u003ccode\u003e$p \\left(w | Context\\left(w\\right)\\right)$\u003c/code\u003e。我们可以将其视为一系列的二分类问题，在到达对应的叶子节点的过程中，经过的每一个非叶子节点均为对应一个取值为 0 或 1 的 Huffman 编码。因此，我们可以将编码为 1 的节点定义为负类，将编码为 0 的节点定义为正类 (即分到左边为负类，分到右边为正类)，则这条路径上对应的标签为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ Label \\left(p_i^w\\right) = 1 - d_i^w, i = 2, 3, ..., l^w $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e则对于一个节点被分为正类的概率为 \u003ccode\u003e$\\sigma \\left(\\mathbf{x}_w^{\\top} \\theta\\right)$\u003c/code\u003e，被分为负类的概率为 \u003ccode\u003e$1 - \\sigma \\left(\\mathbf{x}_w^{\\top} \\theta\\right)$\u003c/code\u003e。则条件概率可以表示为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ p \\left(w | Context\\left(w\\right)\\right) = \\prod_{j=2}^{l^w}{p \\left(d_j^w | \\mathbf{x}_w, \\theta_{j-1}^w\\right)} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ p \\left(d_j^w | \\mathbf{x}_w, \\theta_{j-1}^w\\right) = \\begin{cases} \\sigma \\left(\\mathbf{x}_w^{\\top} \\theta\\right) \u0026amp; d_j^w = 0 \\\\ 1 - \\sigma \\left(\\mathbf{x}_w^{\\top} \\theta\\right) \u0026amp; d_j^w = 1 \\end{cases} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e或表示为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ p \\left(d_j^w | \\mathbf{x}_w, \\theta_{j-1}^w\\right) = \\left[\\sigma \\left(\\mathbf{x}_w^{\\top} \\theta_{j-1}\\right)\\right]^{1 - d_j^w} \\cdot \\left[1 - \\sigma \\left(\\mathbf{x}_w^{\\top} \\theta_{j-1}\\right)\\right]^{d_j^w} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e则对数似然函数为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{equation} \\begin{split} \\mathcal{L} \u0026amp;= \\sum_{w \\in \\mathcal{C}}{\\log \\prod_{j=2}^{l^w}{\\left\\{\\left[\\sigma \\left(\\mathbf{x}_w^{\\top} \\theta_{j-1}\\right)\\right]^{1 - d_j^w} \\cdot \\left[1 - \\sigma \\left(\\mathbf{x}_w^{\\top} \\theta_{j-1}\\right)\\right]^{d_j^w}\\right\\}}} \\\\ \u0026amp;= \\sum_{w \\in \\mathcal{C}}{\\sum_{j=2}^{l^w}{\\left\\{\\left(1 - d_j^w\\right) \\cdot \\log \\left[\\sigma \\left(\\mathbf{x}_w^{\\top} \\theta_{j-1}^w\\right)\\right] + d_j^w \\cdot \\log \\left[1 - \\sigma \\left(\\mathbf{x}_w^{\\top} \\theta_{j-1}^w\\right)\\right]\\right\\}}} \\end{split} \\end{equation} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e记上式花括号中的内容为 \u003ccode\u003e$\\mathcal{L} \\left(w, j\\right)$\u003c/code\u003e，则 \u003ccode\u003e$\\mathcal{L} \\left(w, j\\right)$\u003c/code\u003e 关于 \u003ccode\u003e$\\theta_{j-1}^w$\u003c/code\u003e 的梯度为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{equation} \\begin{split} \\dfrac{\\partial \\mathcal{L} \\left(w, j\\right)}{\\partial \\theta_{j-1}^w} \u0026amp;= \\dfrac{\\partial}{\\partial \\theta_{j-1}^w} \\left\\{\\left(1 - d_j^w\\right) \\cdot \\log \\left[\\sigma \\left(\\mathbf{x}_w^{\\top} \\theta_{j-1}^w\\right)\\right] + d_j^w \\cdot \\log \\left[1 - \\sigma \\left(\\mathbf{x}_w^{\\top} \\theta_{j-1}^w\\right)\\right]\\right\\} \\\\ \u0026amp;= \\left(1 - d_j^w\\right) \\left[1 - \\sigma \\left(\\mathbf{x}_w^{\\top} \\theta_{j-1}^w\\right)\\right] \\mathbf{x}_w - d_j^w \\sigma \\left(\\mathbf{x}_w^{\\top} \\theta_{j-1}^w\\right) \\mathbf{x}_w \\\\ \u0026amp;= \\left\\{\\left(1 - d_j^w\\right) \\left[1 - \\sigma \\left(\\mathbf{x}_w^{\\top} \\theta_{j-1}^w\\right)\\right] - d_j^w \\sigma \\left(\\mathbf{x}_w^{\\top} \\theta_{j-1}^w\\right)\\right\\} \\mathbf{x}_w \\\\ \u0026amp;= \\left[1 - d_j^w - \\sigma \\left(\\mathbf{x}_w^{\\top} \\theta_{j-1}^w\\right)\\right] \\mathbf{x}_w \\end{split} \\end{equation} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e则 \u003ccode\u003e$\\theta_{j-1}^w$\u003c/code\u003e 的更新方式为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\theta_{j-1}^w \\gets \\theta_{j-1}^w + \\eta \\left[1 - d_j^w - \\sigma \\left(\\mathbf{x}_w^{\\top} \\theta_{j-1}^w\\right)\\right] \\mathbf{x}_w $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e同理可得，\u003ccode\u003e$\\mathcal{L} \\left(w, j\\right)$\u003c/code\u003e 关于 \u003ccode\u003e$\\mathbf{x}_w$\u003c/code\u003e 的梯度为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\dfrac{\\partial \\mathcal{L} \\left(w, j\\right)}{\\partial \\mathbf{x}_w} = \\left[1 - d_j^w - \\sigma \\left(\\mathbf{x}_w^{\\top} \\theta_{j-1}^w\\right)\\right] \\theta_{j-1}^w $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e但 \u003ccode\u003e$\\mathbf{x}_w$\u003c/code\u003e 为上下文词汇向量的加和，Word2Vec 的做法是将梯度贡献到上下文中的每个词向量上，即：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\mathbf{v} \\left(u\\right) \\gets \\mathbf{v} \\left(u\\right) + \\eta \\sum_{j=2}^{l^w}{\\dfrac{\\partial \\mathcal{L} \\left(w, j\\right)}{\\partial \\mathbf{x}_w}}, u \\in Context \\left(w\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e基于 Hierarchical Softmax 的 CBOW 模型的随机梯度上升算法伪代码如下：\u003c/p\u003e\n\n\n\u003clink rel=\"stylesheet\" type=\"text/css\" href=\"//cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css\"/\u003e\n\n\n\u003cdiv\u003e\u003cpre class=\"pseudocode\"\u003e\\begin{algorithm}\n\\caption{基于 Hierarchical Softmax 的 CBOW 随机梯度上升算法}\n\\begin{algorithmic}\n\\STATE $\\mathbf{e} = 0$\n\\STATE $\\mathbf{x}_w = \\sum_{u \\in Context \\left(w\\right)}{\\mathbf{v} \\left(u\\right)}$\n\\FOR{$j = 2, 3, ..., l^w$}\n    \\STATE $q = \\sigma \\left(\\mathbf{x}_w^{\\top} \\theta_{j-1}^w\\right)$\n    \\STATE $g = \\eta \\left(1 - d_j^w - q\\right)$\n    \\STATE $\\mathbf{e} \\gets \\mathbf{e} + g \\theta_{j-1}^w$\n    \\STATE $\\theta_{j-1}^w \\gets \\theta_{j-1}^w + g \\mathbf{x}_w$\n\\ENDFOR\n\\FOR{$u \\in Context \\left(w\\right)$}\n    \\STATE $\\mathbf{v} \\left(u\\right) \\gets \\mathbf{v} \\left(u\\right) + \\mathbf{e}$\n\\ENDFOR\n\\end{algorithmic}\n\\end{algorithm}\n\u003c/pre\u003e\u003c/div\u003e\n\n\u003cp\u003e\u003cstrong\u003e基于 Hierarchical Softmax 的 Skip-gram 模型如下\u003c/strong\u003e：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-10-01-word-embeddings/hierarchical-softmax-skipgram.png\" alt=\"Hierarchical-Softmax-Skipgram\"/\u003e\u003c/p\u003e\n\u003cp\u003e对于 Skip-gram 模型，是利用当前词 \u003ccode\u003e$w$\u003c/code\u003e 对上下文 \u003ccode\u003e$Context \\left(w\\right)$\u003c/code\u003e 中的词进行预测，则条件概率为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ p \\left(Context \\left(w\\right) | w\\right) = \\prod_{u \\in Context \\left(w\\right)}{p \\left(u | w\\right)} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e类似于 CBOW 模型的思想，有：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ p \\left(u | w\\right) = \\prod_{j=2}^{l^u}{p \\left(d_j^u | \\mathbf{v} \\left(w\\right), \\theta_{j-1}^u\\right)} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ p \\left(d_j^u | \\mathbf{v} \\left(w\\right), \\theta_{j-1}^u\\right) = \\left[\\sigma \\left(\\mathbf{v} \\left(w\\right)^{\\top} \\theta_{j-1}^u\\right)\\right]^{1 - d_j^u} \\cdot \\left[1 - \\sigma \\left(\\mathbf{v} \\left(w\\right)^{\\top} \\theta_{j-1}^u\\right)\\right]^{d_j^u} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e可得对数似然函数为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{equation} \\begin{split} \\mathcal{L} \u0026amp;= \\sum_{w \\in \\mathcal{C}}{\\log \\prod_{u \\in Context \\left(w\\right)}{\\prod_{j=2}^{l^u}{\\left\\{\\left[\\sigma \\left(\\mathbf{v} \\left(w\\right)^{\\top} \\theta_{j-1}^{u}\\right)\\right]^{1 - d_j^u} \\cdot \\left[1 - \\sigma \\left(\\mathbf{v} \\left(w\\right)^{\\top} \\theta_{j-1}^u\\right)\\right]^{d_j^u}\\right\\}}}} \\\\ \u0026amp;= \\sum_{w \\in \\mathcal{C}}{\\sum_{u \\in Context \\left(w\\right)}{\\sum_{j=2}^{l^u}{\\left\\{\\left(1 - d_j^u\\right) \\cdot \\log \\left[\\sigma \\left(\\mathbf{v} \\left(w\\right)^{\\top} \\theta_{j-1}^{u}\\right)\\right] + d_j^u \\cdot \\log \\left[1 - \\sigma \\left(\\mathbf{v} \\left(w\\right)^{\\top} \\theta_{j-1}^{u}\\right)\\right]\\right\\}}}} \\end{split} \\end{equation} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e记上式花括号中的内容为 \u003ccode\u003e$\\mathcal{L} \\left(w, u, j\\right)$\u003c/code\u003e，在 \u003ccode\u003e$\\mathcal{L} \\left(w, u, j\\right)$\u003c/code\u003e 关于 \u003ccode\u003e$\\theta_{j-1}^u$\u003c/code\u003e 的梯度为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{equation} \\begin{split} \\dfrac{\\partial \\mathcal{L} \\left(w, u, j\\right)}{\\partial \\theta_{j-1}^{u}} \u0026amp;= \\dfrac{\\partial}{\\partial \\theta_{j-1}^{u}} \\left\\{\\left(1 - d_j^u\\right) \\cdot \\log \\left[\\sigma \\left(\\mathbf{v} \\left(w\\right)^{\\top} \\theta_{j-1}^{u}\\right)\\right] + d_j^u \\cdot \\log \\left[1 - \\sigma \\left(\\mathbf{v} \\left(w\\right)^{\\top} \\theta_{j-1}^{u}\\right)\\right]\\right\\} \\\\ \u0026amp;= \\left(1 - d_j^u\\right) \\cdot \\left[1 - \\sigma \\left(\\mathbf{v} \\left(w\\right)^{\\top} \\theta_{j-1}^{u}\\right)\\right] \\mathbf{v} \\left(w\\right) - d_j^u \\sigma \\left(\\mathbf{v} \\left(w\\right)^{\\top} \\theta_{j-1}^{u}\\right) \\mathbf{v} \\left(w\\right) \\\\ \u0026amp;= \\left\\{\\left(1 - d_j^u\\right) \\cdot \\left[1 - \\sigma \\left(\\mathbf{v} \\left(w\\right)^{\\top} \\theta_{j-1}^{u}\\right)\\right] - d_j^u \\sigma \\left(\\mathbf{v} \\left(w\\right)^{\\top} \\theta_{j-1}^{u}\\right)\\right\\} \\mathbf{v} \\left(w\\right) \\\\ \u0026amp;= \\left[1 - d_j^u - \\sigma \\left(\\mathbf{v} \\left(w\\right)^{\\top} \\theta_{j-1}^{u}\\right)\\right] \\mathbf{v} \\left(w\\right) \\end{split} \\end{equation} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e则 \u003ccode\u003e$\\theta_{j-1}^u$\u003c/code\u003e 的更新方式为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\theta_{j-1}^u \\gets \\theta_{j-1}^u + \\eta \\left[1 - d_j^u - \\sigma \\left(\\mathbf{v} \\left(w\\right)^{\\top} \\theta_{j-1}^{u}\\right)\\right] \\mathbf{v} \\left(w\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e同理可得，\u003ccode\u003e$\\mathcal{L} \\left(w, u, j\\right)$\u003c/code\u003e 关于 \u003ccode\u003e$\\mathbf{v} \\left(w\\right)$\u003c/code\u003e 的梯度为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\dfrac{\\partial \\mathcal{L} \\left(w, u, j\\right)}{\\partial \\mathbf{v} \\left(w\\right)} = \\left[1 - d_j^u - \\sigma \\left(\\mathbf{v} \\left(w\\right)^{\\top} \\theta_{j-1}^{u}\\right)\\right] \\theta_{j-1}^u $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e则 \u003ccode\u003e$\\mathbf{v} \\left(w\\right)$\u003c/code\u003e 的更新方式为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\mathbf{v} \\left(w\\right) \\gets \\mathbf{v} \\left(w\\right) + \\eta \\sum_{u \\in Context \\left(w\\right)}{\\sum_{j=2}^{l^u}{\\dfrac{\\partial \\mathcal{L} \\left(w, u, j\\right)}{\\partial \\mathbf{v} \\left(w\\right)}}} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e基于 Hierarchical Softmax 的 Skip-gram 模型的随机梯度上升算法伪代码如下：\u003c/p\u003e\n\n\n\u003cdiv\u003e\u003cpre class=\"pseudocode\"\u003e\\begin{algorithm}\n\\caption{基于 Hierarchical Softmax 的 Skig-gram 随机梯度上升算法}\n\\begin{algorithmic}\n\\STATE $\\mathbf{e} = 0$\n\\FOR{$u \\in Context \\left(w\\right)$}\n    \\FOR{$j = 2, 3, ..., l^u$}\n        \\STATE $q = \\sigma \\left(\\mathbf{x}_w^{\\top} \\theta_{j-1}^u\\right)$\n        \\STATE $g = \\eta \\left(1 - d_j^u - q\\right)$\n        \\STATE $\\mathbf{e} \\gets \\mathbf{e} + g \\theta_{j-1}^u$\n        \\STATE $\\theta_{j-1}^u \\gets \\theta_{j-1}^u + g \\mathbf{v} \\left(w\\right)$\n    \\ENDFOR\n\\ENDFOR\n\\STATE $\\mathbf{v} \\left(w\\right) \\gets \\mathbf{v} \\left(w\\right) + \\mathbf{e}$\n\\end{algorithmic}\n\\end{algorithm}\n\u003c/pre\u003e\u003c/div\u003e\n\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e基于 Negative Sampling 的模型\u003c/strong\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e基于 Negative Sampling (NEG) 的模型相比于基于 Hierarchical Softmax 的模型不再使用复杂的 Huffman 树，而是使用简单的\u003cstrong\u003e随机负采样\u003c/strong\u003e，从而大幅的提高了模型的性能。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e基于 Negative Sampling 的 CBOW 模型如下\u003c/strong\u003e：\u003c/p\u003e\n\u003cp\u003e对于基于 Negative Sampling  CBOW 模型，已知词 \u003ccode\u003e$w$\u003c/code\u003e 的上下文 \u003ccode\u003e$Context \\left(w\\right)$\u003c/code\u003e，预测词 \u003ccode\u003e$w$\u003c/code\u003e，则词 \u003ccode\u003e$w$\u003c/code\u003e 即为一个\u003cstrong\u003e正样本\u003c/strong\u003e，其他词则为\u003cstrong\u003e负样本\u003c/strong\u003e。对于一个给定 \u003ccode\u003e$Context \\left(w\\right)$\u003c/code\u003e 的负样本集合 \u003ccode\u003e$NEG \\left(w\\right) \\neq \\varnothing$\u003c/code\u003e，词典中的任意词 \u003ccode\u003e$\\forall \\tilde{w} \\in \\mathcal{D}$\u003c/code\u003e，其样本的标签定义为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ L^w \\left(\\tilde{w}\\right) =  \\begin{cases} 1, \u0026amp; \\tilde{w} = w \\\\ 0, \u0026amp; \\tilde{w} \\neq w \\end{cases} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e则对于一个正样本 \u003ccode\u003e$\\left(Context, \\left(w\\right)\\right)$\u003c/code\u003e，我们希望最大化：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ g \\left(w\\right) = \\prod_{u \\in \\left\\{w\\right\\} \\cup NEG \\left(w\\right)}{p \\left(u | Context \\left(w\\right)\\right)} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e或表示为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ p \\left(u | Context \\left(w\\right)\\right) = \\left[\\sigma \\left(\\mathbf{x}_w^{\\top} \\theta^u\\right)\\right]^{L^w \\left(w\\right)} \\cdot \\left[1 - \\sigma \\left(\\mathbf{x}_w^{\\top} \\theta^u\\right)\\right]^{1 - L^w \\left(w\\right)} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e即增大正样本概率的同时减少负样本的概率。对于一个给定的语料库 \u003ccode\u003e$\\mathcal{C}$\u003c/code\u003e，对数似然函数为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{equation} \\begin{split} \\mathcal{L} \u0026amp;= \\sum_{w \\in \\mathcal{C}}{\\log g \\left(w\\right)} \\\\ \u0026amp;= \\sum_{w \\in \\mathcal{C}}{\\log \\prod_{u \\in \\left\\{w\\right\\} \\cup NEG \\left(w\\right)}{\\left\\{\\left[\\sigma \\left(\\mathbf{x}_w^{\\top} \\theta^u\\right)\\right]^{L^w \\left(u\\right)} \\cdot \\left[1 - \\sigma \\left(\\mathbf{x}_w^{\\top} \\theta^u\\right)\\right]^{1 - L^w \\left(u\\right)}\\right\\}}} \\\\ \u0026amp;= \\sum_{w \\in \\mathcal{C}}{\\sum_{u \\in \\left\\{w\\right\\} \\cup NEG \\left(w\\right)}{\\left\\{L^w \\left(u\\right) \\cdot \\log \\left[\\sigma \\left(\\mathbf{x}_w^{\\top} \\theta^u\\right)\\right] + \\left[1 - L^w \\left(u\\right)\\right] \\cdot \\log \\left[1 - \\sigma \\left(\\mathbf{x}_w^{\\top} \\theta^u\\right)\\right]\\right\\}}} \\end{split} \\end{equation} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e记上式花括号中的内容为 \u003ccode\u003e$\\mathcal{L} \\left(w, u\\right)$\u003c/code\u003e，则 \u003ccode\u003e$\\mathcal{L} \\left(w, u\\right)$\u003c/code\u003e 关于 \u003ccode\u003e$\\theta^u$\u003c/code\u003e 的梯度为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{equation} \\begin{split} \\dfrac{\\partial \\mathcal{L} \\left(w, u\\right)}{\\partial \\theta^u} \u0026amp;= \\dfrac{\\partial}{\\partial \\theta^u} \\left\\{L^w \\left(u\\right) \\cdot \\log \\left[\\sigma \\left(\\mathbf{x}_w^{\\top} \\theta^u\\right)\\right] + \\left[1 - L^w \\left(u\\right)\\right] \\cdot \\log \\left[1 - \\sigma \\left(\\mathbf{x}_w^{\\top} \\theta^u\\right)\\right]\\right\\} \\\\ \u0026amp;= L^w \\left(u\\right) \\left[1 - \\sigma \\left(\\mathbf{w}_w^{\\top} \\theta^u\\right)\\right] \\mathbf{x}_w - \\left[1 - L^w \\left(u\\right)\\right] \\sigma \\left(\\mathbf{x}_w^{\\top} \\theta^u\\right) \\mathbf{x}_w \\\\ \u0026amp;= \\left\\{L^w \\left(u\\right) \\left[1 - \\sigma \\left(\\mathbf{w}_w^{\\top} \\theta^u\\right)\\right] - \\left[1 - L^w \\left(u\\right)\\right] \\sigma \\left(\\mathbf{x}_w^{\\top} \\theta^u\\right)\\right\\} \\mathbf{x}_w \\\\ \u0026amp;= \\left[L^w \\left(u\\right) - \\sigma \\left(\\mathbf{w}_w^{\\top} \\theta^u\\right)\\right] \\mathbf{x}_w \\end{split} \\end{equation} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e则 \u003ccode\u003e$\\theta^u$\u003c/code\u003e 的更新方式为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\theta^u \\gets \\theta^u + \\eta \\left[L^w \\left(u\\right) - \\sigma \\left(\\mathbf{w}_w^{\\top} \\theta^u\\right)\\right] \\mathbf{x}_w $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e同理可得，\u003ccode\u003e$\\mathcal{L} \\left(w, u\\right)$\u003c/code\u003e 关于 \u003ccode\u003e$\\mathbf{x}_w$\u003c/code\u003e 的梯度为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\dfrac{\\partial \\mathcal{L} \\left(w, u\\right)}{\\partial \\mathbf{x}_w} = \\left[L^w \\left(u\\right) - \\sigma \\left(\\mathbf{w}_w^{\\top} \\theta^u\\right)\\right] \\theta^u $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e则 \u003ccode\u003e$\\mathbf{v} \\left(\\tilde{w}\\right), \\tilde{w} \\in Context \\left(w\\right)$\u003c/code\u003e 的更新方式为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\mathbf{v} \\left(\\tilde{w}\\right) \\gets \\mathbf{v} \\left(\\tilde{w}\\right) + \\eta \\sum_{u \\in \\left\\{w\\right\\} \\cup NEG \\left(w\\right)}{\\dfrac{\\partial \\mathcal{L} \\left(w, u\\right)}{\\partial \\mathbf{x}_w}}, \\tilde{w} \\in Context \\left(w\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e基于 Negative Sampling 的 CBOW 模型的随机梯度上升算法伪代码如下：\u003c/p\u003e\n\n\n\u003cdiv\u003e\u003cpre class=\"pseudocode\"\u003e\\begin{algorithm}\n\\caption{基于 Negative Sampling 的 CBOW 随机梯度上升算法}\n\\begin{algorithmic}\n\\STATE $\\mathbf{e} = 0$\n\\STATE $\\mathbf{x}_w = \\sum_{u \\in Context \\left(w\\right)}{\\mathbf{v} \\left(u\\right)}$\n\\FOR{$u \\in Context \\left\\{w\\right\\} \\cup NEG \\left(w\\right)$}\n    \\STATE $q = \\sigma \\left(\\mathbf{x}_w^{\\top} \\theta^u\\right)$\n    \\STATE $g = \\eta \\left(L^w \\left(u\\right) - q\\right)$\n    \\STATE $\\mathbf{e} \\gets \\mathbf{e} + g \\theta^u$\n    \\STATE $\\theta^u \\gets \\theta^u + g \\mathbf{x}_w$\n\\ENDFOR\n\\FOR{$u \\in Context \\left(w\\right)$}\n    \\STATE $\\mathbf{v} \\left(u\\right) \\gets \\mathbf{v} \\left(u\\right) + \\mathbf{e}$\n\\ENDFOR\n\\end{algorithmic}\n\\end{algorithm}\n\u003c/pre\u003e\u003c/div\u003e\n\n\u003cp\u003e\u003cstrong\u003e基于 Negative Sampling 的 Skip-gram 模型如下\u003c/strong\u003e：\u003c/p\u003e\n\u003cp\u003e对于 Skip-gram 模型，利用当前词 \u003ccode\u003e$w$\u003c/code\u003e 对上下文 \u003ccode\u003e$Context \\left(w\\right)$\u003c/code\u003e 中的词进行预测，则对于一个正样本 \u003ccode\u003e$\\left(Context, \\left(w\\right)\\right)$\u003c/code\u003e，我们希望最大化：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ g \\left(w\\right) = \\prod_{\\tilde{w} \\in Context \\left(w\\right)}{\\prod_{u \\in \\left\\{w\\right\\} \\cup NEG^{\\tilde{w}} \\left(w\\right)}{p \\left(u | \\tilde{w}\\right)}} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$NEG^{\\tilde{w}} \\left(w\\right)$\u003c/code\u003e 为处理词 \u003ccode\u003e$\\tilde{w}$\u003c/code\u003e 时生成的负样本集合，且：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ p \\left(u | \\tilde{w}\\right) =  \\begin{cases} \\sigma \\left(\\mathbf{v}\\left(\\tilde{w}\\right)^{\\top} \\theta^u\\right) \u0026amp; L^w \\left(u\\right) = 1 \\\\ 1 - \\sigma \\left(\\mathbf{v}\\left(\\tilde{w}\\right)^{\\top} \\theta^u\\right) \u0026amp; L^w \\left(u\\right) = 0 \\end{cases} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e或表示为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ p \\left(u | \\tilde{w}\\right) = \\left[\\sigma \\left(\\mathbf{v}\\left(\\tilde{w}\\right)^{\\top} \\theta^u\\right)\\right]^{L^w \\left(u\\right)} \\cdot \\left[1 - \\sigma \\left(\\mathbf{v}\\left(\\tilde{w}\\right)^{\\top} \\theta^u\\right)\\right]^{1 - L^w \\left(u\\right)} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e对于一个给定的语料库 \u003ccode\u003e$\\mathcal{C}$\u003c/code\u003e，对数似然函数为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{equation} \\begin{split} \\mathcal{L} \u0026amp;= \\sum_{w \\in \\mathcal{C}}{\\log g \\left(w\\right)} \\\\ \u0026amp;= \\sum_{w \\in \\mathcal{C}}{\\log \\prod_{\\tilde{w} \\in Context \\left(w\\right)}{\\prod_{u \\in \\left\\{w\\right\\} \\cup NEG^{\\tilde{w}} \\left(w\\right)}{\\left\\{\\left[\\sigma \\left(\\mathbf{v}\\left(\\tilde{w}\\right)^{\\top} \\theta^u\\right)\\right]^{L^w \\left(u\\right)} \\cdot \\left[1 - \\sigma \\left(\\mathbf{v}\\left(\\tilde{w}\\right)^{\\top} \\theta^u\\right)\\right]^{1 - L^w \\left(u\\right)}\\right\\}}}} \\\\ \u0026amp;= \\sum_{w \\in \\mathcal{C}}{\\sum_{\\tilde{w} \\in Context \\left(w\\right)}{\\sum_{u \\in \\left\\{w\\right\\} \\cup NEG^{\\tilde{w}} \\left(w\\right)}{\\left\\{L^w \\left(u\\right) \\cdot \\log \\left[\\sigma \\left(\\mathbf{v}\\left(\\tilde{w}\\right)^{\\top} \\theta^u\\right)\\right] + \\left[1 - L^w \\left(u\\right)\\right] \\cdot \\log \\left[1 - \\sigma \\left(\\mathbf{v}\\left(\\tilde{w}\\right)^{\\top} \\theta^u\\right)\\right]\\right\\}}}} \\end{split} \\end{equation} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e记上式花括号中的内容为 \u003ccode\u003e$\\mathcal{L} \\left(w, \\tilde{w}, u\\right)$\u003c/code\u003e，则 \u003ccode\u003e$\\mathcal{L} \\left(w, \\tilde{w}, u\\right)$\u003c/code\u003e 关于 \u003ccode\u003e$\\theta^u$\u003c/code\u003e 的梯度为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{equation} \\begin{split} \\dfrac{\\partial \\mathcal{L} \\left(w, \\tilde{w}, u\\right)}{\\partial \\theta^u} \u0026amp;= \\dfrac{\\partial}{\\partial \\theta^u} \\left\\{L^w \\left(u\\right) \\cdot \\log \\left[\\sigma \\left(\\mathbf{v}\\left(\\tilde{w}\\right)^{\\top} \\theta^u\\right)\\right] + \\left[1 - L^w \\left(u\\right)\\right] \\cdot \\log \\left[1 - \\sigma \\left(\\mathbf{v}\\left(\\tilde{w}\\right)^{\\top} \\theta^u\\right)\\right]\\right\\} \\\\ \u0026amp;= L^w \\left(u\\right) \\left[1 - \\sigma \\left(\\mathbf{v} \\left(\\tilde{w}\\right)^{\\top} \\theta^u\\right)\\right] \\mathbf{v} \\left(\\tilde{w}\\right) - \\left[1 - L^w \\left(u\\right)\\right] \\sigma \\left(\\mathbf{v} \\left(\\tilde{w}\\right)^{\\top} \\theta^u\\right) \\mathbf{v} \\left(\\tilde{w}\\right) \\\\ \u0026amp;= \\left\\{L^w \\left(u\\right) \\left[1 - \\sigma \\left(\\mathbf{v} \\left(\\tilde{w}\\right)^{\\top} \\theta^u\\right)\\right] - \\left[1 - L^w \\left(u\\right)\\right] \\sigma \\left(\\mathbf{v} \\left(\\tilde{w}\\right)^{\\top} \\theta^u\\right)\\right\\} \\mathbf{v} \\left(\\tilde{w}\\right) \\\\ \u0026amp;= \\left[L^w \\left(u\\right) - \\sigma \\left(\\mathbf{v} \\left(\\tilde{w}\\right)^{\\top} \\theta^u\\right)\\right] \\mathbf{v} \\left(\\tilde{w}\\right) \\end{split} \\end{equation} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e则 \u003ccode\u003e$\\theta^u$\u003c/code\u003e 的更新方式为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\theta^u \\gets \\theta^u + \\eta \\left[L^w \\left(u\\right) - \\sigma \\left(\\mathbf{v} \\left(\\tilde{w}\\right)^{\\top} \\theta^u\\right)\\right] \\mathbf{v} \\left(\\tilde{w}\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e同理可得，\u003ccode\u003e$\\mathcal{L} \\left(w, \\tilde{w}, u\\right)$\u003c/code\u003e 关于 \u003ccode\u003e$\\mathbf{v} \\left(\\tilde{w}\\right)$\u003c/code\u003e 的梯度为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\dfrac{\\partial \\mathcal{L} \\left(w, \\tilde{w}, u\\right)}{\\partial \\mathbf{v} \\left(\\tilde{w}\\right)} = \\left[L^w \\left(u\\right) - \\sigma \\left(\\mathbf{v} \\left(\\tilde{w}\\right)^{\\top} \\theta^u\\right)\\right] \\theta^u $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e则 \u003ccode\u003e$\\mathbf{v} \\left(\\tilde{w}\\right)$\u003c/code\u003e 的更新方式为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\mathbf{v} \\left(\\tilde{w}\\right) \\gets \\mathbf{v} \\left(\\tilde{w}\\right) + \\eta \\sum_{u \\in \\left\\{w\\right\\} \\cup NEG^{\\tilde{w}} \\left(w\\right)}{\\dfrac{\\partial \\mathcal{L} \\left(w, \\tilde{w}, u\\right)}{\\partial \\mathbf{v} \\left(\\tilde{w}\\right)}} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e基于 Negative Sampling 的 Skig-gram 模型的随机梯度上升算法伪代码如下：\u003c/p\u003e\n\n\n\u003cdiv\u003e\u003cpre class=\"pseudocode\"\u003e\\begin{algorithm}\n\\caption{基于 Negative Sampling 的 Skig-gram 随机梯度上升算法}\n\\begin{algorithmic}\n\\STATE $\\mathbf{e} = 0$\n\\FOR{$\\tilde{w} \\in Context \\left(w\\right)$}\n    \\FOR{$u \\in \\left\\{w\\right\\} \\cup NEG^{\\tilde{w}} \\left(w\\right)$}\n        \\STATE $q = \\sigma \\left(\\mathbf{v} \\left(\\tilde{w}\\right)^{\\top} \\theta^u\\right)$\n        \\STATE $g = \\eta \\left(L^w \\left(u\\right) - q\\right)$\n        \\STATE $\\mathbf{e} \\gets \\mathbf{e} + g \\theta^u$\n        \\STATE $\\theta^u \\gets \\theta^u + g \\mathbf{v} \\left(\\tilde{w}\\right)$\n    \\ENDFOR\n\\ENDFOR\n\\STATE $\\mathbf{v} \\left(\\tilde{w}\\right) \\gets \\mathbf{v} \\left(\\tilde{w}\\right) + \\mathbf{e}$\n\\end{algorithmic}\n\\end{algorithm}\n\u003c/pre\u003e\u003c/div\u003e\n\n\u003cp\u003e无论是基于 Negative Sampling 的 CBOW 模型还是 Skip-gram 模型，我们都需要对于给定的词 \u003ccode\u003e$w$\u003c/code\u003e 生成 \u003ccode\u003e$NEG \\left(w\\right)$\u003c/code\u003e，对于一个词典 \u003ccode\u003e$\\mathcal{D}$\u003c/code\u003e 和给定的语料 \u003ccode\u003e$\\mathcal{C}$\u003c/code\u003e，一个词被选择中的概率为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ p_{NEG} \\left(w\\right) = \\dfrac{\\#w}{\\sum_{u \\in \\mathcal{D}}{\\#u}} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中 \u003ccode\u003e$\\#w$\u003c/code\u003e 和 \u003ccode\u003e$\\#u$\u003c/code\u003e 表示词 \u003ccode\u003e$w$\u003c/code\u003e 和 \u003ccode\u003e$u$\u003c/code\u003e 在语料 \u003ccode\u003e$\\mathcal{C}$\u003c/code\u003e 中出现的频次。在 Word2Vec 的 C 代码中 \u003csup id=\"fnref:6\"\u003e\u003ca href=\"#fn:6\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e6\u003c/a\u003e\u003c/sup\u003e，并没有使用词的原始频次，而是对其做了 0.75 次幂，即：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ p_{NEG} \\left(w\\right) = \\dfrac{\\left(\\#w\\right)^{0.75}}{\\sum_{u \\in \\mathcal{D}}{\\left(\\#u\\right)^{0.75}}} $$\u003c/code\u003e\u003c/p\u003e\n\u003clink rel=\"stylesheet\" href=\"/css/admonition.css\"/\u003e\n\u003cdiv class=\"admonition admonition-note  kai\"\u003e\n  \u003cdiv class=\"admonition-content\"\u003e本节内容参考了 licstar 的 \u003ca href=\"http://licstar.net/archives/328\"\u003e博客\u003c/a\u003e 和 peghoty 的 \u003ca href=\"https://www.cnblogs.com/peghoty/p/3857839.html\"\u003e博客\u003c/a\u003e。\u003c/div\u003e\n\u003c/div\u003e\n\u003ch1 id=\"其他-embedding-方法\"\u003e其他 Embedding 方法\u003c/h1\u003e\n\u003ch2 id=\"glove\"\u003eGloVe\u003c/h2\u003e\n\u003cp\u003eGloVe (Global Vector 的简写) 是由 Pennington 等人 \u003csup id=\"fnref:7\"\u003e\u003ca href=\"#fn:7\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e7\u003c/a\u003e\u003c/sup\u003e 提出了一种词向量生成方法，该方法利用了语料的全局统计信息。\u003c/p\u003e\n\u003cp\u003e令 \u003ccode\u003e$X$\u003c/code\u003e 表示词与词之间的共现矩阵，\u003ccode\u003e$X_{ij}$\u003c/code\u003e 表示词 \u003ccode\u003e$j$\u003c/code\u003e 在词 \u003ccode\u003e$i$\u003c/code\u003e 为上下文的情况下出现的频次。则 \u003ccode\u003e$X_i = \\sum_{k}{X_{ik}}$\u003c/code\u003e 表示在词\u003ccode\u003e$i$\u003c/code\u003e 为上下文的情况任意词出现的总次数。令 \u003ccode\u003e$P_{ij} = P \\left(j | i\\right) = X_{ij} / X_i$\u003c/code\u003e 表示词 \u003ccode\u003e$j$\u003c/code\u003e 在词 \u003ccode\u003e$i$\u003c/code\u003e 出现前提下出现的条件概率。\u003c/p\u003e\n\u003cp\u003e例如，我们令 \u003ccode\u003e$i = ice, j = steam$\u003c/code\u003e，则这两个词之间的关系可以利用同其他词 \u003ccode\u003e$k$\u003c/code\u003e 共现概率的比率学习得出。则有：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e与词 \u003ccode\u003eice\u003c/code\u003e 相关，但与词 \u003ccode\u003esteam\u003c/code\u003e 不太相关，例如 \u003ccode\u003e$k = solid$\u003c/code\u003e，则比率 \u003ccode\u003e$P_{ik} / P_{jk}$\u003c/code\u003e 应该较大；类似的当词 \u003ccode\u003e$k$\u003c/code\u003e 与 \u003ccode\u003esteam\u003c/code\u003e 相关，但与词 \u003ccode\u003eice\u003c/code\u003e 不太相关，则比率 \u003ccode\u003e$P_{ik} / P_{jk}$\u003c/code\u003e 应该较小。\u003c/li\u003e\n\u003cli\u003e当与词 \u003ccode\u003eice\u003c/code\u003e 和词 \u003ccode\u003esteam\u003c/code\u003e 均相关或者均不太相关时，例如 \u003ccode\u003e$k = water$\u003c/code\u003e 或 \u003ccode\u003e$k = fashion$\u003c/code\u003e，则比率 \u003ccode\u003e$P_{ik} / P_{jk}$\u003c/code\u003e 应该和 1 接近。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e下表展示了在一个大量语料上的概率及其比率：\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e概率和比例\u003c/th\u003e\n\u003cth\u003e\u003ccode\u003e$k = solid$\u003c/code\u003e\u003c/th\u003e\n\u003cth\u003e\u003ccode\u003e$k = gas$\u003c/code\u003e\u003c/th\u003e\n\u003cth\u003e\u003ccode\u003e$k = water$\u003c/code\u003e\u003c/th\u003e\n\u003cth\u003e\u003ccode\u003e$k = fashion$\u003c/code\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003e$P \\left(k \\vert ice\\right)$\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003e$1.9 \\times 10^{-4}$\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003e$6.6 \\times 10^{-5}$\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003e$3.0 \\times 10^{-3}$\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003e$1.7 \\times 10^{-5}$\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003e$P \\left(k \\vert steam\\right)$\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003e$2.2 \\times 10^{-5}$\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003e$7.8 \\times 10^{-4}$\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003e$2.2 \\times 10^{-3}$\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003e$1.8 \\times 10^{-5}$\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003e$P \\left(k \\vert ice\\right) / P \\left(k \\vert steam\\right)$\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003e$8.9$\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003e$8.5 \\times 10^{-2}$\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003e$1.36$\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003e$0.96$\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e根据如上的假设，我们可以得到一个最基础的模型：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ F \\left(w_i, w_j, \\tilde{w}_k\\right) = \\dfrac{P_{ik}}{P_{jk}} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中 \u003ccode\u003e$w \\in \\mathbb{R}^d$\u003c/code\u003e 为词向量，\u003ccode\u003e$\\tilde{w}_k \\in \\mathbb{R}^d$\u003c/code\u003e 为单独的上下文词的词向量。假设向量空间是一个线性结构，因此 \u003ccode\u003e$F$\u003c/code\u003e 仅依赖于两个向量之间的差异，则模型可以改写为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ F \\left(w_i - w_j, \\tilde{w}_k\\right) = \\dfrac{P_{ik}}{P_{jk}} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e上式中右面是一个标量，如果左面的参数利用一个复杂的模型进行计算，例如神经网络，则会破坏我们希望保留的线性结构。因此，我们对参数采用点积运算，即：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ F \\left(\\left(w_i - w_j\\right)^{\\top} \\tilde{w}_k\\right) = \\dfrac{P_{ik}}{P_{jk}} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e在词之间的共现矩阵中，一个词和其上下文中的一个词之间应该是可以互换角色的。首先我们要保证 \u003ccode\u003e$F$\u003c/code\u003e 在 \u003ccode\u003e$\\left(\\mathbb{R}, +\\right)$\u003c/code\u003e 和 \u003ccode\u003e$\\left(\\mathbb{R}_{\u0026gt;0}, \\times\\right)$\u003c/code\u003e 上是同态的 (homomorphism)，例如：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ F \\left(\\left(w_i - w_j\\right)^{\\top} \\tilde{w}_k\\right) = \\dfrac{F \\left(w_i^{\\top} \\tilde{w}_k\\right)}{F \\left(w_j^{\\top} \\tilde{w}_k\\right)} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中 \u003ccode\u003e$F \\left(w_i^{\\top} \\tilde{w}_k\\right) = P_{ik} = \\dfrac{X_{ik}}{X_i}$\u003c/code\u003e，则上式的一个解为 \u003ccode\u003e$F = \\exp$\u003c/code\u003e，或：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ w_i^{\\top} \\tilde{w}_k = \\log \\left(P_{ik}\\right) = \\log \\left(X_{ik}\\right) - \\log \\left(X_i\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中 \u003ccode\u003e$\\log \\left(X_i\\right)$\u003c/code\u003e 与 \u003ccode\u003e$k$\u003c/code\u003e 无关记为 \u003ccode\u003e$b_i$\u003c/code\u003e，同时为了对称性添加 \u003ccode\u003e$\\tilde{b}_k$\u003c/code\u003e，则上式改写为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ w_i^{\\top} \\tilde{w}_k + b_i + \\tilde{b}_k = \\log \\left(X_{ik}\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e上式中，左侧为词向量的相关运算，右侧为共现矩阵的常量信息，则给出模型的损失函数如下：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ J = \\sum_{i,j=1}^{V}{f \\left(X_{ij}\\right) \\left(w_i^{\\top} \\tilde{w}_k + b_i + \\tilde{b}_k - \\log X_{ij}\\right)^2} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$V$\u003c/code\u003e 为词典中词的个数，\u003ccode\u003e$f$\u003c/code\u003e 为一个权重函数，其应具有如下特点：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003e$f \\left(0\\right) = 0$\u003c/code\u003e。如果 \u003ccode\u003e$f$\u003c/code\u003e 为一个连续函数，则当 \u003ccode\u003e$x \\to 0$\u003c/code\u003e 时 \u003ccode\u003e$\\lim_{x \\to 0}{f \\left(x\\right) \\log^2 x}$\u003c/code\u003e 应足够快地趋近于无穷。\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e$f \\left(x\\right)$\u003c/code\u003e 应为非减函数，以确保稀少的共现不会权重过大。\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e$f \\left(x\\right)$\u003c/code\u003e 对于较大的 \u003ccode\u003e$x$\u003c/code\u003e 应该相对较小，以确保过大的共现不会权重过大。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e文中给出了一个符合要求的函数如下：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ f \\left(x\\right) =  \\begin{cases} \\left(x / x_{\\max}\\right)^{\\alpha} \u0026amp; \\text{if} \\  x \u0026lt; x_{\\max} \\\\ 1 \u0026amp; \\text{otherwise} \\end{cases} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中两个超参数的值建议为 \u003ccode\u003e$x_{\\max} = 100, \\alpha = 0.75$\u003c/code\u003e。\u003c/p\u003e\n\u003ch2 id=\"fasttext\"\u003efastText\u003c/h2\u003e\n\u003cp\u003efastText 是由 Bojanowski 和 Grave 等人 \u003csup id=\"fnref:8\"\u003e\u003ca href=\"#fn:8\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e8\u003c/a\u003e\u003c/sup\u003e 提出的一种词向量表示方法。原始的 Skip-gram 模型忽略了词语内部的结构信息，fastText 利用 N-gram 方法将其考虑在内。\u003c/p\u003e\n\u003cp\u003e对于一个词 \u003ccode\u003e$w$\u003c/code\u003e，利用一系列的 N-gram 进行表示，同时在词的前后添加 \u003ccode\u003e\u0026lt;\u003c/code\u003e 和 \u003ccode\u003e\u0026gt;\u003c/code\u003e 边界符号以同其他文本序列进行区分。同时还将词语本身也包含在这个 N-gram 集合中，从而学习到词语的向量表示。例如，对于词 \u003ccode\u003e$where$\u003c/code\u003e 和 \u003ccode\u003e$n = 3$\u003c/code\u003e，则 N-gram 集合为：\u003ccode\u003e\u0026lt;wh, whe, her, ere, re\u0026gt;\u003c/code\u003e，同时包含词本身 \u003ccode\u003e\u0026lt;where\u0026gt;\u003c/code\u003e。需要注意的是，序列 \u003ccode\u003e\u0026lt;her\u0026gt;\u003c/code\u003e 与词 \u003ccode\u003e$where$\u003c/code\u003e 中的 tri-gram \u003ccode\u003eher\u003c/code\u003e 是两个不同的概念。模型提取所有 \u003ccode\u003e$3 \\leq n \\leq 6$\u003c/code\u003e 的 N-gram 序列。\u003c/p\u003e\n\u003cp\u003e假设 N-gram 词典的大小为 \u003ccode\u003e$G$\u003c/code\u003e，对于一个词 \u003ccode\u003e$w$\u003c/code\u003e，\u003ccode\u003e$\\mathcal{G}_w \\subset \\left\\{1, ..., G\\right\\}$\u003c/code\u003e 表示词中出现的 N-gram 的集合。针对任意一个 N-gram \u003ccode\u003e$g$\u003c/code\u003e，用向量 \u003ccode\u003e$\\mathbf{z}_g$\u003c/code\u003e 表示，则我们利用一个词的所有 N-gram 的向量的加和表示该词。可以得到该模型的评分函数为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ s \\left(w, c\\right) = \\sum_{g \\in \\mathcal{G}_w}{\\mathbf{z}_g^{\\top} \\mathbf{v}_c} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e模型在学习不同词向量时可以共享权重 (不同词的可能包含相同的 N-gram)，使得在学习低频词时也可得到可靠的向量表示。\u003c/p\u003e\n\u003ch2 id=\"wordrank\"\u003eWordRank\u003c/h2\u003e\n\u003cp\u003eWordRank 是由 Ji 等人 \u003csup id=\"fnref:9\"\u003e\u003ca href=\"#fn:9\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e9\u003c/a\u003e\u003c/sup\u003e 提出的一种词向量表示方法，其将词向量学习问题转换成一个排序问题。\u003c/p\u003e\n\u003cp\u003e我们令 \u003ccode\u003e$\\mathbf{u}_w$\u003c/code\u003e 表示当前词 \u003ccode\u003e$w$\u003c/code\u003e 的 \u003ccode\u003e$k$\u003c/code\u003e 维词向量，\u003ccode\u003e$\\mathbf{v}_c$\u003c/code\u003e 表示当前词上下文 \u003ccode\u003e$c$\u003c/code\u003e 的词向量。通过两者的内积 \u003ccode\u003e$\\langle \\mathbf{u}_w, \\mathbf{v}_c \\rangle$\u003c/code\u003e 来捕获词 \u003ccode\u003e$w$\u003c/code\u003e 和上下文 \u003ccode\u003e$c$\u003c/code\u003e 之间的关系，两者越相关则该内积越大。对于一个给定的词 \u003ccode\u003e$w$\u003c/code\u003e，利用上下文集合 \u003ccode\u003e$\\mathcal{C}$\u003c/code\u003e 同词的内积分数进行排序，对于一个给定的上下文 \u003ccode\u003e$c$\u003c/code\u003e，排序为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{equation} \\begin{split} \\text{rank} \\left(w, c\\right) \u0026amp;= \\sum_{c\u0026#39; \\in \\mathcal{C} \\setminus \\left\\{c\\right\\}}{I \\left(\\langle \\mathbf{u}_w, \\mathbf{v}_c \\rangle - \\langle \\mathbf{u}_w, \\mathbf{v}_{c\u0026#39;} \\rangle \\leq 0\\right)} \\\\ \u0026amp;= \\sum_{c\u0026#39; \\in \\mathcal{C} \\setminus \\left\\{c\\right\\}}{I \\left(\\langle \\mathbf{u}_w, \\mathbf{v}_c - \\mathbf{v}_{c\u0026#39;}  \\rangle \\leq 0\\right)} \\end{split} \\end{equation} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$I \\left(x \\leq 0\\right)$\u003c/code\u003e 为一个 0-1 损失函数，当 \u003ccode\u003e$x \\leq 0$\u003c/code\u003e 时为 1 其他情况为 0。由于 \u003ccode\u003e$I \\left(x \\leq 0\\right)$\u003c/code\u003e 为一个非连续函数，因此我们可以将其替换为一个凸上限函数 \u003ccode\u003e$\\ell \\left(\\cdot\\right)$\u003c/code\u003e，其可以为任意的二分类损失函数，构建排序的凸上限如下：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\text{rank} \\left(w, c\\right) \\leq \\overline{\\text{rank}} \\left(w, c\\right) = \\sum_{c\u0026#39; \\in \\mathcal{C} \\setminus \\left\\{c\\right\\}}{\\ell \\left(\\langle \\mathbf{u}_w, \\mathbf{v}_c - \\mathbf{v}_{c\u0026#39;} \\rangle\\right)} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e我们期望排序模型将更相关的上下文排在列表的顶部，基于此构建损失函数如下：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ J \\left(\\mathbf{U}, \\mathbf{V}\\right) := \\sum_{w \\in \\mathcal{W}}{\\sum_{c \\in \\Omega_w}{r_{w, c} \\cdot \\rho \\left(\\dfrac{\\overline{\\text{rank}} \\left(w, c\\right) + \\beta}{\\alpha}\\right)}} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$\\mathcal{W}$\u003c/code\u003e 表示词典，\u003ccode\u003e$\\mathbf{U} := \\left\\{\\mathbf{u}_w\\right\\}_{w \\in \\mathcal{W}}$\u003c/code\u003e 和 \u003ccode\u003e$\\mathbf{V} := \\left\\{\\mathbf{c}_w\\right\\}_{c \\in \\mathcal{C}}$\u003c/code\u003e 分别表示词及其上下文词向量的参数，\u003ccode\u003e$\\Omega_w$\u003c/code\u003e 表示与词 \u003ccode\u003e$w$\u003c/code\u003e 共现的上下文的集合，\u003ccode\u003e$r_{w, c}$\u003c/code\u003e 为衡量 \u003ccode\u003e$w$\u003c/code\u003e 和 \u003ccode\u003e$c$\u003c/code\u003e 之间关系的权重，\u003ccode\u003e$\\rho \\left(\\cdot\\right)$\u003c/code\u003e 为用于衡量排序好坏的单调递增的损失函数，\u003ccode\u003e$\\alpha \\geq 0, \\beta \\geq 0$\u003c/code\u003e 为超参数。可选的有：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ r_{w, c} = \\begin{cases} \\left(X_{w, c} / x_{\\max}\\right)^{\\epsilon} \u0026amp; \\text{if} \\ X_{w, c} \u0026lt; x_{\\max} \\\\ 1 \u0026amp; \\text{otherwise} \\end{cases} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中 \u003ccode\u003e$x_{\\max} = 100, \\epsilon = 0.75$\u003c/code\u003e。根据 \u003ccode\u003e$\\rho \\left(\\cdot\\right)$\u003c/code\u003e 的要求，损失函数在排序的顶部 (rank 值小) 的地方更加敏感，同时对于 rank 值较大的地方不敏感。这可以使得模型变得更加稳健 (避免语法错误和语言的非常规使用造成干扰)，因此可选的有：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{equation} \\begin{split} \\rho \\left(x\\right) \u0026amp;:= \\log_2 \\left(1 + x\\right) \\\\ \\rho \\left(x\\right) \u0026amp;:= 1 - \\dfrac{1}{\\log_2 \\left(2 + x\\right)} \\\\ \\rho \\left(x\\right) \u0026amp;:= \\dfrac{x^{1 - t} - 1}{1 - t}, t \\neq 1 \\end{split} \\end{equation} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e损失函数可以等价的定义为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ J \\left(\\mathbf{U}, \\mathbf{V}\\right) := \\sum_{\\left(w, c\\right) \\in \\Omega}{r_{w, c} \\cdot \\rho \\left(\\dfrac{\\overline{\\text{rank}} \\left(w, c\\right) + \\beta}{\\alpha}\\right)} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e在训练过程中，外层的求和符号容易利用 SDG 算法解决，但对于内层的求和符号除非 \u003ccode\u003e$\\rho \\left(\\cdot\\right)$\u003c/code\u003e 是一个线性函数，否则难以求解。然而，\u003ccode\u003e$\\rho \\left(\\cdot\\right)$\u003c/code\u003e 函数的性质要求其不能是一个线性函数，但我们可以利用其凹函数的特性对其进行一阶泰勒分解，有：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\rho \\left(x\\right) \\leq \\rho \\left(\\xi^{-1}\\right) + \\rho\u0026#39; \\left(\\xi^{-1}\\right) \\cdot \\left(x - \\xi^{-1}\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e对于任意 \u003ccode\u003e$x$\u003c/code\u003e 和 \u003ccode\u003e$\\xi \\neq 0$\u003c/code\u003e 均成立，同时当且仅当 \u003ccode\u003e$\\xi = x^{-1}$\u003c/code\u003e 时等号成立。因此，令 \u003ccode\u003e$\\Xi := \\left\\{\\xi_{w, c}\\right\\}_{\\left(w, c\\right) \\in \\Sigma}$\u003c/code\u003e，则可以得到 \u003ccode\u003e$J \\left(\\mathbf{U}, \\mathbf{V}\\right)$\u003c/code\u003e 的一个上界：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{equation} \\begin{split} \\overline{J} \\left(\\mathbf{U}, \\mathbf{V}, \\Xi\\right) \u0026amp;:= \\sum_{\\left(w, c\\right)  \\in \\Omega}{r_{w, c} \\cdot \\left\\{\\rho \\left(\\xi_{wc}^{-1}\\right) + \\rho\u0026#39; \\left(\\xi_{wc}^{-1}\\right) \\cdot \\left(\\alpha^{-1} \\beta + \\alpha^{-1} \\sum_{c\u0026#39; \\in \\mathcal{C} \\setminus \\left\\{c\\right\\}}{\\ell \\left(\\langle \\mathbf{u}_w, \\mathbf{v}_c - \\mathbf{v}_{c\u0026#39;} \\rangle\\right) - \\xi_{w, c}^{-1}}\\right)\\right\\}} \\\\ \u0026amp;= \\sum_{\\left(w, c, c\u0026#39;\\right)}{r_{w, c} \\cdot \\left(\\dfrac{\\rho \\left(\\xi_{w, c}^{-1}\\right) + \\rho\u0026#39; \\left(\\xi_{w, c}^{-1}\\right) \\cdot \\left(\\alpha^{-1} \\beta - \\xi_{w, c}^{-1}\\right)}{\\lvert \\mathcal{C} \\rvert - 1} + \\dfrac{1}{\\alpha} \\rho\u0026#39; \\left(\\xi_{w, c}^{-1}\\right) \\cdot \\ell \\left(\\langle \\mathbf{u}_w, \\mathbf{v}_c - \\mathbf{v}_{c\u0026#39;} \\rangle\\right)\\right)} \\end{split} \\end{equation} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$\\left(w, c, c\u0026#39;\\right) \\in \\Omega \\times \\left(\\mathcal{C} \\setminus \\left\\{c\\right\\}\\right)$\u003c/code\u003e，至此我们可以通过均匀采样 \u003ccode\u003e$\\left(w, c\\right) \\in \\Sigma$\u003c/code\u003e 和 \u003ccode\u003e$c\u0026#39; \\in \\mathcal{C} \\setminus \\left\\{c\\right\\}$\u003c/code\u003e 解决训练问题。\u003c/p\u003e\n\u003cp\u003e整个 WordRank 算法的伪代码如下：\u003c/p\u003e\n\n\n\u003cdiv\u003e\u003cpre class=\"pseudocode\"\u003e\\begin{algorithm}\n\\caption{WordRank 算法}\n\\begin{algorithmic}\n\\STATE $\\eta$ 为学习率\n\\WHILE{$\\mathbf{U}$，$\\mathbf{V}$ 和 $\\Xi$ 未收敛}\n    \\STATE \\COMMENT{阶段1：更新 $\\mathbf{U}$ 和 $\\mathbf{V}$}\n    \\WHILE{$\\mathbf{U}$ 和 $\\mathbf{V}$ 未收敛}\n        \\STATE 从 $\\Omega$ 中均匀采样 $\\left(w, c\\right)$\n        \\STATE 从 $\\mathcal{C} \\setminus \\left\\{c\\right\\}$ 中均匀采样 $c\u0026#39;$\n        \\STATE \\COMMENT{同时更新如下 3 个参数}\n        \\STATE $\\mathbf{u}_w \\gets \\mathbf{u}_w - \\eta \\cdot r_{w, c} \\cdot \\rho\u0026#39; \\left(\\xi_{w, c}^{-1}\\right) \\cdot \\ell\u0026#39; \\left(\\langle \\mathbf{u}_w, \\mathbf{v}_c - \\mathbf{v}_{c\u0026#39;} \\rangle\\right) \\cdot \\left(\\mathbf{v}_c - \\mathbf{v}_{c\u0026#39;}\\right)$\n        \\STATE $\\mathbf{v}_c \\gets \\mathbf{v}_c - \\eta \\cdot r_{w, c} \\cdot \\rho\u0026#39; \\left(\\xi_{w, c}^{-1}\\right) \\cdot \\ell\u0026#39; \\left(\\langle \\mathbf{u}_w, \\mathbf{v}_c - \\mathbf{v}_{c\u0026#39;} \\rangle\\right) \\cdot \\mathbf{u}_w$\n        \\STATE $\\mathbf{v}_{c\u0026#39;} \\gets \\mathbf{v}_{c\u0026#39;} - \\eta \\cdot r_{w, c} \\cdot \\rho\u0026#39; \\left(\\xi_{w, c}^{-1}\\right) \\cdot \\ell\u0026#39; \\left(\\langle \\mathbf{u}_w, \\mathbf{v}_c - \\mathbf{v}_{c\u0026#39;} \\rangle\\right) \\cdot \\mathbf{u}_w$\n    \\ENDWHILE\n    \\STATE \\COMMENT{阶段2：更新 $\\Xi$}\n    \\FOR{$w \\in \\mathcal{W}$}\n        \\FOR{$c \\in \\mathcal{C}$}\n            \\STATE $\\xi_{w, c} = \\alpha / \\left(\\sum_{c\u0026#39; \\in \\mathcal{C} \\setminus \\left\\{c\\right\\}}{\\ell \\left(\\langle \\mathbf{u}_w, \\mathbf{v}_c - \\mathbf{v}_{c\u0026#39;} \\rangle\\right) + \\beta}\\right)$\n        \\ENDFOR\n    \\ENDFOR\n\\ENDWHILE\n\\end{algorithmic}\n\\end{algorithm}\n\u003c/pre\u003e\u003c/div\u003e\n\n\u003ch2 id=\"cw2vec\"\u003ecw2vec\u003c/h2\u003e\n\u003cp\u003ecw2vec 是由 Cao 等人 \u003csup id=\"fnref:10\"\u003e\u003ca href=\"#fn:10\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e10\u003c/a\u003e\u003c/sup\u003e 提出的一种基于汉字笔画 N-gram 的中文词向量表示方法。该方法根据汉字作为象形文字具有笔画信息的特点，提出了笔画 N-gram 的概念。针对一个词的笔画 N-gram，其生成过程如下图所示：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-10-01-word-embeddings/cw2vec-stroke-n-gram-generation.png\" alt=\"cw2vec-Stroke-N-gram-Generation\"/\u003e\u003c/p\u003e\n\u003cp\u003e共包含 4 个步骤：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e将一个词拆解成单个的汉字，例如：“大人” 拆解为 “大” 和 “人”。\u003c/li\u003e\n\u003cli\u003e将每个汉字拆解成笔画，例如：“大” 和 “人” 拆解为 “一，丿，乀，丿，乀”。\u003c/li\u003e\n\u003cli\u003e将每个笔画映射到对应的编码序列，例如： “一，丿，乀，丿，乀” 映射为 13434。\u003c/li\u003e\n\u003cli\u003e利用编码序列生成笔画 N-gram，例如：134，343，434；1343，3434；13434。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e模型中定义一个词 \u003ccode\u003e$w$\u003c/code\u003e 及其上下文 \u003ccode\u003e$c$\u003c/code\u003e 的相似度如下：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ sim \\left(w, c\\right) = \\sum_{q \\in S\\left(w\\right)}{\\vec{q} \\cdot \\vec{c}} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$S$\u003c/code\u003e 为由笔画 N-gram 构成的词典，\u003ccode\u003e$S \\left(w\\right)$\u003c/code\u003e 为词 \u003ccode\u003e$w$\u003c/code\u003e 对应的笔画 N-gram 集合，\u003ccode\u003e$q$\u003c/code\u003e 为该集合中的一个笔画 N-gram，\u003ccode\u003e$\\vec{q}$\u003c/code\u003e 为 \u003ccode\u003e$q$\u003c/code\u003e 对应的向量。\u003c/p\u003e\n\u003cp\u003e该模型的损失函数为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\mathcal{L} = \\sum_{w \\in D}{\\sum_{c \\in T \\left(w\\right)}{\\log \\sigma \\left(sim \\left(w, c\\right)\\right) + \\lambda \\mathbb{E}_{c\u0026#39; \\sim P} \\left[\\log \\sigma \\left(- sim \\left(w, c\u0026#39;\\right)\\right)\\right]}} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$D$\u003c/code\u003e 为语料中的全部词语，\u003ccode\u003e$T \\left(w\\right)$\u003c/code\u003e 为给定的词 \u003ccode\u003e$w$\u003c/code\u003e 和窗口内的所有上次文词，\u003ccode\u003e$\\sigma \\left(x\\right) = \\left(1 + \\exp \\left(-x\\right)\\right)^{-1}$\u003c/code\u003e，\u003ccode\u003e$\\lambda$\u003c/code\u003e 为负采样的个数，\u003ccode\u003e$\\mathbb{E}_{c\u0026#39; \\sim P} \\left[\\cdot\\right]$\u003c/code\u003e 表示负样本 \u003ccode\u003e$c\u0026#39;$\u003c/code\u003e 按照 \u003ccode\u003e$D$\u003c/code\u003e 中词的分布 \u003ccode\u003e$P$\u003c/code\u003e 进行采样，该分布可以为词的一元模型的分布 \u003ccode\u003e$U$\u003c/code\u003e，同时为了避免数据的稀疏性问题，类似 Word2Vec 中的做法采用 \u003ccode\u003e$U^{0.75}$\u003c/code\u003e。\u003c/p\u003e\n\u003cdiv class=\"footnotes\" role=\"doc-endnotes\"\u003e\n\u003chr/\u003e\n\u003col\u003e\n\u003cli id=\"fn:1\"\u003e\n\u003cp\u003eHinton, G. E. (1986, August). Learning distributed representations of concepts. In \u003cem\u003eProceedings of the eighth annual conference of the cognitive science society\u003c/em\u003e (Vol. 1, p. 12). \u003ca href=\"#fnref:1\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:2\"\u003e\n\u003cp\u003eBengio, Y., Courville, A., \u0026amp; Vincent, P. (2013). Representation learning: A review and new perspectives. \u003cem\u003eIEEE transactions on pattern analysis and machine intelligence\u003c/em\u003e, 35(8), 1798-1828. \u003ca href=\"#fnref:2\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:3\"\u003e\n\u003cp\u003eBengio, Y., Ducharme, R., Vincent, P., \u0026amp; Jauvin, C. (2003). A Neural Probabilistic Language Model. \u003cem\u003eJournal of Machine Learning Research\u003c/em\u003e, 3(Feb), 1137–1155. \u003ca href=\"#fnref:3\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:4\"\u003e\n\u003cp\u003eMikolov, T., Chen, K., Corrado, G., \u0026amp; Dean, J. (2013). Efficient Estimation of Word Representations in Vector Space. \u003cem\u003earXiv preprint arXiv:1301.3781\u003c/em\u003e \u003ca href=\"#fnref:4\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:5\"\u003e\n\u003cp\u003e\u003ca href=\"https://zh.wikipedia.org/zh/%E9%9C%8D%E5%A4%AB%E6%9B%BC%E7%BC%96%E7%A0%81\"\u003ehttps://zh.wikipedia.org/zh/霍夫曼编码\u003c/a\u003e \u003ca href=\"#fnref:5\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:6\"\u003e\n\u003cp\u003e\u003ca href=\"https://code.google.com/archive/p/word2vec/\"\u003ehttps://code.google.com/archive/p/word2vec/\u003c/a\u003e \u003ca href=\"#fnref:6\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:7\"\u003e\n\u003cp\u003ePennington, J., Socher, R., \u0026amp; Manning, C. (2014). Glove: Global Vectors for Word Representation. In \u003cem\u003eProceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)\u003c/em\u003e (pp. 1532–1543). \u003ca href=\"#fnref:7\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:8\"\u003e\n\u003cp\u003eBojanowski, P., Grave, E., Joulin, A., \u0026amp; Mikolov, T. (2017). Enriching Word Vectors with Subword Information. \u003cem\u003eTransactions of the Association for Computational Linguistics\u003c/em\u003e, 5, 135–146. \u003ca href=\"#fnref:8\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:9\"\u003e\n\u003cp\u003eJi, S., Yun, H., Yanardag, P., Matsushima, S., \u0026amp; Vishwanathan, S. V. N. (2016). WordRank: Learning Word Embeddings via Robust Ranking. In \u003cem\u003eProceedings of the 2016 Conference on Empirical Methods in Natural Language Processing\u003c/em\u003e (pp. 658–668). \u003ca href=\"#fnref:9\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:10\"\u003e\n\u003cp\u003eCao, S., Lu, W., Zhou, J., \u0026amp; Li, X. (2018). cw2vec: Learning Chinese Word Embeddings with Stroke n-gram Information. In \u003cem\u003eThirty-Second AAAI Conference on Artificial Intelligence\u003c/em\u003e. \u003ca href=\"#fnref:10\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/div\u003e\n\n\n\n\n\n\u003cdiv class=\"donate\"\u003e\n  \u003cdiv class=\"donate-header\"\u003e\u003c/div\u003e\n  \u003cdiv class=\"donate-slug\" id=\"donate-slug\"\u003eword-embeddings\u003c/div\u003e\n  \u003cbutton class=\"donate-button\"\u003e赞 赏\u003c/button\u003e\n  \u003cdiv class=\"donate-footer\"\u003e「真诚赞赏，手留余香」\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"donate-modal-wrapper\"\u003e\n  \u003cdiv class=\"donate-modal\"\u003e\n    \u003cdiv class=\"donate-box\"\u003e\n      \u003cdiv class=\"donate-box-content\"\u003e\n        \u003cdiv class=\"donate-box-content-inner\"\u003e\n          \u003cdiv class=\"donate-box-header\"\u003e「真诚赞赏，手留余香」\u003c/div\u003e\n          \u003cdiv class=\"donate-box-body\"\u003e\n            \u003cdiv class=\"donate-box-money\"\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-2\" data-v=\"2\" data-unchecked=\"￥ 2\" data-checked=\"2 元\"\u003e￥ 2\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-5\" data-v=\"5\" data-unchecked=\"￥ 5\" data-checked=\"5 元\"\u003e￥ 5\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-10\" data-v=\"10\" data-unchecked=\"￥ 10\" data-checked=\"10 元\"\u003e￥ 10\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-50\" data-v=\"50\" data-unchecked=\"￥ 50\" data-checked=\"50 元\"\u003e￥ 50\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-100\" data-v=\"100\" data-unchecked=\"￥ 100\" data-checked=\"100 元\"\u003e￥ 100\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-custom\" data-v=\"custom\" data-unchecked=\"任意金额\" data-checked=\"任意金额\"\u003e任意金额\u003c/button\u003e\n            \u003c/div\u003e\n            \u003cdiv class=\"donate-box-pay\"\u003e\n              \u003cimg class=\"donate-box-pay-qrcode\" id=\"donate-box-pay-qrcode\" src=\"\"/\u003e\n            \u003c/div\u003e\n          \u003c/div\u003e\n          \u003cdiv class=\"donate-box-footer\"\u003e\n            \u003cdiv class=\"donate-box-pay-method donate-box-pay-method-checked\" data-v=\"wechat-pay\"\u003e\n              \u003cimg class=\"donate-box-pay-method-image\" id=\"donate-box-pay-method-image-wechat-pay\" src=\"\"/\u003e\n            \u003c/div\u003e\n            \u003cdiv class=\"donate-box-pay-method\" data-v=\"alipay\"\u003e\n              \u003cimg class=\"donate-box-pay-method-image\" id=\"donate-box-pay-method-image-alipay\" src=\"\"/\u003e\n            \u003c/div\u003e\n          \u003c/div\u003e\n        \u003c/div\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n    \u003cbutton type=\"button\" class=\"donate-box-close-button\"\u003e\n      \u003csvg class=\"donate-box-close-button-icon\" fill=\"#fff\" viewBox=\"0 0 24 24\" width=\"24\" height=\"24\"\u003e\u003cpath d=\"M13.486 12l5.208-5.207a1.048 1.048 0 0 0-.006-1.483 1.046 1.046 0 0 0-1.482-.005L12 10.514 6.793 5.305a1.048 1.048 0 0 0-1.483.005 1.046 1.046 0 0 0-.005 1.483L10.514 12l-5.208 5.207a1.048 1.048 0 0 0 .006 1.483 1.046 1.046 0 0 0 1.482.005L12 13.486l5.207 5.208a1.048 1.048 0 0 0 1.483-.006 1.046 1.046 0 0 0 .005-1.482L13.486 12z\" fill-rule=\"evenodd\"\u003e\u003c/path\u003e\u003c/svg\u003e\n    \u003c/button\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\u003cscript type=\"text/javascript\" src=\"/js/donate.js\"\u003e\u003c/script\u003e\n\n\n  \u003cfooter\u003e\n  \n\u003cnav class=\"post-nav\"\u003e\n  \u003cspan class=\"nav-prev\"\u003e← \u003ca href=\"/cn/2018/09/rnn/\"\u003e循环神经网络 (Recurrent Neural Network, RNN)\u003c/a\u003e\u003c/span\u003e\n  \u003cspan class=\"nav-next\"\u003e\u003ca href=\"/cn/2018/10/seq2seq-and-attention-machanism/\"\u003e序列到序列 (Seq2Seq) 和注意力机制 (Attention Machanism)\u003c/a\u003e →\u003c/span\u003e\n\u003c/nav\u003e\n\n\n\n\n\u003cins class=\"adsbygoogle\" style=\"display:block; text-align:center;\" data-ad-layout=\"in-article\" data-ad-format=\"fluid\" data-ad-client=\"ca-pub-2608165017777396\" data-ad-slot=\"8302038603\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n  (adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/js-cookie@3.0.5/dist/js.cookie.min.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"/js/toggle-theme.js\"\u003e\u003c/script\u003e\n\n\n\u003cscript src=\"/js/no-highlight.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"/js/math-code.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"/js/heading-anchor.js\"\u003e\u003c/script\u003e\n\n\n\n\u003csection class=\"comments\"\u003e\n\u003cscript src=\"https://giscus.app/client.js\" data-repo=\"leovan/leovan.me\" data-repo-id=\"MDEwOlJlcG9zaXRvcnkxMTMxOTY0Mjc=\" data-category=\"Comments\" data-category-id=\"DIC_kwDOBr89i84CT-R7\" data-mapping=\"pathname\" data-strict=\"1\" data-reactions-enabled=\"1\" data-emit-metadata=\"0\" data-input-position=\"top\" data-theme=\"preferred_color_scheme\" data-lang=\"zh-CN\" data-loading=\"lazy\" crossorigin=\"anonymous\" defer=\"\"\u003e\n\u003c/script\u003e\n\u003c/section\u003e\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"//cdn.jsdelivr.net/npm/prismjs@1.29.0/prism.min.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"//cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"//cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/toolbar/prism-toolbar.min.js\"\u003e\u003c/script\u003e\n\u003cscript\u003e\n  (function() {\n    if (!self.Prism) {\n      return;\n    }\n\n    \n    Prism.languages.dos = Prism.languages.powershell;\n    Prism.languages.gremlin = Prism.languages.groovy;\n\n    let languages = {\n      'r': 'R', 'python': 'Python', 'xml': 'XML', 'html': 'HTML',\n      'yaml': 'YAML', 'latex': 'LaTeX', 'tex': 'TeX',\n      'powershell': 'PowerShell', 'javascript': 'JavaScript',\n      'dos': 'DOS', 'qml': 'QML', 'json': 'JSON', 'bash': 'Bash',\n      'text': 'Text', 'txt': 'Text', 'sparql': 'SPARQL',\n      'gremlin': 'Gremlin', 'cypher': 'Cypher', 'ngql': 'nGQL',\n      'shell': 'Shell', 'sql': 'SQL', 'apacheconf': 'Apache Configuration', 'c': 'C', 'css': 'CSS'\n    };\n\n    Prism.hooks.add('before-highlight', function(env) {\n      if (env.language !== 'plain') {\n        let language = languages[env.language] || env.language;\n        env.element.setAttribute('data-language', language);\n      }\n    });\n\n    \n    let ClipboardJS = window.ClipboardJS || undefined;\n\n    Prism.plugins.toolbar.registerButton('copy-to-clipboard', function(env) {\n      let linkCopy = document.createElement('button');\n      linkCopy.classList.add('prism-button-copy');\n\n      registerClipboard();\n\n      return linkCopy;\n\n      function registerClipboard() {\n        let clip = new ClipboardJS(linkCopy, {\n          'text': function () {\n            return env.code;\n          }\n        });\n\n        clip.on('success', function() {\n          linkCopy.classList.add('prism-button-copy-success');\n          resetText();\n        });\n        clip.on('error', function () {\n          linkCopy.classList.add('prism-button-copy-error');\n          resetText();\n        });\n      }\n\n      function resetText() {\n        setTimeout(function () {\n          linkCopy.classList.remove('prism-button-copy-success');\n          linkCopy.classList.remove('prism-button-copy-error');\n        }, 1600);\n      }\n    });\n  })();\n\u003c/script\u003e\n\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js\"\u003e\u003c/script\u003e\n\u003cscript type=\"text/javascript\"\u003e\nlet pseudocodeCaptionCount = 0;\n(function(d) {\n  d.querySelectorAll(\".pseudocode\").forEach(function(elem) {\n    let pseudocode_options = {\n      indentSize: '1.2em',\n      commentDelimiter: '\\/\\/',\n      lineNumber:  true ,\n      lineNumberPunc: ':',\n      noEnd:  false \n    };\n    pseudocode_options.captionCount = pseudocodeCaptionCount;\n    pseudocodeCaptionCount += 1;\n    pseudocode.renderElement(elem, pseudocode_options);\n  });\n})(document);\n\u003c/script\u003e\n\n\n\n\n\n\n\n\n\n\n\n\u003cscript async=\"\" src=\"/js/center-img.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/right-quote.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/external-link.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/alt-title.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/figure.js\"\u003e\u003c/script\u003e\n\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js\"\u003e\u003c/script\u003e\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/vanilla-back-to-top@latest/dist/vanilla-back-to-top.min.js\"\u003e\u003c/script\u003e\n\u003cscript\u003e\naddBackToTop({\n  diameter: 48\n});\n\u003c/script\u003e\n\n  \u003chr/\u003e\n  \u003cdiv class=\"copyright no-border-bottom\"\u003e\n    \u003cdiv class=\"copyright-author-year\"\u003e\n      \u003cspan\u003eCopyright © 2017-2024 \u003ca href=\"/\"\u003e范叶亮 | Leo Van\u003c/a\u003e\u003c/span\u003e\n    \u003c/div\u003e\n  \u003c/div\u003e\n  \u003c/footer\u003e\n  \u003c/article\u003e",
  "Date": "2018-10-01T00:00:00Z",
  "Author": "范叶亮"
}