{
  "Source": "leovan.me",
  "Title": "集成学习算法 (Ensemble Learning)",
  "Link": "https://leovan.me/cn/2018/12/ensemble-learning/",
  "Content": "\u003carticle class=\"main\"\u003e\n    \u003cheader class=\"content-title\"\u003e\n    \n\u003ch1 class=\"title\"\u003e\n  \n  集成学习算法 (Ensemble Learning)\n  \n\u003c/h1\u003e\n\n\n\n\n\n\n\n\u003ch2 class=\"author-date\"\u003e范叶亮 / \n2018-12-08\u003c/h2\u003e\n\n\n\n\u003ch3 class=\"post-meta\"\u003e\n\n\n\u003cstrong\u003e分类: \u003c/strong\u003e\n\u003ca href=\"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0\"\u003e机器学习\u003c/a\u003e\n\n\n\n\n/\n\n\n\n\n\u003cstrong\u003e标签: \u003c/strong\u003e\n\u003cspan\u003e集成学习\u003c/span\u003e, \u003cspan\u003eEnsemble Learning\u003c/span\u003e, \u003cspan\u003eBagging\u003c/span\u003e, \u003cspan\u003eBootstrap Aggregating\u003c/span\u003e, \u003cspan\u003eBoosting\u003c/span\u003e, \u003cspan\u003e模型融合\u003c/span\u003e, \u003cspan\u003eAveraging\u003c/span\u003e, \u003cspan\u003eVoting\u003c/span\u003e, \u003cspan\u003eStacking\u003c/span\u003e, \u003cspan\u003eStacked Generalization\u003c/span\u003e, \u003cspan\u003e随机森林\u003c/span\u003e, \u003cspan\u003eRandom Forest\u003c/span\u003e, \u003cspan\u003eAdaboost\u003c/span\u003e, \u003cspan\u003e梯度提升\u003c/span\u003e, \u003cspan\u003eGradient Boosting\u003c/span\u003e, \u003cspan\u003eGBDT\u003c/span\u003e, \u003cspan\u003eGBM\u003c/span\u003e, \u003cspan\u003eGBRT\u003c/span\u003e, \u003cspan\u003eMART\u003c/span\u003e, \u003cspan\u003eXGBoost\u003c/span\u003e, \u003cspan\u003eLightGBM\u003c/span\u003e, \u003cspan\u003eGradient-based One-Side Sampling\u003c/span\u003e, \u003cspan\u003eGOSS\u003c/span\u003e, \u003cspan\u003eExclusive Feature Bundling\u003c/span\u003e, \u003cspan\u003eEFB\u003c/span\u003e, \u003cspan\u003eLevel-wise Tree Growth\u003c/span\u003e, \u003cspan\u003eDepth-wise Tree Growth\u003c/span\u003e, \u003cspan\u003eLeaf-wise Tree Growth\u003c/span\u003e, \u003cspan\u003eBest-first Tree Growth\u003c/span\u003e, \u003cspan\u003eCatBoost\u003c/span\u003e\n\n\n\n\n/\n\n\n\u003cstrong\u003e字数: \u003c/strong\u003e\n10102\n\u003c/h3\u003e\n\n\n\n\u003chr/\u003e\n\n\n\n    \n    \n    \u003cins class=\"adsbygoogle\" style=\"display:block; text-align:center;\" data-ad-layout=\"in-article\" data-ad-format=\"fluid\" data-ad-client=\"ca-pub-2608165017777396\" data-ad-slot=\"1261604535\"\u003e\u003c/ins\u003e\n    \u003cscript\u003e\n    (adsbygoogle = window.adsbygoogle || []).push({});\n    \u003c/script\u003e\n    \n    \n    \u003c/header\u003e\n\n\n\n\n\u003cp\u003e传统机器学习算法 (例如：决策树，人工神经网络，支持向量机，朴素贝叶斯等) 的目标都是寻找一个最优分类器尽可能的将训练数据分开。集成学习 (Ensemble Learning) 算法的基本思想就是将多个分类器组合，从而实现一个预测效果更好的集成分类器。集成算法可以说从一方面验证了中国的一句老话：三个臭皮匠，赛过诸葛亮。\u003c/p\u003e\n\u003cp\u003eThomas G. Dietterich \u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e \u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e 指出了集成算法在统计，计算和表示上的有效原因：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e统计上的原因\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e一个学习算法可以理解为在一个假设空间 \u003ccode\u003e$\\mathcal{H}$\u003c/code\u003e 中选找到一个最好的假设。但是，当训练样本的数据量小到不够用来精确的学习到目标假设时，学习算法可以找到很多满足训练样本的分类器。所以，学习算法选择任何一个分类器都会面临一定错误分类的风险，因此将多个假设集成起来可以降低选择错误分类器的风险。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e计算上的原因\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e很多学习算法在进行最优化搜索时很有可能陷入局部最优的错误中，因此对于学习算法而言很难得到一个全局最优的假设。事实上人工神经网络和决策树已经被证实为是一 个NP 问题 \u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e \u003csup id=\"fnref:4\"\u003e\u003ca href=\"#fn:4\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e4\u003c/a\u003e\u003c/sup\u003e。集成算法可以从多个起始点进行局部搜索，从而分散陷入局部最优的风险。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e表示上的原因\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e在多数应用场景中，假设空间 \u003ccode\u003e$\\mathcal{H}$\u003c/code\u003e 中的任意一个假设都无法表示 (或近似表示) 真正的分类函数 \u003ccode\u003e$f$\u003c/code\u003e。因此，对于不同的假设条件，通过加权的形式可以扩大假设空间，从而学习算法可以在一个无法表示或近似表示真正分类函数 \u003ccode\u003e$f$\u003c/code\u003e 的假设空间中找到一个逼近函数 \u003ccode\u003e$f$\u003c/code\u003e 的近似值。\u003c/p\u003e\n\u003cp\u003e集成算法大致可以分为：Bagging，Boosting 和 Stacking 等类型。\u003c/p\u003e\n\u003ch1 id=\"bagging\"\u003eBagging\u003c/h1\u003e\n\u003cp\u003eBagging (Boostrap Aggregating) 是由 Breiman 于 1996 年提出 \u003csup id=\"fnref:5\"\u003e\u003ca href=\"#fn:5\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e5\u003c/a\u003e\u003c/sup\u003e，基本思想如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e每次采用有放回的抽样从训练集中取出 \u003ccode\u003e$n$\u003c/code\u003e 个训练样本组成新的训练集。\u003c/li\u003e\n\u003cli\u003e利用新的训练集，训练得到 \u003ccode\u003e$M$\u003c/code\u003e 个子模型 \u003ccode\u003e$\\{h_1, h_2, ..., h_M\\}$\u003c/code\u003e。\u003c/li\u003e\n\u003cli\u003e对于分类问题，采用投票的方法，得票最多子模型的分类类别为最终的类别；对于回归问题，采用简单的平均方法得到预测值。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eBagging 算法如下所示：\u003c/p\u003e\n\n\n\u003clink rel=\"stylesheet\" type=\"text/css\" href=\"//cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css\"/\u003e\n\n\n\u003cdiv\u003e\u003cpre class=\"pseudocode\"\u003e\\begin{algorithm}\n\\caption{Bagging 算法}\n\\begin{algorithmic}\n\\REQUIRE \\\\\n    学习算法 $L$ \\\\\n    子模型个数 $M$ \\\\\n    训练数据集 $T = \\{(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)\\}$\n\\ENSURE \\\\\n    Bagging 算法 $h_f\\left(x\\right)$\n\\FUNCTION{Bagging}{$L, M, T$}\n\\FOR{$m = 1$ \\TO $M$}\n    \\STATE $T_m \\gets $ boostrap sample from training set $T$\n    \\STATE $h_m \\gets L\\left(T_m\\right)$\n\\ENDFOR\n\\STATE $h_f\\left(x\\right) \\gets \\text{sign} \\left(\\sum_{m=1}^{M} h_m\\left(x\\right)\\right)$\n\\RETURN $h_f\\left(x\\right)$\n\\ENDFUNCTION\n\\end{algorithmic}\n\\end{algorithm}\n\u003c/pre\u003e\u003c/div\u003e\n\n\u003cp\u003e假设对于一个包含 \u003ccode\u003e$M$\u003c/code\u003e 个样本的数据集 \u003ccode\u003e$T$\u003c/code\u003e，利用自助采样，则一个样本始终不被采用的概率是 \u003ccode\u003e$\\left(1 - \\frac{1}{M}\\right)^M$\u003c/code\u003e，取极限有：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\lim_{x \\to \\infty} \\left(1 - \\dfrac{1}{M}\\right)^M = \\dfrac{1}{e} \\approx 0.368 $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e即每个学习器仅用到了训练集中 \u003ccode\u003e$63.2\\%$\u003c/code\u003e 的数据集，剩余的 \u003ccode\u003e$36.8\\%$\u003c/code\u003e 的训练集样本可以用作验证集对于学习器的泛化能力进行包外估计 (out-of-bag estimate)。\u003c/p\u003e\n\u003ch2 id=\"随机森林-random-forests\"\u003e随机森林 (Random Forests)\u003c/h2\u003e\n\u003cp\u003e随机森林 (Random Forests) \u003csup id=\"fnref:6\"\u003e\u003ca href=\"#fn:6\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e6\u003c/a\u003e\u003c/sup\u003e 是一种利用决策树作为基学习器的 Bagging 集成学习算法。随机森林模型的构建过程如下：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e数据采样\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e作为一种 Bagging 集成算法，随机森林同样采用有放回的采样，对于总体训练集 \u003ccode\u003e$T$\u003c/code\u003e，抽样一个子集 \u003ccode\u003e$T_{sub}$\u003c/code\u003e 作为训练样本集。除此之外，假设训练集的特征个数为 \u003ccode\u003e$d$\u003c/code\u003e，每次仅选择 \u003ccode\u003e$k\\left(k \u0026lt; d\\right)$\u003c/code\u003e 个构建决策树。因此，随机森林除了能够做到样本扰动外，还添加了特征扰动，对于特征的选择个数，推荐值为 \u003ccode\u003e$k = \\log_2 d$\u003c/code\u003e \u003csup id=\"fnref1:6\"\u003e\u003ca href=\"#fn:6\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e6\u003c/a\u003e\u003c/sup\u003e。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e树的构建\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e每次根据采样得到的数据和特征构建一棵决策树。在构建决策树的过程中，会让决策树生长完全而不进行剪枝。构建出的若干棵决策树则组成了最终的随机森林。\u003c/p\u003e\n\u003cp\u003e随机森林在众多分类算法中表现十分出众 \u003csup id=\"fnref:7\"\u003e\u003ca href=\"#fn:7\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e7\u003c/a\u003e\u003c/sup\u003e，其主要的优点包括：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e由于随机森林引入了样本扰动和特征扰动，从而很大程度上提高了模型的泛化能力，尽可能地避免了过拟合现象的出现。\u003c/li\u003e\n\u003cli\u003e随机森林可以处理高维数据，无需进行特征选择，在训练过程中可以得出不同特征对模型的重要性程度。\u003c/li\u003e\n\u003cli\u003e随机森林的每个基分类器采用决策树，方法简单且容易实现。同时每个基分类器之间没有相互依赖关系，整个算法易并行化。\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch1 id=\"boosting\"\u003eBoosting\u003c/h1\u003e\n\u003cp\u003eBoosting 是一种提升算法，可以将弱的学习算法提升 (boost) 为强的学习算法。基本思路如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e利用初始训练样本集训练得到一个基学习器。\u003c/li\u003e\n\u003cli\u003e提高被基学习器误分的样本的权重，使得那些被错误分类的样本在下一轮训练中可以得到更大的关注，利用调整后的样本训练得到下一个基学习器。\u003c/li\u003e\n\u003cli\u003e重复上述步骤，直至得到 \u003ccode\u003e$M$\u003c/code\u003e 个学习器。\u003c/li\u003e\n\u003cli\u003e对于分类问题，采用有权重的投票方式；对于回归问题，采用加权平均得到预测值。\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"adaboost\"\u003eAdaboost\u003c/h2\u003e\n\u003cp\u003eAdaboost \u003csup id=\"fnref:8\"\u003e\u003ca href=\"#fn:8\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e8\u003c/a\u003e\u003c/sup\u003e 是 Boosting 算法中有代表性的一个。原始的 Adaboost 算法用于解决二分类问题，因此对于一个训练集\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ T = \\{\\left(x_1, y_1\\right), \\left(x_2, y_2\\right), ..., \\left(x_n, y_n\\right)\\} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中 \u003ccode\u003e$x_i \\in \\mathcal{X} \\subseteq \\mathbb{R}^n, y_i \\in \\mathcal{Y} = \\{-1, +1\\}$\u003c/code\u003e，首先初始化训练集的权重\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{equation} \\begin{split} D_1 =\u0026amp; \\left(w_{11}, w_{12}, ..., w_{1n}\\right) \\\\ w_{1i} =\u0026amp; \\dfrac{1}{n}, i = 1, 2, ..., n \\end{split} \\end{equation} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e根据每一轮训练集的权重 \u003ccode\u003e$D_m$\u003c/code\u003e，对训练集数据进行抽样得到 \u003ccode\u003e$T_m$\u003c/code\u003e，再根据 \u003ccode\u003e$T_m$\u003c/code\u003e 训练得到每一轮的基学习器 \u003ccode\u003e$h_m$\u003c/code\u003e。通过计算可以得出基学习器 \u003ccode\u003e$h_m$\u003c/code\u003e 的误差为 \u003ccode\u003e$\\epsilon_m$\u003c/code\u003e，根据基学习器的误差计算得出该基学习器在最终学习器中的权重系数\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\alpha_m = \\dfrac{1}{2} \\ln \\dfrac{1 - \\epsilon_m}{\\epsilon_m} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e更新训练集的权重\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{equation} \\begin{split} D_{m+1} =\u0026amp; \\left(w_{m+1, 1}, w_{m+1, 2}, ..., w_{m+1, n}\\right) \\\\ w_{m+1, i} =\u0026amp; \\dfrac{w_{m, i}}{Z_m} \\exp \\left(-\\alpha_m y_i h_m\\left(x_i\\right)\\right) \\end{split} \\end{equation} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中 \u003ccode\u003e$Z_m$\u003c/code\u003e 为规范化因子\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ Z_m = \\sum_{i = 1}^{n} w_{m, i} \\exp \\left(-\\alpha_m y_i h_m \\left(x_i\\right)\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e从而保证 \u003ccode\u003e$D_{m+1}$\u003c/code\u003e 为一个概率分布。最终根据构建的 \u003ccode\u003e$M$\u003c/code\u003e 个基学习器得到最终的学习器：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ h_f\\left(x\\right) = \\text{sign} \\left(\\sum_{m=1}^{M} \\alpha_m h_m\\left(x\\right)\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eAdaBoost 算法过程如下所示：\u003c/p\u003e\n\n\n\u003cdiv\u003e\u003cpre class=\"pseudocode\"\u003e\\begin{algorithm}\n\\caption{AdaBoost 算法}\n\\begin{algorithmic}\n\\REQUIRE \\\\\n    学习算法 $L$ \\\\\n    子模型个数 $M$ \\\\\n    训练数据集 $T = \\{(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)\\}$\n\\ENSURE \\\\\n    AdaBoost 算法 $h_f\\left(x\\right)$\n\\FUNCTION{AdaBoost}{$L, M, T$}\n\\STATE $D_1\\left(x\\right) \\gets 1 / n$\n\\FOR{$m = 1$ \\TO $M$}\n    \\STATE $T_{sub} \\gets $ sample from training set $T$ with weights\n    \\STATE $h_m \\gets L\\left(T_{sub}\\right)$\n    \\STATE $\\epsilon_m\\gets Error\\left(h_m\\right)$\n    \\IF{$\\epsilon_m \u0026gt; 0.5$}\n        \\BREAK\n    \\ENDIF\n    \\STATE $\\alpha_m \\gets \\dfrac{1}{2} \\ln \\dfrac{1 - \\epsilon_m}{\\epsilon_m}$\n    \\STATE $D_{m+1} \\gets \\dfrac{D_m \\exp \\left(-\\alpha_m y h_m\\left(x\\right)\\right)}{Z_m}$\n\\ENDFOR\n\\STATE $h_f\\left(x\\right) \\gets \\text{sign} \\left(\\sum_{m=1}^{M} \\alpha_m h_m\\left(x\\right)\\right)$\n\\RETURN $h_f\\left(x\\right)$\n\\ENDFUNCTION\n\\end{algorithmic}\n\\end{algorithm}\n\u003c/pre\u003e\u003c/div\u003e\n\n\u003ch2 id=\"gbdt-gbm-gbrt-mart\"\u003eGBDT (GBM, GBRT, MART)\u003c/h2\u003e\n\u003cp\u003eGBDT (Gradient Boosting Decision Tree) 是另一种基于 Boosting 思想的集成算法，除此之外 GBDT 还有很多其他的叫法，例如：GBM (Gradient Boosting Machine)，GBRT (Gradient Boosting Regression Tree)，MART (Multiple Additive Regression Tree) 等等。GBDT 算法由 3 个主要概念构成：Gradient Boosting (GB)，Regression Decision Tree (DT 或 RT) 和 Shrinkage。\u003c/p\u003e\n\u003cp\u003e从 GBDT 的众多别名中可以看出，GBDT 中使用的决策树并非我们最常用的分类树，而是回归树。分类树主要用于处理响应变量为因子型的数据，例如天气 (可以为晴，阴或下雨等)。回归树主要用于处理响应变量为数值型的数据，例如商品的价格。当然回归树也可以用于二分类问题，对于回归树预测出的数值结果，通过设置一个阈值即可以将数值型的预测结果映射到二分类问题标签上，即 \u003ccode\u003e$\\mathcal{Y} = \\{-1, +1\\}$\u003c/code\u003e。\u003c/p\u003e\n\u003cp\u003e对于 Gradient Boosting 而言，首先，Boosting 并不是 Adaboost 中 Boost 的概念，也不是 Random Forest 中的重抽样。在 Adaboost 中，Boost 是指在生成每个新的基学习器时，根据上一轮基学习器分类对错对训练集设置不同的权重，使得在上一轮中分类错误的样本在生成新的基学习器时更被重视。GBDT 中在应用 Boost 概念时，每一轮所使用的数据集没有经过重抽样，也没有更新样本的权重，而是每一轮选择了不用的回归目标，即上一轮计算得到的残差 (Residual)。其次，Gradient 是指在新一轮中在残差减少的梯度 (Gradient) 上建立新的基学习器。\u003c/p\u003e\n\u003cp\u003e下面我们通过一个年龄预测的 \u003ca href=\"http://suanfazu.com/t/gbdt-die-dai-jue-ce-shu-ru-men-jiao-cheng/135\"\u003e示例\u003c/a\u003e (较之原示例有修改) 简单介绍 GBDT 的工作流程。\u003c/p\u003e\n\u003cp\u003e假设存在 4 个人 \u003ccode\u003e$P = \\{p_1, p_2, p_3, p_4\\}$\u003c/code\u003e，他们的年龄分别为 \u003ccode\u003e$14, 16, 24, 26$\u003c/code\u003e。其中 \u003ccode\u003e$p_1, p_2$\u003c/code\u003e 分别是高一和高三学生，\u003ccode\u003e$p_3, p_4$\u003c/code\u003e 分别是应届毕业生和工作两年的员工。利用原始的决策树模型进行训练可以得到如下图所示的结果：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-12-08-ensemble-learning/gbdt-decision-tree-1.png\" alt=\"GBDT-Descision-Tree-1\"/\u003e\u003c/p\u003e\n\u003cp\u003e利用 GBDT 训练模型，由于数据量少，在此我们限定每个基学习器中的叶子节点最多为 2 个，即树的深度最大为 1 层。训练得到的结果如下图所示：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-12-08-ensemble-learning/gbdt-decision-tree-2.png\" alt=\"GBDT-Descision-Tree-2\"/\u003e\u003c/p\u003e\n\u003cp\u003e在训练第一棵树过程中，利用年龄作为预测值，根据计算可得由于 \u003ccode\u003e$p_1, p_2$\u003c/code\u003e 年龄相近，\u003ccode\u003e$p_3, p_4$\u003c/code\u003e 年龄相近被划分为两组。通过计算两组中真实年龄和预测的年龄的差值，可以得到第一棵树的残差 \u003ccode\u003e$R = \\{-1, 1, -1, 1\\}$\u003c/code\u003e。因此在训练第二棵树的过程中，利用第一棵树的残差作为标签值，最终所有人的年龄均正确被预测，即最终的残差均为 \u003ccode\u003e$0$\u003c/code\u003e。\u003c/p\u003e\n\u003cp\u003e则对于训练集中的 4 个人，利用训练得到的 GBDT 模型进行预测，结果如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003e$p_1$\u003c/code\u003e ：14 岁高一学生。购物较少，经常问学长问题，预测年龄 \u003ccode\u003e$Age = 15 - 1 = 14$\u003c/code\u003e。\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e$p_2$\u003c/code\u003e ：16 岁高三学生。购物较少，经常回答学弟问题，预测年龄 \u003ccode\u003e$Age = 15 + 1 = 16$\u003c/code\u003e。\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e$p_3$\u003c/code\u003e ：24 岁应届毕业生。购物较多，经常问别人问题，预测年龄 \u003ccode\u003e$Age = 25 - 1 = 24$\u003c/code\u003e。\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e$p_4$\u003c/code\u003e ：26 岁 2 年工作经验员工。购物较多，经常回答别人问题，预测年龄 \u003ccode\u003e$Age = 25 + 1 = 26$\u003c/code\u003e。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e整个 GBDT 算法流程如下所示：\u003c/p\u003e\n\n\n\u003cdiv\u003e\u003cpre class=\"pseudocode\"\u003e\\begin{algorithm}\n\\caption{GBDT 算法}\n\\begin{algorithmic}\n\\REQUIRE \\\\\n    子模型个数 $M$ \\\\\n    训练数据集 $T = \\{(x_1, y_1), (x_2, y_2), ..., (x_N, y_N)\\}$\n\\ENSURE \\\\\n    GBDT 算法 $h_f\\left(x\\right)$\n\\FUNCTION{GBDT}{$M, T$}\n\\STATE $F_1\\left(x\\right) \\gets \\sum_{i = 1}^{N} y_i / N$\n\\FOR{$m = 1$ \\TO $M$}\n\\STATE $r_m \\gets y - F_m \\left(x\\right)$\n\\STATE $T_m \\gets \\left(x, r_m\\right)$\n\\STATE $h_m \\gets RegressionTree \\left(T_m\\right)$\n\\STATE $\\alpha_m \\gets \\dfrac{\\sum_{i = 1}^{N} r_{im} h_m \\left(x_i\\right)}{\\sum_{i = 1}^{N} h_m \\left(x_i\\right)^2}$\n\\STATE $F_m \\left(x\\right) = F_{m-1} \\left(x\\right) + \\alpha_m h_m \\left(x\\right)$\n\\ENDFOR\n\\STATE $h_f\\left(x\\right) =  F_M \\left(x\\right)$\n\\RETURN $h_f\\left(x\\right)$\n\\ENDFUNCTION\n\\end{algorithmic}\n\\end{algorithm}\n\u003c/pre\u003e\u003c/div\u003e\n\n\u003cp\u003eGBDT 中也应用到了 Shrinkage 的思想，其基本思想可以理解为每一轮利用残差学习得到的回归树仅学习到了一部分知识，因此我们无法完全信任一棵树的结果。Shrinkage 思想认为在新的一轮学习中，不能利用全部残差训练模型，而是仅利用其中一部分，即：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ r_m = y - s F_m \\left(x\\right), 0 \\leq s \\leq 1 $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e注意，这里的 Shrinkage 和学习算法中 Gradient 的步长是两个不一样的概念。Shrinkage 设置小一些可以避免发生过拟合现象；而 Gradient 中的步长如果设置太小则会陷入局部最优，如果设置过大又容易结果不收敛。\u003c/p\u003e\n\u003ch2 id=\"xgboost\"\u003eXGBoost\u003c/h2\u003e\n\u003cp\u003eXGBoost 是由 Chen 等人 \u003csup id=\"fnref:9\"\u003e\u003ca href=\"#fn:9\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e9\u003c/a\u003e\u003c/sup\u003e 提出的一种梯度提升树模型框架。XGBoost 的基本思想同 GBDT 一样，对于一个包含 \u003ccode\u003e$n$\u003c/code\u003e 个样本和 \u003ccode\u003e$m$\u003c/code\u003e 个特征的数据集 \u003ccode\u003e$\\mathcal{D} = \\left\\{\\left(\\mathbf{x}_i, y_i\\right)\\right\\}$\u003c/code\u003e，其中 \u003ccode\u003e$\\left|\\mathcal{D}\\right| = n, \\mathbf{x}_i \\in \\mathbb{R}^m, y_i \\in \\mathbb{R}$\u003c/code\u003e，一个集成树模型可以用 \u003ccode\u003e$K$\u003c/code\u003e 个加法函数预测输出：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\hat{y}_i = \\phi \\left(\\mathbf{x}_i\\right) = \\sum_{k=1}^{K}{f_k \\left(\\mathbf{x}_i\\right)}, f_k \\in \\mathcal{F} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$\\mathcal{F} = \\left\\{f \\left(\\mathbf{x}\\right) = w_{q \\left(\\mathbf{x}\\right)}\\right\\} \\left(q: \\mathbb{R}^m \\to T, w \\in \\mathbb{R}^T\\right)$\u003c/code\u003e 为回归树 (CART)，\u003ccode\u003e$q$\u003c/code\u003e 表示每棵树的结构，其将一个样本映射到最终的叶子节点，\u003ccode\u003e$T$\u003c/code\u003e 为叶子节点的数量，每个 \u003ccode\u003e$f_w$\u003c/code\u003e 单独的对应一棵结构为 \u003ccode\u003e$q$\u003c/code\u003e 和权重为 \u003ccode\u003e$w$\u003c/code\u003e 的树。不同于决策树，每棵回归树的每个叶子节点上包含了一个连续的分值，我们用 \u003ccode\u003e$w_i$\u003c/code\u003e 表示第 \u003ccode\u003e$i$\u003c/code\u003e 个叶子节点上的分值。\u003c/p\u003e\n\u003cp\u003eXGBoost 首先对损失函数进行了改进，添加了 L2 正则项，同时进行了二阶泰勒展开。损失函数表示为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{equation} \\begin{split} \\mathcal{L} \\left(\\phi\\right) = \\sum_{i}{l \\left(\\hat{y}_i, y_i\\right)} + \\sum_{k}{\\Omega \\left(f_k\\right)} \\\\ \\text{where} \\ \\Omega \\left(f\\right) = \\gamma T + \\dfrac{1}{2} \\lambda \\left\\| w \\right\\|^2 \\end{split} \\end{equation} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$l$\u003c/code\u003e 为衡量预测值 \u003ccode\u003e$\\hat{y}_i$\u003c/code\u003e 和真实值 \u003ccode\u003e$y_i$\u003c/code\u003e 之间差异的函数，\u003ccode\u003e$\\Omega$\u003c/code\u003e 为惩罚项，\u003ccode\u003e$\\gamma$\u003c/code\u003e 和 \u003ccode\u003e$\\lambda$\u003c/code\u003e 为惩罚项系数。\u003c/p\u003e\n\u003cp\u003e我们用 \u003ccode\u003e$\\hat{y}_i^{\\left(t\\right)}$\u003c/code\u003e 表示第 \u003ccode\u003e$t$\u003c/code\u003e 次迭代的第 \u003ccode\u003e$i$\u003c/code\u003e 个实例，我们需要增加 \u003ccode\u003e$f_t$\u003c/code\u003e 来最小化如下的损失函数：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\mathcal{L}^{\\left(t\\right)} = \\sum_{i=1}^{n}{l \\left(y_i, \\hat{y}_i^{\\left(t-1\\right)} + f_t \\left(\\mathbf{x}_i\\right)\\right)} + \\Omega \\left(f_t\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e对上式进行二阶泰勒展开有：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\mathcal{L}^{\\left(t\\right)} \\simeq \\sum_{i=1}^{n}{\\left[l \\left(y_i, \\hat{y}_i^{\\left(t-1\\right)}\\right) + g_i f_t \\left(\\mathbf{x}_i\\right) + \\dfrac{1}{2} h_i f_t^2 \\left(\\mathbf{x}_i\\right)\\right]} + \\Omega \\left(f_t\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$g_i = \\partial_{\\hat{y}^{\\left(t-1\\right)}} l \\left(y_i, \\hat{y}^{\\left(t-1\\right)}\\right), h_i = \\partial_{\\hat{y}^{\\left(t-1\\right)}}^{2} l \\left(y_i, \\hat{y}^{\\left(t-1\\right)}\\right)$\u003c/code\u003e 分别为损失函数的一阶梯度和二阶梯度。去掉常数项，第 \u003ccode\u003e$t$\u003c/code\u003e 步的损失函数可以简化为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\tilde{\\mathcal{L}}^{\\left(t\\right)} = \\sum_{i=1}^{n}{\\left[ g_i f_t \\left(\\mathbf{x}_i\\right) + \\dfrac{1}{2} h_i f_t^2 \\left(\\mathbf{x}_i\\right)\\right]} + \\Omega \\left(f_t\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e令 \u003ccode\u003e$I_j = \\left\\{i \\ | \\ q \\left(\\mathbf{x}_i\\right) = j\\right\\}$\u003c/code\u003e 表示叶子节点 \u003ccode\u003e$j$\u003c/code\u003e 的实例集合，上式可重写为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{equation} \\begin{split} \\tilde{\\mathcal{L}}^{\\left(t\\right)} \u0026amp;= \\sum_{i=1}^{n}{\\left[ g_i f_t \\left(\\mathbf{x}_i\\right) + \\dfrac{1}{2} h_i f_t^2 \\left(\\mathbf{x}_i\\right)\\right]} + \\gamma T + \\dfrac{1}{2} \\lambda \\sum_{j=1}^{T}{w_j^2} \\\\ \u0026amp;= \\sum_{j=1}^{T}{\\left[\\left(\\sum_{i \\in I_j}{g_i}\\right) w_j + \\dfrac{1}{2} \\left(\\sum_{i \\in I_j}{h_i + \\lambda}\\right) w_j^2\\right]} + \\gamma T \\end{split} \\end{equation} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e对于一个固定的结构 \u003ccode\u003e$q \\left(\\mathbf{x}\\right)$\u003c/code\u003e，可以通过下式计算叶子节点 \u003ccode\u003e$j$\u003c/code\u003e 的最优权重 \u003ccode\u003e$w_j^*$\u003c/code\u003e：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ w_j^* = - \\dfrac{\\sum_{i \\in I_j}{g_i}}{\\sum_{i \\in I_j}{h_i} + \\lambda} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e进而计算对应的最优值：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\tilde{\\mathcal{L}}^{\\left(t\\right)} \\left(q\\right) = - \\dfrac{1}{2} \\sum_{j=1}^{T}{\\dfrac{\\left(\\sum_{i \\in I_j}{g_i}\\right)^2}{\\sum_{i \\in I_j}{h_i} + \\lambda}} + \\gamma T $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e上式可以作为评价树的结构 \u003ccode\u003e$q$\u003c/code\u003e 的评分函数。通常情况下很难枚举所有可能的树结构，一个贪心的算法是从一个节点出发，逐层的选择最佳的分裂节点。令 \u003ccode\u003e$I_L$\u003c/code\u003e 和 \u003ccode\u003e$I_R$\u003c/code\u003e 分别表示分裂后左侧和右侧的节点集合，令 \u003ccode\u003e$I = I_L \\cup I_R$\u003c/code\u003e，则分裂后损失的减少量为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\mathcal{L}_{\\text{split}} = \\dfrac{1}{2} \\left[\\dfrac{\\left(\\sum_{i \\in I_L}{g_i}\\right)^2}{\\sum_{i \\in I_L}{h_i} + \\lambda} + \\dfrac{\\left(\\sum_{i \\in I_R}{g_i}\\right)^2}{\\sum_{i \\in I_R}{h_i} + \\lambda} - \\dfrac{\\left(\\sum_{i \\in I}{g_i}\\right)^2}{\\sum_{i \\in I}{h_i} + \\lambda}\\right] - \\gamma $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003eXGBoost 也采用了 Shrinkage 的思想减少每棵树的影响，为后续树模型留下更多的改进空间。同时 XGBoost 也采用了随机森林中的特征下采样 (列采样) 方法用于避免过拟合，同时 XGBoost 也支持样本下采样 (行采样)。XGBoost 在分裂点的查找上也进行了优化，使之能够处理无法将全部数据读入内存的情况，同时能够更好的应对一些由于数据缺失，大量零值和 One-Hot 编码导致的特征稀疏问题。除此之外，XGBoost 在系统实现，包括：并行化，Cache-Aware 加速和数据的核外计算 (Out-of-Core Computation) 等方面也进行了大量优化，相关具体实现请参见论文和 \u003ca href=\"https://xgboost.readthedocs.io/en/latest/\"\u003e文档\u003c/a\u003e。\u003c/p\u003e\n\u003ch2 id=\"lightgbm\"\u003eLightGBM\u003c/h2\u003e\n\u003cp\u003eLightGBM 是由微软研究院的 Ke 等人 \u003csup id=\"fnref:10\"\u003e\u003ca href=\"#fn:10\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e10\u003c/a\u003e\u003c/sup\u003e 提出了一种梯度提升树模型框架。之前的 GBDT 模型在查找最优分裂点时需要扫描所有的样本计算信息增益，因此其计算复杂度与样本的数量和特征的数量成正比，这使得在处理大数据量的问题时非常耗时。LightGBM 针对这个问题提出了两个算法：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eGradient-based One-Side Sampling (GOSS)\u003c/li\u003e\n\u003cli\u003eExclusive Feature Bundling (EFB)\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"gradient-based-one-side-sampling\"\u003eGradient-based One-Side Sampling\u003c/h3\u003e\n\u003cp\u003e在 AdaBoost 中，样本的权重很好的诠释了数据的重要性，但在 GBDT 中并没有这样的权重，因此无法直接应用 AdaBoost 的采样方法。幸运的是 GBDT 中每个样本的梯度可以为我们的数据采样提供有用的信息。当一个样本具有较小的梯度时，其训练的误差也较小，表明其已经训练好了。一个直观的想法就是丢弃这些具有较小梯度的样本，但是这样操作会影响整个数据的分布，从而对模型的精度造成损失。\u003c/p\u003e\n\u003cp\u003eGOSS 的做法是保留具有较大梯度的样本，并从具有较小梯度的样本中随机采样。同时为了补偿对数据分布的影响，在计算信息增益的时候，GOSS 针对梯度较小的样本引入了一个常数乘子。这样就保证了模型更多的关注未得到较好训练的数据，同时又不会对原始数据分布改变过多。整个算法流程如下：\u003c/p\u003e\n\n\n\u003cdiv\u003e\u003cpre class=\"pseudocode\"\u003e\\begin{algorithm}\n\\caption{GOSS 算法}\n\\begin{algorithmic}\n\\INPUT \\\\\n    训练数据 $I$ \\\\\n    迭代次数 $d$ \\\\\n    具有较大梯度数据的采样比例 $a$ \\\\\n    具有较小梯度数据的采样比例 $b$ \\\\\n    损失函数 $loss$ \\\\\n    基学习器 $L$\n\\FUNCTION{GOSS}{$I, d, a, b, loss, L$}\n\\STATE $\\text{models} \\gets \\varnothing$\n\\STATE $\\text{fact} \\gets \\dfrac{1-a}{b}$\n\\STATE $\\text{topN} \\gets a \\times \\text{len} \\left(I\\right)$\n\\STATE $\\text{randN} \\gets b \\times \\text{len} \\left(I\\right)$\n\\FOR{$i = 1$ \\TO $d$}\n    \\STATE $\\text{preds} \\gets \\text{models.predict} \\left(I\\right)$\n    \\STATE $\\text{g} \\gets loss \\left(I, \\text{preds}\\right)$\n    \\STATE $\\text{w} \\gets \\left\\{1, 1, \\dotsc\\right\\}$\n    \\STATE $\\text{sorted} \\gets \\text{GetSortedIndices} \\left(\\text{abs} \\left(\\text{g}\\right)\\right)$\n    \\STATE $\\text{topSet} \\gets \\text{sorted[1:topN]}$\n    \\STATE $\\text{randSet} \\gets \\text{RandomPick} \\left(\\text{sorted[topN:len}\\left(I\\right)\\text{]}, \\text{randN}\\right)$\n    \\STATE $\\text{usedSet} \\gets \\text{topSet} \\cup \\text{randSet}$\n    \\STATE $\\text{w[randSet]} \\gets \\text{w[randSet]} \\times \\text{fact}$\n    \\STATE $\\text{newModel} \\gets L \\left(I \\text{[usedSet]}, - \\text{g[usedSet]}, \\text{w[usedSet]}\\right)$\n    \\STATE $\\text{models} \\gets \\text{models} \\cup \\text{newModel}$\n\\ENDFOR\n\\ENDFUNCTION\n\\end{algorithmic}\n\\end{algorithm}\n\u003c/pre\u003e\u003c/div\u003e\n\n\u003ch3 id=\"exclusive-feature-bundling\"\u003eExclusive Feature Bundling\u003c/h3\u003e\n\u003cp\u003e高维数据往往是稀疏的，特征空间的稀疏性为我们提供了可能的近似无损的特征降维实现。进一步而言，在稀疏的特征空间中，很多特征之间是互斥的，也就是说它们不同时取非零值。因此，我们就可以将这些互斥的特征绑定成一个特征。由于 \u003ccode\u003e$\\#bundle \\ll \\#feature$\u003c/code\u003e，因此构建直方图的复杂度就可以从 \u003ccode\u003e$O \\left(\\#data \\times \\#features\\right)$\u003c/code\u003e 减小至 \u003ccode\u003e$O \\left(\\#data \\times \\#bundle\\right)$\u003c/code\u003e，从而在不损失精度的情况下加速模型的训练。这样我们就需要解决如下两个问题：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e确定对哪些特征进行绑定。\u003c/li\u003e\n\u003cli\u003e如果对这些特征进行绑定。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e对哪些特征进行绑定可以利用 \u003ca href=\"https://en.wikipedia.org/wiki/Graph_coloring\"\u003e图着色问题\u003c/a\u003e 进行解决。对于一个图 \u003ccode\u003e$G = \\left(V, E\\right)$\u003c/code\u003e，将 \u003ccode\u003e$G$\u003c/code\u003e 的 \u003ca href=\"https://en.wikipedia.org/wiki/Incidence_matrix\"\u003e关联矩阵\u003c/a\u003e 中的每一行看成特征，得到 \u003ccode\u003e$|V|$\u003c/code\u003e 个特征，从而可以得出图中颜色相同的节点即为互斥的特征。算法如下：\u003c/p\u003e\n\n\n\u003cdiv\u003e\u003cpre class=\"pseudocode\"\u003e\\begin{algorithm}\n\\caption{Greedy Bundling}\n\\begin{algorithmic}\n\\INPUT \\\\\n    特征 $F$ \\\\\n    最大冲突数量 $K$\n\\OUTPUT \\\\\n    需要绑定的特征 $bundles$\n\\FUNCTION{GreedyBundling}{$F, K$}\n\\STATE Construct graph $G$\n\\STATE $\\text{searchOrder} \\gets G.\\text{sortByDegree}()$\n\\STATE $\\text{bundles} \\gets \\varnothing$\n\\STATE $\\text{bundlesConflict} \\gets \\varnothing$\n\\FOR{$i$ $\\in$ searchOrder}\n    \\STATE $\\text{needNew} \\gets$ \\TRUE\n    \\FOR{$j = 1$ \\TO len(bundles)}\n        \\STATE $\\text{cnt} \\gets$ ConflictCnt(bundles[$j$],F[$i$])\n        \\IF{cnt $+$ bundlesConflict[$i$] $\\leq K$}\n            \\STATE bundles[$j$].add($F[i]$)\n            \\STATE $\\text{needNew} \\gets$ \\FALSE\n            \\BREAK\n        \\ENDIF\n    \\ENDFOR\n    \\IF{needNew}\n        \\STATE $bundles \\gets bundles \\cup F[i]$\n    \\ENDIF\n\\ENDFOR\n\\RETURN $bundles$\n\\ENDFUNCTION\n\\end{algorithmic}\n\\end{algorithm}\n\u003c/pre\u003e\u003c/div\u003e\n\n\u003cp\u003e上述算法的复杂度为 \u003ccode\u003e$O \\left(\\#feature^2\\right)$\u003c/code\u003e ，并且仅在模型训练前运行一次。对于特征数不是很大的情况是可以接受的，但当特征数量很大时算法效率并不令人满意。进一步的优化是在不构造图的情况下进行高效的排序，即根据非零值的数量进行排序，更多的非零值意味着更高的冲突概率。\u003c/p\u003e\n\u003cp\u003e合并特征的关键在于确保原始特征的值能够从合并后的特征之中识别出来。由于基于直方图的算法保存的是原始特征的离散桶，而非连续的值，因此我们可以将互斥的特征置于不同的桶内。算法如下：\u003c/p\u003e\n\n\n\u003cdiv\u003e\u003cpre class=\"pseudocode\"\u003e\\begin{algorithm}\n\\caption{Merge Exclusive Features}\n\\begin{algorithmic}\n\\REQUIRE \\\\\n    数据数量 $numData$ \\\\\n    一组互斥特征 $F$\n\\ENSURE \\\\\n    新的分箱 $newBin$ \\\\\n    分箱范围 $binRanges$\n\\FUNCTION{MergeExclusiveFeatures}{$numData, F$}\n\\STATE $\\text{binRages} \\gets \\left\\{0\\right\\}$\n\\STATE $\\text{totalBin} \\gets 0$\n\\FOR{$f$ $\\in$ $F$}\n    \\STATE $\\text{totalBin} \\gets \\text{totalBin} + \\text{f.numBin}$\n    \\STATE $\\text{binRanges} \\gets \\text{binRanges} \\cup \\text{totalBin}$\n\\ENDFOR\n\\STATE $\\text{newBin} \\gets \\text{Bin} \\left(numData\\right)$\n\\FOR{$i = 1$ \\TO $numData$}\n    \\STATE $\\text{newBin}[i] \\gets 0$\n    \\FOR{$j = 1$ \\TO $\\text{len} \\left(F\\right)$}\n        \\IF{$F[j].\\text{bin}[i] \\neq 0$}\n            \\STATE $\\text{newBin}[i] \\gets F[j].\\text{bin}[i] + \\text{binRanges}[j]$\n        \\ENDIF\n    \\ENDFOR\n\\ENDFOR\n\\RETURN $newBin, binRanges$\n\\ENDFUNCTION\n\\end{algorithmic}\n\\end{algorithm}\n\u003c/pre\u003e\u003c/div\u003e\n\n\u003cp\u003eEFB 算法可以将大量的互斥特征合并为少量的稠密特征，从而通过避免对零值特征的计算提高算法的运行效率。\u003c/p\u003e\n\u003ch3 id=\"tree-growth\"\u003eTree Growth\u003c/h3\u003e\n\u003cp\u003e大多的决策树算法通过逐层 (Level-wise / Depth-wise) 的方法生成树，如下图所示：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-12-08-ensemble-learning/tree-growth-level-wise.png\" alt=\"Level-Wise-Tree-Growth\"/\u003e\u003c/p\u003e\n\u003cp\u003eLightGBM 采用了另外一种 Leaf-wise (或称 Best-first) 的方式生成 \u003csup id=\"fnref:11\"\u003e\u003ca href=\"#fn:11\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e11\u003c/a\u003e\u003c/sup\u003e，如下图所示：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-12-08-ensemble-learning/tree-growth-leaf-wise.png\" alt=\"Leaf-Wise-Tree-Growth\"/\u003e\u003c/p\u003e\n\u003cp\u003e该方法想选择具有最大 Delta Loss 值的叶子节点进行生长。在固定叶子节点数量的情况下，Leaf-wise 的生长方式比 Level-wise 的方式更容易获得较低的损失值。Leaf-wise 的生长方式在数据量较小时容易产生过拟合的现象，在 LightGBM 中可以通过限制树的最大深度减轻该问题。\u003c/p\u003e\n\u003cp\u003e更多有关 LightGBM 的优化请参见论文和 \u003ca href=\"https://github.com/Microsoft/LightGBM/blob/master/docs/Features.rst\"\u003e文档\u003c/a\u003e。\u003c/p\u003e\n\u003ch2 id=\"catboost\"\u003eCatBoost\u003c/h2\u003e\n\u003cp\u003eCatBoost 是由俄罗斯 Yandex 公司 \u003csup id=\"fnref:12\"\u003e\u003ca href=\"#fn:12\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e12\u003c/a\u003e\u003c/sup\u003e \u003csup id=\"fnref:13\"\u003e\u003ca href=\"#fn:13\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e13\u003c/a\u003e\u003c/sup\u003e 提出的一种梯度提升树模型框架。相比于之前的实现，CatBoost 的优化主要包括如下几点：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e提出了一种处理分类特征 (Categorical Features) 的算法。\u003c/li\u003e\n\u003cli\u003e提出了一种解决预测偏移 (Prediction Shift) 问题的算法。\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"分类特征\"\u003e分类特征\u003c/h3\u003e\n\u003cp\u003e分类特征是由一些离散的值构成的集合，其无法直接应用在提升树模型中，一个常用的方法是利用 One-Hot 编码对分类特征进行预处理，将其转化成值特征。\u003c/p\u003e\n\u003cp\u003e另一种方法是根据样本的标签值计算分类特征的一些统计量 (Target Statistics, TS)。令 \u003ccode\u003e$\\mathcal{D} = \\left\\{\\left(\\mathbf{x}_k, y_k\\right)\\right\\}_{k=1, \\dotsc, n}$\u003c/code\u003e 为一个数据集，其中 \u003ccode\u003e$\\mathbf{x}_k = \\left(x_k^1, \\dotsc, x_k^m\\right)$\u003c/code\u003e 为一个包含 \u003ccode\u003e$m$\u003c/code\u003e 个特征的向量 (包含值特征和分类特征)，\u003ccode\u003e$y_k \\in \\mathbb{R}$\u003c/code\u003e 为标签值。最简单的做法是将分类特征替换为全量训练数据上对应特征值相同的标签值的均值，即 \u003ccode\u003e$\\hat{x}_k^i \\approx \\mathbb{E} \\left(y \\ | \\ x^i = x_k^i\\right)$\u003c/code\u003e。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eGreedy TS\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e一个简单估计 \u003ccode\u003e$\\mathbb{E} \\left(y \\ | \\ x^i = x_k^i\\right)$\u003c/code\u003e 的方法是对具有相同类型 \u003ccode\u003e$x_k^i$\u003c/code\u003e 的样本的标签值求均值。但这种估计对于低频的分类噪音较大，因此我们可以通过一个先验 \u003ccode\u003e$P$\u003c/code\u003e 来进行平滑：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\hat{x}_k^i = \\dfrac{\\sum_{j=1}^{n}{\\boldsymbol{1}_{\\left\\{x_j^i = x_k^i\\right\\}} \\cdot y_j} + a P}{\\sum_{j=1}^{n}{\\boldsymbol{1}_{\\left\\{x_j^i = x_k^i\\right\\}}} + a} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$a \u0026gt; 0$\u003c/code\u003e 为先验系数，\u003ccode\u003e$\\boldsymbol{1}$\u003c/code\u003e 为指示函数，通常 \u003ccode\u003e$P$\u003c/code\u003e 取整个数据集标签值的均值。\u003c/p\u003e\n\u003cp\u003e上述贪婪 (Greedy) 的做法的问题在于存在目标泄露 (Target Leakage)，即特征 \u003ccode\u003e$\\hat{x}_k^i$\u003c/code\u003e 是通过 \u003ccode\u003e$\\mathbf{x}_k$\u003c/code\u003e 的目标 \u003ccode\u003e$y_k$\u003c/code\u003e 计算所得。这会导致条件偏移 (Conditional Shift) 的问题 \u003csup id=\"fnref:14\"\u003e\u003ca href=\"#fn:14\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e14\u003c/a\u003e\u003c/sup\u003e，即 \u003ccode\u003e$\\hat{x}^i \\ | \\ y$\u003c/code\u003e 的分布在训练集和测试集上不同。因此在计算 TS 时需要满足如下特性：\u003c/p\u003e\n\u003clink rel=\"stylesheet\" href=\"/css/admonition.css\"/\u003e\n\u003cdiv class=\"admonition admonition-note admonition-no-icon kai\"\u003e\n  \u003cp class=\"admonition-title\"\u003e\u003cstrong\u003e特性 1\u003c/strong\u003e\u003c/p\u003e\n  \u003cdiv class=\"admonition-content\"\u003e\u003ccode\u003e$\\mathbb{E} \\left(\\hat{x}^i \\ | \\ y = v\\right) = \\mathbb{E} \\left(\\hat{x}_k^i \\ | \\ y_k = v\\right)$\u003c/code\u003e，其中 \u003ccode\u003e$\\left(x_k, y_k\\right)$\u003c/code\u003e 为第 \u003ccode\u003e$k$\u003c/code\u003e 个训练样本。\u003c/div\u003e\n\u003c/div\u003e\n\u003cp\u003e一种修正方法是在计算 TS 时使用排除掉 \u003ccode\u003e$\\mathbf{x}_k$\u003c/code\u003e 的一个子集，令 \u003ccode\u003e$\\mathcal{D}_k \\subset \\mathcal{D} \\setminus \\left\\{\\mathbf{x}_k\\right\\}$\u003c/code\u003e，有：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\hat{x}_k^i = \\dfrac{\\sum_{\\mathbf{x}_j \\in \\mathcal{D}_k}{\\boldsymbol{1}_{\\left\\{x_j^i = x_k^i\\right\\}} \\cdot y_j} + a P}{\\sum_{\\mathbf{x}_j \\in \\mathcal{D}_k}{\\boldsymbol{1}_{\\left\\{x_j^i = x_k^i\\right\\}}} + a} $$\u003c/code\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eHoldout TS\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e另一种方法是将训练集划分为两部分 \u003ccode\u003e$\\mathcal{D} = \\hat{\\mathcal{D}}_0 \\sqcup \\hat{\\mathcal{D}_1}$\u003c/code\u003e，利用 \u003ccode\u003e$\\mathcal{D}_k = \\hat{\\mathcal{D}}_0$\u003c/code\u003e 计算 TS，利用 \u003ccode\u003e$\\hat{\\mathcal{D}_1}$\u003c/code\u003e 进行训练。虽然满足了 \u003cstrong\u003e特性 1\u003c/strong\u003e，但是这会导致计算 TS 和用于训练的数据均显著减少，因此还需要满足另一个特性：\u003c/p\u003e\n\u003cdiv class=\"admonition admonition-note admonition-no-icon kai\"\u003e\n  \u003cp class=\"admonition-title\"\u003e\u003cstrong\u003e特性 2\u003c/strong\u003e\u003c/p\u003e\n  \u003cdiv class=\"admonition-content\"\u003e有效地利用所有的训练数据计算 TS 和训练模型。\u003c/div\u003e\n\u003c/div\u003e\n\u003cul\u003e\n\u003cli\u003eLeave-one-out TS\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e对于训练样本 \u003ccode\u003e$\\mathbf{x}_k$\u003c/code\u003e 令 \u003ccode\u003e$\\mathcal{D}_k = \\mathcal{D} \\setminus \\mathbf{x}_k$\u003c/code\u003e，对于测试集，令 \u003ccode\u003e$\\mathcal{D}_k = \\mathcal{D}$\u003c/code\u003e，但这并没有解决 Target Leakage 问题。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eOrdered TS\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eCatboost 采用了一种更有效的策略：首先对于训练样本进行随机排列，得到排列下标 \u003ccode\u003e$\\sigma$\u003c/code\u003e，之后对于每个训练样本仅利用“历史”样本来计算 TS，即：\u003ccode\u003e$\\mathcal{D}_k = \\left\\{\\mathbf{x}_j: \\sigma \\left(j\\right) \u0026lt; \\sigma \\left(k\\right)\\right\\}$\u003c/code\u003e，对于每个测试样本 \u003ccode\u003e$\\mathcal{D}_k = \\mathcal{D}$\u003c/code\u003e。\u003c/p\u003e\n\u003ch3 id=\"prediction-shift-ordered-boosting\"\u003ePrediction Shift \u0026amp; Ordered Boosting\u003c/h3\u003e\n\u003cp\u003e类似计算 TS，Prediction Shift 是由一种特殊的 Target Leakage 所导致的。对于第 \u003ccode\u003e$t$\u003c/code\u003e 次迭代，我们优化的目标为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ h^t = \\mathop{\\arg\\min}_{h \\in H} \\mathbb{E} \\left(-g^t \\left(\\mathbf{x}, y\\right) - h \\left(\\mathbf{x}\\right)\\right)^2  \\label{eq:catboost-obj} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$g^t \\left(\\mathbf{x}, y\\right) := \\dfrac{\\partial L \\left(y, s\\right)}{\\partial s} \\bigg\\vert_{s = F^{t-1} \\left(\\mathbf{x}\\right)}$\u003c/code\u003e。通常情况下会使用相同的数据集 \u003ccode\u003e$\\mathcal{D}$\u003c/code\u003e 进行估计：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ h^t = \\mathop{\\arg\\min}_{h \\in H} \\dfrac{1}{n} \\sum_{k=1}^{n}{\\left(-g^t \\left(\\mathbf{x}_k, y_k\\right) - h \\left(\\mathbf{x}_k\\right)\\right)^2} \\label{eq:catboost-obj-approx} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e整个偏移的链条如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e梯度的条件分布 \u003ccode\u003e$g^t \\left(\\mathbf{x}_k, y_k\\right) \\ | \\ \\mathbf{x}_k$\u003c/code\u003e 同测试样本对应的分布 \u003ccode\u003e$g^t \\left(\\mathbf{x}, y\\right) \\ | \\ \\mathbf{x}$\u003c/code\u003e 存在偏移。\u003c/li\u003e\n\u003cli\u003e由式 \u003ccode\u003e$\\ref{eq:catboost-obj}$\u003c/code\u003e 定义的基学习器 \u003ccode\u003e$h^t$\u003c/code\u003e 同由式 \u003ccode\u003e$\\ref{eq:catboost-obj-approx}$\u003c/code\u003e 定义的估计方法存在偏移。\u003c/li\u003e\n\u003cli\u003e最终影响训练模型 \u003ccode\u003e$F^t$\u003c/code\u003e 的泛化能力。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e每一步梯度的估计所使用的标签值同构建当前模型 \u003ccode\u003e$F^{t-1}$\u003c/code\u003e 使用的相同。但是，对于一个训练样本 \u003ccode\u003e$\\mathbf{x}_k$\u003c/code\u003e 而言，条件分布 \u003ccode\u003e$F^{t-1} \\left(\\mathbf{x}_k \\ | \\ \\mathbf{x}_k\\right)$\u003c/code\u003e 相对一个测试样本 \u003ccode\u003e$\\mathbf{x}$\u003c/code\u003e 对应的分布 \u003ccode\u003e$F^{t-1} \\left(\\mathbf{x}\\right) \\ | \\ \\mathbf{x}$\u003c/code\u003e 发生了偏移，我们称这为预测偏移 (Prediction Shift)。\u003c/p\u003e\n\u003cp\u003eCatBoost 提出了一种解决 Prediction Shift 的算法：Ordered Boosting。假设对于训练数据进行随机排序得到 \u003ccode\u003e$\\sigma$\u003c/code\u003e，并有 \u003ccode\u003e$n$\u003c/code\u003e 个不同的模型 \u003ccode\u003e$M_1, \\dotsc, M_n$\u003c/code\u003e，每个模型 \u003ccode\u003e$M_i$\u003c/code\u003e 仅利用随机排序后的前 \u003ccode\u003e$i$\u003c/code\u003e 个样本训练得到。算法如下：\u003c/p\u003e\n\n\n\u003cdiv\u003e\u003cpre class=\"pseudocode\"\u003e\\begin{algorithm}\n\\caption{Ordered Boosting}\n\\begin{algorithmic}\n\\INPUT \\\\\n    训练集 $\\left\\{\\left(\\mathbf{x}_k, y_k\\right)\\right\\}_{k=1}^{n}$ \\\\\n    树的个数 $I$\n\\OUTPUT \\\\\n    模型 $M_n$\n\\FUNCTION{OrderedBoosting}{$numData, F$}\n\\STATE $\\sigma \\gets \\text{random permutation of} \\left[1, n\\right]$\n\\STATE $M_i \\gets 0$ for $i = 1, \\dotsc, n$\n\\FOR{$t = 1$ \\TO $I$}\n    \\FOR{$i = 1$ \\TO $n$}\n        \\STATE $r_i \\gets y_i - M_{\\sigma \\left(i\\right) -1} \\left(\\mathbf{x}_i\\right)$\n    \\ENDFOR\n    \\FOR{$i = 1$ \\TO $n$}\n        \\STATE $\\Delta M \\gets \\text{LearnModel} \\left(\\left(\\mathbf{x}_j, r_j\\right): \\sigma \\left(j\\right) \\leq i\\right)$\n        \\STATE $M_i \\gets M_i + \\Delta M$\n    \\ENDFOR\n\\ENDFOR\n\\RETURN $M_n$\n\\ENDFUNCTION\n\\end{algorithmic}\n\\end{algorithm}\n\u003c/pre\u003e\u003c/div\u003e\n\n\u003cp\u003e在计算 TS 和进行 Ordered Boosting 时我们均使用了随机排列并得到 \u003ccode\u003e$\\sigma_{cat}$\u003c/code\u003e 和 \u003ccode\u003e$\\sigma_{boost}$\u003c/code\u003e。需要注意的是在将两部分合并为一个算法时，我们需要令 \u003ccode\u003e$\\sigma_{cat} = \\sigma_{boost}$\u003c/code\u003e 避免 Prediction Shift。这样可以保证目标 \u003ccode\u003e$y_i$\u003c/code\u003e 不用于训练模型 \u003ccode\u003e$M_i$\u003c/code\u003e (既不参与计算 TS，也不用于梯度估计)。\u003c/p\u003e\n\u003cp\u003e更多 CatBoost 的实现细节请参见论文和 \u003ca href=\"https://tech.yandex.com/catboost/\"\u003e文档\u003c/a\u003e。\u003c/p\u003e\n\u003ch2 id=\"不同实现的比较\"\u003e不同实现的比较\u003c/h2\u003e\n\u003cp\u003e针对 \u003ca href=\"https://github.com/scikit-learn/scikit-learn\"\u003escikit-learn\u003c/a\u003e，\u003ca href=\"https://github.com/dmlc/xgboost\"\u003eXGBoost\u003c/a\u003e，\u003ca href=\"https://github.com/Microsoft/LightGBM\"\u003eLightGBM\u003c/a\u003e 和 \u003ca href=\"https://github.com/catboost/catboost\"\u003eCatBoost\u003c/a\u003e 4 种 GBDT 的具体实现，下表汇总了各自的相关特性：\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003escikit-learn\u003c/th\u003e\n\u003cth\u003eXGBoost\u003c/th\u003e\n\u003cth\u003eLightGBM\u003c/th\u003e\n\u003cth\u003eCatBoost\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e当前版本\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e0.20.1\u003c/td\u003e\n\u003ctd\u003e0.81\u003c/td\u003e\n\u003ctd\u003e2.2.2\u003c/td\u003e\n\u003ctd\u003e0.11.1\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e实现语言\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eC, C++, Python\u003c/td\u003e\n\u003ctd\u003eC, C++\u003c/td\u003e\n\u003ctd\u003eC, C++\u003c/td\u003e\n\u003ctd\u003eC++\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eAPI 语言\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003ePython\u003c/td\u003e\n\u003ctd\u003ePython, R, Java, Scala, C++ and more\u003c/td\u003e\n\u003ctd\u003ePython, R\u003c/td\u003e\n\u003ctd\u003ePython, R\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e模型导出\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eJPMML \u003csup id=\"fnref:15\"\u003e\u003ca href=\"#fn:15\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e15\u003c/a\u003e\u003c/sup\u003e\u003c/td\u003e\n\u003ctd\u003eJPMML \u003csup id=\"fnref:16\"\u003e\u003ca href=\"#fn:16\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e16\u003c/a\u003e\u003c/sup\u003e, ONNX \u003csup id=\"fnref:17\"\u003e\u003ca href=\"#fn:17\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e17\u003c/a\u003e\u003c/sup\u003e \u003csup id=\"fnref:18\"\u003e\u003ca href=\"#fn:18\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e18\u003c/a\u003e\u003c/sup\u003e\u003c/td\u003e\n\u003ctd\u003eONNX \u003csup id=\"fnref1:17\"\u003e\u003ca href=\"#fn:17\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e17\u003c/a\u003e\u003c/sup\u003e \u003csup id=\"fnref:19\"\u003e\u003ca href=\"#fn:19\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e19\u003c/a\u003e\u003c/sup\u003e\u003c/td\u003e\n\u003ctd\u003eCoreML, Python, C++, JSON \u003csup id=\"fnref:20\"\u003e\u003ca href=\"#fn:20\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e20\u003c/a\u003e\u003c/sup\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e多线程\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eNo\u003c/td\u003e\n\u003ctd\u003eYes\u003c/td\u003e\n\u003ctd\u003eYes\u003c/td\u003e\n\u003ctd\u003eYes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eGPU\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eNo\u003c/td\u003e\n\u003ctd\u003eYes \u003csup id=\"fnref:21\"\u003e\u003ca href=\"#fn:21\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e21\u003c/a\u003e\u003c/sup\u003e\u003c/td\u003e\n\u003ctd\u003eYes \u003csup id=\"fnref:22\"\u003e\u003ca href=\"#fn:22\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e22\u003c/a\u003e\u003c/sup\u003e\u003c/td\u003e\n\u003ctd\u003eYes \u003csup id=\"fnref:23\"\u003e\u003ca href=\"#fn:23\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e23\u003c/a\u003e\u003c/sup\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e多 GPU\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eNo\u003c/td\u003e\n\u003ctd\u003eYes \u003csup id=\"fnref1:21\"\u003e\u003ca href=\"#fn:21\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e21\u003c/a\u003e\u003c/sup\u003e\u003c/td\u003e\n\u003ctd\u003eNo\u003c/td\u003e\n\u003ctd\u003eYes \u003csup id=\"fnref1:23\"\u003e\u003ca href=\"#fn:23\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e23\u003c/a\u003e\u003c/sup\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eBoosting 类型\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eGradient Boosted Tree (\u003cstrong\u003eGBDT\u003c/strong\u003e)\u003c/td\u003e\n\u003ctd\u003e\u003cstrong\u003eGBDT\u003c/strong\u003e (booster: gbtree) \u003cbr/\u003eGeneralized Linear Model, \u003cstrong\u003eGLM\u003c/strong\u003e (booster: gbliner) \u003cbr/\u003eDropout Additive Regression Tree, \u003cstrong\u003eDART\u003c/strong\u003e (booster: dart)\u003c/td\u003e\n\u003ctd\u003e\u003cstrong\u003eGBDT\u003c/strong\u003e (boosting: gbdt) \u003cbr/\u003e\u003cstrong\u003eRandom Forest\u003c/strong\u003e (boosting: rf) \u003cbr/\u003e\u003cstrong\u003eDART\u003c/strong\u003e (boosting: dart) \u003cbr/\u003eGradient-based One-Side Sampling, \u003cstrong\u003eGOSS\u003c/strong\u003e (bossting: goss)\u003c/td\u003e\n\u003ctd\u003e\u003cstrong\u003eOrdered\u003c/strong\u003e (boosting_type: Ordered) \u003cbr/\u003e\u003cstrong\u003ePlain\u003c/strong\u003e (bossting_type: Plain)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eLevel-wise (Depth-wise) Split\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eYes\u003c/td\u003e\n\u003ctd\u003eYes \u003cbr/\u003e(grow_policy: depthwise)\u003c/td\u003e\n\u003ctd\u003eNo\u003c/td\u003e\n\u003ctd\u003eYes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eLeaf-wise (Best-first) Split\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eNo\u003c/td\u003e\n\u003ctd\u003eYes \u003cbr/\u003e(grow_policy: lossguide)\u003c/td\u003e\n\u003ctd\u003eYes\u003c/td\u003e\n\u003ctd\u003eNo\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003eHistogram-based Split\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eNo\u003c/td\u003e\n\u003ctd\u003eYes \u003cbr/\u003e(tree_method: hist / gpu_hist)\u003c/td\u003e\n\u003ctd\u003eYes\u003c/td\u003e\n\u003ctd\u003eYes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e过拟合控制\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eYes \u003cbr/\u003e(max_depth, …)\u003c/td\u003e\n\u003ctd\u003eYes \u003cbr/\u003e(max_depth, max_leaves, gamma, reg_alpha, reg_lamda, …)\u003c/td\u003e\n\u003ctd\u003eYes \u003cbr/\u003e(max_depth, num_leaves, gamma, reg_alpha, reg_lamda, drop_rate, …)\u003c/td\u003e\n\u003ctd\u003eYes \u003cbr/\u003e(max_depth, reg_lambda, …)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e分类特征\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eNo\u003c/td\u003e\n\u003ctd\u003eNo\u003c/td\u003e\n\u003ctd\u003eYes \u003cbr/\u003e(categorical_feature)\u003c/td\u003e\n\u003ctd\u003eYes \u003cbr/\u003e(cat_features)\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e缺失值处理\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eNo\u003c/td\u003e\n\u003ctd\u003eYes\u003c/td\u003e\n\u003ctd\u003eYes \u003cbr/\u003e(use_missing)\u003c/td\u003e\n\u003ctd\u003eYes\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e不均衡数据\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003eNo\u003c/td\u003e\n\u003ctd\u003eYes \u003cbr/\u003e(scale_pos_weight, max_delta_step)\u003c/td\u003e\n\u003ctd\u003eYes \u003cbr/\u003e(scale_pos_weight, poisson_max_delta_step)\u003c/td\u003e\n\u003ctd\u003eYes \u003cbr/\u003e(scale_pos_weight)\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e不同实现的性能分析和比较可参见如下文章，括号中内容为分析的实现库：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://xgboost.ai/2016/12/14/GPU-accelerated-xgboost.html\"\u003eGPU Accelerated XGBoost\u003c/a\u003e (XGBoost)\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://xgboost.ai/2018/07/04/gpu-xgboost-update.html\"\u003eUpdates to the XGBoost GPU algorithms\u003c/a\u003e (XGBoost)\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://lightgbm.readthedocs.io/en/latest/Experiments.html\"\u003eLightGBM Experiments\u003c/a\u003e (XGBoost, LightGBM)，\u003ca href=\"https://github.com/guolinke/boosting_tree_benchmarks\"\u003e代码\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://lightgbm.readthedocs.io/en/latest/GPU-Performance.html\"\u003eGPU Tunning Guide and Performance Comparision\u003c/a\u003e (LightGBM)\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://blogs.technet.microsoft.com/machinelearning/2017/07/25/lessons-learned-benchmarking-fast-machine-learning-algorithms/\"\u003eLessons Learned From Benchmarking Fast Machine Learning Algorithms\u003c/a\u003e (XGBoost, LightGBM), \u003ca href=\"https://github.com/Azure/fast_retraining\"\u003e代码\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/catboost/benchmarks\"\u003eCatBoost Benchmarks\u003c/a\u003e (XGBoost, LightGBM, CatBoost, H2O)\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://arxiv.org/abs/1809.04559\"\u003eBenchmarking and Optimization of Gradient Boosted Decision Tree Algorithms\u003c/a\u003e (XGBoost, LightGBM, CatBoost)\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://sites.google.com/view/lauraepp/home\"\u003eLaurae++: xgboost / LightGBM\u003c/a\u003e (XGBoost, LightGBM), \u003ca href=\"https://github.com/Laurae2/gbt_benchmarks\"\u003e代码\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/szilard/GBM-perf\"\u003eGBM Performance\u003c/a\u003e (XGBoost, LightGBM, H2O), \u003ca href=\"https://github.com/szilard/GBM-perf\"\u003e代码\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"stacking\"\u003eStacking\u003c/h1\u003e\n\u003cp\u003eStacking 本身是一种集成学习方法，同时也是一种模型组合策略，我们首先介绍一些相对简单的模型组合策略：\u003cstrong\u003e平均法\u003c/strong\u003e 和 \u003cstrong\u003e投票法\u003c/strong\u003e。\u003c/p\u003e\n\u003cp\u003e对于 \u003cstrong\u003e数值型的输出\u003c/strong\u003e \u003ccode\u003e$h_i \\left(\\mathbf{x}\\right) \\in \\mathbb{R}$\u003c/code\u003e，\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e简单平均法 (Simple Averaging)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ccode\u003e$$ H \\left(\\mathbf{x}\\right) = \\dfrac{1}{M} \\sum_{i=1}^{M}{h_i \\left(\\mathbf{x}\\right)} $$\u003c/code\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e加权平均法 (Weighted Averaging)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ccode\u003e$$ H \\left(\\mathbf{x}\\right) = \\sum_{i=1}^{M}{w_i h_i \\left(\\mathbf{x}\\right)} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$w_i$\u003c/code\u003e 为学习器 \u003ccode\u003e$h_i$\u003c/code\u003e 的权重，且 \u003ccode\u003e$w_i \\geq 0, \\sum_{i=1}^{T}{w_i} = 1$\u003c/code\u003e。\u003c/p\u003e\n\u003cp\u003e对于 \u003cstrong\u003e分类型的任务\u003c/strong\u003e，学习器 \u003ccode\u003e$h_i$\u003c/code\u003e 从类别集合 \u003ccode\u003e$\\left\\{c_1, c_2, \\dotsc, c_N\\right\\}$\u003c/code\u003e 中预测一个标签。我们将 \u003ccode\u003e$h_i$\u003c/code\u003e 在样本 \u003ccode\u003e$\\mathbf{x}$\u003c/code\u003e 上的预测输出表示为一个 \u003ccode\u003e$N$\u003c/code\u003e 维向量 \u003ccode\u003e$\\left(h_i^1 \\left(\\mathbf{x}\\right); h_i^2 \\left(\\mathbf{x}\\right); \\dotsc, h_i^N \\left(\\mathbf{x}\\right)\\right)$\u003c/code\u003e，其中 \u003ccode\u003e$h_i^j \\left(\\mathbf{x}\\right)$\u003c/code\u003e 为 \u003ccode\u003e$h_i$\u003c/code\u003e 在类型标签 \u003ccode\u003e$c_j$\u003c/code\u003e 上的输出。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e绝对多数投票法 (Majority Voting)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ccode\u003e$$ H \\left(\\mathbf{x}\\right) = \\begin{cases} c_j, \u0026amp; \\displaystyle\\sum_{i=1}^{M}{h_i^j \\left(\\mathbf{x}\\right) \u0026gt; 0.5 \\displaystyle\\sum_{k=1}^{N}{\\displaystyle\\sum_{i=1}^{M}{h_i^k \\left(\\mathbf{x}\\right)}}} \\\\ \\text{refuse}, \u0026amp; \\text{other wise} \\end{cases} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e即如果一个类型的标记得票数过半，则预测为该类型，否则拒绝预测。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e相对多数投票法 (Plurality Voting)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ccode\u003e$$ H \\left(\\mathbf{x}\\right) = c_{\\arg\\max_j \\sum_{i=1}^{M}{h_i^j \\left(\\mathbf{x}\\right)}} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e即预测为得票数最多的类型，如果同时有多个类型获得相同最高票数，则从中随机选取一个。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e加权投票法 （Weighted Voting)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ccode\u003e$$ H \\left(\\mathbf{x}\\right) = c_{\\arg\\max_j \\sum_{i=1}^{M}{w_i h_i^j \\left(\\mathbf{x}\\right)}} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$w_i$\u003c/code\u003e 为学习器 \u003ccode\u003e$h_i$\u003c/code\u003e 的权重，且 \u003ccode\u003e$w_i \\geq 0, \\sum_{i=1}^{M}{w_i} = 1$\u003c/code\u003e。\u003c/p\u003e\n\u003cp\u003e绝对多数投票提供了“拒绝预测”，这为可靠性要求较高的学习任务提供了一个很好的机制，但如果学习任务要求必须有预测结果时则只能选择相对多数投票法和加权投票法。在实际任务中，不同类型的学习器可能产生不同类型的 \u003ccode\u003e$h_i^j \\left(\\boldsymbol{x}\\right)$\u003c/code\u003e 值，常见的有：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e类标记，\u003ccode\u003e$h_i^j \\left(\\mathbf{x}\\right) \\in \\left\\{0, 1\\right\\}$\u003c/code\u003e，若 \u003ccode\u003e$h_i$\u003c/code\u003e 将样本 \u003ccode\u003e$\\mathbf{x}$\u003c/code\u003e 预测为类型 \u003ccode\u003e$c_j$\u003c/code\u003e 则取值为 1，否则取值为 0。使用类型标记的投票称之为 \u003cstrong\u003e“硬投票” (Hard Voting)\u003c/strong\u003e。\u003c/li\u003e\n\u003cli\u003e类概率，\u003ccode\u003e$h_i^j \\left(\\mathbf{x}\\right) \\in \\left[0, 1\\right]$\u003c/code\u003e，相当于对后验概率 \u003ccode\u003e$P \\left(c_j \\ | \\ \\mathbf{x}\\right)$\u003c/code\u003e 的一个估计。使用类型概率的投票称之为 \u003cstrong\u003e“软投票” (Soft Voting)\u003c/strong\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eStacking \u003csup id=\"fnref:24\"\u003e\u003ca href=\"#fn:24\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e24\u003c/a\u003e\u003c/sup\u003e \u003csup id=\"fnref:25\"\u003e\u003ca href=\"#fn:25\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e25\u003c/a\u003e\u003c/sup\u003e 方法又称为 Stacked Generalization，是一种基于分层模型组合的集成算法。Stacking 算法的基本思想如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e利用初级学习算法对原始数据集进行学习，同时生成一个新的数据集。\u003c/li\u003e\n\u003cli\u003e根据从初级学习算法生成的新数据集，利用次级学习算法学习并得到最终的输出。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e对于初级学习器，可以是相同类型也可以是不同类型的。在新的数据集中，初级学习器的输出被用作次级学习器的输入特征，初始样本的标记仍被用作次级学习器学习样本的标记。Stacking 算法的流程如下图所示：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-12-08-ensemble-learning/stacking.png\" alt=\"Stacking\"/\u003e\u003c/p\u003e\n\u003cp\u003eStacking 算法过程如下：\u003c/p\u003e\n\n\n\u003cdiv\u003e\u003cpre class=\"pseudocode\"\u003e\\begin{algorithm}\n\\caption{Stacking 算法}\n\\begin{algorithmic}\n\\REQUIRE \\\\\n    初级学习算法 $L = \\{L_1, L_2, ... L_M\\}$ \\\\\n    次级学习算法 $L\u0026#39;$ \\\\\n    训练数据集 $T = \\{(\\mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2), ..., (\\mathbf{x}_N, y_N)\\}$\n\\ENSURE \\\\\n    Stacking 算法 $h_f\\left(x\\right)$\n\\FUNCTION{Stacking}{$L, L\u0026#39;, T$}\n\\FOR{$m$ = $1$ to $M$}\n  \\STATE $h_t \\gets L_m \\left(T\\right)$\n\\ENDFOR\n\\STATE $T\u0026#39; \\gets \\varnothing$\n\\FOR{$i$ = $1$ to $N$}\n  \\FOR{$m$ = $1$ to $M$}\n    \\STATE $z_{im} \\gets h_m(\\mathbf{x}_i)$\n  \\ENDFOR\n  \\STATE $T\u0026#39; \\gets T\u0026#39; \\cup \\left(\\left(z_{i1}, z_{i2}, ..., z_{iM}\\right), y_i\\right)$\n\\ENDFOR\n\\STATE $h\u0026#39; \\gets L\u0026#39; \\left(T\u0026#39;\\right)$\n\\STATE $h_f\\left(\\mathbf{x}\\right) \\gets h\u0026#39; \\left(h_1\\left(\\mathbf{x}\\right), h_2\\left(\\mathbf{x}\\right), ..., h_M\\left(\\mathbf{x}\\right)\\right)$\n\\RETURN $h_f\\left(\\mathbf{x}\\right)$\n\\ENDFUNCTION\n\\end{algorithmic}\n\\end{algorithm}\n\u003c/pre\u003e\u003c/div\u003e\n\n\u003cp\u003e次级学习器的训练集是有初级学习器产生的，如果直接利用初级学习器的训练集生成次级学习器的训练集，过拟合风险会比较大 \u003csup id=\"fnref:26\"\u003e\u003ca href=\"#fn:26\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e26\u003c/a\u003e\u003c/sup\u003e。因此，一般利用在训练初级学习器中未使用过的样本来生成次级学习器的训练样本。以 \u003ccode\u003e$k$\u003c/code\u003e 折交叉检验为例：初始的训练集 \u003ccode\u003e$T$\u003c/code\u003e 被随机划分为 \u003ccode\u003e$k$\u003c/code\u003e 个大小相近的集合 \u003ccode\u003e$T_1, T_2, ..., T_k$\u003c/code\u003e。令 \u003ccode\u003e$T_j$\u003c/code\u003e 和 \u003ccode\u003e$\\overline{T}_j = T \\setminus T_j$\u003c/code\u003e 表示第 \u003ccode\u003e$j$\u003c/code\u003e 折的测试集和训练集。则对于 \u003ccode\u003e$M$\u003c/code\u003e 个初级学习算法，学习器 \u003ccode\u003e$h_m^{\\left(j\\right)}$\u003c/code\u003e 是根据训练集 \u003ccode\u003e$\\overline{T}_j$\u003c/code\u003e 生成的，对于测试集 \u003ccode\u003e$T_j$\u003c/code\u003e 中的每个样本 \u003ccode\u003e$\\mathbf{x}_i$\u003c/code\u003e，得到 \u003ccode\u003e$z_{im} = h_m^{\\left(j\\right)} \\left(\\mathbf{x}_i\\right)$\u003c/code\u003e。则根据 \u003ccode\u003e$\\mathbf{x}_i$\u003c/code\u003e 所产生的次级学习器的训练样本为 \u003ccode\u003e$\\mathbf{z}_i = \\left(\\left(z_{i1}, z_{i2}, ..., z_{iM}\\right), y_i\\right)$\u003c/code\u003e。最终利用 \u003ccode\u003e$M$\u003c/code\u003e 个初级学习器产生的训练集 \u003ccode\u003e$T\u0026#39; = \\{\\left(\\mathbf{z}_i, y_i\\right)\\}_{i=1}^N$\u003c/code\u003e 训练次级学习器。\u003c/p\u003e\n\u003cp\u003e下图展示了一些基础分类器以及 Soft Voting 和 Stacking 两种融合策略的模型在 Iris 数据集分类任务上的决策区域。数据选取 Iris 数据集中的 Sepal Length 和 Petal Length 两个特征，Stacking 中的次级学习器选择 Logistic Regression，详细实现请参见 \u003ca href=\"https://github.com/leovan/leovan.me/blob/main/static/scripts/cn/2018-12-08-ensemble-learning/clfs-decision-regions.py\"\u003e这里\u003c/a\u003e。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-12-08-ensemble-learning/clfs-decision-regions.png\" alt=\"Classifiers-Decision-Regions\"/\u003e\u003c/p\u003e\n\u003cdiv class=\"footnotes\" role=\"doc-endnotes\"\u003e\n\u003chr/\u003e\n\u003col\u003e\n\u003cli id=\"fn:1\"\u003e\n\u003cp\u003eDietterich, T. G. (2000, June). Ensemble methods in machine learning. In \u003cem\u003eInternational workshop on multiple classifier systems\u003c/em\u003e (pp. 1-15). \u003ca href=\"#fnref:1\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:2\"\u003e\n\u003cp\u003eDietterich, T. G. (2002). \u003cem\u003eEnsemble Learning, The Handbook of Brain Theory and Neural Networks\u003c/em\u003e, MA Arbib. \u003ca href=\"#fnref:2\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:3\"\u003e\n\u003cp\u003eLaurent, H., \u0026amp; Rivest, R. L. (1976). Constructing optimal binary decision trees is NP-complete. \u003cem\u003eInformation processing letters\u003c/em\u003e, 5(1), 15-17. \u003ca href=\"#fnref:3\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:4\"\u003e\n\u003cp\u003eBlum, A., \u0026amp; Rivest, R. L. (1989). Training a 3-node neural network is NP-complete. In \u003cem\u003eAdvances in neural information processing systems\u003c/em\u003e (pp. 494-501). \u003ca href=\"#fnref:4\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:5\"\u003e\n\u003cp\u003eBreiman, L. (1996). Bagging predictors. \u003cem\u003eMachine learning, 24\u003c/em\u003e(2), 123-140. \u003ca href=\"#fnref:5\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:6\"\u003e\n\u003cp\u003eBreiman, L. (2001). Random forests. \u003cem\u003eMachine learning, 45\u003c/em\u003e(1), 5-32. \u003ca href=\"#fnref:6\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e \u003ca href=\"#fnref1:6\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:7\"\u003e\n\u003cp\u003eFernández-Delgado, M., Cernadas, E., Barro, S., \u0026amp; Amorim, D. (2014). Do we need hundreds of classifiers to solve real world classification problems?. \u003cem\u003eThe Journal of Machine Learning Research, 15\u003c/em\u003e(1), 3133-3181. \u003ca href=\"#fnref:7\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:8\"\u003e\n\u003cp\u003eFreund, Y., \u0026amp; Schapire, R. E. (1997). A decision-theoretic generalization of on-line learning and an application to boosting. \u003cem\u003eJournal of computer and system sciences, 55\u003c/em\u003e(1), 119-139. \u003ca href=\"#fnref:8\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:9\"\u003e\n\u003cp\u003eChen, T., \u0026amp; Guestrin, C. (2016). XGBoost: A Scalable Tree Boosting System. In \u003cem\u003eProceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining\u003c/em\u003e (pp. 785–794). \u003ca href=\"#fnref:9\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:10\"\u003e\n\u003cp\u003eKe, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., … Liu, T.-Y. (2017). LightGBM: A Highly Efficient Gradient Boosting Decision Tree. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, \u0026amp; R. Garnett (Eds.), \u003cem\u003eAdvances in Neural Information Processing Systems 30\u003c/em\u003e (pp. 3146–3154). \u003ca href=\"#fnref:10\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:11\"\u003e\n\u003cp\u003eShi, H. (2007). \u003cem\u003eBest-first Decision Tree Learning\u003c/em\u003e (Thesis). The University of Waikato. \u003ca href=\"#fnref:11\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:12\"\u003e\n\u003cp\u003eDorogush, A. V., Ershov, V., \u0026amp; Gulin, A. (2018). CatBoost: gradient boosting with categorical features support. \u003cem\u003earXiv preprint arXiv:1810.11363\u003c/em\u003e \u003ca href=\"#fnref:12\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:13\"\u003e\n\u003cp\u003eProkhorenkova, L., Gusev, G., Vorobev, A., Dorogush, A. V., \u0026amp; Gulin, A. (2018). CatBoost: unbiased boosting with categorical features. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, \u0026amp; R. Garnett (Eds.), \u003cem\u003eAdvances in Neural Information Processing Systems 31\u003c/em\u003e (pp. 6637–6647). \u003ca href=\"#fnref:13\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:14\"\u003e\n\u003cp\u003eZhang, K., Schölkopf, B., Muandet, K., \u0026amp; Wang, Z. (2013, February). Domain adaptation under target and conditional shift. In \u003cem\u003eInternational Conference on Machine Learning\u003c/em\u003e (pp. 819-827). \u003ca href=\"#fnref:14\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:15\"\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/jpmml/jpmml-sklearn\"\u003ehttps://github.com/jpmml/jpmml-sklearn\u003c/a\u003e \u003ca href=\"#fnref:15\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:16\"\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/jpmml/jpmml-xgboost\"\u003ehttps://github.com/jpmml/jpmml-xgboost\u003c/a\u003e \u003ca href=\"#fnref:16\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:17\"\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/onnx/onnx\"\u003ehttps://github.com/onnx/onnx\u003c/a\u003e \u003ca href=\"#fnref:17\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e \u003ca href=\"#fnref1:17\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:18\"\u003e\n\u003cp\u003e\u003ca href=\"https://pypi.org/project/winmltools\"\u003ehttps://pypi.org/project/winmltools\u003c/a\u003e \u003ca href=\"#fnref:18\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:19\"\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/onnx/onnxmltools\"\u003ehttps://github.com/onnx/onnxmltools\u003c/a\u003e \u003ca href=\"#fnref:19\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:20\"\u003e\n\u003cp\u003e\u003ca href=\"https://tech.yandex.com/catboost/doc/dg/concepts/python-reference_catboost_save_model-docpage\"\u003ehttps://tech.yandex.com/catboost/doc/dg/concepts/python-reference_catboost_save_model-docpage\u003c/a\u003e \u003ca href=\"#fnref:20\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:21\"\u003e\n\u003cp\u003e\u003ca href=\"https://xgboost.readthedocs.io/en/latest/gpu/index.html\"\u003ehttps://xgboost.readthedocs.io/en/latest/gpu/index.html\u003c/a\u003e \u003ca href=\"#fnref:21\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e \u003ca href=\"#fnref1:21\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:22\"\u003e\n\u003cp\u003e\u003ca href=\"https://lightgbm.readthedocs.io/en/latest/GPU-Tutorial.html\"\u003ehttps://lightgbm.readthedocs.io/en/latest/GPU-Tutorial.html\u003c/a\u003e \u003ca href=\"#fnref:22\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:23\"\u003e\n\u003cp\u003e\u003ca href=\"https://tech.yandex.com/catboost/doc/dg/features/training-on-gpu-docpage\"\u003ehttps://tech.yandex.com/catboost/doc/dg/features/training-on-gpu-docpage\u003c/a\u003e \u003ca href=\"#fnref:23\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e \u003ca href=\"#fnref1:23\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:24\"\u003e\n\u003cp\u003eWolpert, D. H. (1992). Stacked generalization. \u003cem\u003eNeural networks, 5\u003c/em\u003e(2), 241-259. \u003ca href=\"#fnref:24\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:25\"\u003e\n\u003cp\u003eBreiman, L. (1996). Stacked regressions. \u003cem\u003eMachine learning, 24\u003c/em\u003e(1), 49-64. \u003ca href=\"#fnref:25\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:26\"\u003e\n\u003cp\u003e周志华. (2016). \u003cem\u003e机器学习\u003c/em\u003e. 清华大学出版社. \u003ca href=\"#fnref:26\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/div\u003e\n\n\n\n\n\n\u003cdiv class=\"donate\"\u003e\n  \u003cdiv class=\"donate-header\"\u003e\u003c/div\u003e\n  \u003cdiv class=\"donate-slug\" id=\"donate-slug\"\u003eensemble-learning\u003c/div\u003e\n  \u003cbutton class=\"donate-button\"\u003e赞 赏\u003c/button\u003e\n  \u003cdiv class=\"donate-footer\"\u003e「真诚赞赏，手留余香」\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"donate-modal-wrapper\"\u003e\n  \u003cdiv class=\"donate-modal\"\u003e\n    \u003cdiv class=\"donate-box\"\u003e\n      \u003cdiv class=\"donate-box-content\"\u003e\n        \u003cdiv class=\"donate-box-content-inner\"\u003e\n          \u003cdiv class=\"donate-box-header\"\u003e「真诚赞赏，手留余香」\u003c/div\u003e\n          \u003cdiv class=\"donate-box-body\"\u003e\n            \u003cdiv class=\"donate-box-money\"\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-2\" data-v=\"2\" data-unchecked=\"￥ 2\" data-checked=\"2 元\"\u003e￥ 2\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-5\" data-v=\"5\" data-unchecked=\"￥ 5\" data-checked=\"5 元\"\u003e￥ 5\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-10\" data-v=\"10\" data-unchecked=\"￥ 10\" data-checked=\"10 元\"\u003e￥ 10\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-50\" data-v=\"50\" data-unchecked=\"￥ 50\" data-checked=\"50 元\"\u003e￥ 50\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-100\" data-v=\"100\" data-unchecked=\"￥ 100\" data-checked=\"100 元\"\u003e￥ 100\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-custom\" data-v=\"custom\" data-unchecked=\"任意金额\" data-checked=\"任意金额\"\u003e任意金额\u003c/button\u003e\n            \u003c/div\u003e\n            \u003cdiv class=\"donate-box-pay\"\u003e\n              \u003cimg class=\"donate-box-pay-qrcode\" id=\"donate-box-pay-qrcode\" src=\"\"/\u003e\n            \u003c/div\u003e\n          \u003c/div\u003e\n          \u003cdiv class=\"donate-box-footer\"\u003e\n            \u003cdiv class=\"donate-box-pay-method donate-box-pay-method-checked\" data-v=\"wechat-pay\"\u003e\n              \u003cimg class=\"donate-box-pay-method-image\" id=\"donate-box-pay-method-image-wechat-pay\" src=\"\"/\u003e\n            \u003c/div\u003e\n            \u003cdiv class=\"donate-box-pay-method\" data-v=\"alipay\"\u003e\n              \u003cimg class=\"donate-box-pay-method-image\" id=\"donate-box-pay-method-image-alipay\" src=\"\"/\u003e\n            \u003c/div\u003e\n          \u003c/div\u003e\n        \u003c/div\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n    \u003cbutton type=\"button\" class=\"donate-box-close-button\"\u003e\n      \u003csvg class=\"donate-box-close-button-icon\" fill=\"#fff\" viewBox=\"0 0 24 24\" width=\"24\" height=\"24\"\u003e\u003cpath d=\"M13.486 12l5.208-5.207a1.048 1.048 0 0 0-.006-1.483 1.046 1.046 0 0 0-1.482-.005L12 10.514 6.793 5.305a1.048 1.048 0 0 0-1.483.005 1.046 1.046 0 0 0-.005 1.483L10.514 12l-5.208 5.207a1.048 1.048 0 0 0 .006 1.483 1.046 1.046 0 0 0 1.482.005L12 13.486l5.207 5.208a1.048 1.048 0 0 0 1.483-.006 1.046 1.046 0 0 0 .005-1.482L13.486 12z\" fill-rule=\"evenodd\"\u003e\u003c/path\u003e\u003c/svg\u003e\n    \u003c/button\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\u003cscript type=\"text/javascript\" src=\"/js/donate.js\"\u003e\u003c/script\u003e\n\n\n  \u003cfooter\u003e\n  \n\u003cnav class=\"post-nav\"\u003e\n  \u003cspan class=\"nav-prev\"\u003e← \u003ca href=\"/cn/2018/11/computational-complexity-and-dynamic-programming/\"\u003e计算复杂性 (Computational Complexity) 与动态规划 (Dynamic Programming)\u003c/a\u003e\u003c/span\u003e\n  \u003cspan class=\"nav-next\"\u003e\u003ca href=\"/cn/2019/01/similarity-and-distance-measurement/\"\u003e相似性和距离度量 (Similarity \u0026amp; Distance Measurement)\u003c/a\u003e →\u003c/span\u003e\n\u003c/nav\u003e\n\n\n\n\n\u003cins class=\"adsbygoogle\" style=\"display:block; text-align:center;\" data-ad-layout=\"in-article\" data-ad-format=\"fluid\" data-ad-client=\"ca-pub-2608165017777396\" data-ad-slot=\"8302038603\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n  (adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/js-cookie@3.0.5/dist/js.cookie.min.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"/js/toggle-theme.js\"\u003e\u003c/script\u003e\n\n\n\u003cscript src=\"/js/no-highlight.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"/js/math-code.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"/js/heading-anchor.js\"\u003e\u003c/script\u003e\n\n\n\n\u003csection class=\"comments\"\u003e\n\u003cscript src=\"https://giscus.app/client.js\" data-repo=\"leovan/leovan.me\" data-repo-id=\"MDEwOlJlcG9zaXRvcnkxMTMxOTY0Mjc=\" data-category=\"Comments\" data-category-id=\"DIC_kwDOBr89i84CT-R7\" data-mapping=\"pathname\" data-strict=\"1\" data-reactions-enabled=\"1\" data-emit-metadata=\"0\" data-input-position=\"top\" data-theme=\"preferred_color_scheme\" data-lang=\"zh-CN\" data-loading=\"lazy\" crossorigin=\"anonymous\" defer=\"\"\u003e\n\u003c/script\u003e\n\u003c/section\u003e\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"//cdn.jsdelivr.net/npm/prismjs@1.29.0/prism.min.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"//cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"//cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/toolbar/prism-toolbar.min.js\"\u003e\u003c/script\u003e\n\u003cscript\u003e\n  (function() {\n    if (!self.Prism) {\n      return;\n    }\n\n    \n    Prism.languages.dos = Prism.languages.powershell;\n    Prism.languages.gremlin = Prism.languages.groovy;\n\n    let languages = {\n      'r': 'R', 'python': 'Python', 'xml': 'XML', 'html': 'HTML',\n      'yaml': 'YAML', 'latex': 'LaTeX', 'tex': 'TeX',\n      'powershell': 'PowerShell', 'javascript': 'JavaScript',\n      'dos': 'DOS', 'qml': 'QML', 'json': 'JSON', 'bash': 'Bash',\n      'text': 'Text', 'txt': 'Text', 'sparql': 'SPARQL',\n      'gremlin': 'Gremlin', 'cypher': 'Cypher', 'ngql': 'nGQL',\n      'shell': 'Shell', 'sql': 'SQL', 'apacheconf': 'Apache Configuration', 'c': 'C', 'css': 'CSS'\n    };\n\n    Prism.hooks.add('before-highlight', function(env) {\n      if (env.language !== 'plain') {\n        let language = languages[env.language] || env.language;\n        env.element.setAttribute('data-language', language);\n      }\n    });\n\n    \n    let ClipboardJS = window.ClipboardJS || undefined;\n\n    Prism.plugins.toolbar.registerButton('copy-to-clipboard', function(env) {\n      let linkCopy = document.createElement('button');\n      linkCopy.classList.add('prism-button-copy');\n\n      registerClipboard();\n\n      return linkCopy;\n\n      function registerClipboard() {\n        let clip = new ClipboardJS(linkCopy, {\n          'text': function () {\n            return env.code;\n          }\n        });\n\n        clip.on('success', function() {\n          linkCopy.classList.add('prism-button-copy-success');\n          resetText();\n        });\n        clip.on('error', function () {\n          linkCopy.classList.add('prism-button-copy-error');\n          resetText();\n        });\n      }\n\n      function resetText() {\n        setTimeout(function () {\n          linkCopy.classList.remove('prism-button-copy-success');\n          linkCopy.classList.remove('prism-button-copy-error');\n        }, 1600);\n      }\n    });\n  })();\n\u003c/script\u003e\n\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js\"\u003e\u003c/script\u003e\n\u003cscript type=\"text/javascript\"\u003e\nlet pseudocodeCaptionCount = 0;\n(function(d) {\n  d.querySelectorAll(\".pseudocode\").forEach(function(elem) {\n    let pseudocode_options = {\n      indentSize: '1.2em',\n      commentDelimiter: '\\/\\/',\n      lineNumber:  true ,\n      lineNumberPunc: ':',\n      noEnd:  false \n    };\n    pseudocode_options.captionCount = pseudocodeCaptionCount;\n    pseudocodeCaptionCount += 1;\n    pseudocode.renderElement(elem, pseudocode_options);\n  });\n})(document);\n\u003c/script\u003e\n\n\n\n\n\n\n\n\n\n\n\n\u003cscript async=\"\" src=\"/js/center-img.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/right-quote.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/external-link.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/alt-title.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/figure.js\"\u003e\u003c/script\u003e\n\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js\"\u003e\u003c/script\u003e\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/vanilla-back-to-top@latest/dist/vanilla-back-to-top.min.js\"\u003e\u003c/script\u003e\n\u003cscript\u003e\naddBackToTop({\n  diameter: 48\n});\n\u003c/script\u003e\n\n  \u003chr/\u003e\n  \u003cdiv class=\"copyright no-border-bottom\"\u003e\n    \u003cdiv class=\"copyright-author-year\"\u003e\n      \u003cspan\u003eCopyright © 2017-2024 \u003ca href=\"/\"\u003e范叶亮 | Leo Van\u003c/a\u003e\u003c/span\u003e\n    \u003c/div\u003e\n  \u003c/div\u003e\n  \u003c/footer\u003e\n  \u003c/article\u003e",
  "Date": "2018-12-08T00:00:00Z",
  "Author": "范叶亮"
}