{
  "Source": "leovan.me",
  "Title": "利用动态规划求解马尔可夫决策过程 (Planning by Dynamic Programming)",
  "Link": "https://leovan.me/cn/2020/06/planning-by-dynamic-programming/",
  "Content": "\u003carticle class=\"main\"\u003e\n    \u003cheader class=\"content-title\"\u003e\n    \n\u003ch1 class=\"title\"\u003e\n  \n  利用动态规划求解马尔可夫决策过程 (Planning by Dynamic Programming)\n  \n\u003c/h1\u003e\n\u003ch2 class=\"subtitle\"\u003e强化学习系列\u003c/h2\u003e\n\n\n\n\n\n\n\u003ch2 class=\"author-date\"\u003e范叶亮 / \n2020-06-13\u003c/h2\u003e\n\n\n\n\u003ch3 class=\"post-meta\"\u003e\n\n\n\u003cstrong\u003e分类: \u003c/strong\u003e\n\u003ca href=\"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0\"\u003e机器学习\u003c/a\u003e, \u003ca href=\"/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0\"\u003e强化学习\u003c/a\u003e\n\n\n\n\n/\n\n\n\n\n\u003cstrong\u003e标签: \u003c/strong\u003e\n\u003cspan\u003e动态规划\u003c/span\u003e, \u003cspan\u003eDynamic Programming\u003c/span\u003e, \u003cspan\u003eDP\u003c/span\u003e, \u003cspan\u003e马尔可夫决策过程\u003c/span\u003e, \u003cspan\u003eMarkov Decision Process\u003c/span\u003e, \u003cspan\u003eMDP\u003c/span\u003e, \u003cspan\u003e策略评估\u003c/span\u003e, \u003cspan\u003ePolicy Evaluation\u003c/span\u003e, \u003cspan\u003e策略改进\u003c/span\u003e, \u003cspan\u003ePolicy Improvement\u003c/span\u003e, \u003cspan\u003e策略迭代\u003c/span\u003e, \u003cspan\u003ePolicy Iteration\u003c/span\u003e, \u003cspan\u003e价值迭代\u003c/span\u003e, \u003cspan\u003eValue Iteration\u003c/span\u003e, \u003cspan\u003e异步动态规划\u003c/span\u003e, \u003cspan\u003eAsynchronous Dynamic Programming\u003c/span\u003e, \u003cspan\u003e广义策略迭代\u003c/span\u003e, \u003cspan\u003eGeneralised Policy Interation\u003c/span\u003e\n\n\n\n\n/\n\n\n\u003cstrong\u003e字数: \u003c/strong\u003e\n3656\n\u003c/h3\u003e\n\n\n\n\u003chr/\u003e\n\n\n\n    \n    \n    \u003cins class=\"adsbygoogle\" style=\"display:block; text-align:center;\" data-ad-layout=\"in-article\" data-ad-format=\"fluid\" data-ad-client=\"ca-pub-2608165017777396\" data-ad-slot=\"1261604535\"\u003e\u003c/ins\u003e\n    \u003cscript\u003e\n    (adsbygoogle = window.adsbygoogle || []).push({});\n    \u003c/script\u003e\n    \n    \n    \u003c/header\u003e\n\n\n\n\n\u003cblockquote\u003e\n\u003cp\u003e本文为\u003ca href=\"/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/\"\u003e《强化学习系列》\u003c/a\u003e文章\u003cbr/\u003e\n本文内容主要参考自：\u003cbr/\u003e\n1.《强化学习》\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e\u003cbr/\u003e\n2. CS234: Reinforcement Learning \u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e\u003cbr/\u003e\n3. UCL Course on RL \u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch1 id=\"动态规划\"\u003e动态规划\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"/cn/2018/11/computational-complexity-and-dynamic-programming/\"\u003e\u003cstrong\u003e动态规划\u003c/strong\u003e\u003c/a\u003e（Dynamic Programming，DP）是一种用于解决具有如下两个特性问题的通用算法：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e优化问题可以分解为子问题。\u003c/li\u003e\n\u003cli\u003e子问题出现多次并可以被缓存和复用。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e马尔可夫决策过程正符合这两个特性：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e贝尔曼方程给定了迭代过程的分解。\u003c/li\u003e\n\u003cli\u003e价值函数保存并复用了解决方案。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e在强化学习中，DP 的核心思想是使用价值函数来结构化地组织对最优策略的搜索。一旦得到了满足贝尔曼最优方程的价值函数 \u003ccode\u003e$v_*$\u003c/code\u003e 或 \u003ccode\u003e$q_*$\u003c/code\u003e，得到最优策略就容易了。对于任意 \u003ccode\u003e$s \\in \\mathcal{S}$\u003c/code\u003e（状态集合），\u003ccode\u003e$a \\in \\mathcal{A} \\left(s\\right)$\u003c/code\u003e（动作集合）和 \u003ccode\u003e$s\u0026#39; \\in \\mathcal{S}^{+}$\u003c/code\u003e（在分幕式任务下 \u003ccode\u003e$\\mathcal{S}$\u003c/code\u003e 加上一个终止状态），有：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{aligned} v_{*}(s) \u0026amp;=\\max _{a} \\mathbb{E}\\left[R_{t+1}+\\gamma v_{*}\\left(S_{t+1}\\right) | S_{t}=s, A_{t}=a\\right] \\\\ \u0026amp;=\\max _{a} \\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r | s, a\\right)\\left[r+\\gamma v_{*}\\left(s^{\\prime}\\right)\\right] \\end{aligned} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{aligned} q_{*}(s, a) \u0026amp;=\\mathbb{E}\\left[R_{t+1}+\\gamma \\max _{a^{\\prime}} q_{*}\\left(S_{t+1}, a^{\\prime}\\right) | S_{t}=s, A_{t}=a\\right] \\\\ \u0026amp;\\left.=\\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r | s, a\\right)\\left[r+\\gamma \\max _{a^{\\prime}}\\right] q_{*}\\left(s^{\\prime}, a^{\\prime}\\right)\\right] \\end{aligned} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e将贝尔曼方程转化成为近似逼近理想价值函数的递归更新公式，我们就得到了 DP 算法。\u003c/p\u003e\n\u003ch1 id=\"策略评估\"\u003e策略评估\u003c/h1\u003e\n\u003cp\u003e对于一个策略 \u003ccode\u003e$\\pi$\u003c/code\u003e，如何计算其状态价值函数 \u003ccode\u003e$v_{\\pi}$\u003c/code\u003e 被称为\u003cstrong\u003e策略评估\u003c/strong\u003e。对于任意 \u003ccode\u003e$s \\in \\mathcal{S}$\u003c/code\u003e，有：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{aligned} v_{\\pi}(s) \u0026amp; \\doteq \\mathbb{E}_{\\pi}\\left[G_{t} | S_{t}=s\\right] \\\\ \u0026amp;=\\mathbb{E}_{\\pi}\\left[R_{t+1}+\\gamma G_{t+1} | S_{t}=s\\right] \\\\ \u0026amp;=\\mathbb{E}_{\\pi}\\left[R_{t+1}+\\gamma v_{\\pi}\\left(S_{t+1}\\right) | S_{t}=s\\right] \\\\ \u0026amp;=\\sum_{a} \\pi(a | s) \\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r | s, a\\right)\\left[r+\\gamma v_{\\pi}\\left(s^{\\prime}\\right)\\right] \\end{aligned} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中 \u003ccode\u003e$\\pi \\left(a | s\\right)$\u003c/code\u003e 表示在环境 \u003ccode\u003e$s$\u003c/code\u003e 中智能体在策略 \u003ccode\u003e$\\pi$\u003c/code\u003e 下采取动作 \u003ccode\u003e$a$\u003c/code\u003e 的概率。只要 \u003ccode\u003e$\\gamma \u0026lt; 1$\u003c/code\u003e 或者任何状态在 \u003ccode\u003e$\\pi$\u003c/code\u003e 下都能保证最后终止，则 \u003ccode\u003e$v_{\\pi}$\u003c/code\u003e 唯一存在。\u003c/p\u003e\n\u003cp\u003e考虑一个近似的价值函数序列 \u003ccode\u003e$v_0, v_1, \\cdots$\u003c/code\u003e，从 \u003ccode\u003e$\\mathcal{S}^{+}$\u003c/code\u003e 映射到 \u003ccode\u003e$\\mathbb{R}$\u003c/code\u003e，初始的近似值 \u003ccode\u003e$v_0$\u003c/code\u003e 可以任意选取（除了终止状态必须为 0 外）。下一轮迭代的近似可以使用 \u003ccode\u003e$v_{\\pi}$\u003c/code\u003e 的贝尔曼方程进行更新，对于任意 \u003ccode\u003e$s \\in \\mathcal{S}$\u003c/code\u003e 有：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{aligned} v_{k+1}(s) \u0026amp; \\doteq \\mathbb{E}_{\\pi}\\left[R_{t+1}+\\gamma v_{k}\\left(S_{t+1}\\right) | S_{t}=s\\right] \\\\ \u0026amp;=\\sum_{a} \\pi(a | s) \\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r | s, a\\right)\\left[r+\\gamma v_{k}\\left(s^{\\prime}\\right)\\right] \\end{aligned} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e显然，\u003ccode\u003e$v_k = v_{\\pi}$\u003c/code\u003e 是这个更新规则的一个不动点。在保证 \u003ccode\u003e$v_{\\pi}$\u003c/code\u003e 存在的条件下，序列 \u003ccode\u003e$\\left\\{v_k\\right\\}$\u003c/code\u003e 在 \u003ccode\u003e$k \\to \\infty$\u003c/code\u003e 时将会收敛到 \u003ccode\u003e$v_{\\pi}$\u003c/code\u003e，这个算法称作 \u003cstrong\u003e迭代策略评估\u003c/strong\u003e。\u003c/p\u003e\n\u003ch1 id=\"策略改进\"\u003e策略改进\u003c/h1\u003e\n\u003cp\u003e对于任意一个确定的策略 \u003ccode\u003e$\\pi$\u003c/code\u003e，我们已经确定了它的价值函数 \u003ccode\u003e$v_{\\pi}$\u003c/code\u003e。对于某个状态 \u003ccode\u003e$s$\u003c/code\u003e，我们想知道是否应该选择一个不同于给定的策略的动作 \u003ccode\u003e$a \\neq \\pi \\left(s\\right)$\u003c/code\u003e。如果从状态 \u003ccode\u003e$s$\u003c/code\u003e 继续使用现有策略，则最后的结果就是 \u003ccode\u003e$v \\left(s\\right)$\u003c/code\u003e，但我们并不知道换成一个新策略后是得到更好的结果还是更坏的结果。一种解决方法是在状态 \u003ccode\u003e$s$\u003c/code\u003e 选择动作 \u003ccode\u003e$a$\u003c/code\u003e 后，继续遵循现有的策略 \u003ccode\u003e$\\pi$\u003c/code\u003e，则这种方法的价值为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{aligned} q_{\\pi}(s, a) \u0026amp; \\doteq \\mathbb{E}\\left[R_{t+1}+\\gamma v_{\\pi}\\left(S_{t+1}\\right) | S_{t}=s, A_{t}=a\\right] \\\\ \u0026amp;=\\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r | s, a\\right)\\left[r+\\gamma v_{\\pi}\\left(s^{\\prime}\\right)\\right] \\end{aligned} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e一个关键的准则就是这个值是大于还是小于 \u003ccode\u003e$v_{\\pi} \\left(s\\right)$\u003c/code\u003e。如果这个值更大，则说明在状态 \u003ccode\u003e$s$\u003c/code\u003e 选择动作 \u003ccode\u003e$a$\u003c/code\u003e，然后继续使用策略 \u003ccode\u003e$\\pi$\u003c/code\u003e 会比使用始终使用策略 \u003ccode\u003e$\\pi$\u003c/code\u003e 更优。\u003c/p\u003e\n\u003cp\u003e上述情况是\u003cstrong\u003e策略改进定理\u003c/strong\u003e的一个特例，一般来说，如果 \u003ccode\u003e$\\pi$\u003c/code\u003e 和 \u003ccode\u003e$\\pi\u0026#39;$\u003c/code\u003e 是任意两个确定的策略，对于任意 \u003ccode\u003e$s \\in \\mathcal{S}$\u003c/code\u003e：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ q_{\\pi}\\left(s, \\pi^{\\prime}(s)\\right) \\geq v_{\\pi}(s) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e则称策略 \u003ccode\u003e$\\pi\u0026#39;$\u003c/code\u003e 相比于 \u003ccode\u003e$\\pi$\u003c/code\u003e 一样好或更好。也就是说，对于任意状态 \u003ccode\u003e$s \\in \\mathcal{S}$\u003c/code\u003e，这样肯定能得到一样或更好的期望回报：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ v_{\\pi^{\\prime}}(s) \\geq v_{\\pi}(s) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e延伸到所有状态和所有可能的动作，即在每个状态下根据 \u003ccode\u003e$q_{\\pi} \\left(s, a\\right)$\u003c/code\u003e 选择一个最优的，换言之，考虑一个新的\u003cstrong\u003e贪心\u003c/strong\u003e策略 \u003ccode\u003e$\\pi\u0026#39;$\u003c/code\u003e，满足：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{aligned} \\pi^{\\prime}(s) \u0026amp; \\doteq \\underset{a}{\\arg \\max } q_{\\pi}(s, a) \\\\ \u0026amp;=\\underset{a}{\\arg \\max } \\mathbb{E}\\left[R_{t+1}+\\gamma v_{\\pi}\\left(S_{t+1}\\right) | S_{t}=s, A_{t}=a\\right] \\\\ \u0026amp;=\\underset{a}{\\arg \\max } \\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r | s, a\\right)\\left[r+\\gamma v_{\\pi}\\left(s^{\\prime}\\right)\\right] \\end{aligned} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e这样构造出的贪心策略满足策略改进定理的条件，所以它和原策略相比一样好或更好。这种根据原策略的价值函数执行贪心算法，来构造一个更好策略的过程称之为\u003cstrong\u003e策略改进\u003c/strong\u003e。如果新的贪心策略 \u003ccode\u003e$\\pi\u0026#39;$\u003c/code\u003e 和原策略 \u003ccode\u003e$\\pi$\u003c/code\u003e 一样好而不是更好，则有 \u003ccode\u003e$v_{\\pi} = v_{\\pi\u0026#39;}$\u003c/code\u003e，对任意 \u003ccode\u003e$s \\in \\mathcal{S}$\u003c/code\u003e：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{aligned} v_{\\pi^{\\prime}}(s) \u0026amp;=\\max _{a} \\mathbb{E}\\left[R_{t+1}+\\gamma v_{\\pi^{\\prime}}\\left(S_{t+1}\\right) | S_{t}=s, A_{t}=a\\right] \\\\ \u0026amp;=\\max _{a} \\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r | s, a\\right)\\left[r+\\gamma v_{\\pi^{\\prime}}\\left(s^{\\prime}\\right)\\right] \\end{aligned} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e这同贝尔曼方程完全相同，因此 \u003ccode\u003e$v_{\\pi}$\u003c/code\u003e 一定与 \u003ccode\u003e$v_*$\u003c/code\u003e 相同，\u003ccode\u003e$\\pi$\u003c/code\u003e 与 \u003ccode\u003e$\\pi\u0026#39;$\u003c/code\u003e 均必须为最优策略。因此，在除了原策略即为最优策略的情况下，策略改进一定会给出一个更优的结果。\u003c/p\u003e\n\u003ch1 id=\"策略迭代\"\u003e策略迭代\u003c/h1\u003e\n\u003cp\u003e一个策略 \u003ccode\u003e$\\pi$\u003c/code\u003e 根据 \u003ccode\u003e$v_{\\pi}$\u003c/code\u003e 产生了一个更好的策略 \u003ccode\u003e$\\pi\u0026#39;$\u003c/code\u003e，进而我们可以通过计算 \u003ccode\u003e$v_{\\pi\u0026#39;}$\u003c/code\u003e 来得到一个更优的策略 \u003ccode\u003e$\\pi\u0026#39;\u0026#39;$\u003c/code\u003e。这样一个链式的方法可以得到一个不断改进的策略和价值函数序列：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\pi_{0} \\stackrel{E}{\\longrightarrow} v_{\\pi_{0}} \\stackrel{I}{\\longrightarrow} \\pi_{1} \\stackrel{E}{\\longrightarrow} v_{\\pi_{1}} \\stackrel{I}{\\longrightarrow} \\pi_{2} \\stackrel{E}{\\longrightarrow} \\cdots \\stackrel{I}{\\longrightarrow} \\pi_{*} \\stackrel{E}{\\longrightarrow} v_{*} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中 \u003ccode\u003e$\\stackrel{E}{\\longrightarrow}$\u003c/code\u003e 表示策略评估，\u003ccode\u003e$\\stackrel{I}{\\longrightarrow}$\u003c/code\u003e 表示策略改进。每一个策略都能保证同前一个一样或者更优，由于一个有限 MDP 必然只有有限种策略，所以在有限次的迭代后，这种方法一定收敛到一个最优的策略与最优价值函数。这种寻找最优策略的方法叫做\u003cstrong\u003e策略迭代\u003c/strong\u003e。整个策略迭代算法如下：\u003c/p\u003e\n\n\n\u003clink rel=\"stylesheet\" type=\"text/css\" href=\"//cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css\"/\u003e\n\n\n\u003cdiv\u003e\u003cpre class=\"pseudocode\"\u003e\\begin{algorithm}\n\\caption{迭代策略算法}\n\\begin{algorithmic}\n\\FUNCTION{PolicyIteration}{}\n\\STATE \\COMMENT{初始化}\n\\FOR{$s \\in \\mathcal{S}$}\n  \\STATE 初始化 $V \\left(s\\right) \\in \\mathbb{R}$\n  \\STATE 初始化 $\\pi \\left(s\\right) \\in \\mathcal{A} \\left(s\\right)$\n\\ENDFOR\n\\WHILE{true}\n  \\STATE \\COMMENT{策略评估}\n  \\REPEAT\n    \\STATE $\\Delta \\gets 0$\n    \\FOR{$s \\in \\mathcal{S}$}\n      \\STATE $v \\gets V \\left(s\\right)$\n      \\STATE $V \\left(s\\right) \\gets \\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r | s, \\pi \\left(s\\right)\\right)\\left[r+\\gamma V\\left(s^{\\prime}\\right)\\right]$\n      \\STATE $\\Delta \\gets \\max\\left(\\Delta, \\left|v - V \\left(s\\right)\\right|\\right)$\n    \\ENDFOR\n  \\UNTIL{$\\Delta \u0026lt; \\theta$}\n  \\STATE \\COMMENT{策略改进}\n  \\STATE policy-stable $\\gets$ true\n  \\FOR{$s \\in \\mathcal{S}$}\n    \\STATE $\\pi\u0026#39; \\left(s\\right) \\gets \\pi \\left(s\\right)$\n    \\STATE $\\pi \\left(s\\right) \\gets \\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r | s, a\\right)\\left[r+\\gamma V\\left(s^{\\prime}\\right)\\right]$\n    \\IF{$\\pi\u0026#39; \\left(s\\right) \\neq \\pi \\left(s\\right)$}\n      \\STATE policy-stable $\\gets$ flase\n    \\ENDIF\n  \\ENDFOR\n  \\IF{policy-stable $=$ true}\n    \\BREAK\n  \\ENDIF\n\\ENDWHILE\n\\RETURN $V, \\pi$\n\\ENDFUNCTION\n\\end{algorithmic}\n\\end{algorithm}\n\u003c/pre\u003e\u003c/div\u003e\n\n\u003cp\u003e以\u003cstrong\u003e杰克租车（Jack’s Car）问题\u003c/strong\u003e为例：杰克在两地运营租车公司，每租出一辆车获得 10 元收益，为了保证每个地点有车可用，杰克需要夜间在两地之间移动车辆，每辆车的移动代价为 2 元。假设每个地点租车和还车的数量符合泊松分布 \u003ccode\u003e$\\dfrac{\\lambda^n}{n!} e^{- \\lambda}$\u003c/code\u003e，其中 \u003ccode\u003e$\\lambda$\u003c/code\u003e 为期望值，租车的 \u003ccode\u003e$\\lambda$\u003c/code\u003e 在两地分别为 3 和 4，还车的 \u003ccode\u003e$\\lambda$\u003c/code\u003e 在两地分别为 3 和 2。假设任何一个地点不超过 20 辆车，每天最多移动 5 辆车，折扣率 \u003ccode\u003e$\\gamma = 0.9$\u003c/code\u003e，将问题描述为一个持续的有限 MPD，时刻按天计算，状态为每天结束时每个地点的车辆数，动作则为夜间在两个地点之间移动的车辆数。策略从不移动任何车辆开始，整个策略迭代过程如下图所示：\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-06-13-planning-by-dynamic-programming/car-rental-policy-history.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n\u003c/figure\u003e\n\u003cp\u003e上例代码实现请参见\u003ca href=\"https://github.com/leovan/leovan.me/blob/main/static/scripts/cn/2020-06-13-planning-by-dynamic-programming/car_rental.py\"\u003e这里\u003c/a\u003e。\u003c/p\u003e\n\u003ch1 id=\"价值迭代\"\u003e价值迭代\u003c/h1\u003e\n\u003cp\u003e策略迭代算法的一个缺点是每一次迭代都涉及了策略评估，这是一个需要多次遍历状态集合的迭代过程。如果策略评估是迭代进行的，那么收敛到 \u003ccode\u003e$v_{\\pi}$\u003c/code\u003e 理论上在极限处才成立，实际中不必等到其完全收敛，可以提前截断策略评估过程。有多种方式可以截断策略迭代中的策略评估步骤，并且不影响其收敛，一种重要的特殊情况是在一次遍历后即刻停止策略评估，该算法称为\u003cstrong\u003e价值迭代\u003c/strong\u003e。可以将此表示为结合了策略改进与阶段策略评估的简单更新公式，对任意 \u003ccode\u003e$s \\in \\mathcal{S}$\u003c/code\u003e：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{aligned} v_{k+1}(s) \u0026amp; \\doteq \\max _{a} \\mathbb{E}\\left[R_{t+1}+\\gamma v_{k}\\left(S_{t+1}\\right) | S_{t}=s, A_{t}=a\\right] \\\\ \u0026amp;=\\max _{a} \\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r | s, a\\right)\\left[r+\\gamma v_{k}\\left(s^{\\prime}\\right)\\right] \\end{aligned} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e可以证明，对任意 \u003ccode\u003e$v_0$\u003c/code\u003e，在 \u003ccode\u003e$v_*$\u003c/code\u003e 存在的条件下，序列 \u003ccode\u003e$\\left\\{v_k\\right\\}$\u003c/code\u003e 都可以收敛到 \u003ccode\u003e$v_*$\u003c/code\u003e。整个价值迭代算法如下：\u003c/p\u003e\n\n\n\u003cdiv\u003e\u003cpre class=\"pseudocode\"\u003e\\begin{algorithm}\n\\caption{价值迭代算法}\n\\begin{algorithmic}\n\\FUNCTION{ValueIteration}{}\n\\STATE \\COMMENT{初始化}\n\\FOR{$s \\in \\mathcal{S}^{+}$}\n  \\STATE 初始化 $V \\left(s\\right)$，其中 $V \\left(\\text{终止状态}\\right) = 0$\n\\ENDFOR\n\\STATE \\COMMENT{价值迭代}\n\\REPEAT\n  \\STATE $\\Delta \\gets 0$\n  \\FOR{$s \\in \\mathcal{S}$}\n    \\STATE $v \\gets V \\left(s\\right)$\n    \\STATE $V \\left(s\\right) \\gets\\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r | s, a\\right)\\left[r+\\gamma V\\left(s^{\\prime}\\right)\\right]$\n    \\STATE $\\Delta \\gets \\max\\left(\\Delta, \\left|v - V \\left(s\\right)\\right|\\right)$\n  \\ENDFOR\n\\UNTIL{$\\Delta \u0026lt; \\theta$}\n\\STATE 输出一个确定的策略 $\\pi \\approx \\pi_*$ 使得 $\\pi(s)=\\arg \\max _{a} \\sum_{s^{\\prime}, r} p\\left(s^{\\prime}, r | s, a\\right)\\left[r+\\gamma V\\left(s^{\\prime}\\right)\\right]$\n\\RETURN $V, \\pi$\n\\ENDFUNCTION\n\\end{algorithmic}\n\\end{algorithm}\n\u003c/pre\u003e\u003c/div\u003e\n\n\u003cp\u003e以**赌徒问题（Gambler’s Problem）**为例：一个赌徒下注猜一系列抛硬币实验的结果，如果正面朝上则获得这一次下注的钱，如果背面朝上则失去这一次下注的钱，游戏在达到目标收益 100 元或全部输光时结束。每抛一次硬币，赌徒必须从他的赌资中选取一个整数来下注，这个问题可以表示为一个非折扣的分幕式有限 MDP。状态为赌徒的赌资 \u003ccode\u003e$s \\in \\left\\{1, 2, \\cdots, 99\\right\\}$\u003c/code\u003e，动作为赌徒下注的金额 \u003ccode\u003e$a \\in \\left\\{0, 1, \\cdots, \\min \\left(s, 100 - s\\right)\\right\\}$\u003c/code\u003e，收益在一般情况下为 0，只有在赌徒达到获利 100 元的终止状态时为 1。\u003c/p\u003e\n\u003cp\u003e令 \u003ccode\u003e$p_h$\u003c/code\u003e 为抛硬币正面朝上的概率，如果 \u003ccode\u003e$p_h$\u003c/code\u003e 已知，那么整个问题可以由价值迭代或其他类似算法解决。下图为当 \u003ccode\u003e$p_h = 0.4$\u003c/code\u003e 时，价值迭代连续遍历得到的价值函数和最后的策略。\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-06-13-planning-by-dynamic-programming/gamblers-problem-value-iteration.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n\u003c/figure\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-06-13-planning-by-dynamic-programming/gamblers-problem-optimal-policy.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n\u003c/figure\u003e\n\u003cp\u003e上例代码实现请参见\u003ca href=\"https://github.com/leovan/leovan.me/blob/main/static/scripts/cn/2020-06-13-planning-by-dynamic-programming/gamblers_problem.py\"\u003e这里\u003c/a\u003e。\u003c/p\u003e\n\u003ch1 id=\"异步动态规划\"\u003e异步动态规划\u003c/h1\u003e\n\u003cp\u003e之前讨论的 DP 方法的一个主要缺点是它们涉及对 MDP 的整个状态集的操作，如果状态集很大，即使单次遍历也会十分昂贵。\u003cstrong\u003e异步动态规划\u003c/strong\u003e算法是一类就地迭代的 DP 算法，其不以系统遍历状态集的形式来组织算法。这些算法使用任意可用的状态值，以任意顺序来更新状态值，在某些状态的值更新一次之前，另一些状态的值可能已经更新了好几次。然而为了正确收敛，异步算法必须要不断地更新所有状态的值：在某个计算节点后，它不能忽略任何一个状态。\u003c/p\u003e\n\u003ch1 id=\"广义策略迭代\"\u003e广义策略迭代\u003c/h1\u003e\n\u003cp\u003e策略迭代包含两个同时进行的相互作用的流程，一个使得价值函数与当前策略一致（策略评估），另一个根据当前价值函数贪心地更新策略（策略改进）。在策略迭代中，这两个流程交替进行，每个流程都在另一个开始前完成。然而这也不是必须的，在异步方法中，评估和改进流程则以更细的粒度交替进行。我们利用**广义策略迭代（GPI）**一词来指代策略评估和策略改进相互作用的一般思路，与这两个流程的力度和其他细节无关。\u003c/p\u003e\n\u003cp\u003e几乎所有的强化学习方法都可以被描述为 GPI，几乎所有方法都包含明确定义的策略和价值函数。策略总是基于特定的价值函数进行改进，价值函数也始终会向对应特定策略的真实价值函数收敛。\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-06-13-planning-by-dynamic-programming/generalized-policy-iteration.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n\u003c/figure\u003e\n\u003cp\u003eGPI 的评估和改进流程可以视为两个约束或目标之间的相互作用的流程。每个流程都把价值函数或策略推向其中的一条线，该线代表了对于两个目标中的某一个目标的解决方案，如下图所示：\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-06-13-planning-by-dynamic-programming/policy-improvement.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n\u003c/figure\u003e\n\u003cdiv class=\"footnotes\" role=\"doc-endnotes\"\u003e\n\u003chr/\u003e\n\u003col\u003e\n\u003cli id=\"fn:1\"\u003e\n\u003cp\u003eSutton, R. S., \u0026amp; Barto, A. G. (2018). \u003cem\u003eReinforcement learning: An introduction\u003c/em\u003e. MIT press. \u003ca href=\"#fnref:1\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:2\"\u003e\n\u003cp\u003eCS234: Reinforcement Learning \u003ca href=\"http://web.stanford.edu/class/cs234/index.html\"\u003ehttp://web.stanford.edu/class/cs234/index.html\u003c/a\u003e \u003ca href=\"#fnref:2\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:3\"\u003e\n\u003cp\u003eUCL Course on RL \u003ca href=\"https://www.davidsilver.uk/teaching\"\u003ehttps://www.davidsilver.uk/teaching\u003c/a\u003e \u003ca href=\"#fnref:3\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/div\u003e\n\n\n\n\n\n\u003cdiv class=\"donate\"\u003e\n  \u003cdiv class=\"donate-header\"\u003e\u003c/div\u003e\n  \u003cdiv class=\"donate-slug\" id=\"donate-slug\"\u003eplanning-by-dynamic-programming\u003c/div\u003e\n  \u003cbutton class=\"donate-button\"\u003e赞 赏\u003c/button\u003e\n  \u003cdiv class=\"donate-footer\"\u003e「真诚赞赏，手留余香」\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"donate-modal-wrapper\"\u003e\n  \u003cdiv class=\"donate-modal\"\u003e\n    \u003cdiv class=\"donate-box\"\u003e\n      \u003cdiv class=\"donate-box-content\"\u003e\n        \u003cdiv class=\"donate-box-content-inner\"\u003e\n          \u003cdiv class=\"donate-box-header\"\u003e「真诚赞赏，手留余香」\u003c/div\u003e\n          \u003cdiv class=\"donate-box-body\"\u003e\n            \u003cdiv class=\"donate-box-money\"\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-2\" data-v=\"2\" data-unchecked=\"￥ 2\" data-checked=\"2 元\"\u003e￥ 2\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-5\" data-v=\"5\" data-unchecked=\"￥ 5\" data-checked=\"5 元\"\u003e￥ 5\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-10\" data-v=\"10\" data-unchecked=\"￥ 10\" data-checked=\"10 元\"\u003e￥ 10\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-50\" data-v=\"50\" data-unchecked=\"￥ 50\" data-checked=\"50 元\"\u003e￥ 50\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-100\" data-v=\"100\" data-unchecked=\"￥ 100\" data-checked=\"100 元\"\u003e￥ 100\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-custom\" data-v=\"custom\" data-unchecked=\"任意金额\" data-checked=\"任意金额\"\u003e任意金额\u003c/button\u003e\n            \u003c/div\u003e\n            \u003cdiv class=\"donate-box-pay\"\u003e\n              \u003cimg class=\"donate-box-pay-qrcode\" id=\"donate-box-pay-qrcode\" src=\"\"/\u003e\n            \u003c/div\u003e\n          \u003c/div\u003e\n          \u003cdiv class=\"donate-box-footer\"\u003e\n            \u003cdiv class=\"donate-box-pay-method donate-box-pay-method-checked\" data-v=\"wechat-pay\"\u003e\n              \u003cimg class=\"donate-box-pay-method-image\" id=\"donate-box-pay-method-image-wechat-pay\" src=\"\"/\u003e\n            \u003c/div\u003e\n            \u003cdiv class=\"donate-box-pay-method\" data-v=\"alipay\"\u003e\n              \u003cimg class=\"donate-box-pay-method-image\" id=\"donate-box-pay-method-image-alipay\" src=\"\"/\u003e\n            \u003c/div\u003e\n          \u003c/div\u003e\n        \u003c/div\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n    \u003cbutton type=\"button\" class=\"donate-box-close-button\"\u003e\n      \u003csvg class=\"donate-box-close-button-icon\" fill=\"#fff\" viewBox=\"0 0 24 24\" width=\"24\" height=\"24\"\u003e\u003cpath d=\"M13.486 12l5.208-5.207a1.048 1.048 0 0 0-.006-1.483 1.046 1.046 0 0 0-1.482-.005L12 10.514 6.793 5.305a1.048 1.048 0 0 0-1.483.005 1.046 1.046 0 0 0-.005 1.483L10.514 12l-5.208 5.207a1.048 1.048 0 0 0 .006 1.483 1.046 1.046 0 0 0 1.482.005L12 13.486l5.207 5.208a1.048 1.048 0 0 0 1.483-.006 1.046 1.046 0 0 0 .005-1.482L13.486 12z\" fill-rule=\"evenodd\"\u003e\u003c/path\u003e\u003c/svg\u003e\n    \u003c/button\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\u003cscript type=\"text/javascript\" src=\"/js/donate.js\"\u003e\u003c/script\u003e\n\n\n  \u003cfooter\u003e\n  \n\u003cnav class=\"post-nav\"\u003e\n  \u003cspan class=\"nav-prev\"\u003e← \u003ca href=\"/cn/2020/06/bayesian-optimization/\"\u003e贝叶斯优化 (Bayesian Optimization)\u003c/a\u003e\u003c/span\u003e\n  \u003cspan class=\"nav-next\"\u003e\u003ca href=\"/cn/2020/07/model-free-policy-prediction-and-control-monte-carlo-learning/\"\u003e无模型策略预测和控制 - 蒙特卡洛方法 (Model-Free Policy Prediction and Control - Monte-Carlo Learning)\u003c/a\u003e →\u003c/span\u003e\n\u003c/nav\u003e\n\n\n\n\n\u003cins class=\"adsbygoogle\" style=\"display:block; text-align:center;\" data-ad-layout=\"in-article\" data-ad-format=\"fluid\" data-ad-client=\"ca-pub-2608165017777396\" data-ad-slot=\"8302038603\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n  (adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/js-cookie@3.0.5/dist/js.cookie.min.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"/js/toggle-theme.js\"\u003e\u003c/script\u003e\n\n\n\u003cscript src=\"/js/no-highlight.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"/js/math-code.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"/js/heading-anchor.js\"\u003e\u003c/script\u003e\n\n\n\n\u003csection class=\"comments\"\u003e\n\u003cscript src=\"https://giscus.app/client.js\" data-repo=\"leovan/leovan.me\" data-repo-id=\"MDEwOlJlcG9zaXRvcnkxMTMxOTY0Mjc=\" data-category=\"Comments\" data-category-id=\"DIC_kwDOBr89i84CT-R7\" data-mapping=\"pathname\" data-strict=\"1\" data-reactions-enabled=\"1\" data-emit-metadata=\"0\" data-input-position=\"top\" data-theme=\"preferred_color_scheme\" data-lang=\"zh-CN\" data-loading=\"lazy\" crossorigin=\"anonymous\" defer=\"\"\u003e\n\u003c/script\u003e\n\u003c/section\u003e\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"//cdn.jsdelivr.net/npm/prismjs@1.29.0/prism.min.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"//cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"//cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/toolbar/prism-toolbar.min.js\"\u003e\u003c/script\u003e\n\u003cscript\u003e\n  (function() {\n    if (!self.Prism) {\n      return;\n    }\n\n    \n    Prism.languages.dos = Prism.languages.powershell;\n    Prism.languages.gremlin = Prism.languages.groovy;\n\n    let languages = {\n      'r': 'R', 'python': 'Python', 'xml': 'XML', 'html': 'HTML',\n      'yaml': 'YAML', 'latex': 'LaTeX', 'tex': 'TeX',\n      'powershell': 'PowerShell', 'javascript': 'JavaScript',\n      'dos': 'DOS', 'qml': 'QML', 'json': 'JSON', 'bash': 'Bash',\n      'text': 'Text', 'txt': 'Text', 'sparql': 'SPARQL',\n      'gremlin': 'Gremlin', 'cypher': 'Cypher', 'ngql': 'nGQL',\n      'shell': 'Shell', 'sql': 'SQL', 'apacheconf': 'Apache Configuration', 'c': 'C', 'css': 'CSS'\n    };\n\n    Prism.hooks.add('before-highlight', function(env) {\n      if (env.language !== 'plain') {\n        let language = languages[env.language] || env.language;\n        env.element.setAttribute('data-language', language);\n      }\n    });\n\n    \n    let ClipboardJS = window.ClipboardJS || undefined;\n\n    Prism.plugins.toolbar.registerButton('copy-to-clipboard', function(env) {\n      let linkCopy = document.createElement('button');\n      linkCopy.classList.add('prism-button-copy');\n\n      registerClipboard();\n\n      return linkCopy;\n\n      function registerClipboard() {\n        let clip = new ClipboardJS(linkCopy, {\n          'text': function () {\n            return env.code;\n          }\n        });\n\n        clip.on('success', function() {\n          linkCopy.classList.add('prism-button-copy-success');\n          resetText();\n        });\n        clip.on('error', function () {\n          linkCopy.classList.add('prism-button-copy-error');\n          resetText();\n        });\n      }\n\n      function resetText() {\n        setTimeout(function () {\n          linkCopy.classList.remove('prism-button-copy-success');\n          linkCopy.classList.remove('prism-button-copy-error');\n        }, 1600);\n      }\n    });\n  })();\n\u003c/script\u003e\n\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js\"\u003e\u003c/script\u003e\n\u003cscript type=\"text/javascript\"\u003e\nlet pseudocodeCaptionCount = 0;\n(function(d) {\n  d.querySelectorAll(\".pseudocode\").forEach(function(elem) {\n    let pseudocode_options = {\n      indentSize: '1.2em',\n      commentDelimiter: '\\/\\/',\n      lineNumber:  true ,\n      lineNumberPunc: ':',\n      noEnd:  false \n    };\n    pseudocode_options.captionCount = pseudocodeCaptionCount;\n    pseudocodeCaptionCount += 1;\n    pseudocode.renderElement(elem, pseudocode_options);\n  });\n})(document);\n\u003c/script\u003e\n\n\n\n\n\n\n\n\n\n\n\n\u003cscript async=\"\" src=\"/js/center-img.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/right-quote.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/external-link.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/alt-title.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/figure.js\"\u003e\u003c/script\u003e\n\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js\"\u003e\u003c/script\u003e\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/vanilla-back-to-top@latest/dist/vanilla-back-to-top.min.js\"\u003e\u003c/script\u003e\n\u003cscript\u003e\naddBackToTop({\n  diameter: 48\n});\n\u003c/script\u003e\n\n  \u003chr/\u003e\n  \u003cdiv class=\"copyright no-border-bottom\"\u003e\n    \u003cdiv class=\"copyright-author-year\"\u003e\n      \u003cspan\u003eCopyright © 2017-2024 \u003ca href=\"/\"\u003e范叶亮 | Leo Van\u003c/a\u003e\u003c/span\u003e\n    \u003c/div\u003e\n  \u003c/div\u003e\n  \u003c/footer\u003e\n  \u003c/article\u003e",
  "Date": "2020-06-13T00:00:00Z",
  "Author": "范叶亮"
}