{
  "Source": "leovan.me",
  "Title": "马尔可夫决策过程 (Markov Decision Process)",
  "Link": "https://leovan.me/cn/2020/05/markov-decision-process/",
  "Content": "\u003carticle class=\"main\"\u003e\n    \u003cheader class=\"content-title\"\u003e\n    \n\u003ch1 class=\"title\"\u003e\n  \n  马尔可夫决策过程 (Markov Decision Process)\n  \n\u003c/h1\u003e\n\u003ch2 class=\"subtitle\"\u003e强化学习系列\u003c/h2\u003e\n\n\n\n\n\n\n\u003ch2 class=\"author-date\"\u003e范叶亮 / \n2020-05-23\u003c/h2\u003e\n\n\n\n\u003ch3 class=\"post-meta\"\u003e\n\n\n\u003cstrong\u003e分类: \u003c/strong\u003e\n\u003ca href=\"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0\"\u003e机器学习\u003c/a\u003e, \u003ca href=\"/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0\"\u003e强化学习\u003c/a\u003e\n\n\n\n\n/\n\n\n\n\n\u003cstrong\u003e标签: \u003c/strong\u003e\n\u003cspan\u003e强化学习\u003c/span\u003e, \u003cspan\u003eReinforcement Learning\u003c/span\u003e, \u003cspan\u003e马尔可夫模型\u003c/span\u003e, \u003cspan\u003eMarkov Model\u003c/span\u003e, \u003cspan\u003e马尔可夫链\u003c/span\u003e, \u003cspan\u003eMarkov Chain\u003c/span\u003e, \u003cspan\u003eMC\u003c/span\u003e, \u003cspan\u003e隐马尔可夫模型\u003c/span\u003e, \u003cspan\u003eHidden Markov Model\u003c/span\u003e, \u003cspan\u003eHMM\u003c/span\u003e, \u003cspan\u003e马尔可夫奖励过程\u003c/span\u003e, \u003cspan\u003eMarkov Reward Process\u003c/span\u003e, \u003cspan\u003eMRP\u003c/span\u003e, \u003cspan\u003e分幕\u003c/span\u003e, \u003cspan\u003e马尔可夫决策过程\u003c/span\u003e, \u003cspan\u003eMarkov Decision Process\u003c/span\u003e, \u003cspan\u003eMDP\u003c/span\u003e, \u003cspan\u003e部分可观测马尔可夫决策过程\u003c/span\u003e, \u003cspan\u003ePartially Observable Markov Decision Process\u003c/span\u003e, \u003cspan\u003ePOMDP\u003c/span\u003e\n\n\n\n\n/\n\n\n\u003cstrong\u003e字数: \u003c/strong\u003e\n2940\n\u003c/h3\u003e\n\n\n\n\u003chr/\u003e\n\n\n\n    \n    \n    \u003cins class=\"adsbygoogle\" style=\"display:block; text-align:center;\" data-ad-layout=\"in-article\" data-ad-format=\"fluid\" data-ad-client=\"ca-pub-2608165017777396\" data-ad-slot=\"1261604535\"\u003e\u003c/ins\u003e\n    \u003cscript\u003e\n    (adsbygoogle = window.adsbygoogle || []).push({});\n    \u003c/script\u003e\n    \n    \n    \u003c/header\u003e\n\n\n\n\n\u003cblockquote\u003e\n\u003cp\u003e本文为\u003ca href=\"/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/\"\u003e《强化学习系列》\u003c/a\u003e文章\u003cbr/\u003e\n本文内容主要参考自：\u003cbr/\u003e\n1.《强化学习》\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e\u003cbr/\u003e\n2. CS234: Reinforcement Learning \u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e\u003cbr/\u003e\n3. UCL Course on RL \u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch1 id=\"马尔可夫模型\"\u003e马尔可夫模型\u003c/h1\u003e\n\u003cp\u003e马尔可夫模型是一种用于序列数据建模的随机模型，其假设未来的状态仅取决于当前的状态，即：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\mathbb{P} \\left[S_{t+1} | S_t\\right] = \\mathbb{P} \\left[S_{t+1} | S_1, \\cdots, S_t\\right] $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e也就是认为当前状态捕获了历史中所有相关的信息。根据系统状态是否完全可被观测以及系统是自动的还是受控的，可以将马尔可夫模型分为 4 种，如下表所示：\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003e状态状态完全可被观测\u003c/th\u003e\n\u003cth\u003e系统状态不是完全可被观测\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e状态是自动的\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e马尔可夫链（MC）\u003c/td\u003e\n\u003ctd\u003e隐马尔可夫模型（HMM）\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003cstrong\u003e系统是受控的\u003c/strong\u003e\u003c/td\u003e\n\u003ctd\u003e马尔可夫决策过程（MDP）\u003c/td\u003e\n\u003ctd\u003e部分可观测马尔可夫决策过程（POMDP）\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e马尔可夫链（Markov Chain，MC）为从一个状态到另一个状态转换的随机过程，当马尔可夫链的状态只能部分被观测到时，即为\u003ca href=\"/cn/2020/05/hmm-crf-and-sequence-labeling/\"\u003e隐马尔可夫模型（Hidden Markov Model，HMM）\u003c/a\u003e，也就是说观测值与系统状态有关，但通常不足以精确地确定状态。马尔可夫决策过程（Markov Decision Process，MDP）也是马尔可夫链，但其状态转移取决于当前状态和采取的动作，通常一个马尔可夫决策过程用于计算依据期望回报最大化某些效用的行动策略。部分可观测马尔可夫决策过程（Partially Observable Markov Decision Process，POMDP）即为系统状态仅部分可见情况下的马尔可夫决策过程。\u003c/p\u003e\n\u003ch1 id=\"马尔可夫过程\"\u003e马尔可夫过程\u003c/h1\u003e\n\u003cp\u003e对于一个马尔可夫状态 \u003ccode\u003e$s$\u003c/code\u003e 和一个后继状态 \u003ccode\u003e$s\u0026#39;$\u003c/code\u003e，状态转移概率定义为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\mathcal{P}_{ss\u0026#39;} = \\mathbb{P} \\left[S_t = s\u0026#39; | S_{t-1} = s\\right] $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e状态概率矩阵\u003c/strong\u003e \u003ccode\u003e$\\mathcal{P}$\u003c/code\u003e 定义了从所有状态 \u003ccode\u003e$s$\u003c/code\u003e 到后继状态 \u003ccode\u003e$s\u0026#39;$\u003c/code\u003e 的转移概率：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\mathcal{P} = \\left[\\begin{array}{ccc} \\mathcal{P}_{11} \u0026amp; \\cdots \u0026amp; \\mathcal{P}_{1 n} \\\\ \\vdots \u0026amp; \u0026amp; \\\\ \\mathcal{P}_{n 1} \u0026amp; \\cdots \u0026amp; \\mathcal{P}_{n n} \\end{array}\\right] $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中每一行的加和为 1。\u003c/p\u003e\n\u003cp\u003e**马尔可夫过程（马尔可夫链）**是一个无记忆的随机过程，一个马尔可夫过程可以定义为 \u003ccode\u003e$\\langle \\mathcal{S}, \\mathcal{P} \\rangle$\u003c/code\u003e，其中 \u003ccode\u003e$\\mathcal{S}$\u003c/code\u003e 是一个有限状态集合，\u003ccode\u003e$\\mathcal{P}_{ss\u0026#39;} = \\mathbb{P} \\left[S_t = s\u0026#39; | S_{t-1} = s\\right]$\u003c/code\u003e，\u003ccode\u003e$\\mathcal{P}$\u003c/code\u003e 为状态转移概率矩阵。以一个学生的日常生活为例，Class \u003ccode\u003e$i$\u003c/code\u003e 表示第 \u003ccode\u003e$i$\u003c/code\u003e 门课程，Facebook 表示在 Facebook 上进行社交，Pub 表示去酒吧，Pass 表示通过考试，Sleep 表示睡觉，这个马尔可夫过程如下图所示：\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-05-23-markov-decision-process/student-markov-chain.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n\u003c/figure\u003e\n\u003cp\u003e从而可以产生多种不同的序列，例如：\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eC1 -\u0026gt; C2 -\u0026gt; C3 -\u0026gt; Pass -\u0026gt; Sleep\nC1 -\u0026gt; FB -\u0026gt; FB -\u0026gt; C1 -\u0026gt; C2 -\u0026gt; Sleep\nC1 -\u0026gt; C2 -\u0026gt; C3 -\u0026gt; Pub -\u0026gt; C2 -\u0026gt; C3 -\u0026gt; Pass -\u0026gt; Sleep\n\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e状态转移概率矩阵如下所示：\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-05-23-markov-decision-process/student-markov-chain-transition-matrix.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n\u003c/figure\u003e\n\u003cp\u003e据此我们可以定义\u003cstrong\u003e马尔可夫奖励过程（Markov Reward Process，MRP）\u003cstrong\u003e为 \u003ccode\u003e$\\langle \\mathcal{S, P, R}, \\gamma \\rangle$\u003c/code\u003e，其中 \u003ccode\u003e$\\mathcal{S}$\u003c/code\u003e 和 \u003ccode\u003e$\\mathcal{P}$\u003c/code\u003e 同马尔可夫过程定义中的参数相同，\u003ccode\u003e$\\mathcal{R}$\u003c/code\u003e 为收益函数，\u003ccode\u003e$\\mathcal{R}_s = \\mathbb{E} \\left[R_t | S_{t-1} = s\\right]$\u003c/code\u003e，\u003ccode\u003e$\\gamma \\in \\left[0, 1\\right]$\u003c/code\u003e 为\u003c/strong\u003e折扣率\u003c/strong\u003e。如下图所示：\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-05-23-markov-decision-process/student-mrp.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n\u003c/figure\u003e\n\u003cp\u003e\u003cstrong\u003e期望回报\u003c/strong\u003e \u003ccode\u003e$G_t$\u003c/code\u003e 定义为从时刻 \u003ccode\u003e$t$\u003c/code\u003e 之后的所有衰减的收益之和，即：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ G_t = R_{t+1} + \\gamma R_{t+2} + \\cdots = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e当 \u003ccode\u003e$\\gamma$\u003c/code\u003e 接近 \u003ccode\u003e$0$\u003c/code\u003e 时，智能体更倾向于近期收益，当 \u003ccode\u003e$\\gamma$\u003c/code\u003e 接近 \u003ccode\u003e$1$\u003c/code\u003e 时，智能体更侧重考虑长远收益。邻接时刻的收益可以按如下递归方式表示：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ G_t = R_{t+1} + \\gamma G_{t+1} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e对于存在“最终时刻”的应用中，智能体和环境的交互能被自然地分成一个系列子序列，每个子序列称之为“\u003cstrong\u003e幕（episodes）\u003c/strong\u003e”，例如一盘游戏、一次走迷宫的过程，每幕都以一种特殊状态结束，称之为\u003cstrong\u003e终结状态\u003c/strong\u003e。这些幕可以被认为在同样的终结状态下结束，只是对不同的结果有不同的收益，具有这种\u003cstrong\u003e分幕\u003c/strong\u003e重复特性的任务称之为\u003cstrong\u003e分幕式任务\u003c/strong\u003e。\u003c/p\u003e\n\u003cp\u003eMRP 的状态价值函数 \u003ccode\u003e$v \\left(s\\right)$\u003c/code\u003e 给出了状态 \u003ccode\u003e$s$\u003c/code\u003e 的长期价值，定义为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{aligned} v(s) \u0026amp;=\\mathbb{E}\\left[G_{t} | S_{t}=s\\right] \\\\ \u0026amp;=\\mathbb{E}\\left[R_{t+1}+\\gamma R_{t+2}+\\gamma^{2} R_{t+3}+\\ldots | S_{t}=s\\right] \\\\ \u0026amp;=\\mathbb{E}\\left[R_{t+1}+\\gamma\\left(R_{t+2}+\\gamma R_{t+3}+\\ldots\\right) | S_{t}=s\\right] \\\\ \u0026amp;=\\mathbb{E}\\left[R_{t+1}+\\gamma G_{t+1} | S_{t}=s\\right] \\\\ \u0026amp;=\\mathbb{E}\\left[R_{t+1}+\\gamma v\\left(S_{t+1}\\right) | S_{t}=s\\right] \\end{aligned} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e价值函数可以分解为两部分：即时收益 \u003ccode\u003e$R_{t+1}$\u003c/code\u003e 和后继状态的折扣价值 \u003ccode\u003e$\\gamma v \\left(S_{t+1}\\right)$\u003c/code\u003e。上式我们称之为\u003cstrong\u003e贝尔曼方程（Bellman Equation）\u003c/strong\u003e，其衡量了状态价值和后继状态价值之间的关系。\u003c/p\u003e\n\u003ch1 id=\"马尔可夫决策过程\"\u003e马尔可夫决策过程\u003c/h1\u003e\n\u003cp\u003e一个**马尔可夫决策过程（Markov Decision Process，MDP）**定义为包含决策的马尔可夫奖励过程 \u003ccode\u003e$\\langle\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma\\rangle$\u003c/code\u003e，在这个环境中所有的状态均具有马尔可夫性。其中，\u003ccode\u003e$\\mathcal{S}$\u003c/code\u003e 为有限的状态集合，\u003ccode\u003e$\\mathcal{A}$\u003c/code\u003e 为有限的动作集合，\u003ccode\u003e$\\mathcal{P}$\u003c/code\u003e 为状态转移概率矩阵，\u003ccode\u003e$\\mathcal{P}_{s s^{\\prime}}^{a}=\\mathbb{P}\\left[S_{t+1}=s^{\\prime} | S_{t}=s, A_{t}=a\\right]$\u003c/code\u003e，\u003ccode\u003e$\\mathcal{R}$\u003c/code\u003e 为奖励函数，\u003ccode\u003e$\\mathcal{R}_{s}^{a}=\\mathbb{E}\\left[R_{t+1} | S_{t}=s, A_{t}=a\\right]$\u003c/code\u003e，\u003ccode\u003e$\\gamma \\in \\left[0, 1\\right]$\u003c/code\u003e 为折扣率。上例中的马尔可夫决策过程如下图所示：\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-05-23-markov-decision-process/student-mdp.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n\u003c/figure\u003e\n\u003cp\u003e**策略（Policy）**定义为给定状态下动作的概率分布：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\pi \\left(a | s\\right) = \\mathbb{P} \\left[A_t = a | S_t = s\\right] $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e一个策略完全确定了一个智能体的行为，同时 MDP 策略仅依赖于当前状态。给定一个 MDP \u003ccode\u003e$\\mathcal{M}=\\langle\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma\\rangle$\u003c/code\u003e 和一个策略 \u003ccode\u003e$\\pi$\u003c/code\u003e，状态序列 \u003ccode\u003e$S_1, S_2, \\cdots$\u003c/code\u003e 为一个马尔可夫过程 \u003ccode\u003e$\\langle \\mathcal{S}, \\mathcal{P}^{\\pi} \\rangle$\u003c/code\u003e，状态和奖励序列 \u003ccode\u003e$S_1, R_2, S_2, \\cdots$\u003c/code\u003e 为一个马尔可夫奖励过程 \u003ccode\u003e$\\left\\langle\\mathcal{S}, \\mathcal{P}^{\\pi}, \\mathcal{R}^{\\pi}, \\gamma\\right\\rangle$\u003c/code\u003e，其中\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{aligned} \\mathcal{P}_{s s^{\\prime}}^{\\pi} \u0026amp;=\\sum_{a \\in \\mathcal{A}} \\pi(a | s) \\mathcal{P}_{s s^{\\prime}}^{a} \\\\ \\mathcal{R}_{s}^{\\pi} \u0026amp;=\\sum_{a \\in \\mathcal{A}} \\pi(a | s) \\mathcal{R}_{s}^{a} \\end{aligned} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e在策略 \u003ccode\u003e$\\pi$\u003c/code\u003e 下，状态 \u003ccode\u003e$s$\u003c/code\u003e 的价值函数记为 \u003ccode\u003e$v_{\\pi} \\left(s\\right)$\u003c/code\u003e，即从状态 \u003ccode\u003e$s$\u003c/code\u003e 开始，智能体按照策略进行决策所获得的回报的概率期望值，对于 MDP 其定义为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{aligned} v_{\\pi} \\left(s\\right) \u0026amp;= \\mathbb{E}_{\\pi} \\left[G_t | S_t = s\\right] \\\\ \u0026amp;= \\mathbb{E}_{\\pi} \\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} | S_t = s\\right] \\end{aligned} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e在策略 \u003ccode\u003e$\\pi$\u003c/code\u003e 下，在状态 \u003ccode\u003e$s$\u003c/code\u003e 时采取动作 \u003ccode\u003e$a$\u003c/code\u003e 的价值记为 \u003ccode\u003e$q_\\pi \\left(s, a\\right)$\u003c/code\u003e，即根据策略 \u003ccode\u003e$\\pi$\u003c/code\u003e，从状态 \u003ccode\u003e$s$\u003c/code\u003e 开始，执行动作 \u003ccode\u003e$a$\u003c/code\u003e 之后，所有可能的决策序列的期望回报：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{aligned} q_\\pi \\left(s, a\\right) \u0026amp;= \\mathbb{E}_{\\pi} \\left[G_t | S_t = s, A_t = a\\right] \\\\ \u0026amp;= \\mathbb{E}_{\\pi} \\left[\\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} | S_t = s, A_t = a\\right] \\end{aligned} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e状态价值函数 \u003ccode\u003e$v_{\\pi}$\u003c/code\u003e 和动作价值函数 \u003ccode\u003e$q_{\\pi}$\u003c/code\u003e 都能从经验中估计得到，两者都可以分解为当前和后继两个部分：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{aligned} v_{\\pi}(s) \u0026amp;= \\mathbb{E}_{\\pi}\\left[R_{t+1}+\\gamma v_{\\pi}\\left(S_{t+1}\\right) | S_{t}=s\\right] \\\\ q_{\\pi}(s, a) \u0026amp;= \\mathbb{E}_{\\pi}\\left[R_{t+1}+\\gamma q_{\\pi}\\left(S_{t+1}, A_{t+1}\\right) | S_{t}=s, A_{t}=a\\right] \\end{aligned} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e从一个状态 \u003ccode\u003e$s$\u003c/code\u003e 出发，采取一个行动 \u003ccode\u003e$a$\u003c/code\u003e，状态价值函数为：\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-05-23-markov-decision-process/bellman-expection-eq-state-value-1.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n\u003c/figure\u003e\n\u003cp\u003e\u003ccode\u003e$$ v_{\\pi}(s)=\\sum_{a \\in \\mathcal{A}} \\pi(a | s) q_{\\pi}(s, a) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e从一个动作 \u003ccode\u003e$s$\u003c/code\u003e 出发，再采取一个行动 \u003ccode\u003e$a$\u003c/code\u003e 后，动作价值函数为：\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-05-23-markov-decision-process/bellman-expection-eq-action-value-1.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n\u003c/figure\u003e\n\u003cp\u003e\u003ccode\u003e$$ q_{\\pi}(s, a)=\\mathcal{R}_{s}^{a}+\\gamma \\sum_{s^{\\prime} \\in \\mathcal{S}} \\mathcal{P}_{s s^{\\prime}}^{a} v_{\\pi}\\left(s^{\\prime}\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e利用后继状态价值函数表示当前状态价值函数为：\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-05-23-markov-decision-process/bellman-expection-eq-state-value-2.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n\u003c/figure\u003e\n\u003cp\u003e\u003ccode\u003e$$ v_{\\pi}(s)=\\sum_{a \\in \\mathcal{A}} \\pi(a | s)\\left(\\mathcal{R}_{s}^{a}+\\gamma \\sum_{s^{\\prime} \\in \\mathcal{S}} \\mathcal{P}_{s s^{\\prime}}^{a} v_{\\pi}\\left(s^{\\prime}\\right)\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e利用后继动作价值函数表示当前动作价值函数为：\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-05-23-markov-decision-process/bellman-expection-eq-action-value-2.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n\u003c/figure\u003e\n\u003cp\u003e\u003ccode\u003e$$ q_{\\pi}(s, a)=\\mathcal{R}_{s}^{a}+\\gamma \\sum_{s^{\\prime} \\in \\mathcal{S}} \\mathcal{P}_{s s^{\\prime}}^{a} \\sum_{a^{\\prime} \\in \\mathcal{A}} \\pi\\left(a^{\\prime} | s^{\\prime}\\right) q_{\\pi}\\left(s^{\\prime}, a^{\\prime}\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e最优状态价值函数\u003c/strong\u003e \u003ccode\u003e$v_* \\left(s\\right)$\u003c/code\u003e 定义为所有策略上最大值的状态价值函数：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ v_* \\left(s\\right) = \\mathop{\\max}_{\\pi} v_{\\pi} \\left(s\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e最优动作价值函数\u003c/strong\u003e \u003ccode\u003e$q_* \\left(s, a\\right)$\u003c/code\u003e 定义为所有策略上最大值的动作价值函数：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ q_* \\left(s, a\\right) = \\mathop{\\max}_{\\pi} q_{\\pi} \\left(s, a\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e定义不同策略之间的大小关系为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\pi \\geq \\pi^{\\prime} \\text { if } v_{\\pi}(s) \\geq v_{\\pi^{\\prime}}(s), \\forall s $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e对于任意一个马尔可夫决策过程有：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e存在一个比其他策略更优或相等的策略，\u003ccode\u003e$\\pi_* \\geq \\pi, \\forall \\pi$\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e所有的最优策略均能够获得最优的状态价值函数，\u003ccode\u003e$v_{\\pi_*} \\left(s\\right) = v_* \\left(s\\right)$\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e所有的最优策略均能够获得最优的动作价值函数，\u003ccode\u003e$q_{\\pi_*} \\left(s, a\\right) = q_* \\left(s, a\\right)$\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e一个最优策略可以通过最大化 \u003ccode\u003e$q_* \\left(s, a\\right)$\u003c/code\u003e 获得：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\pi_{*}(a | s)=\\left\\{\\begin{array}{ll} 1 \u0026amp; \\text { if } a=\\underset{a \\in \\mathcal{A}}{\\operatorname{argmax}} q_{*}(s, a) \\\\ 0 \u0026amp; \\text { otherwise } \\end{array}\\right. $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e对于任意一个 MDP 均会有一个确定的最优策略，如果已知 \u003ccode\u003e$q_* \\left(s, a\\right)$\u003c/code\u003e 即可知晓最优策略。\u003c/p\u003e\n\u003cp\u003e最优状态价值函数循环依赖于贝尔曼最优方程：\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-05-23-markov-decision-process/bellman-optimality-eq-state-value-1.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n\u003c/figure\u003e\n\u003cp\u003e\u003ccode\u003e$$ v_{*}(s)=\\max _{a} q_{*}(s, a) $$\u003c/code\u003e\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-05-23-markov-decision-process/bellman-optimality-eq-action-value-1.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n\u003c/figure\u003e\n\u003cp\u003e\u003ccode\u003e$$ q_{*}(s, a)=\\mathcal{R}_{s}^{a}+\\gamma \\sum_{s^{\\prime} \\in \\mathcal{S}} \\mathcal{P}_{s s^{\\prime}}^{a} v_{*}\\left(s^{\\prime}\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-05-23-markov-decision-process/bellman-optimality-eq-state-value-2.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n\u003c/figure\u003e\n\u003cp\u003e\u003ccode\u003e$$ v_{*}(s)=\\max _{a} \\mathcal{R}_{s}^{a}+\\gamma \\sum_{s^{\\prime} \\in \\mathcal{S}} \\mathcal{P}_{s s^{\\prime}}^{a} v_{*}\\left(s^{\\prime}\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-05-23-markov-decision-process/bellman-optimality-eq-action-value-2.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n\u003c/figure\u003e\n\u003cp\u003e\u003ccode\u003e$$ q_{*}(s, a)=\\mathcal{R}_{s}^{a}+\\gamma \\sum_{s^{\\prime} \\in \\mathcal{S}} \\mathcal{P}_{s s^{\\prime}}^{a} \\max _{a^{\\prime}} q_{*}\\left(s^{\\prime}, a^{\\prime}\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e显式求解贝尔曼最优方程给出了找到一个最优策略的方法，但这种解法至少依赖于三条实际情况很难满足的假设：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e准确地知道环境的动态变化特性\u003c/li\u003e\n\u003cli\u003e有足够的计算资源来求解\u003c/li\u003e\n\u003cli\u003e马尔可夫性质\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e尤其是假设 2 很难满足，现实问题中状态的数量一般很大，即使利用最快的计算机也需要花费难以接受的时间才能求解完成。\u003c/p\u003e\n\u003cdiv class=\"footnotes\" role=\"doc-endnotes\"\u003e\n\u003chr/\u003e\n\u003col\u003e\n\u003cli id=\"fn:1\"\u003e\n\u003cp\u003eSutton, R. S., \u0026amp; Barto, A. G. (2018). \u003cem\u003eReinforcement learning: An introduction\u003c/em\u003e. MIT press. \u003ca href=\"#fnref:1\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:2\"\u003e\n\u003cp\u003eCS234: Reinforcement Learning \u003ca href=\"http://web.stanford.edu/class/cs234/index.html\"\u003ehttp://web.stanford.edu/class/cs234/index.html\u003c/a\u003e \u003ca href=\"#fnref:2\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:3\"\u003e\n\u003cp\u003eUCL Course on RL \u003ca href=\"https://www.davidsilver.uk/teaching\"\u003ehttps://www.davidsilver.uk/teaching\u003c/a\u003e \u003ca href=\"#fnref:3\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/div\u003e\n\n\n\n\n\n\u003cdiv class=\"donate\"\u003e\n  \u003cdiv class=\"donate-header\"\u003e\u003c/div\u003e\n  \u003cdiv class=\"donate-slug\" id=\"donate-slug\"\u003emarkov-decision-process\u003c/div\u003e\n  \u003cbutton class=\"donate-button\"\u003e赞 赏\u003c/button\u003e\n  \u003cdiv class=\"donate-footer\"\u003e「真诚赞赏，手留余香」\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"donate-modal-wrapper\"\u003e\n  \u003cdiv class=\"donate-modal\"\u003e\n    \u003cdiv class=\"donate-box\"\u003e\n      \u003cdiv class=\"donate-box-content\"\u003e\n        \u003cdiv class=\"donate-box-content-inner\"\u003e\n          \u003cdiv class=\"donate-box-header\"\u003e「真诚赞赏，手留余香」\u003c/div\u003e\n          \u003cdiv class=\"donate-box-body\"\u003e\n            \u003cdiv class=\"donate-box-money\"\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-2\" data-v=\"2\" data-unchecked=\"￥ 2\" data-checked=\"2 元\"\u003e￥ 2\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-5\" data-v=\"5\" data-unchecked=\"￥ 5\" data-checked=\"5 元\"\u003e￥ 5\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-10\" data-v=\"10\" data-unchecked=\"￥ 10\" data-checked=\"10 元\"\u003e￥ 10\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-50\" data-v=\"50\" data-unchecked=\"￥ 50\" data-checked=\"50 元\"\u003e￥ 50\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-100\" data-v=\"100\" data-unchecked=\"￥ 100\" data-checked=\"100 元\"\u003e￥ 100\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-custom\" data-v=\"custom\" data-unchecked=\"任意金额\" data-checked=\"任意金额\"\u003e任意金额\u003c/button\u003e\n            \u003c/div\u003e\n            \u003cdiv class=\"donate-box-pay\"\u003e\n              \u003cimg class=\"donate-box-pay-qrcode\" id=\"donate-box-pay-qrcode\" src=\"\"/\u003e\n            \u003c/div\u003e\n          \u003c/div\u003e\n          \u003cdiv class=\"donate-box-footer\"\u003e\n            \u003cdiv class=\"donate-box-pay-method donate-box-pay-method-checked\" data-v=\"wechat-pay\"\u003e\n              \u003cimg class=\"donate-box-pay-method-image\" id=\"donate-box-pay-method-image-wechat-pay\" src=\"\"/\u003e\n            \u003c/div\u003e\n            \u003cdiv class=\"donate-box-pay-method\" data-v=\"alipay\"\u003e\n              \u003cimg class=\"donate-box-pay-method-image\" id=\"donate-box-pay-method-image-alipay\" src=\"\"/\u003e\n            \u003c/div\u003e\n          \u003c/div\u003e\n        \u003c/div\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n    \u003cbutton type=\"button\" class=\"donate-box-close-button\"\u003e\n      \u003csvg class=\"donate-box-close-button-icon\" fill=\"#fff\" viewBox=\"0 0 24 24\" width=\"24\" height=\"24\"\u003e\u003cpath d=\"M13.486 12l5.208-5.207a1.048 1.048 0 0 0-.006-1.483 1.046 1.046 0 0 0-1.482-.005L12 10.514 6.793 5.305a1.048 1.048 0 0 0-1.483.005 1.046 1.046 0 0 0-.005 1.483L10.514 12l-5.208 5.207a1.048 1.048 0 0 0 .006 1.483 1.046 1.046 0 0 0 1.482.005L12 13.486l5.207 5.208a1.048 1.048 0 0 0 1.483-.006 1.046 1.046 0 0 0 .005-1.482L13.486 12z\" fill-rule=\"evenodd\"\u003e\u003c/path\u003e\u003c/svg\u003e\n    \u003c/button\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\u003cscript type=\"text/javascript\" src=\"/js/donate.js\"\u003e\u003c/script\u003e\n\n\n  \u003cfooter\u003e\n  \n\u003cnav class=\"post-nav\"\u003e\n  \u003cspan class=\"nav-prev\"\u003e← \u003ca href=\"/cn/2020/05/multi-armed-bandit/\"\u003e多臂赌博机 (Multi-armed Bandit)\u003c/a\u003e\u003c/span\u003e\n  \u003cspan class=\"nav-next\"\u003e\u003ca href=\"/cn/2020/06/bayesian-optimization/\"\u003e贝叶斯优化 (Bayesian Optimization)\u003c/a\u003e →\u003c/span\u003e\n\u003c/nav\u003e\n\n\n\n\n\u003cins class=\"adsbygoogle\" style=\"display:block; text-align:center;\" data-ad-layout=\"in-article\" data-ad-format=\"fluid\" data-ad-client=\"ca-pub-2608165017777396\" data-ad-slot=\"8302038603\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n  (adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/js-cookie@3.0.5/dist/js.cookie.min.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"/js/toggle-theme.js\"\u003e\u003c/script\u003e\n\n\n\u003cscript src=\"/js/no-highlight.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"/js/math-code.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"/js/heading-anchor.js\"\u003e\u003c/script\u003e\n\n\n\n\u003csection class=\"comments\"\u003e\n\u003cscript src=\"https://giscus.app/client.js\" data-repo=\"leovan/leovan.me\" data-repo-id=\"MDEwOlJlcG9zaXRvcnkxMTMxOTY0Mjc=\" data-category=\"Comments\" data-category-id=\"DIC_kwDOBr89i84CT-R7\" data-mapping=\"pathname\" data-strict=\"1\" data-reactions-enabled=\"1\" data-emit-metadata=\"0\" data-input-position=\"top\" data-theme=\"preferred_color_scheme\" data-lang=\"zh-CN\" data-loading=\"lazy\" crossorigin=\"anonymous\" defer=\"\"\u003e\n\u003c/script\u003e\n\u003c/section\u003e\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"//cdn.jsdelivr.net/npm/prismjs@1.29.0/prism.min.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"//cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"//cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/toolbar/prism-toolbar.min.js\"\u003e\u003c/script\u003e\n\u003cscript\u003e\n  (function() {\n    if (!self.Prism) {\n      return;\n    }\n\n    \n    Prism.languages.dos = Prism.languages.powershell;\n    Prism.languages.gremlin = Prism.languages.groovy;\n\n    let languages = {\n      'r': 'R', 'python': 'Python', 'xml': 'XML', 'html': 'HTML',\n      'yaml': 'YAML', 'latex': 'LaTeX', 'tex': 'TeX',\n      'powershell': 'PowerShell', 'javascript': 'JavaScript',\n      'dos': 'DOS', 'qml': 'QML', 'json': 'JSON', 'bash': 'Bash',\n      'text': 'Text', 'txt': 'Text', 'sparql': 'SPARQL',\n      'gremlin': 'Gremlin', 'cypher': 'Cypher', 'ngql': 'nGQL',\n      'shell': 'Shell', 'sql': 'SQL', 'apacheconf': 'Apache Configuration', 'c': 'C', 'css': 'CSS'\n    };\n\n    Prism.hooks.add('before-highlight', function(env) {\n      if (env.language !== 'plain') {\n        let language = languages[env.language] || env.language;\n        env.element.setAttribute('data-language', language);\n      }\n    });\n\n    \n    let ClipboardJS = window.ClipboardJS || undefined;\n\n    Prism.plugins.toolbar.registerButton('copy-to-clipboard', function(env) {\n      let linkCopy = document.createElement('button');\n      linkCopy.classList.add('prism-button-copy');\n\n      registerClipboard();\n\n      return linkCopy;\n\n      function registerClipboard() {\n        let clip = new ClipboardJS(linkCopy, {\n          'text': function () {\n            return env.code;\n          }\n        });\n\n        clip.on('success', function() {\n          linkCopy.classList.add('prism-button-copy-success');\n          resetText();\n        });\n        clip.on('error', function () {\n          linkCopy.classList.add('prism-button-copy-error');\n          resetText();\n        });\n      }\n\n      function resetText() {\n        setTimeout(function () {\n          linkCopy.classList.remove('prism-button-copy-success');\n          linkCopy.classList.remove('prism-button-copy-error');\n        }, 1600);\n      }\n    });\n  })();\n\u003c/script\u003e\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cscript async=\"\" src=\"/js/center-img.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/right-quote.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/external-link.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/alt-title.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/figure.js\"\u003e\u003c/script\u003e\n\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js\"\u003e\u003c/script\u003e\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/vanilla-back-to-top@latest/dist/vanilla-back-to-top.min.js\"\u003e\u003c/script\u003e\n\u003cscript\u003e\naddBackToTop({\n  diameter: 48\n});\n\u003c/script\u003e\n\n  \u003chr/\u003e\n  \u003cdiv class=\"copyright no-border-bottom\"\u003e\n    \u003cdiv class=\"copyright-author-year\"\u003e\n      \u003cspan\u003eCopyright © 2017-2024 \u003ca href=\"/\"\u003e范叶亮 | Leo Van\u003c/a\u003e\u003c/span\u003e\n    \u003c/div\u003e\n  \u003c/div\u003e\n  \u003c/footer\u003e\n  \u003c/article\u003e",
  "Date": "2020-05-23T00:00:00Z",
  "Author": "范叶亮"
}