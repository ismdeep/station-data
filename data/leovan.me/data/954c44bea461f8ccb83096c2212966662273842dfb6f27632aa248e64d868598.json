{
  "Source": "leovan.me",
  "Title": "贝叶斯优化 (Bayesian Optimization)",
  "Link": "https://leovan.me/cn/2020/06/bayesian-optimization/",
  "Content": "\u003carticle class=\"main\"\u003e\n    \u003cheader class=\"content-title\"\u003e\n    \n\u003ch1 class=\"title\"\u003e\n  \n  贝叶斯优化 (Bayesian Optimization)\n  \n\u003c/h1\u003e\n\n\n\n\n\n\n\n\u003ch2 class=\"author-date\"\u003e范叶亮 / \n2020-06-06\u003c/h2\u003e\n\n\n\n\u003ch3 class=\"post-meta\"\u003e\n\n\n\u003cstrong\u003e分类: \u003c/strong\u003e\n\u003ca href=\"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0\"\u003e机器学习\u003c/a\u003e, \u003ca href=\"/categories/%E6%9C%80%E4%BC%98%E5%8C%96\"\u003e最优化\u003c/a\u003e\n\n\n\n\n/\n\n\n\n\n\u003cstrong\u003e标签: \u003c/strong\u003e\n\u003cspan\u003e高斯分布\u003c/span\u003e, \u003cspan\u003eGaussian Distribution\u003c/span\u003e, \u003cspan\u003e正态分布\u003c/span\u003e, \u003cspan\u003eNormal Distribution\u003c/span\u003e, \u003cspan\u003e边缘化\u003c/span\u003e, \u003cspan\u003eMarginalization\u003c/span\u003e, \u003cspan\u003e条件化\u003c/span\u003e, \u003cspan\u003eConditioning\u003c/span\u003e, \u003cspan\u003e高斯过程\u003c/span\u003e, \u003cspan\u003eGaussian Processes\u003c/span\u003e, \u003cspan\u003e高斯过程回归\u003c/span\u003e, \u003cspan\u003eGaussian Processes Regression\u003c/span\u003e, \u003cspan\u003e主动学习\u003c/span\u003e, \u003cspan\u003eActive Learning\u003c/span\u003e, \u003cspan\u003e代理模型\u003c/span\u003e, \u003cspan\u003eSurrogate Model\u003c/span\u003e, \u003cspan\u003e贝叶斯优化\u003c/span\u003e, \u003cspan\u003eBayesian Optimization\u003c/span\u003e, \u003cspan\u003e采集函数\u003c/span\u003e, \u003cspan\u003eAcquisition Functions\u003c/span\u003e\n\n\n\n\n/\n\n\n\u003cstrong\u003e字数: \u003c/strong\u003e\n5045\n\u003c/h3\u003e\n\n\n\n\u003chr/\u003e\n\n\n\n    \n    \n    \u003cins class=\"adsbygoogle\" style=\"display:block; text-align:center;\" data-ad-layout=\"in-article\" data-ad-format=\"fluid\" data-ad-client=\"ca-pub-2608165017777396\" data-ad-slot=\"1261604535\"\u003e\u003c/ins\u003e\n    \u003cscript\u003e\n    (adsbygoogle = window.adsbygoogle || []).push({});\n    \u003c/script\u003e\n    \n    \n    \u003c/header\u003e\n\n\n\n\n\u003cblockquote\u003e\n\u003cp\u003e本文内容主要参考自：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"https://zhuanlan.zhihu.com/p/139478368\"\u003e从高斯分布到高斯过程、高斯过程回归、贝叶斯优化\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://distill.pub/2019/visual-exploration-gaussian-processes/\"\u003eA Visual Exploration of Gaussian Processes\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.aidanscannell.com/post/gaussian-process-regression/\"\u003eGaussian Process Regression\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://distill.pub/2020/bayesian-optimization/\"\u003eExploring Bayesian Optimization\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/blockquote\u003e\n\u003ch1 id=\"高斯分布\"\u003e高斯分布\u003c/h1\u003e\n\u003ch2 id=\"一元高斯分布\"\u003e一元高斯分布\u003c/h2\u003e\n\u003cp\u003e若随机变量 \u003ccode\u003e$X$\u003c/code\u003e 服从一个均值为 \u003ccode\u003e$\\mu$\u003c/code\u003e，方差为 \u003ccode\u003e$\\sigma^2$\u003c/code\u003e 的高斯分布，则记为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ X \\sim N \\left(\\mu, \\sigma^2\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其概率密度函数为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ f \\left(x\\right) = \\dfrac{1}{\\sigma \\sqrt{2 \\pi}} e^{- \\dfrac{\\left(x - \\mu\\right)^2}{2 \\sigma^2}} $$\u003c/code\u003e\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-06-06-bayesian-optimization/univariate-gaussian-distribution.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n  \u003cfigcaption class=\"kai\"\u003e图片来源：https://zh.wikipedia.org/wiki/正态分布\u003c/figcaption\u003e\n  \n\u003c/figure\u003e\n\u003ch2 id=\"二元高斯分布\"\u003e二元高斯分布\u003c/h2\u003e\n\u003cp\u003e若随机变量 \u003ccode\u003e$X, Y$\u003c/code\u003e 服从均值为 \u003ccode\u003e$\\mu = \\left(\\mu_X, \\mu_Y\\right)^{\\top}$\u003c/code\u003e，方差为 \u003ccode\u003e$\\mu = \\left(\\sigma_X, \\sigma_Y\\right)^{\\top}$\u003c/code\u003e 的高斯分布，则记为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\left(X, Y\\right) \\sim \\mathcal{N} \\left(\\mu, \\sigma\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其概率密度函数为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ f(x, y)=\\frac{1}{2 \\pi \\sigma_{X} \\sigma_{Y} \\sqrt{1-\\rho^{2}}} e^{-\\dfrac{1}{2\\left(1-\\rho^{2}\\right)}\\left[\\dfrac{\\left(x-\\mu_{X}\\right)^{2}}{\\sigma_{X}^{2}}+\\dfrac{\\left(y-\\mu_{Y}\\right)^{2}}{\\sigma_{Y}^{2}}-\\dfrac{2 \\rho\\left(x-\\mu_{X}\\right)\\left(y-\\mu_{X}\\right)}{\\sigma_{X} \\sigma_{Y}}\\right]} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$\\rho$\u003c/code\u003e 是 \u003ccode\u003e$X$\u003c/code\u003e 和 \u003ccode\u003e$Y$\u003c/code\u003e 之间的相关系数，\u003ccode\u003e$\\sigma_X \u0026gt; 0$\u003c/code\u003e 且 \u003ccode\u003e$\\sigma_Y \u0026gt; 0$\u003c/code\u003e。\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-06-06-bayesian-optimization/bivariate-gaussian-distribution.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n  \u003cfigcaption class=\"kai\"\u003e图片来源：Bayesian tracking of multiple point targets using expectation maximization\u003c/figcaption\u003e\n  \n\u003c/figure\u003e\n\u003ch2 id=\"多元高斯分布\"\u003e多元高斯分布\u003c/h2\u003e\n\u003cp\u003e若 \u003ccode\u003e$K$\u003c/code\u003e 维随机向量 \u003ccode\u003e$X = \\left[X_1, \\cdots, X_K\\right]^{\\top}$\u003c/code\u003e 服从多元高斯分布，则必须满足如下三个等价条件：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e任何线性组合 \u003ccode\u003e$Y = a_1 X_1 + \\cdots a_K X_K$\u003c/code\u003e 均服从高斯分布。\u003c/li\u003e\n\u003cli\u003e存在随机向量 \u003ccode\u003e$Z = \\left[Z_1, \\cdots, Z_L\\right]^{\\top}$\u003c/code\u003e（每个元素服从独立标准高斯分布），向量 \u003ccode\u003e$\\mu = \\left[\\mu_1, \\cdots, \\mu_K\\right]^{\\top}$\u003c/code\u003e 以及 \u003ccode\u003e$K \\times L$\u003c/code\u003e 的矩阵 \u003ccode\u003e$A$\u003c/code\u003e，满足 \u003ccode\u003e$X = A Z + \\mu$\u003c/code\u003e。\u003c/li\u003e\n\u003cli\u003e存在 \u003ccode\u003e$\\mu$\u003c/code\u003e 和一个对称半正定矩阵 \u003ccode\u003e$\\Sigma$\u003c/code\u003e 满足 \u003ccode\u003e$X$\u003c/code\u003e 的特征函数 \u003ccode\u003e$\\phi_X \\left(u; \\mu, \\Sigma\\right) = \\exp \\left(i \\mu^{\\top} u - \\dfrac{1}{2} u^{\\top} \\Sigma u\\right)$\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e如果 \u003ccode\u003e$\\Sigma$\u003c/code\u003e 是非奇异的，则概率密度函数为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ f \\left(x_1, \\cdots, x_k\\right) = \\dfrac{1}{\\sqrt{\\left(2 \\pi\\right)^k \\lvert\\Sigma\\rvert}} e^{- \\dfrac{1}{2} \\left(x - \\mu\\right)^{\\top} \\Sigma^{-1} \\left(x - \\mu\\right)} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中 \u003ccode\u003e$\\lvert\\Sigma\\rvert$\u003c/code\u003e 表示协方差矩阵的行列式。\u003c/p\u003e\n\u003ch2 id=\"边缘化和条件化\"\u003e边缘化和条件化\u003c/h2\u003e\n\u003cp\u003e高斯分布具有一个优秀的代数性质，即在边缘化和条件化下是闭合的，也就是说从这些操作中获取的结果分布也是高斯的。**边缘化（Marginalization）\u003cstrong\u003e和\u003c/strong\u003e条件化（Conditioning）**都作用于原始分布的子集上：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ P_{X, Y}=\\left[\\begin{array}{l} X \\\\ Y \\end{array}\\right] \\sim \\mathcal{N}(\\mu, \\Sigma)=\\mathcal{N}\\left(\\left[\\begin{array}{l} \\mu_{X} \\\\ \\mu_{Y} \\end{array}\\right],\\left[\\begin{array}{l} \\Sigma_{X X} \\Sigma_{X Y} \\\\ \\Sigma_{Y X} \\Sigma_{Y Y} \\end{array}\\right]\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$X$\u003c/code\u003e 和 \u003ccode\u003e$Y$\u003c/code\u003e 表示原始随机变量的子集。\u003c/p\u003e\n\u003cp\u003e对于随机向量 \u003ccode\u003e$X$\u003c/code\u003e 和 \u003ccode\u003e$Y$\u003c/code\u003e 的高斯概率分布 \u003ccode\u003e$P \\left(X, Y\\right)$\u003c/code\u003e，其边缘概率分布为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{array}{l} X \\sim \\mathcal{N}\\left(\\mu_{X}, \\Sigma_{X X}\\right) \\\\ Y \\sim \\mathcal{N}\\left(\\mu_{Y}, \\Sigma_{Y Y}\\right) \\end{array} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$X$\u003c/code\u003e 和 \u003ccode\u003e$Y$\u003c/code\u003e 两个子集各自只依赖于 \u003ccode\u003e$\\mu$\u003c/code\u003e 和 \u003ccode\u003e$\\Sigma$\u003c/code\u003e 中它们对应的值。因此从高斯分布中边缘化一个随机变量仅需从 \u003ccode\u003e$\\mu$\u003c/code\u003e 和 \u003ccode\u003e$\\Sigma$\u003c/code\u003e 中舍弃相应的变量即可：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ p_{X}(x)=\\int_{y} p_{X, Y}(x, y) d y=\\int_{y} p_{X | Y}(x | y) p_{Y}(y) d y $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e条件化可以用于得到一个变量在另一个变量条件下的概率分布：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{array}{l} X | Y \\sim \\mathcal{N}\\left(\\mu_{X}+\\Sigma_{X Y} \\Sigma_{Y Y}^{-1}\\left(Y-\\mu_{Y}\\right), \\Sigma_{X X}-\\Sigma_{X Y} \\Sigma_{Y Y}^{-1} \\Sigma_{Y X}\\right) \\\\ Y | X \\sim \\mathcal{N}\\left(\\mu_{Y}+\\Sigma_{Y X} \\Sigma_{X X}^{-1}\\left(X-\\mu_{X}\\right), \\Sigma_{Y Y}-\\Sigma_{Y X} \\Sigma_{X X}^{-1} \\Sigma_{X Y}\\right) \\end{array} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e需要注意新的均值仅依赖于作为条件的变量，协方差矩阵和这个变量无关。\u003c/p\u003e\n\u003cp\u003e边缘化可以理解为在高斯分布的一个维度上的累加，条件化可以理解为在多元分布上切一刀从而获得一个维数更少的高斯分布，如下图所示：\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-06-06-bayesian-optimization/marginalization-and-conditioning.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n\u003c/figure\u003e\n\u003ch1 id=\"高斯过程\"\u003e高斯过程\u003c/h1\u003e\n\u003cp\u003e**高斯过程（Gaussian Process）**是观测值出现在一个连续域（例如时间或空间）的随机过程。在高斯过程中，连续输入空间中每个点都是与一个正态分布的随机变量相关联。此外，这些随机变量的每个有限集合都有一个多元正态分布，换句话说它们的任意有限线性组合是一个正态分布。高斯过程的分布是所有那些（无限多个）随机变量的联合分布，正因如此，它是连续域（例如时间或空间）上函数的分布。\u003c/p\u003e\n\u003cp\u003e简单而言，高斯过程即为一系列随机变量，这些随机变量的任意有限集合均为一个多元高斯分布。从\u003cstrong\u003e一元高斯分布\u003c/strong\u003e到\u003cstrong\u003e多元高斯分布\u003c/strong\u003e相当于增加了空间维度，从\u003cstrong\u003e高斯分布\u003c/strong\u003e到\u003cstrong\u003e高斯过程\u003c/strong\u003e相当于引入了时间维度。一个高斯过程可以被均值函数 \u003ccode\u003e$m \\left(x\\right)$\u003c/code\u003e 和协方差函数 \u003ccode\u003e$K \\left(x, x\u0026#39;\\right)$\u003c/code\u003e 共同唯一确定：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{aligned} m(x) \u0026amp;=\\mathbb{E}[f(x)] \\\\ K\\left(x, x\u0026#39;\\right) \u0026amp;=\\mathbb{E}\\left[(f(x)-m(x))\\left(f\\left(x^{\\prime}\\right)-m\\left(x^{\\prime}\\right)\\right)\\right] \\end{aligned} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e则高斯过程可以表示为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ f \\left(x\\right) \\sim \\mathcal{GP} \\left(m \\left(x\\right), K \\left(x, x\u0026#39;\\right)\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e均值函数决定了样本出现的整体位置，如果为零则表示以 \u003ccode\u003e$y = 0$\u003c/code\u003e 为基准线。协方差函数描述了不同点之间的关系，从而可以利用输入的训练数据预测未知点的值。常用的协方差函数有：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e常数：\u003ccode\u003e$K_c \\left(x, x\u0026#39;\\right) = C$\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e线性：\u003ccode\u003e$K_L \\left(x, x\u0026#39;\\right) = x^{\\top} x\u0026#39;$\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e高斯噪声：\u003ccode\u003e$K_{GN} \\left(x, x\u0026#39;\\right) = \\sigma^2 \\delta_{x, x\u0026#39;}$\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e指数平方：\u003ccode\u003e$K_{\\mathrm{SE}}\\left(x, x^{\\prime}\\right)=\\exp \\left(-\\dfrac{|d|^{2}}{2 \\ell^{2}}\\right)$\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eOrnstein-Uhlenbeck：\u003ccode\u003e$K_{\\mathrm{OU}}\\left(x, x^{\\prime}\\right)=\\exp \\left(-\\dfrac{|d|}{\\ell}\\right)$\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003eMatérn：\u003ccode\u003e$K_{\\text {Matern }}\\left(x, x^{\\prime}\\right)=\\dfrac{2^{1-\\nu}}{\\Gamma(\\nu)}\\left(\\dfrac{\\sqrt{2 \\nu}|d|}{\\ell}\\right)^{\\nu} K_{\\nu}\\left(\\dfrac{\\sqrt{2 \\nu}|d|}{\\ell}\\right)$\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e周期：\u003ccode\u003e$K_{\\mathrm{P}}\\left(x, x^{\\prime}\\right)=\\exp \\left(-\\dfrac{2 \\sin ^{2}\\left(\\dfrac{d}{2}\\right)}{\\ell^{2}}\\right)$\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e有理平方：\u003ccode\u003e$K_{\\mathrm{RQ}}\\left(x, x^{\\prime}\\right)=\\left(1+|d|^{2}\\right)^{-\\alpha}, \\quad \\alpha \\geq 0$\u003c/code\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"高斯过程回归\"\u003e高斯过程回归\u003c/h1\u003e\n\u003cp\u003e回归任务的目标是给定一个输入变量 \u003ccode\u003e$x \\in \\mathbb{R}^D$\u003c/code\u003e 预测一个或多个连续目标变量 \u003ccode\u003e$y$\u003c/code\u003e 的值。更确切的说，给定一个包含 \u003ccode\u003e$N$\u003c/code\u003e 个观测值的训练集 \u003ccode\u003e$\\mathbf{X} = \\left\\{x_n\\right\\}^N_1$\u003c/code\u003e 和对应的目标值 \u003ccode\u003e$\\mathbf{Y} = \\left\\{y_n\\right\\}^N_1$\u003c/code\u003e，回归的目标是对于一个新的 \u003ccode\u003e$x$\u003c/code\u003e 预测对应的 \u003ccode\u003e$y$\u003c/code\u003e。目标值和观测值之间通过一个映射进行关联：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ f: X \\to Y $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e在贝叶斯模型中，我们通过观测数据 \u003ccode\u003e$\\mathcal{D} = \\left\\{\\left(\\mathbf{x}_n, \\mathbf{y}_n\\right)\\right\\}^N_{n=1}$\u003c/code\u003e 更新先验分布 \u003ccode\u003e$P \\left(\\mathbf{\\Theta}\\right)$\u003c/code\u003e。通过贝叶斯公式我们可以利用先验概率 \u003ccode\u003e$P \\left(\\mathbf{\\Theta}\\right)$\u003c/code\u003e 和似然函数 \u003ccode\u003e$P \\left(\\mathcal{D} | \\mathbf{\\Theta}\\right)$\u003c/code\u003e 推导出后验概率：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ p\\left(\\mathbf{\\Theta} | \\mathcal{D}\\right)=\\frac{p\\left(\\mathcal{D} | \\mathbf{\\Theta}\\right) p\\left(\\mathbf{\\Theta}\\right)}{p\\left(\\mathcal{D}\\right)} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中 \u003ccode\u003e$p\\left(\\mathcal{D}\\right)$\u003c/code\u003e 为边际似然。在贝叶斯回归中我们不仅希望获得未知输入对应的预测值 \u003ccode\u003e$\\mathbf{y}_*$\u003c/code\u003e ，还希望知道预测的不确定性。因此我们需要利用联合分布和边缘化模型参数 \u003ccode\u003e$\\mathbf{\\Theta}$\u003c/code\u003e 来构造预测分布：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ p\\left(\\mathbf{y}_{*} | \\mathbf{x}_{*}, \\mathcal{D}\\right)=\\int p\\left(\\mathbf{y}_{*}, \\mathbf{\\Theta} | \\mathbf{x}_{*}, \\mathcal{D}\\right) \\mathrm{d} \\Theta=\\int p\\left(\\mathbf{y}_{*} | \\mathbf{x}_{*}, \\mathbf{\\Theta}, \\mathcal{D}\\right) p(\\mathbf{\\Theta} | \\mathcal{D}) \\mathrm{d} \\mathbf{\\Theta} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e通常情况下，由于积分形式 \u003ccode\u003e$p \\left(\\Theta | \\mathcal{D}\\right)$\u003c/code\u003e 不具有解析可解性（Analytically Tractable）：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ p\\left(\\mathcal{D}\\right)=\\int p\\left(\\mathcal{D} | \\mathbf{\\Theta}\\right) p\\left(\\mathbf{\\Theta}\\right) d \\Theta $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e但在高斯似然和高斯过程先验的前提下，后验采用函数的高斯过程的形式，同时是解析可解的。\u003c/p\u003e\n\u003cp\u003e对于高斯过程回归，我们构建一个贝叶斯模型，首先定义函数输出的先验为一个高斯过程：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ p \\left(f | \\mathbf{X}, \\theta\\right) = \\mathcal{N} \\left(\\mathbf{0}, K \\left(\\mathbf{X}, \\mathbf{X}\\right)\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中 \u003ccode\u003e$K \\left(\\cdot, \\cdot\\right)$\u003c/code\u003e 为协方差函数，\u003ccode\u003e$\\theta$\u003c/code\u003e 为过程的超参数。假设数据已经变换为零均值，因此我们不需要在先验中设置均值函数，则令似然形式如下：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ p \\left(\\mathbf{Y} | f\\right) \\sim \\mathcal{N} \\left(f, \\sigma^2_n \\mathbf{I}\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e假设观测值为独立同分布的高斯噪音的累加，则整个模型的联合分布为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ p \\left(\\mathbf{Y} , f | \\mathbf{X}, \\theta\\right) = p \\left(\\mathbf{Y} | f\\right) p \\left(f | \\mathbf{X}, \\theta\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e虽然我们并不关心变量 \u003ccode\u003e$f$\u003c/code\u003e，但由于我们需要对不确定性进行建模，我们仍需考虑 \u003ccode\u003e$\\mathbf{Y}$\u003c/code\u003e 和 \u003ccode\u003e$f$\u003c/code\u003e 以及 \u003ccode\u003e$f$\u003c/code\u003e 和 \u003ccode\u003e$\\mathbf{X}$\u003c/code\u003e 之间的关系。高斯过程作为一个非参数模型，其先验分布构建于映射 \u003ccode\u003e$f$\u003c/code\u003e 之上，\u003ccode\u003e$f$\u003c/code\u003e 仅依赖于核函数的超参数 \u003ccode\u003e$\\theta$\u003c/code\u003e，且这些超参数可以通过数据进行估计。我们可以将超参数作为先验，即：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ p \\left(\\mathbf{Y} , f | \\mathbf{X}, \\theta\\right) = p \\left(\\mathbf{Y} | f\\right) p \\left(f | \\mathbf{X}, \\theta\\right) p \\left(\\theta\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e然后进行贝叶斯推断和模型选择，但是通常情况下这是不可解的。David MacKay 引入了一个利用最优化边际似然来估计贝叶斯平均的框架，即计算如下积分：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ p \\left(\\mathbf{Y} | \\mathbf{X}, \\theta\\right) = \\int p \\left(\\mathbf{Y} | f\\right) p \\left(f | \\mathbf{X}, \\theta\\right) df $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，高斯似然 \u003ccode\u003e$p \\left(\\mathbf{Y} | f\\right)$\u003c/code\u003e 表示模型拟合数据的程度，\u003ccode\u003e$p \\left(f | \\mathbf{X}, \\theta\\right)$\u003c/code\u003e 为高斯过程先验。经过边缘化后，\u003ccode\u003e$\\mathbf{Y}$\u003c/code\u003e 不在依赖于 \u003ccode\u003e$f$\u003c/code\u003e 而仅依赖于 \u003ccode\u003e$\\theta$\u003c/code\u003e。\u003c/p\u003e\n\u003cp\u003e假设采用零均值函数，对于一个高斯过程先验，我们仅需指定一个协方差函数。以指数平方协方差函数为例，选择一系列测试输入点 \u003ccode\u003e$X_*$\u003c/code\u003e，利用协方差矩阵和测试输入点可以生成一个高斯向量：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\mathbf{f}_* \\sim \\mathcal{N} \\left(\\mathbf{0}, K \\left(X_*, X_*\\right)\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e从高斯先验中进行采样，我们首先需要利用标准正态来表示多元正态：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\mathbf{f}_* \\sim \\mu + \\mathbf{B} \\mathcal{N} \\left(0, \\mathbf{I}\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$\\mathbf{BB}^{\\top} = K \\left(X_*, X_*\\right)$\u003c/code\u003e，\u003ccode\u003e$\\mathbf{B}$\u003c/code\u003e 本质上是协方差矩阵的平方根，可以通过 \u003ca href=\"https://zh.wikipedia.org/wiki/Cholesky%E5%88%86%E8%A7%A3\"\u003eCholesky 分解\u003c/a\u003e获得。\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-06-06-bayesian-optimization/gp-prior.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n\u003c/figure\u003e\n\u003cp\u003e上图（左）为从高斯先验中采样的 10 个序列，上图（右）为先验的协方差。如果输入点 \u003ccode\u003e$x_n$\u003c/code\u003e 和 \u003ccode\u003e$x_m$\u003c/code\u003e 接近，则对应的 \u003ccode\u003e$f \\left(x_n\\right)$\u003c/code\u003e 和 \u003ccode\u003e$f \\left(x_m\\right)$\u003c/code\u003e 相比于不接近的点是强相关的。\u003c/p\u003e\n\u003cp\u003e我们关注的并不是这些随机的函数，而是如何将训练数据中的信息同先验进行合并。假设观测数据为 \u003ccode\u003e$\\left\\{\\left(\\mathbf{x}_{i}, f_{i}\\right) | i=1, \\ldots, n\\right\\}$\u003c/code\u003e，则训练目标 \u003ccode\u003e$\\mathbf{f}$\u003c/code\u003e 和测试目标 \u003ccode\u003e$\\mathbf{f}_*$\u003c/code\u003e 之间的联合分布为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\left[\\begin{array}{l} \\mathbf{f} \\\\ \\mathbf{f}_{*} \\end{array}\\right] \\sim \\mathcal{N}\\left(\\mathbf{0},\\left[\\begin{array}{ll} K(X, X) \u0026amp; K\\left(X, X_{*}\\right) \\\\ K\\left(X_{*}, X\\right) \u0026amp; K\\left(X_{*}, X_{*}\\right) \\end{array}\\right]\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e根据观测值对联合高斯先验分布进行条件化处理可以得到高斯过程回归的关键预测方程：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\mathbf{f}_{*} | X, X_{*}, \\mathbf{f} \\sim \\mathcal{N}\\left(\\overline{\\mathbf{f}}_{*}, \\operatorname{cov}\\left(\\mathbf{f}_{*}\\right)\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{aligned} \\overline{\\mathbf{f}}_{*} \u0026amp; \\triangleq \\mathbb{E}\\left[\\mathbf{f}_{*} | X, X_{*}, \\mathbf{f}\\right]=K\\left(X_{*}, X\\right) K(X, X)^{-1} \\mathbf{f} \\\\ \\operatorname{cov}\\left(\\mathbf{f}_{*}\\right) \u0026amp;=K\\left(X_{*}, X_{*}\\right)-K\\left(X_{*}, X\\right) K(X, X)^{-1} K\\left(X, X_{*}\\right) \\end{aligned} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e函数值可以通过对联合后验分布采样获得。\u003c/p\u003e\n\u003cp\u003e我们以三角函数作为给定的函数，并随机采样一些训练数据 \u003ccode\u003e$\\left\\{\\left(\\mathbf{x}_{i}, f_{i}\\right) | i=1, \\ldots, n\\right\\}$\u003c/code\u003e，如下图所示：\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-06-06-bayesian-optimization/underlying-functions-and-training-points.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n\u003c/figure\u003e\n\u003cp\u003e我们希望将训练数据和高斯过程先验进行合并得到联合后验分布，我们可以通过在观测值上条件化联合高斯先验分布，预测的均值和协方差为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{aligned} \\overline{\\mathbf{f}}_{*} \u0026amp;=K\\left(X_{*}, X\\right) K(X, X)^{-1} \\mathbf{f} \\\\ \\operatorname{cov}\\left(\\mathbf{f}_{*}\\right) \u0026amp;=K\\left(X_{*}, X_{*}\\right)-K\\left(X_{*}, X\\right) K(X, X)^{-1} K\\left(X, X_{*}\\right) \\end{aligned} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"http://www.gaussianprocess.org/gpml/\"\u003eRasmussen 和 Williams\u003c/a\u003e 给出了一个实现高斯过程回归的实用方法：\u003c/p\u003e\n\n\n\u003clink rel=\"stylesheet\" type=\"text/css\" href=\"//cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.css\"/\u003e\n\n\n\u003cdiv\u003e\u003cpre class=\"pseudocode\"\u003e\\begin{algorithm}\n\\caption{高斯过程回归算法}\n\\begin{algorithmic}\n\\REQUIRE \\\\\n    输入 $\\mathbf{X}$ \\\\\n    目标 $\\mathbf{y}$ \\\\\n    协方差函数 $k$ \\\\\n    噪音水平 $\\sigma^2_n$ \\\\\n    测试输入 $\\mathbf{x}_*$\n\\ENSURE \\\\\n    均值 $\\bar{f}_*$ \\\\\n    方差 $\\mathbb{V}\\left[f_{*}\\right]$\n\\FUNCTION{GaussianProcessRegression}{$\\mathbf{X}, \\mathbf{y}, k, \\sigma^2_n, \\mathbf{x}_*$}\n\\STATE $L \\gets \\text{cholesky} \\left(K + \\sigma^2_n I\\right)$\n\\STATE $\\alpha \\gets L^{\\top} \\setminus \\left(L \\setminus \\mathbf{y}\\right)$\n\\STATE $\\bar{f}_* \\gets \\mathbf{k}^{\\top}_* \\alpha$\n\\STATE $\\mathbf{v} \\gets L \\setminus \\mathbf{k}_*$\n\\STATE $\\mathbb{V}\\left[f_{*}\\right] \\gets k \\left(\\mathbf{x}_*, \\mathbf{x}_*\\right) - \\mathbf{v}^{\\top} \\mathbf{v}$\n\\RETURN $\\bar{f}_*, \\mathbb{V}\\left[f_{*}\\right]$\n\\ENDFUNCTION\n\\end{algorithmic}\n\\end{algorithm}\n\u003c/pre\u003e\u003c/div\u003e\n\n\u003cp\u003e高斯过程后验和采样的序列如下图所示：\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-06-06-bayesian-optimization/gp-posterior.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n\u003c/figure\u003e\n\u003cp\u003e先验的协方差矩阵和后验的协方差矩阵可视化如下图所示：\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-06-06-bayesian-optimization/prior-posterior-convariance.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n\u003c/figure\u003e\n\u003cp\u003e本小结代码请参见\u003ca href=\"https://github.com/leovan/leovan.me/blob/main/static/scripts/cn/2020-06-06-bayesian-optimization/gaussian-process-regression.py\"\u003e这里\u003c/a\u003e。\u003c/p\u003e\n\u003ch1 id=\"贝叶斯优化\"\u003e贝叶斯优化\u003c/h1\u003e\n\u003ch2 id=\"主动学习\"\u003e主动学习\u003c/h2\u003e\n\u003cp\u003e在很多机器学习问题中，数据标注往往需要耗费很大成本。**主动学习（Active Learning）**在最大化模型准确率时最小化标注成本，例如对不确定性最高的数据进行标注。由于我们仅知道少量数据点，因此我们需要一个代理模型（Surrogate Model）来建模真正的模型。高斯过程因其灵活性和具有估计不确定性估计的特性不失为一个常用的代理模型。\u003c/p\u003e\n\u003cp\u003e在估计 \u003ccode\u003e$f \\left(x\\right)$\u003c/code\u003e 的过程中，我们希望最小化评估的次数，因此我们可以通过主动学习来“智能”地选择下一个评估的数据点。通过不断的选择具有最高不确定性的数据点来获得 \u003ccode\u003e$f \\left(x\\right)$\u003c/code\u003e 更准确的估计，直至收敛或达到停止条件。下图展示了利用主动学习估计真实数据分布的过程：\u003c/p\u003e\n\n  \n  \u003clink rel=\"stylesheet\" href=\"/css/figure-slider.css\"/\u003e\n\n\n\u003cdiv class=\"figure-slider\"\u003e\n  \u003cdiv class=\"figure-slider-parameters\"\u003e\n    \u003cspan class=\"base-url\"\u003e/images/cn/2020-06-06-bayesian-optimization/\u003c/span\u003e\n    \u003cspan class=\"image-filename-prefix\"\u003eactive-gp-\u003c/span\u003e\n    \u003cspan class=\"image-format\"\u003epng\u003c/span\u003e\n    \u003cspan class=\"milliseconds\"\u003e300\u003c/span\u003e\n  \u003c/div\u003e\n  \u003cdiv class=\"figure-slider-image-container\"\u003e\n    \u003cimg class=\"figure-slider-image\" src=\"/images/cn/2020-06-06-bayesian-optimization/active-gp-0.png\"/\u003e\n  \u003c/div\u003e\n  \u003cdiv class=\"figure-slider-controls\"\u003e\n    \u003cdiv class=\"figure-slider-buttons\"\u003e\n      \u003cbutton class=\"figure-slider-button-previous\"\u003e\n        \u003cspan class=\"material-symbols material-symbols-skip-previous-outline\"\u003e\u003c/span\u003e\n      \u003c/button\u003e\n    \u003c/div\u003e\n    \u003cdiv class=\"figure-slider-scroll-bar-container\"\u003e\n      \u003cinput class=\"figure-slider-scroll-bar\" type=\"range\" min=\"0\" max=\"9\" value=\"0\"/\u003e\n    \u003c/div\u003e\n    \u003cdiv class=\"figure-slider-buttons\"\u003e\n      \u003cbutton class=\"figure-slider-button-next\"\u003e\n        \u003cspan class=\"material-symbols material-symbols-skip-next-outline\"\u003e\u003c/span\u003e\n      \u003c/button\u003e\n    \u003c/div\u003e\n    \u003cdiv class=\"figure-slider-buttons\"\u003e\n      \u003cbutton class=\"figure-slider-button-play-pause figure-slider-button-play\"\u003e\n        \u003cspan class=\"material-symbols material-symbols-play-outline\"\u003e\u003c/span\u003e\n      \u003c/button\u003e\n    \u003c/div\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\u003ch2 id=\"贝叶斯优化问题\"\u003e贝叶斯优化问题\u003c/h2\u003e\n\u003cp\u003e贝叶斯优化的核心问题是：基于现有的已知情况，如果选择下一步评估的数据点？在主动学习中我们选择不确定性最大的点，但在贝叶斯优化中我们需要在探索不确定性区域（探索）和关注已知具有较优目标值的区域之间进行权衡（开发）。这种评价的依据称之为\u003cstrong\u003e采集函数（Acquisition Functions）\u003c/strong\u003e，采集函数通过当前模型启发式的评估是否选择一个数据点。\u003c/p\u003e\n\u003cp\u003e贝叶斯优化的目标是找到一个函数 \u003ccode\u003e$f: \\mathbb{R}^d \\mapsto \\mathbb{R}$\u003c/code\u003e 最大值（或最小值）对应的位置 \u003ccode\u003e$x \\in \\mathbb{R}^d$\u003c/code\u003e。为了解决这个问题，我们遵循如下算法：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e选择一个代理模型用于建模真实函数 \u003ccode\u003e$f$\u003c/code\u003e 和定义其先验。\u003c/li\u003e\n\u003cli\u003e给定观测集合，利用贝叶斯公式获取后验。\u003c/li\u003e\n\u003cli\u003e利用采集函数 \u003ccode\u003e$\\alpha \\left(x\\right)$\u003c/code\u003e 确性下一个采样点 \u003ccode\u003e$x_t = \\arg\\max_x \\alpha \\left(x\\right)$\u003c/code\u003e。\u003c/li\u003e\n\u003cli\u003e将采样的点加入观测集合，重复步骤 2 直至收敛或达到停止条件。\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"采集函数\"\u003e采集函数\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003eProbability of Improvement (PI)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eProbability of Improvement (PI) 采集函数会选择具有最大可能性提高当前最大的 \u003ccode\u003e$f \\left(x^{+}\\right)$\u003c/code\u003e 值的点作为下一个查询点，即：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ x_{t+1} = \\arg\\max \\left(\\alpha_{PI} \\left(x\\right)\\right) = \\arg\\max \\left(P \\left(f \\left(x\\right)\\right) \\geq \\left(f \\left(x^{+}\\right) + \\epsilon\\right)\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$P \\left(\\cdot\\right)$\u003c/code\u003e 表示概率，\u003ccode\u003e$\\epsilon$\u003c/code\u003e 为一个较小的正数，\u003ccode\u003e$x^{+} = \\arg\\max_{x_i \\in x_{1:t}} f \\left(x_i\\right)$\u003c/code\u003e，\u003ccode\u003e$x_i$\u003c/code\u003e 为第 \u003ccode\u003e$i$\u003c/code\u003e 步查询点的位置。如果采用高斯过程作为代理模型，上式则转变为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ x_{t+1} = \\arg\\max_x \\Phi \\left(\\dfrac{\\mu_t \\left(x\\right) - f \\left(x^{+}\\right) - \\epsilon}{\\sigma_t \\left(x\\right)}\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$\\Phi \\left(\\cdot\\right)$\u003c/code\u003e 表示标准正态分布累积分布函数。PI 利用 \u003ccode\u003e$\\epsilon$\u003c/code\u003e 来权衡探索和开发，增加 \u003ccode\u003e$\\epsilon$\u003c/code\u003e 的值会更加倾向进行探索。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eExpected Improvement (EI)\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003ePI 仅关注了有多大的可能性能够提高，而没有关注能够提高多少。Expected Improvement (EI) 则会选择具有最大期望提高的点作为下一个查询点，即：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ x_{t+1} = \\arg\\min_x \\mathbb{E} \\left(\\left\\|h_{t+1} \\left(x\\right) - f \\left(x^*\\right)\\right\\| | \\mathcal{D}_t\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$f$\u003c/code\u003e 为真实函数，\u003ccode\u003e$h_{t+1}$\u003c/code\u003e 为代理模型在 \u003ccode\u003e$t+1$\u003c/code\u003e 步的后验均值，\u003ccode\u003e$\\mathcal{D}_t = \\left\\{\\left(x_i, f\\left(x_i\\right)\\right)\\right\\}, \\forall x \\in x_{1:t}$\u003c/code\u003e 为训练数据，\u003ccode\u003e$x^*$\u003c/code\u003e 为 \u003ccode\u003e$f$\u003c/code\u003e 取得最大值的真实位置。\u003c/p\u003e\n\u003cp\u003e上式中我们希望选择能够最小化与最大目标值之间距离的点，由于我们并不知道真实函数 \u003ccode\u003e$f$\u003c/code\u003e，Mockus \u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e 提出了一种解决办法：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ x_{t+1} = \\arg\\max_x \\mathbb{E} \\left(\\max \\left\\{0, h_{t+1} \\left(x\\right) - f \\left(x^{+}\\right)\\right\\} | \\mathcal{D}_t\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$f \\left(x^{+}\\right)$\u003c/code\u003e 为到目前为止遇见的最大函数值，如果采用高斯过程作为代理模型，上式则转变为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{aligned} EI(x) \u0026amp;= \\left\\{\\begin{array}{ll} \\left(\\mu_{t}(x)-f\\left(x^{+}\\right)-\\epsilon\\right) \\Phi(Z)+\\sigma_{t}(x) \\phi(Z), \u0026amp; \\text { if } \\sigma_{t}(x)\u0026gt;0 \\\\ 0 \u0026amp; \\text { if } \\sigma_{t}(x)=0 \\end{array}\\right. \\\\ Z \u0026amp;= \\frac{\\mu_{t}(x)-f\\left(x^{+}\\right)-\\epsilon}{\\sigma_{t}(x)} \\end{aligned} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中 \u003ccode\u003e$\\Phi \\left(\\cdot\\right)$\u003c/code\u003e 表示标准正态分布累积分布函数，\u003ccode\u003e$\\phi \\left(\\cdot\\right)$\u003c/code\u003e 表示标准正态分布概率密度函数。类似 PI，EI 也可以利用 \u003ccode\u003e$\\epsilon$\u003c/code\u003e 来权衡探索和开发，增加 \u003ccode\u003e$\\epsilon$\u003c/code\u003e 的值会更加倾向进行探索。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e对比和其他采集函数\u003c/li\u003e\n\u003c/ul\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-06-06-bayesian-optimization/pi-vs-ei.svg\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n\u003c/figure\u003e\n\u003cp\u003e上图展示了在仅包含一个训练观测数据 \u003ccode\u003e$\\left(0.5, f \\left(0.5\\right)\\right)$\u003c/code\u003e 情况下不同点的采集函数值。可以看出 \u003ccode\u003e$\\alpha_{EI}$\u003c/code\u003e 和 \u003ccode\u003e$\\alpha_{PI}$\u003c/code\u003e 的最大值分别为 0.3 和 0.47。选择一个具有较小的 \u003ccode\u003e$\\alpha_{PI}$\u003c/code\u003e 和一个较大的 \u003ccode\u003e$\\alpha_{EI}$\u003c/code\u003e 的点可以理解为一个高的风险和高的回报。因此，当多个点具有相同的 \u003ccode\u003e$\\alpha_{EI}$\u003c/code\u003e 时，我们应该优先选择具有较小风险（高 \u003ccode\u003e$\\alpha_{PI}$\u003c/code\u003e）的点，类似的，当多个点具有相同的 \u003ccode\u003e$\\alpha_{PI}$\u003c/code\u003e 时，我们应该优先选择具有较大回报（高 \u003ccode\u003e$\\alpha_{EI}$\u003c/code\u003e）的点。\u003c/p\u003e\n\u003cp\u003e其他采集函数还有 Thompson Sampling \u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e，Upper Confidence Bound (UCB)，Gaussian Process Upper Confidence Bound (GP-UCB) \u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e，Entropy Search \u003csup id=\"fnref:4\"\u003e\u003ca href=\"#fn:4\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e4\u003c/a\u003e\u003c/sup\u003e，Predictive Entropy Search \u003csup id=\"fnref:5\"\u003e\u003ca href=\"#fn:5\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e5\u003c/a\u003e\u003c/sup\u003e 等，细节请参见原始论文或 A Tutorial on Bayesian Optimization \u003csup id=\"fnref:6\"\u003e\u003ca href=\"#fn:6\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e6\u003c/a\u003e\u003c/sup\u003e。\u003c/p\u003e\n\u003ch1 id=\"开放资源\"\u003e开放资源\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/scikit-optimize/scikit-optimize\"\u003escikit-optimize/scikit-optimize\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/hyperopt/hyperopt\"\u003ehyperopt/hyperopt\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/automl/SMAC3\"\u003eautoml/SMAC3\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/fmfn/BayesianOptimization\"\u003efmfn/BayesianOptimization\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/pytorch/botorch\"\u003epytorch/botorch\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/GPflow/GPflowOpt\"\u003eGPflow/GPflowOpt\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/keras-team/keras-tuner\"\u003ekeras-team/keras-tuner\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/tobegit3hub/advisor\"\u003etobegit3hub/advisor\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"footnotes\" role=\"doc-endnotes\"\u003e\n\u003chr/\u003e\n\u003col\u003e\n\u003cli id=\"fn:1\"\u003e\n\u003cp\u003eMockus, J. B., \u0026amp; Mockus, L. J. (1991). Bayesian approach to global optimization and application to multiobjective and constrained problems. \u003cem\u003eJournal of Optimization Theory and Applications, 70\u003c/em\u003e(1), 157-172. \u003ca href=\"#fnref:1\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:2\"\u003e\n\u003cp\u003eThompson, W. R. (1933). On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. \u003cem\u003eBiometrika, 25\u003c/em\u003e(3/4), 285-294. \u003ca href=\"#fnref:2\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:3\"\u003e\n\u003cp\u003eAuer, P. (2002). Using confidence bounds for exploitation-exploration trade-offs. \u003cem\u003eJournal of Machine Learning Research, 3\u003c/em\u003e(Nov), 397-422. \u003ca href=\"#fnref:3\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:4\"\u003e\n\u003cp\u003eHennig, P., \u0026amp; Schuler, C. J. (2012). Entropy search for information-efficient global optimization. \u003cem\u003eJournal of Machine Learning Research, 13\u003c/em\u003e(Jun), 1809-1837. \u003ca href=\"#fnref:4\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:5\"\u003e\n\u003cp\u003eHernández-Lobato, J. M., Hoffman, M. W., \u0026amp; Ghahramani, Z. (2014). Predictive entropy search for efficient global optimization of black-box functions. \u003cem\u003eIn Advances in neural information processing systems\u003c/em\u003e (pp. 918-926). \u003ca href=\"#fnref:5\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:6\"\u003e\n\u003cp\u003eFrazier, P. I. (2018). A tutorial on bayesian optimization. \u003cem\u003earXiv preprint arXiv:1807.02811\u003c/em\u003e. \u003ca href=\"#fnref:6\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/div\u003e\n\n\n\n\n\n\u003cdiv class=\"donate\"\u003e\n  \u003cdiv class=\"donate-header\"\u003e\u003c/div\u003e\n  \u003cdiv class=\"donate-slug\" id=\"donate-slug\"\u003ebayesian-optimization\u003c/div\u003e\n  \u003cbutton class=\"donate-button\"\u003e赞 赏\u003c/button\u003e\n  \u003cdiv class=\"donate-footer\"\u003e「真诚赞赏，手留余香」\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"donate-modal-wrapper\"\u003e\n  \u003cdiv class=\"donate-modal\"\u003e\n    \u003cdiv class=\"donate-box\"\u003e\n      \u003cdiv class=\"donate-box-content\"\u003e\n        \u003cdiv class=\"donate-box-content-inner\"\u003e\n          \u003cdiv class=\"donate-box-header\"\u003e「真诚赞赏，手留余香」\u003c/div\u003e\n          \u003cdiv class=\"donate-box-body\"\u003e\n            \u003cdiv class=\"donate-box-money\"\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-2\" data-v=\"2\" data-unchecked=\"￥ 2\" data-checked=\"2 元\"\u003e￥ 2\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-5\" data-v=\"5\" data-unchecked=\"￥ 5\" data-checked=\"5 元\"\u003e￥ 5\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-10\" data-v=\"10\" data-unchecked=\"￥ 10\" data-checked=\"10 元\"\u003e￥ 10\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-50\" data-v=\"50\" data-unchecked=\"￥ 50\" data-checked=\"50 元\"\u003e￥ 50\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-100\" data-v=\"100\" data-unchecked=\"￥ 100\" data-checked=\"100 元\"\u003e￥ 100\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-custom\" data-v=\"custom\" data-unchecked=\"任意金额\" data-checked=\"任意金额\"\u003e任意金额\u003c/button\u003e\n            \u003c/div\u003e\n            \u003cdiv class=\"donate-box-pay\"\u003e\n              \u003cimg class=\"donate-box-pay-qrcode\" id=\"donate-box-pay-qrcode\" src=\"\"/\u003e\n            \u003c/div\u003e\n          \u003c/div\u003e\n          \u003cdiv class=\"donate-box-footer\"\u003e\n            \u003cdiv class=\"donate-box-pay-method donate-box-pay-method-checked\" data-v=\"wechat-pay\"\u003e\n              \u003cimg class=\"donate-box-pay-method-image\" id=\"donate-box-pay-method-image-wechat-pay\" src=\"\"/\u003e\n            \u003c/div\u003e\n            \u003cdiv class=\"donate-box-pay-method\" data-v=\"alipay\"\u003e\n              \u003cimg class=\"donate-box-pay-method-image\" id=\"donate-box-pay-method-image-alipay\" src=\"\"/\u003e\n            \u003c/div\u003e\n          \u003c/div\u003e\n        \u003c/div\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n    \u003cbutton type=\"button\" class=\"donate-box-close-button\"\u003e\n      \u003csvg class=\"donate-box-close-button-icon\" fill=\"#fff\" viewBox=\"0 0 24 24\" width=\"24\" height=\"24\"\u003e\u003cpath d=\"M13.486 12l5.208-5.207a1.048 1.048 0 0 0-.006-1.483 1.046 1.046 0 0 0-1.482-.005L12 10.514 6.793 5.305a1.048 1.048 0 0 0-1.483.005 1.046 1.046 0 0 0-.005 1.483L10.514 12l-5.208 5.207a1.048 1.048 0 0 0 .006 1.483 1.046 1.046 0 0 0 1.482.005L12 13.486l5.207 5.208a1.048 1.048 0 0 0 1.483-.006 1.046 1.046 0 0 0 .005-1.482L13.486 12z\" fill-rule=\"evenodd\"\u003e\u003c/path\u003e\u003c/svg\u003e\n    \u003c/button\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\u003cscript type=\"text/javascript\" src=\"/js/donate.js\"\u003e\u003c/script\u003e\n\n\n  \u003cfooter\u003e\n  \n\u003cnav class=\"post-nav\"\u003e\n  \u003cspan class=\"nav-prev\"\u003e← \u003ca href=\"/cn/2020/05/markov-decision-process/\"\u003e马尔可夫决策过程 (Markov Decision Process)\u003c/a\u003e\u003c/span\u003e\n  \u003cspan class=\"nav-next\"\u003e\u003ca href=\"/cn/2020/06/planning-by-dynamic-programming/\"\u003e利用动态规划求解马尔可夫决策过程 (Planning by Dynamic Programming)\u003c/a\u003e →\u003c/span\u003e\n\u003c/nav\u003e\n\n\n\n\n\u003cins class=\"adsbygoogle\" style=\"display:block; text-align:center;\" data-ad-layout=\"in-article\" data-ad-format=\"fluid\" data-ad-client=\"ca-pub-2608165017777396\" data-ad-slot=\"8302038603\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n  (adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/js-cookie@3.0.5/dist/js.cookie.min.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"/js/toggle-theme.js\"\u003e\u003c/script\u003e\n\n\n\u003cscript src=\"/js/no-highlight.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"/js/math-code.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"/js/heading-anchor.js\"\u003e\u003c/script\u003e\n\n\n\n\u003csection class=\"comments\"\u003e\n\u003cscript src=\"https://giscus.app/client.js\" data-repo=\"leovan/leovan.me\" data-repo-id=\"MDEwOlJlcG9zaXRvcnkxMTMxOTY0Mjc=\" data-category=\"Comments\" data-category-id=\"DIC_kwDOBr89i84CT-R7\" data-mapping=\"pathname\" data-strict=\"1\" data-reactions-enabled=\"1\" data-emit-metadata=\"0\" data-input-position=\"top\" data-theme=\"preferred_color_scheme\" data-lang=\"zh-CN\" data-loading=\"lazy\" crossorigin=\"anonymous\" defer=\"\"\u003e\n\u003c/script\u003e\n\u003c/section\u003e\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"//cdn.jsdelivr.net/npm/prismjs@1.29.0/prism.min.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"//cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"//cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/toolbar/prism-toolbar.min.js\"\u003e\u003c/script\u003e\n\u003cscript\u003e\n  (function() {\n    if (!self.Prism) {\n      return;\n    }\n\n    \n    Prism.languages.dos = Prism.languages.powershell;\n    Prism.languages.gremlin = Prism.languages.groovy;\n\n    let languages = {\n      'r': 'R', 'python': 'Python', 'xml': 'XML', 'html': 'HTML',\n      'yaml': 'YAML', 'latex': 'LaTeX', 'tex': 'TeX',\n      'powershell': 'PowerShell', 'javascript': 'JavaScript',\n      'dos': 'DOS', 'qml': 'QML', 'json': 'JSON', 'bash': 'Bash',\n      'text': 'Text', 'txt': 'Text', 'sparql': 'SPARQL',\n      'gremlin': 'Gremlin', 'cypher': 'Cypher', 'ngql': 'nGQL',\n      'shell': 'Shell', 'sql': 'SQL', 'apacheconf': 'Apache Configuration', 'c': 'C', 'css': 'CSS'\n    };\n\n    Prism.hooks.add('before-highlight', function(env) {\n      if (env.language !== 'plain') {\n        let language = languages[env.language] || env.language;\n        env.element.setAttribute('data-language', language);\n      }\n    });\n\n    \n    let ClipboardJS = window.ClipboardJS || undefined;\n\n    Prism.plugins.toolbar.registerButton('copy-to-clipboard', function(env) {\n      let linkCopy = document.createElement('button');\n      linkCopy.classList.add('prism-button-copy');\n\n      registerClipboard();\n\n      return linkCopy;\n\n      function registerClipboard() {\n        let clip = new ClipboardJS(linkCopy, {\n          'text': function () {\n            return env.code;\n          }\n        });\n\n        clip.on('success', function() {\n          linkCopy.classList.add('prism-button-copy-success');\n          resetText();\n        });\n        clip.on('error', function () {\n          linkCopy.classList.add('prism-button-copy-error');\n          resetText();\n        });\n      }\n\n      function resetText() {\n        setTimeout(function () {\n          linkCopy.classList.remove('prism-button-copy-success');\n          linkCopy.classList.remove('prism-button-copy-error');\n        }, 1600);\n      }\n    });\n  })();\n\u003c/script\u003e\n\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/pseudocode@latest/build/pseudocode.min.js\"\u003e\u003c/script\u003e\n\u003cscript type=\"text/javascript\"\u003e\nlet pseudocodeCaptionCount = 0;\n(function(d) {\n  d.querySelectorAll(\".pseudocode\").forEach(function(elem) {\n    let pseudocode_options = {\n      indentSize: '1.2em',\n      commentDelimiter: '\\/\\/',\n      lineNumber:  true ,\n      lineNumberPunc: ':',\n      noEnd:  false \n    };\n    pseudocode_options.captionCount = pseudocodeCaptionCount;\n    pseudocodeCaptionCount += 1;\n    pseudocode.renderElement(elem, pseudocode_options);\n  });\n})(document);\n\u003c/script\u003e\n\n\n\n\n\n\n\n\u003cscript src=\"/js/figure-slider.js\"\u003e\u003c/script\u003e\n\n\n\n\n\n\u003cscript async=\"\" src=\"/js/center-img.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/right-quote.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/external-link.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/alt-title.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/figure.js\"\u003e\u003c/script\u003e\n\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js\"\u003e\u003c/script\u003e\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/vanilla-back-to-top@latest/dist/vanilla-back-to-top.min.js\"\u003e\u003c/script\u003e\n\u003cscript\u003e\naddBackToTop({\n  diameter: 48\n});\n\u003c/script\u003e\n\n  \u003chr/\u003e\n  \u003cdiv class=\"copyright no-border-bottom\"\u003e\n    \u003cdiv class=\"copyright-author-year\"\u003e\n      \u003cspan\u003eCopyright © 2017-2024 \u003ca href=\"/\"\u003e范叶亮 | Leo Van\u003c/a\u003e\u003c/span\u003e\n    \u003c/div\u003e\n  \u003c/div\u003e\n  \u003c/footer\u003e\n  \u003c/article\u003e",
  "Date": "2020-06-06T00:00:00Z",
  "Author": "范叶亮"
}