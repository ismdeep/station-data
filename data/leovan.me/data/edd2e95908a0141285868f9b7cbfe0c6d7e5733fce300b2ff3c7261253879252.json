{
  "Source": "leovan.me",
  "Title": "隐马尔可夫 (Hidden Markov Model, HMM)，条件随机场 (Conditional Random Fields, CRF) 和序列标注 (Sequence Labeling)",
  "Link": "https://leovan.me/cn/2020/05/hmm-crf-and-sequence-labeling/",
  "Content": "\u003carticle class=\"main\"\u003e\n    \u003cheader class=\"content-title\"\u003e\n    \n\u003ch1 class=\"title\"\u003e\n  \n  隐马尔可夫 (Hidden Markov Model, HMM)，条件随机场 (Conditional Random Fields, CRF) 和序列标注 (Sequence Labeling)\n  \n\u003c/h1\u003e\n\n\n\n\n\n\n\n\u003ch2 class=\"author-date\"\u003e范叶亮 / \n2020-05-02\u003c/h2\u003e\n\n\n\n\u003ch3 class=\"post-meta\"\u003e\n\n\n\u003cstrong\u003e分类: \u003c/strong\u003e\n\u003ca href=\"/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0\"\u003e机器学习\u003c/a\u003e, \u003ca href=\"/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0\"\u003e深度学习\u003c/a\u003e, \u003ca href=\"/categories/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86\"\u003e自然语言处理\u003c/a\u003e\n\n\n\n\n/\n\n\n\n\n\u003cstrong\u003e标签: \u003c/strong\u003e\n\u003cspan\u003e隐马尔可夫\u003c/span\u003e, \u003cspan\u003eHidden Markov Model\u003c/span\u003e, \u003cspan\u003eHMM\u003c/span\u003e, \u003cspan\u003e条件随机场\u003c/span\u003e, \u003cspan\u003eConditional Random Fields\u003c/span\u003e, \u003cspan\u003eCRF\u003c/span\u003e, \u003cspan\u003e生成式\u003c/span\u003e, \u003cspan\u003eGenerative\u003c/span\u003e, \u003cspan\u003e判别式\u003c/span\u003e, \u003cspan\u003eDiscriminative\u003c/span\u003e, \u003cspan\u003e序列标注\u003c/span\u003e, \u003cspan\u003eSequence Labeling\u003c/span\u003e, \u003cspan\u003e词性标注\u003c/span\u003e, \u003cspan\u003ePart-of-speech Tagging\u003c/span\u003e, \u003cspan\u003ePOS Tagging\u003c/span\u003e, \u003cspan\u003ePOS\u003c/span\u003e, \u003cspan\u003e命名实体识别\u003c/span\u003e, \u003cspan\u003eNamed Entity Recognition\u003c/span\u003e, \u003cspan\u003eNER\u003c/span\u003e, \u003cspan\u003eBiLSTM-CRF\u003c/span\u003e, \u003cspan\u003eLattice LSTM\u003c/span\u003e, \u003cspan\u003eNCRF++\u003c/span\u003e, \u003cspan\u003eTENER\u003c/span\u003e\n\n\n\n\n/\n\n\n\u003cstrong\u003e字数: \u003c/strong\u003e\n9039\n\u003c/h3\u003e\n\n\n\n\u003chr/\u003e\n\n\n\n    \n    \n    \u003cins class=\"adsbygoogle\" style=\"display:block; text-align:center;\" data-ad-layout=\"in-article\" data-ad-format=\"fluid\" data-ad-client=\"ca-pub-2608165017777396\" data-ad-slot=\"1261604535\"\u003e\u003c/ins\u003e\n    \u003cscript\u003e\n    (adsbygoogle = window.adsbygoogle || []).push({});\n    \u003c/script\u003e\n    \n    \n    \u003c/header\u003e\n\n\n\n\n\u003ch1 id=\"隐马尔可夫\"\u003e隐马尔可夫\u003c/h1\u003e\n\u003cp\u003e隐马尔可夫模型（Hidden Markov Model，HMM）是一个描述包含隐含未知参数的马尔可夫过程的统计模型。马尔可夫过程（Markov Process）是因俄国数学家安德雷·安德耶维齐·马尔可夫（Андрей Андреевич Марков）而得名一个随机过程，在该随机过程中，给定当前状态和过去所有状态的条件下，其下一个状态的条件概率分布仅依赖于当前状态，通常具备离散状态的马尔可夫过程称之为马尔可夫链（Markov Chain）。因此，马尔可夫链可以理解为一个有限状态机，给定了当前状态为 \u003ccode\u003e$S_i$\u003c/code\u003e 时，下一时刻状态为 \u003ccode\u003e$S_j$\u003c/code\u003e 的概率，不同状态之间变换的概率称之为转移概率。下图描述了 3 个状态 \u003ccode\u003e$S_a, S_b, S_c$\u003c/code\u003e 之间转换状态的马尔可夫链。\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-05-02-hmm-and-crf/hmm-markov-chain-example.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n\u003c/figure\u003e\n\u003cp\u003e隐马尔可夫模型中包含两种序列：随机生成的状态构成的序列称之为状态序列（state sequence），状态序列是不可被观测到的；每个状态对应的观测值组成的序列称之为观测序列（observation sequence）。令 \u003ccode\u003e$I = \\left(i_1, i_2, \\cdots, i_T\\right)$\u003c/code\u003e 为状态序列，其中 \u003ccode\u003e$i_t$\u003c/code\u003e 为第 \u003ccode\u003e$t$\u003c/code\u003e 时刻系统的状态值，对应的有 \u003ccode\u003e$O = \\left(o_1, o_2, \\cdots, o_T\\right)$\u003c/code\u003e 为观测序列，其中 \u003ccode\u003e$o_t$\u003c/code\u003e 为第 \u003ccode\u003e$t$\u003c/code\u003e 时刻系统的观测值，系统的所有可能的状态集合为 \u003ccode\u003e$Q = \\{q_1, q_2, \\cdots, q_N\\}$\u003c/code\u003e，所有可能的观测集合为 \u003ccode\u003e$V= \\{v_1, v_2, \\cdots, v_M\\}$\u003c/code\u003e。\u003c/p\u003e\n\u003cp\u003e隐马尔可夫模型主要由三组参数构成：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e状态转移矩阵：\n\u003ccode\u003e$$ A = \\left[a_{ij}\\right]_{N \\times N} $$\u003c/code\u003e\n其中，\n\u003ccode\u003e$$ a_{ij} = P \\left(i_{t+1} = q_j | i_t = q_i\\right), 1 \\leq i, j \\leq N $$\u003c/code\u003e\n表示 \u003ccode\u003e$t$\u003c/code\u003e 时刻状态为 \u003ccode\u003e$q_i$\u003c/code\u003e 的情况下，在 \u003ccode\u003e$t+1$\u003c/code\u003e 时刻状态转移到 \u003ccode\u003e$q_j$\u003c/code\u003e 的概率。\u003c/li\u003e\n\u003cli\u003e观测概率矩阵：\n\u003ccode\u003e$$ B = \\left[b_j \\left(k\\right)\\right]_{N \\times M} $$\u003c/code\u003e\n其中，\n\u003ccode\u003e$$ b_j \\left(k\\right) = P \\left(o_t = v_k | i_t = q_j\\right), k = 1, 2, \\cdots, M, j = 1, 2, \\cdots, N $$\u003c/code\u003e\n表示 \u003ccode\u003e$t$\u003c/code\u003e 时刻状态为 \u003ccode\u003e$q_i$\u003c/code\u003e 的情况下，观测值为 \u003ccode\u003e$v_k$\u003c/code\u003e 的概率。\u003c/li\u003e\n\u003cli\u003e初始状态概率向量：\n\u003ccode\u003e$$ \\pi = \\left(\\pi_i\\right) $$\u003c/code\u003e\n其中，\n\u003ccode\u003e$$ \\pi_i = P \\left(i_1 = q_i\\right), i = 1, 2, \\cdots, N $$\u003c/code\u003e\n表示 \u003ccode\u003e$t = 1$\u003c/code\u003e 时刻，系统处于状态 \u003ccode\u003e$q_i$\u003c/code\u003e 的概率。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e初始状态概率向量 \u003ccode\u003e$\\pi$\u003c/code\u003e 和状态转移矩阵 \u003ccode\u003e$A$\u003c/code\u003e 决定了状态序列，观测概率矩阵 \u003ccode\u003e$B$ \u003c/code\u003e 决定了状态序列对应的观测序列，因此马尔可夫模型可以表示为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\lambda = \\left(A, B, \\pi\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e对于马尔可夫模型 \u003ccode\u003e$\\lambda = \\left(A, B, \\pi\\right)$\u003c/code\u003e，通过如下步骤生成观测序列 \u003ccode\u003e$\\{o_1, o_2, \\cdots, o_T\\}$\u003c/code\u003e：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e按照初始状态分布 \u003ccode\u003e$\\pi$\u003c/code\u003e 产生状态 \u003ccode\u003e$i_1$\u003c/code\u003e.\u003c/li\u003e\n\u003cli\u003e令 \u003ccode\u003e$t = 1$\u003c/code\u003e。\u003c/li\u003e\n\u003cli\u003e按照状态 \u003ccode\u003e$i_t$\u003c/code\u003e 的观测概率分布 \u003ccode\u003e$b_{i_t} \\left(k\\right)$\u003c/code\u003e 生成 \u003ccode\u003e$o_t$\u003c/code\u003e。\u003c/li\u003e\n\u003cli\u003e按照状态 \u003ccode\u003e$i_t$\u003c/code\u003e 的状态转移概率分布 \u003ccode\u003e$\\left\\{a_{i_t i_{t+1}}\\right\\}$\u003c/code\u003e 产生状态 \u003ccode\u003e$i_{t+1}$\u003c/code\u003e，\u003ccode\u003e$i_{t+1} = 1, 2, \\cdots, N$\u003c/code\u003e。\u003c/li\u003e\n\u003cli\u003e令 \u003ccode\u003e$t = t + 1$\u003c/code\u003e，如果 \u003ccode\u003e$t \u0026lt; T$\u003c/code\u003e，转步骤 3；否则，终止。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e马尔可夫模型在应用过程中有 3 个基本问题 \u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e概率计算问题。给定模型 \u003ccode\u003e$\\lambda = \\left(A, B, \\pi\\right)$\u003c/code\u003e 和观测序列 \u003ccode\u003e$O = \\{o_1, o_2, \\cdots, o_T\\}$\u003c/code\u003e，计算在模型 \u003ccode\u003e$\\lambda$\u003c/code\u003e 下观测序列 \u003ccode\u003e$O$\u003c/code\u003e 出现的概率 \u003ccode\u003e$P\\left(O | \\lambda \\right)$\u003c/code\u003e。\u003c/li\u003e\n\u003cli\u003e学习问题。已知观测序列 \u003ccode\u003e$O = \\{o_1, o_2, \\cdots, o_T\\}$\u003c/code\u003e，估计模型 \u003ccode\u003e$\\lambda = \\left(A, B, \\pi\\right)$\u003c/code\u003e 参数，使得在该模型下观测序列概率 \u003ccode\u003e$P\\left(X | \\lambda \\right)$\u003c/code\u003e 最大。即用极大似然估计的方法估计参数。\u003c/li\u003e\n\u003cli\u003e预测问题，也称为解码（decoding）问题。已知模型 \u003ccode\u003e$\\lambda = \\left(A, B, \\pi\\right)$\u003c/code\u003e 和观测序列 \u003ccode\u003e$O = \\{o_1, o_2, \\cdots, o_T\\}$\u003c/code\u003e，求对给定观测序列条件概率 \u003ccode\u003e$P \\left(I | O\\right)$\u003c/code\u003e 最大的状态序列 \u003ccode\u003e$I = \\{i_1, i_2, \\cdots, i_T\\}$\u003c/code\u003e。即给定观测序列，求最有可能的对应的状态序列。\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"概率计算\"\u003e概率计算\u003c/h2\u003e\n\u003ch3 id=\"直接计算法\"\u003e直接计算法\u003c/h3\u003e\n\u003cp\u003e给定模型 \u003ccode\u003e$\\lambda = \\left(A, B, \\pi \\right)$\u003c/code\u003e 和观测序列 \u003ccode\u003e$O = \\{o_1, o_2, ..., o_T\\}$\u003c/code\u003e，计算在模型 \u003ccode\u003e$\\lambda$\u003c/code\u003e 下观测序列 \u003ccode\u003e$O$\u003c/code\u003e 出现的概率 \u003ccode\u003e$P\\left(O | \\lambda \\right)$\u003c/code\u003e。最简单的办法就是列举出左右可能的状态序列 \u003ccode\u003e$I = \\{i_1, i_2, ..., i_T\\}$\u003c/code\u003e，再根据观测概率矩阵 \u003ccode\u003e$B$\u003c/code\u003e，计算每种状态序列对应的联合概率 \u003ccode\u003e$P \\left(O, I | \\lambda\\right)$\u003c/code\u003e，对其进行求和得到概率 \u003ccode\u003e$P\\left(O | \\lambda \\right)$\u003c/code\u003e。\u003c/p\u003e\n\u003cp\u003e状态序列 \u003ccode\u003e$I = \\{i_1, i_2, ..., i_T\\}$\u003c/code\u003e 的概率是：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ P \\left(I | \\lambda \\right) = \\pi_{y_1} \\prod_{t = 1}^{T - 1} a_{{i_t}{i_{t+1}}} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e对于固定的状态序列 \u003ccode\u003e$I = \\{i_1, i_2, ..., i_T\\}$\u003c/code\u003e，观测序列 \u003ccode\u003e$O = \\{o_1, o_2, ..., o_T\\}$\u003c/code\u003e 的概率是：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ P \\left(O | I, \\lambda \\right) = \\prod_{t = 1}^{T} b_{i_t} \\left(o_t\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$O$\u003c/code\u003e 和 \u003ccode\u003e$I$\u003c/code\u003e 同时出现的联合概率为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{split} P \\left(O, I | \\lambda \\right) \u0026amp;= P \\left(O | I, \\lambda \\right) P \\left(I | \\lambda \\right) \\\\ \u0026amp;= \\pi_{y_1} \\prod_{t = 1}^{T - 1} a_{{i_t}{i_{t+1}}} \\prod_{t = 1}^{T} b_{i_t} \\left(o_t\\right) \\end{split} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e然后，对于所有可能的状态序列 \u003ccode\u003e$I$\u003c/code\u003e 求和，得到观测序列 \u003ccode\u003e$O$\u003c/code\u003e 的概率 \u003ccode\u003e$P \\left(O | \\lambda\\right)$\u003c/code\u003e，即：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{split} P\\left(O | \\lambda \\right) \u0026amp;= \\sum_{I} P \\left(O | I, \\lambda \\right) P \\left(I | \\lambda \\right)  \\\\ \u0026amp;= \\sum_{i_1, i_2, \\cdots, i_T} \\pi_{y_1} \\prod_{t = 1}^{T - 1} a_{{i_t}{i_{t+1}}} \\prod_{t = 1}^{T} b_{i_t} \\left(o_t\\right) \\end{split} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e但利用上式的计算量很大，是 \u003ccode\u003e$O \\left(T N^T\\right)$\u003c/code\u003e 阶的，这种算法不可行。\u003c/p\u003e\n\u003ch3 id=\"前向算法\"\u003e前向算法\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e前向概率\u003c/strong\u003e：给定马尔可夫模型 \u003ccode\u003e$\\lambda$\u003c/code\u003e，给定到时刻 \u003ccode\u003e$t$\u003c/code\u003e 部分观测序列为 \u003ccode\u003e$o_1, o_2, \\cdots, o_t$\u003c/code\u003e 且状态为 \u003ccode\u003e$q_i$\u003c/code\u003e 的概率为前向概率，记作：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\alpha_t \\left(i\\right) = P \\left(o_1, o_2, \\cdots, o_t, i_t = q_i | \\lambda\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e可以递推地求得前向概率 \u003ccode\u003e$\\alpha_t \\left(i\\right)$\u003c/code\u003e 及观测序列概率 \u003ccode\u003e$P \\left(O | \\lambda\\right)$\u003c/code\u003e，前向算法如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e初值\n\u003ccode\u003e$$ \\alpha_{1}(i)=\\pi_{i} b_{i}\\left(o_{1}\\right), \\quad i=1,2, \\cdots, N $$\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e递推，对 \u003ccode\u003e$t = 1, 2, \\cdots, T-1$\u003c/code\u003e\n\u003ccode\u003e$$ \\alpha_{t+1}(i)=\\left[\\sum_{j=1}^{N} \\alpha_{t}(j) a_{j i}\\right] b_{i}\\left(o_{t+1}\\right), \\quad i=1,2, \\cdots, N $$\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e终止\n\u003ccode\u003e$$ P(O | \\lambda)=\\sum_{i=1}^{N} \\alpha_{T}(i) $$\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"后向算法\"\u003e后向算法\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e后向概率\u003c/strong\u003e：给定隐马尔可夫模型 \u003ccode\u003e$\\lambda$\u003c/code\u003e，给定在时刻 \u003ccode\u003e$t$\u003c/code\u003e 状态为 \u003ccode\u003e$q_i$\u003c/code\u003e 的条件下，从 \u003ccode\u003e$t+1$\u003c/code\u003e 到 \u003ccode\u003e$T$\u003c/code\u003e 的部分观测序列为 \u003ccode\u003e$o_{t+1}, o_{t+2}, \\cdots, o_T$\u003c/code\u003e 的概率为后向概率，记作：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\beta_{t}(i)=P\\left(o_{t+1}, o_{t+2}, \\cdots, o_{T} | i_{t}=q_{i}, \\lambda\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e可以递推地求得后向概率 \u003ccode\u003e$\\alpha_t \\left(i\\right)$\u003c/code\u003e 及观测序列概率 \u003ccode\u003e$P \\left(O | \\lambda\\right)$\u003c/code\u003e，后向算法如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e初值\n\u003ccode\u003e$$ \\beta_{T}(i)=1, \\quad i=1,2, \\cdots, N $$\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e递推，对 \u003ccode\u003e$t = T-1, T-2, \\cdots, 1$\u003c/code\u003e\n\u003ccode\u003e$$ \\beta_{t}(i)=\\sum_{j=1}^{N} a_{i j} b_{j}\\left(o_{t+1}\\right) \\beta_{t+1}(j), \\quad i=1,2, \\cdots, N $$\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e终止\n\u003ccode\u003e$$ P(O | \\lambda)=\\sum_{i=1}^{N} \\pi_{i} b_{i}\\left(o_{1}\\right) \\beta_{1}(i) $$\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"学习算法\"\u003e学习算法\u003c/h2\u003e\n\u003ch3 id=\"监督学习算法\"\u003e监督学习算法\u003c/h3\u003e\n\u003cp\u003e假设以给训练数据包含 \u003ccode\u003e$S$\u003c/code\u003e 个长度相同的观测序列和对应的状态序列 \u003ccode\u003e$\\left\\{\\left(O_1, I_1\\right), \\left(O_2, I_2\\right), \\cdots, \\left(O_S, I_S\\right)\\right\\}$\u003c/code\u003e，那么可以利用极大似然估计法来估计隐马尔可夫模型的参数。\u003c/p\u003e\n\u003cp\u003e设样本中时刻 \u003ccode\u003e$t$\u003c/code\u003e 处于状态 \u003ccode\u003e$i$\u003c/code\u003e 时刻 \u003ccode\u003e$t+1$\u003c/code\u003e 转移到状态 \u003ccode\u003e$j$\u003c/code\u003e 的频数为 \u003ccode\u003e$A_{ij}$\u003c/code\u003e，那么转移概率 \u003ccode\u003e$a_{ij}$\u003c/code\u003e 的估计是：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\hat{a}_{i j}=\\frac{A_{i j}}{\\sum_{j=1}^{N} A_{i j}}, \\quad i=1,2, \\cdots, N ; \\quad j=1,2, \\cdots, N $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e设样本中状态为 \u003ccode\u003e$j$\u003c/code\u003e 并观测为 \u003ccode\u003e$k$\u003c/code\u003e 的频数是 \u003ccode\u003e$B_{jk}$\u003c/code\u003e，那么状态为 \u003ccode\u003e$j$\u003c/code\u003e 观测为 \u003ccode\u003e$k$\u003c/code\u003e 的概率 \u003ccode\u003e$b_j \\left(k\\right)$\u003c/code\u003e 的估计是：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\hat{b}_{j}(k)=\\frac{B_{j k}}{\\sum_{k=1}^{M} B_{j k}}, \\quad j=1,2, \\cdots, N ; \\quad k=1,2, \\cdots, M $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e初始状态概率 \u003ccode\u003e$\\pi_i$\u003c/code\u003e 的估计 \u003ccode\u003e$\\hat{\\pi}_i$\u003c/code\u003e 为 \u003ccode\u003e$S$\u003c/code\u003e 个样本中初始状态为 \u003ccode\u003e$q_i$\u003c/code\u003e 的频率。\u003c/p\u003e\n\u003ch3 id=\"无监督学习算法\"\u003e无监督学习算法\u003c/h3\u003e\n\u003cp\u003e假设给定训练数据值包含 \u003ccode\u003e$S$\u003c/code\u003e 个长度为 \u003ccode\u003e$T$\u003c/code\u003e 的观测序列 \u003ccode\u003e$\\left\\{O_1, O_2, \\cdots, O_S\\right\\}$\u003c/code\u003e 而没有对应的状态序例，目标是学习隐马尔可夫模型 \u003ccode\u003e$\\lambda = \\left(A, B, \\pi\\right)$\u003c/code\u003e 的参数。我们将观测序列数据看做观测数据 \u003ccode\u003e$O$\u003c/code\u003e，状态序列数据看作不可观测的隐数据 \u003ccode\u003e$I$\u003c/code\u003e，那么马尔可夫模型事实上是一个含有隐变量的概率模型：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ P(O | \\lambda)=\\sum_{I} P(O | I, \\lambda) P(I | \\lambda) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e它的参数学习可以由 EM 算法实现。EM 算法在隐马尔可夫模型学习中的具体实现为 Baum-Welch 算法：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e初始化。对 \u003ccode\u003e$n = 0$\u003c/code\u003e，选取 \u003ccode\u003e$a_{i j}^{(0)}, b_{j}(k)^{(0)}, \\pi_{i}^{(0)}$\u003c/code\u003e，得到模型 \u003ccode\u003e$\\lambda^{(0)}=\\left(A^{(0)}, B^{(0)}, \\pi^{(0)}\\right)$\u003c/code\u003e。\u003c/li\u003e\n\u003cli\u003e递推。对 \u003ccode\u003e$n = 1, 2, \\cdots$\u003c/code\u003e：\n\u003ccode\u003e$$ \\begin{aligned} a_{i j}^{(n+1)} \u0026amp;= \\frac{\\sum_{t=1}^{T-1} \\xi_{t}(i, j)}{\\sum_{t=1}^{T-1} \\gamma_{t}(i)} \\\\ b_{j}(k)^{(n+1)} \u0026amp;= \\frac{\\sum_{t=1, o_{t}=v_{k}}^{T} \\gamma_{t}(j)}{\\sum_{t=1}^{T} \\gamma_{t}(j)} \\\\ \\pi_{i}^{(n+1)} \u0026amp;= \\gamma_{1}(i) \\end{aligned} $$\u003c/code\u003e\n右端各按照观测 \u003ccode\u003e$O=\\left(o_{1}, o_{2}, \\cdots, o_{T}\\right)$\u003c/code\u003e 和模型 \u003ccode\u003e$\\lambda^{(n)}=\\left(A^{(n)}, B^{(n)}, \\pi^{(n)}\\right)$\u003c/code\u003e 计算，\n\u003ccode\u003e$$ \\begin{aligned} \\gamma_{t}(i) \u0026amp;= \\frac{\\alpha_{t}(i) \\beta_{t}(i)}{P(O | \\lambda)}=\\frac{\\alpha_{t}(i) \\beta_{t}(i)}{\\sum_{j=1}^{N} \\alpha_{t}(j) \\beta_{t}(j)} \\\\ \\xi_{t}(i, j) \u0026amp;= \\frac{\\alpha_{t}(i) a_{i j} b_{j}\\left(o_{t+1}\\right) \\beta_{t+1}(j)}{\\sum_{i=1}^{N} \\sum_{j=1}^{N} \\alpha_{t}(i) a_{i j} b_{j}\\left(o_{t+1}\\right) \\beta_{t+1}(j)} \\end{aligned} $$\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e终止。得到模型参数 \u003ccode\u003e$\\lambda^{(n+1)}=\\left(A^{(n+1)}, B^{(n+1)}, \\pi^{(n+1)}\\right)$\u003c/code\u003e。\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"预测算法\"\u003e预测算法\u003c/h2\u003e\n\u003ch3 id=\"近似算法\"\u003e近似算法\u003c/h3\u003e\n\u003cp\u003e近似算法的思想是，在每个时刻 \u003ccode\u003e$t$\u003c/code\u003e 选择在该时刻最有可能出现的状态 \u003ccode\u003e$i_t^*$\u003c/code\u003e，从而得到一个状态序列 \u003ccode\u003e$I^{*}=\\left(i_{1}^{*}, i_{2}^{*}, \\cdots, i_{T}^{*}\\right)$\u003c/code\u003e，将它作为预测的结果。给定隐马尔可夫模型 \u003ccode\u003e$\\lambda$\u003c/code\u003e 和观测序列 \u003ccode\u003e$O$\u003c/code\u003e，在时刻 \u003ccode\u003e$t$\u003c/code\u003e 处于状态 \u003ccode\u003e$q_i$\u003c/code\u003e 的概率 \u003ccode\u003e$\\gamma_t \\left(i\\right)$\u003c/code\u003e 是：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\gamma_{t}(i)=\\frac{\\alpha_{t}(i) \\beta_{t}(i)}{P(O | \\lambda)}=\\frac{\\alpha_{t}(i) \\beta_{t}(i)}{\\sum_{j=1}^{N} \\alpha_{t}(j) \\beta_{t}(j)} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e在每一时刻 \u003ccode\u003e$t$\u003c/code\u003e 最有可能的状态 \u003ccode\u003e$i_t^*$\u003c/code\u003e 是：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ i_{t}^{*}=\\arg \\max _{1 \\leqslant i \\leqslant N}\\left[\\gamma_{t}(i)\\right], \\quad t=1,2, \\cdots, T $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e从而得到状态序列 \u003ccode\u003e$I^{*}=\\left(i_{1}^{*}, i_{2}^{*}, \\cdots, i_{T}^{*}\\right)$\u003c/code\u003e。\u003c/p\u003e\n\u003cp\u003e近似算法的优点是计算简单，其缺点是不能保证预测的状态序列整体是最有可能的状态序列，因为预测的状态序列可能有实际不发生的部分。事实上，上述方法得到的状态序列中有可能存在转移概率为0的相邻状态，即对某些 \u003ccode\u003e$i, j, a_{ij} = 0$\u003c/code\u003e 。尽管如此，近似算法仍然是有用的。\u003c/p\u003e\n\u003ch3 id=\"维特比算法\"\u003e维特比算法\u003c/h3\u003e\n\u003cp\u003e维特比算法（Viterbi Algorithm）实际是用动态规划（Dynamic Programming）解隐马尔可夫模型预测问题，即用动态规划求概率最大路径（最优路径）。这时一条路径对应着一个状态序列。\u003c/p\u003e\n\u003cp\u003e首先导入两个变量 \u003ccode\u003e$\\sigma$\u003c/code\u003e 和 \u003ccode\u003e$\\Psi$\u003c/code\u003e。定义在时刻 \u003ccode\u003e$t$\u003c/code\u003e 状态为 \u003ccode\u003e$i$\u003c/code\u003e 的所有单个路径 \u003ccode\u003e$\\left(i_1, i_2, \\cdots, i_t\\right)$\u003c/code\u003e 中概率最大值为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\delta_{t}(i)=\\max _{i_{1}, i_{2}, \\cdots, i_{t-1}} P\\left(i_{t}=i, i_{t-1}, \\cdots, i_{1}, o_{t}, \\cdots, o_{1} | \\lambda\\right), \\quad i=1,2, \\cdots, N $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e由定义可得变量 \u003ccode\u003e$\\sigma$\u003c/code\u003e 的递推公式：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{aligned} \\delta_{t+1}(i) \u0026amp;=\\max _{i_{1}, i_{2}, \\cdots, i_{t}} P\\left(i_{t+1}=i, i_{t}, \\cdots, i_{1}, o_{t+1}, \\cdots, o_{1} | \\lambda\\right) \\\\ \u0026amp;=\\max _{1 \\leqslant j \\leqslant N}\\left[\\delta_{t}(j) a_{j i}\\right] b_{i}\\left(o_{t+1}\\right), \\quad i=1,2, \\cdots, N ; \\quad t=1,2, \\cdots, T-1 \\end{aligned} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e定义在时刻 \u003ccode\u003e$t$\u003c/code\u003e 状态为 \u003ccode\u003e$i$\u003c/code\u003e 的所有单个路径 \u003ccode\u003e$\\left(i_1, i_2, \\cdots, i_{t-1}, i\\right)$\u003c/code\u003e 中概率最大的路径的第 \u003ccode\u003e$t - 1$\u003c/code\u003e 个结点为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\Psi_{t}(i)=\\arg \\max _{1 \\leqslant j \\leqslant N}\\left[\\delta_{t-1}(j) a_{j i}\\right], \\quad i=1,2, \\cdots, N $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e维特比算法流程如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e初始化\n\u003ccode\u003e$$ \\begin{array}{c} \\delta_{1}(i)=\\pi_{i} b_{i}\\left(o_{1}\\right), \\quad i=1,2, \\cdots, N \\\\ \\Psi_{1}(i)=0, \\quad i=1,2, \\cdots, N \\end{array} $$\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e递推。对 \u003ccode\u003e$t = 2, 3, \\cdots, T$\u003c/code\u003e\n\u003ccode\u003e$$ \\begin{array}{c} \\delta_{t}(i)=\\max _{1 \\leqslant j \\leqslant N}\\left[\\delta_{t-1}(j) a_{j i}\\right] b_{i}\\left(o_{t}\\right), \\quad i=1,2, \\cdots, N \\\\ \\Psi_{t}(i)=\\arg \\max _{1 \\leqslant j \\leqslant N}\\left[\\delta_{t-1}(j) a_{j i}\\right], \\quad i=1,2, \\cdots, N \\end{array} $$\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e终止。\n\u003ccode\u003e$$ \\begin{array}{c} P^{*}=\\max _{1 \\leqslant i \\leqslant N} \\delta_{T}(i) \\\\ i_{T}^{*}=\\arg \\max _{1 \\leqslant i \\leqslant N}\\left[\\delta_{T}(i)\\right] \\end{array} $$\u003c/code\u003e\u003c/li\u003e\n\u003cli\u003e最优路径回溯。对 \u003ccode\u003e$t = T - 1, T - 2, \\cdots, 1$\u003c/code\u003e\n\u003ccode\u003e$$ i_{t}^{*}=\\Psi_{t+1}\\left(i_{t+1}^{*}\\right) $$\u003c/code\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e求的最优路径 \u003ccode\u003e$I^{*}=\\left(i_{1}^{*}, i_{2}^{*}, \\cdots, i_{T}^{*}\\right)$\u003c/code\u003e。\u003c/p\u003e\n\u003ch1 id=\"条件随机场\"\u003e条件随机场\u003c/h1\u003e\n\u003cp\u003e概率无向图模型（Probabilistic Undirected Graphical Model）又称为马尔可夫随机场（Markov Random Field），是一个可以由无向图表示的联合概率分布。概率图模型（Probabilistic Graphical Model）是由图表示的概率分布，设有联合概率分布 \u003ccode\u003e$P \\left(Y\\right), Y \\in \\mathcal{Y}$\u003c/code\u003e 是一组随机变量。由无向图 \u003ccode\u003e$G = \\left(V, E\\right)$\u003c/code\u003e 表示概率分布 \u003ccode\u003e$P \\left(Y\\right)$\u003c/code\u003e，即在图 \u003ccode\u003e$G$\u003c/code\u003e 中，结点 \u003ccode\u003e$v \\in V$\u003c/code\u003e 表示一个随机变量 \u003ccode\u003e$Y_v, Y = \\left(Y_v\\right)_{v \\in V}$\u003c/code\u003e，边 \u003ccode\u003e$e \\in E$\u003c/code\u003e 表示随机变量之间的概率依赖关系。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e成对马尔可夫性\u003c/strong\u003e：设 \u003ccode\u003e$u$\u003c/code\u003e 和 \u003ccode\u003e$v$\u003c/code\u003e 是无向图 \u003ccode\u003e$G$\u003c/code\u003e 中任意两个没有边连接的结点，结点 \u003ccode\u003e$u$\u003c/code\u003e 和 \u003ccode\u003e$v$\u003c/code\u003e 分别对应随机变量 \u003ccode\u003e$Y_u$\u003c/code\u003e 和 \u003ccode\u003e$Y_v$\u003c/code\u003e。其他所有结点为 \u003ccode\u003e$O$\u003c/code\u003e，对应的随机变量组是 \u003ccode\u003e$Y_O$\u003c/code\u003e。成对马尔可夫是指给定随机变量组 \u003ccode\u003e$Y_O$\u003c/code\u003e 的条件下随机变量 \u003ccode\u003e$Y_u$\u003c/code\u003e 和 \u003ccode\u003e$Y_v$\u003c/code\u003e 是条件独立的，即：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ P\\left(Y_{u}, Y_{v} | Y_{O}\\right)=P\\left(Y_{u} | Y_{O}\\right) P\\left(Y_{v} | Y_{O}\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e局部马尔可夫性\u003c/strong\u003e：设 \u003ccode\u003e$v \\in V$\u003c/code\u003e 是无向图 \u003ccode\u003e$G$\u003c/code\u003e 中任意一个结点，\u003ccode\u003e$W$\u003c/code\u003e 是与 \u003ccode\u003e$v$\u003c/code\u003e 有边连接的所有结点，\u003ccode\u003e$O$\u003c/code\u003e 是 \u003ccode\u003e$v$\u003c/code\u003e 和 \u003ccode\u003e$W$\u003c/code\u003e 以外的其他所有结点。\u003ccode\u003e$v$\u003c/code\u003e 表示的随机变量是 \u003ccode\u003e$Y_v$\u003c/code\u003e，\u003ccode\u003e$W$\u003c/code\u003e 表示的随机变量组是 \u003ccode\u003e$Y_W$\u003c/code\u003e，\u003ccode\u003e$O$\u003c/code\u003e 表示的随机变量组是 \u003ccode\u003e$Y_O$\u003c/code\u003e。局部马尔可夫性是指在给定随机变量组 \u003ccode\u003e$Y_W$\u003c/code\u003e 的条件下随机变量 \u003ccode\u003e$Y_v$\u003c/code\u003e 与随机变量组 \u003ccode\u003e$Y_O$\u003c/code\u003e 是独立的，即：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ P\\left(Y_{v}, Y_{O} | Y_{W}\\right)=P\\left(Y_{v} | Y_{W}\\right) P\\left(Y_{O} | Y_{W}\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e在 \u003ccode\u003e$P \\left(Y_O | Y_W\\right) \u0026gt; 0$\u003c/code\u003e 时，等价地：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ P\\left(Y_{v} | Y_{W}\\right)=P\\left(Y_{v} | Y_{W}, Y_{O}\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e局部马尔可夫性如下图所示：\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-05-02-hmm-and-crf/local-markov.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n\u003c/figure\u003e\n\u003cp\u003e\u003cstrong\u003e全局马尔可夫性\u003c/strong\u003e：设结点结合 \u003ccode\u003e$A, B$\u003c/code\u003e 是在无向图 \u003ccode\u003e$G$\u003c/code\u003e 中被结点集合 \u003ccode\u003e$C$\u003c/code\u003e 分开的任意结点集合，如下图所示。结点集合 \u003ccode\u003e$A, B$\u003c/code\u003e 和 \u003ccode\u003e$C$\u003c/code\u003e 所对应的随机变量组分别是 \u003ccode\u003e$Y_A, Y_B$\u003c/code\u003e 和 \u003ccode\u003e$Y_C$\u003c/code\u003e。全局马尔可夫性是指给定随机变量组 \u003ccode\u003e$Y_C$\u003c/code\u003e 条件下随机变量组 \u003ccode\u003e$Y_A$\u003c/code\u003e 和 \u003ccode\u003e$Y_B$\u003c/code\u003e 是条件独立的，即：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ P\\left(Y_{A}, Y_{B} | Y_{C}\\right)=P\\left(Y_{A} | Y_{C}\\right) P\\left(Y_{B} | Y_{C}\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-05-02-hmm-and-crf/global-markov.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n\u003c/figure\u003e\n\u003cp\u003e\u003cstrong\u003e概率无向图模型\u003c/strong\u003e定义为：设有联合概率分布 \u003ccode\u003e$P \\left(Y\\right)$\u003c/code\u003e，由无向图 \u003ccode\u003e$G = \\left(V, E\\right)$\u003c/code\u003e 表示，在图 \u003ccode\u003e$G$\u003c/code\u003e 中，结点表示随机变量，边表示随机变量之间的依赖关系。如果联合概率分布 \u003ccode\u003e$P \\left(Y\\right)$\u003c/code\u003e 满足成对、局部或全局马尔可夫性，就称此联合概率分布为概率无向图模型（Probabilistic Undirected Graphical Model），或马尔可夫随机场（Markov Random Field）。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e团与最大团\u003c/strong\u003e：无向图 \u003ccode\u003e$G$\u003c/code\u003e 中任何两个结点均有边连接的结点子集称为团（Clique）。若 \u003ccode\u003e$C$\u003c/code\u003e 是无向图 \u003ccode\u003e$G$\u003c/code\u003e 的一个团，并且不能再加进任何一个 \u003ccode\u003e$G$\u003c/code\u003e 的结点时期成为一个更大的团，则称此 \u003ccode\u003e$C$\u003c/code\u003e 为最大团（Maximal Clique）。\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-05-02-hmm-and-crf/clique.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n  \u003cfigcaption class=\"kai\"\u003e无向图的团和最大团\u003c/figcaption\u003e\n  \n\u003c/figure\u003e\n\u003cp\u003e上图表示 4 个结点组成的无向图。图中有 2 个结点组成的团有 5 个：\u003ccode\u003e$\\left\\{Y_1, Y_2\\right\\}$\u003c/code\u003e，\u003ccode\u003e$\\left\\{Y_2, Y_3\\right\\}$\u003c/code\u003e，\u003ccode\u003e$\\left\\{Y_3, Y_4\\right\\}$\u003c/code\u003e，\u003ccode\u003e$\\left\\{Y_4, Y_2\\right\\}$\u003c/code\u003e 和 \u003ccode\u003e$\\left\\{Y_1, Y_3\\right\\}$\u003c/code\u003e。有 2 个最大团：\u003ccode\u003e$\\left\\{Y_1, Y_2, Y_3\\right\\}$\u003c/code\u003e 和 \u003ccode\u003e$\\left\\{Y_2, Y_3, Y_4\\right\\}$\u003c/code\u003e。而 \u003ccode\u003e$\\left\\{Y_1, Y_2, Y_3, Y_4\\right\\}$\u003c/code\u003e 不是一个团，因为 \u003ccode\u003e$Y_1$\u003c/code\u003e 和 \u003ccode\u003e$Y_4$\u003c/code\u003e 没有边连接。\u003c/p\u003e\n\u003cp\u003e将概率无向图模型的联合概率分布表示为其最大团上的随机变量的函数的乘积形式的操作，称为概率无向图模型的因子分解。给定无向图模型，设其无向图为 \u003ccode\u003e$G$\u003c/code\u003e，\u003ccode\u003e$C$\u003c/code\u003e 为 \u003ccode\u003e$G$\u003c/code\u003e 上的最大团，\u003ccode\u003e$Y_C$\u003c/code\u003e 表示 \u003ccode\u003e$C$\u003c/code\u003e 对应的随机变量。那么概率无向图模型的联合概率分布 \u003ccode\u003e$P \\left(Y\\right)$\u003c/code\u003e 可以写作图中所有最大团 \u003ccode\u003e$C$\u003c/code\u003e 上的函数 \u003ccode\u003e$\\Psi_C \\left(Y_C\\right)$\u003c/code\u003e 的乘积形式，即：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ P(Y)=\\frac{1}{Z} \\prod_{C} \\Psi_{C}\\left(Y_{C}\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$Z$\u003c/code\u003e 是规范化因子：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ Z=\\sum_{Y} \\prod_{C} \\Psi_{C}\\left(Y_{C}\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e规范化因子保证 \u003ccode\u003e$P \\left(Y\\right)$\u003c/code\u003e 构成一个概率分布。函数 \u003ccode\u003e$\\Psi_C \\left(Y_C\\right)$\u003c/code\u003e 称为\u003cstrong\u003e势函数\u003c/strong\u003e，这里要求势函数 \u003ccode\u003e$\\Psi_C \\left(Y_C\\right)$\u003c/code\u003e 是严格正的，通常定义为指数函数：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\Psi_{C}\\left(Y_{C}\\right)=\\exp \\left\\{-E\\left(Y_{C}\\right)\\right\\} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e概率无向图模型的因子分解由这个 Hammersley-Clifford 定理来保证。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e条件随机场\u003c/strong\u003e（Conditional Random Field）是给定随机变量 \u003ccode\u003e$X$\u003c/code\u003e 条件下，随机变量 \u003ccode\u003e$Y$\u003c/code\u003e 的马尔可夫随机场。设 \u003ccode\u003e$X$\u003c/code\u003e 与 \u003ccode\u003e$Y$\u003c/code\u003e 是随机变量，\u003ccode\u003e$P \\left(Y | X\\right)$\u003c/code\u003e 是给定 \u003ccode\u003e$X$\u003c/code\u003e 的条件下 \u003ccode\u003e$Y$\u003c/code\u003e 的条件概率分布。若随机变量 \u003ccode\u003e$Y$\u003c/code\u003e 构成一个有无向图 \u003ccode\u003e$G = \\left(V, E\\right)$\u003c/code\u003e 表示的马尔可夫随机场，即：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ P\\left(Y_{v} | X, Y_{w}, w \\neq v\\right)=P\\left(Y_{v} | X, Y_{w}, w \\sim v\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e对任意结点 \u003ccode\u003e$v$\u003c/code\u003e 成立，则称条件概率分布 \u003ccode\u003e$P \\left(Y | X\\right)$\u003c/code\u003e 为条件随机场。其中，\u003ccode\u003e$w \\sim v$\u003c/code\u003e 表示在图 \u003ccode\u003e$G = \\left(V, E\\right)$\u003c/code\u003e 中与结点 \u003ccode\u003e$v$\u003c/code\u003e 有边连接的所有结点 \u003ccode\u003e$w$\u003c/code\u003e，\u003ccode\u003e$w \\neq v$\u003c/code\u003e 表示结点 \u003ccode\u003e$v$\u003c/code\u003e 以外的所有结点，\u003ccode\u003e$Y_v, Y_u$\u003c/code\u003e 与 \u003ccode\u003e$Y_w$\u003c/code\u003e 为结点 \u003ccode\u003e$v, u$\u003c/code\u003e 和 \u003ccode\u003e$w$\u003c/code\u003e 对应的随机变量。\u003c/p\u003e\n\u003cp\u003e定义中并没有要求 \u003ccode\u003e$X$\u003c/code\u003e 和 \u003ccode\u003e$Y$\u003c/code\u003e 具有相同的结构，一般假设 \u003ccode\u003e$X$\u003c/code\u003e 和 \u003ccode\u003e$Y$\u003c/code\u003e 有相同的图结构，下图展示了无向图的线性链情况，即：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ G=(V=\\{1,2, \\cdots, n\\}, E=\\{(i, i+1)\\}), \\quad i=1,2, \\cdots, n-1 $$\u003c/code\u003e\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-05-02-hmm-and-crf/linear-crf-1.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n  \u003cfigcaption class=\"kai\"\u003e线性链条件随机场\u003c/figcaption\u003e\n  \n\u003c/figure\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-05-02-hmm-and-crf/linear-crf-2.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n  \u003cfigcaption class=\"kai\"\u003eX 和 Y 有相同的图结构的线性链条件随机场\u003c/figcaption\u003e\n  \n\u003c/figure\u003e\n\u003cp\u003e此情况下，\u003ccode\u003e$X=\\left(X_{1}, X_{2}, \\cdots, X_{n}\\right), Y=\\left(Y_{1}, Y_{2}, \\cdots, Y_{n}\\right)$\u003c/code\u003e，最大团是相邻两个结点的集合。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e线性链条件随机场\u003c/strong\u003e：设 \u003ccode\u003e$X=\\left(X_{1}, X_{2}, \\cdots, X_{n}\\right), Y=\\left(Y_{1}, Y_{2}, \\cdots, Y_{n}\\right)$\u003c/code\u003e 均为线性链表示的随机变量序列，若在给定随机变量序列 \u003ccode\u003e$X$\u003c/code\u003e 的条件下，随机变量序列 \u003ccode\u003e$Y$\u003c/code\u003e 的条件概率分布 \u003ccode\u003e$P \\left(Y | X\\right)$\u003c/code\u003e 构成条件随机场，即满足马尔可夫性：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{array}{c} P\\left(Y_{i} | X, Y_{1}, \\cdots, Y_{i-1}, Y_{i+1}, \\cdots, Y_{n}\\right)=P\\left(Y_{i} | X, Y_{i-1}, Y_{i+1}\\right) \\\\ i=1,2, \\cdots, n \\quad (\\text { 在 } i=1 \\text { 和 } n \\text { 时只考虑单边 }) \\end{array} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e则称 \u003ccode\u003e$P \\left(Y | X\\right)$\u003c/code\u003e 为线性链条件随机场。在标注问题中，\u003ccode\u003e$X$\u003c/code\u003e 表示输入观测序列，\u003ccode\u003e$Y$\u003c/code\u003e 表示对应的输出标记序列或状态序列。\u003c/p\u003e\n\u003cp\u003e根据 Hammersley-Clifford 定理，设 \u003ccode\u003e$P \\left(Y | X\\right)$\u003c/code\u003e 为线性链条件随机场，则在随机变量 \u003ccode\u003e$X$\u003c/code\u003e 取值为 \u003ccode\u003e$x$\u003c/code\u003e 的条件下，随机变量 \u003ccode\u003e$Y$\u003c/code\u003e 取值为 \u003ccode\u003e$y$\u003c/code\u003e 的条件概率有如下形式：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ P(y | x)=\\frac{1}{Z(x)} \\exp \\left(\\sum_{i, k} \\lambda_{k} t_{k}\\left(y_{i-1}, y_{i}, x, i\\right)+\\sum_{i, l} \\mu_{l} s_{l}\\left(y_{i}, x, i\\right)\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ Z(x)=\\sum_{y} \\exp \\left(\\sum_{i, k} \\lambda_{k} t_{k}\\left(y_{i-1}, y_{i}, x, i\\right)+\\sum_{i, l} \\mu_{l} s_{l}\\left(y_{i}, x, i\\right)\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$t_k$\u003c/code\u003e 和 \u003ccode\u003e$s_l$\u003c/code\u003e 是特征函数，\u003ccode\u003e$\\lambda_k$\u003c/code\u003e 和 \u003ccode\u003e$\\mu_l$\u003c/code\u003e 是对应的权值。\u003ccode\u003e$Z \\left(x\\right)$\u003c/code\u003e 是规范化因子，求和是在所有可能的输出序列上进行的。\u003c/p\u003e\n\u003cp\u003e条件随机场的概率计算，学习算法和预测算法类似隐马尔可夫模型，在此不进行过多赘述，有兴趣的同学可以参见 \u003csup id=\"fnref1:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e。\u003c/p\u003e\n\u003cp\u003e综上所述，隐马尔可夫模型和条件随机场的主要联系和区别如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eHMM 是概率有向图，CRF 是概率无向图\u003c/li\u003e\n\u003cli\u003eHMM 是生成模型，CRF 是判别模型\u003c/li\u003e\n\u003c/ol\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-05-02-hmm-and-crf/relationship-between-nb-lr-hmm-lcrf-gdm-gcrf.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n  \u003cfigcaption class=\"kai\"\u003e图片来源：An Introduction to Conditional Random Fields\u003c/figcaption\u003e\n  \n\u003c/figure\u003e\n\u003cp\u003e如上图所示，上面部分为生成式模型，下面部分为判别式模型，生成式模型尝试构建联合分布 \u003ccode\u003e$P \\left(Y, X\\right)$\u003c/code\u003e，而判别模型则尝试构建条件分布 \u003ccode\u003e$P \\left(Y | X\\right)$\u003c/code\u003e。\u003c/p\u003e\n\u003ch1 id=\"序列标注\"\u003e序列标注\u003c/h1\u003e\n\u003cp\u003e序列标注（Sequence Labeling）是自然语言处理中的一项重要任务，对于给定的文本序列需要给出对应的标注序列。常见的序列标注任务包含：组块分析（Chunking），词性标注（Part-of-Speech，POS）和命名实体识别（Named Entity Recognition，NER）。\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-05-02-hmm-and-crf/pos-ner-demo.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n\u003c/figure\u003e\n\u003cp\u003e上图为一段文本的词性标注和命名实体识别的结果。\u003c/p\u003e\n\u003ch2 id=\"词性标注\"\u003e词性标注\u003c/h2\u003e\n\u003cp\u003e词性标注是指为分词结果中的每个单词标注一个正确的词性，即确定每个词是名词、动词、形容词或其他词性的过程。\u003c/p\u003e\n\u003cp\u003e一些常用中文标注规范如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e北京大学现代汉语语料库基本加工规范 \u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e\u003c/li\u003e\n\u003cli\u003e北大语料库加工规范：切分·词性标注·注音 \u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e\u003c/li\u003e\n\u003cli\u003e计算所汉语词性标记集 3.0（ICTPOS 3.0）\u003csup id=\"fnref:4\"\u003e\u003ca href=\"#fn:4\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e4\u003c/a\u003e\u003c/sup\u003e\u003c/li\u003e\n\u003cli\u003eThe Part-Of-Speech Tagging Guidelines for the Penn Chinese Treebank (3.0) \u003csup id=\"fnref:5\"\u003e\u003ca href=\"#fn:5\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e5\u003c/a\u003e\u003c/sup\u003e\u003c/li\u003e\n\u003cli\u003e中文文本标注规范（微软亚洲研究院）\u003csup id=\"fnref:6\"\u003e\u003ca href=\"#fn:6\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e6\u003c/a\u003e\u003c/sup\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"命名实体识别\"\u003e命名实体识别\u003c/h2\u003e\n\u003cp\u003e命名实体识别，又称作“专名识别”，是指识别文本中具有特定意义的实体，主要包括人名、地名、机构名、专有名词等。简单的讲，就是识别自然文本中的实体指称的边界和类别。\u003c/p\u003e\n\u003cp\u003e常用的标注标准有 IO，BIO，BIOES，BMEWO 和 BMEWO+ 等。（参考自：\u003ca href=\"https://lingpipe-blog.com/2009/10/14/coding-chunkers-as-taggers-io-bio-bmewo-and-bmewo/\"\u003eCoding Chunkers as Taggers: IO, BIO, BMEWO, and BMEWO+\u003c/a\u003e）\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eIO 标注标准是最简单的标注方式，对于命名实体类别 X 标注为 \u003ccode\u003eI_X\u003c/code\u003e，其他则标注为 \u003ccode\u003eO\u003c/code\u003e。由于没有标签界线表示，这种方式无法表示两个相邻的同类命名实体。\u003c/li\u003e\n\u003cli\u003eBIO 标注标准将命名实体的起始部分标记为 \u003ccode\u003eB_X\u003c/code\u003e，其余部分标记为 \u003ccode\u003eI_X\u003c/code\u003e。\u003c/li\u003e\n\u003cli\u003eBIOES 标注标准将命名实体的起始部分标记为 \u003ccode\u003eB_X\u003c/code\u003e，中间部分标记为 \u003ccode\u003eI_X\u003c/code\u003e，结尾部分标记为 \u003ccode\u003eE_X\u003c/code\u003e，对于单个字符成为命名实体的情况标记为 \u003ccode\u003eS_X\u003c/code\u003e。\u003c/li\u003e\n\u003cli\u003eBMEWO 标注标准将命名实体的起始部分标记为 \u003ccode\u003eB_X\u003c/code\u003e，中间部分标记为 \u003ccode\u003eM_X\u003c/code\u003e，结尾部分标记为 \u003ccode\u003eE_X\u003c/code\u003e，对于单个字符成为命名实体的情况标记为 \u003ccode\u003eW_X\u003c/code\u003e。\u003c/li\u003e\n\u003cli\u003eBMEWO+ 标注标准在 BMEWO 的基础上针对不同情况的非命名实体标签的标注进行了扩展，同时增加了一个句外（out-of-sentence）标签 \u003ccode\u003eW_OOS\u003c/code\u003e，句子起始标签 \u003ccode\u003eBB_O_OOS\u003c/code\u003e 和句子结束标签 \u003ccode\u003eWW_O_OOS\u003c/code\u003e，如 \u003ca href=\"http://www.alias-i.com/lingpipe/docs/api/com/aliasi/chunk/HmmChunker.html\"\u003e下表\u003c/a\u003e 所示：\u003c/li\u003e\n\u003c/ol\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e标签\u003c/th\u003e\n\u003cth\u003e描述\u003c/th\u003e\n\u003cth\u003e可能上接的标签\u003c/th\u003e\n\u003cth\u003e可能下接的标签\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eB_X\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e命名实体类型 X 的起始\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eE_Y, W_Y, EE_O_X, WW_O_X\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eM_X, W_X\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eM_X\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e命名实体类型 X 的中间\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eB_X, M_X\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eM_X, W_X\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eE_X\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e命名实体类型 X 的结尾\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eB_X, M_X\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eB_Y, W_Y, BB_O_X, WW_O_X\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eW_X\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e命名实体类型 X 的单个字符\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eE_Y, W_Y, EE_O_X, WW_O_X\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eB_Y, W_Y, BB_O_X, WW_O_X\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eBB_O_X\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e非命名实体的起始，上接命名实体类型 X\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eE_X, W_X\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eMM_O, EE_O_Y\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eMM_O\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e非命名实体的中间\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eBB_O_Y, MM_O\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eMM_O, EE_O_Y\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eEE_O_X\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e非命名实体的结尾，下接命名实体类型 X\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eBB_O_Y, MM_O\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eB_X, W_X\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eWW_O_X\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e非命名实体，上接命名实体，下接命名实体类型 X\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eE_X, W_X\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eB_Y, W_Y\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e不同标注标准的差别示例如下：\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e字符\u003c/th\u003e\n\u003cth\u003eIO\u003c/th\u003e\n\u003cth\u003eBIO\u003c/th\u003e\n\u003cth\u003eBIOES\u003c/th\u003e\n\u003cth\u003eBMEWO\u003c/th\u003e\n\u003cth\u003eBMEWO+\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eW_OOS\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eYesterday\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eO\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eO\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eO\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eO\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eBB_O_OOS\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eafternoon\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eO\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eO\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eO\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eO\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eMM_O\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e,\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eO\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eO\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eO\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eO\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eEE_O_PER\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eJohn\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eI_PER\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eB_PER\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eB_PER\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eB_PER\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eB_PER\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eJ\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eI_PER\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eI_PER\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eI_PER\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eM_PER\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eM_PER\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e.\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eI_PER\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eI_PER\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eI_PER\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eM_PER\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eM_PER\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eSmith\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eI_PER\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eI_PER\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eE_PER\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eE_PER\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eE_PER\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003etraveled\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eO\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eO\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eO\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eO\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eBB_O_PER\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eto\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eO\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eO\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eO\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eO\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eEE_O_LOC\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eWashington\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eI_LOC\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eB_LOC\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eS_LOC\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eW_LOC\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eW_LOC\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e.\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eO\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eO\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eO\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eO\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eWW_O_OOS\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003c/td\u003e\n\u003ctd\u003e\u003ccode\u003eW_OOS\u003c/code\u003e\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e不同标准的标签数量如下表所示：\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e标注标准\u003c/th\u003e\n\u003cth\u003e标签数量\u003c/th\u003e\n\u003cth\u003eN=1\u003c/th\u003e\n\u003cth\u003eN=3\u003c/th\u003e\n\u003cth\u003eN=20\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003eIO\u003c/td\u003e\n\u003ctd\u003eN+1\u003c/td\u003e\n\u003ctd\u003e2\u003c/td\u003e\n\u003ctd\u003e4\u003c/td\u003e\n\u003ctd\u003e21\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eBIO\u003c/td\u003e\n\u003ctd\u003e2N+1\u003c/td\u003e\n\u003ctd\u003e3\u003c/td\u003e\n\u003ctd\u003e7\u003c/td\u003e\n\u003ctd\u003e41\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eBIOES\u003c/td\u003e\n\u003ctd\u003e4N+1\u003c/td\u003e\n\u003ctd\u003e5\u003c/td\u003e\n\u003ctd\u003e13\u003c/td\u003e\n\u003ctd\u003e81\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eBMEWO\u003c/td\u003e\n\u003ctd\u003e4N+1\u003c/td\u003e\n\u003ctd\u003e5\u003c/td\u003e\n\u003ctd\u003e13\u003c/td\u003e\n\u003ctd\u003e81\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003eBMEWO+\u003c/td\u003e\n\u003ctd\u003e7N+3\u003c/td\u003e\n\u003ctd\u003e10\u003c/td\u003e\n\u003ctd\u003e24\u003c/td\u003e\n\u003ctd\u003e143\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e其中，N 为命名实体类型的数量。\u003c/p\u003e\n\u003ch3 id=\"bilstm-crf-huang2015bidirectional\"\u003eBiLSTM CRF \u003csup id=\"fnref:7\"\u003e\u003ca href=\"#fn:7\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e7\u003c/a\u003e\u003c/sup\u003e\u003c/h3\u003e\n\u003cblockquote\u003e\n\u003cp\u003e本小节内容参考和修改自 \u003ca href=\"https://github.com/createmomo/CRF-Layer-on-the-Top-of-BiLSTM\"\u003eCRF-Layer-on-the-Top-of-BiLSTM\u003c/a\u003e。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eHuang 等人提出了一种基于 BiLSTM 和 CRF 的神经网络模型用于序例标注。整个网络如下图所示：\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-05-02-hmm-and-crf/bilstm-crf.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n\u003c/figure\u003e\n\u003cp\u003e关于模型中的 BiLSTM 部分在此不过多赘述，相关细节可以参见之前的博客：\u003ca href=\"/cn/2018/09/rnn/\"\u003e循环神经网络 (Recurrent Neural Network, RNN)\u003c/a\u003e 和 \u003ca href=\"/cn/2020/03/pre-trained-model-for-nlp/\"\u003e预训练自然语言模型 (Pre-trained Models for NLP)\u003c/a\u003e。BiLSTM-CRF 模型的输入是词嵌入向量，输出是对应的预测标注标签，如下图所示：\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-05-02-hmm-and-crf/bilstm-crf-1.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n\u003c/figure\u003e\n\u003cp\u003eBiLSTM 层的输出为每个标签的分数，对于 \u003ccode\u003e$w_0$\u003c/code\u003e，BiLSTM 的输出为 1.5 (\u003ccode\u003eB_PER\u003c/code\u003e)，0.9 (\u003ccode\u003eI_PER\u003c/code\u003e)，0.1 (\u003ccode\u003eB_ORG\u003c/code\u003e)，0.08 (\u003ccode\u003eI_ORG\u003c/code\u003e) 和 0.05 (\u003ccode\u003eO\u003c/code\u003e)，这些分数为 CRF 层的输入，如下图所示：\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-05-02-hmm-and-crf/bilstm-crf-2.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n\u003c/figure\u003e\n\u003cp\u003e经过 CRF 层后，具有最高分数的预测序列被选择为最优预测结果。如果没有 CRF 层，我们可以直接选择 BiLSTM 层输出分数的最大值对应的序列为预测结果。例如，对于 \u003ccode\u003e$w_0$\u003c/code\u003e，最高分数为 1.5，对应的预测标签则为 \u003ccode\u003eB_PER\u003c/code\u003e，类似的 \u003ccode\u003e$w_1, w_2, w_3, w_4$\u003c/code\u003e 对应的预测标签为 \u003ccode\u003eI_PER, O, B_ORG, O\u003c/code\u003e，如下图所示：\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-05-02-hmm-and-crf/bilstm-crf-3.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n\u003c/figure\u003e\n\u003cp\u003e虽然我们在上例中得到了正确的结果，但通常情况下并非如此。对于如下的示例，预测结果为 \u003ccode\u003eI_ORG, I_PER, O, I_ORG, I_PER\u003c/code\u003e，这显然是不正确的。\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-05-02-hmm-and-crf/bilstm-crf-4.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n\u003c/figure\u003e\n\u003cp\u003eCRF 层在进行预测时可以添加一些约束，这些约束可以在训练时被 CRF 层学习得到。可能的约束有：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e句子的第一个词的标签可以是 \u003ccode\u003eB_X\u003c/code\u003e 或 \u003ccode\u003eO\u003c/code\u003e，而非 \u003ccode\u003eI_X\u003c/code\u003e。\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003eB_X, I_X\u003c/code\u003e 是有效的标签，而 \u003ccode\u003eB_X, I_Y\u003c/code\u003e 是无效的标签。\u003c/li\u003e\n\u003cli\u003e一个命名实体的起始标签应为 \u003ccode\u003eB_X\u003c/code\u003e 而非 \u003ccode\u003eI_X\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eCRF 层的损失包含两部分，这两部分构成了 CRF 层的关键：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e发射分数（Emission Score）\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e发射分数即为 BiLSTM 层的输出分数，例如 \u003ccode\u003e$w_0$\u003c/code\u003e 对应的标签 \u003ccode\u003eB_PER\u003c/code\u003e 的分数为 1.5。为了方便起见，对于每类标签给定一个索引：\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e标签\u003c/th\u003e\n\u003cth\u003e索引\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eB_PER\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eI_PER\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e1\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eB_ORG\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e2\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eI_ORG\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e3\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eO\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e4\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e我们利用 \u003ccode\u003e$x_{i y_{j}}$\u003c/code\u003e 表示发射分数，\u003ccode\u003e$i$\u003c/code\u003e 为词的索引，\u003ccode\u003e$y_i$\u003c/code\u003e 为标注标签的索引。例如：\u003ccode\u003e$x_{i=1, y_{j}=2} = x_{w_1, \\text{B_ORG}} = 0.1$\u003c/code\u003e，表示 \u003ccode\u003e$w_1$\u003c/code\u003e 为 \u003ccode\u003eB_ORG\u003c/code\u003e 的分数为 0.1。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e转移分数（Transition Score）\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e我们利用 \u003ccode\u003e$t_{y_i, y_j}$\u003c/code\u003e 表示转移分数，例如 \u003ccode\u003e$t_{\\text{B_PER}, \\text{I_PER}} = 0.9$\u003c/code\u003e 表示由标签 \u003ccode\u003eB_PER\u003c/code\u003e 转移到 \u003ccode\u003eI_PER\u003c/code\u003e 的分数为 0.9。因此，需要一个转移分数矩阵用于存储所有标注标签之间的转移分数。为了使得转移分数矩阵更加鲁棒，需要添加两个标签 \u003ccode\u003eSTART\u003c/code\u003e 和 \u003ccode\u003eEND\u003c/code\u003e，分别表示一个句子的开始和结束。下表为一个转移分数矩阵的示例：\u003c/p\u003e\n\u003ctable\u003e\n\u003cthead\u003e\n\u003ctr\u003e\n\u003cth\u003e\u003c/th\u003e\n\u003cth\u003e\u003ccode\u003eSTART\u003c/code\u003e\u003c/th\u003e\n\u003cth\u003e\u003ccode\u003eB-PER\u003c/code\u003e\u003c/th\u003e\n\u003cth\u003e\u003ccode\u003eI-PER\u003c/code\u003e\u003c/th\u003e\n\u003cth\u003e\u003ccode\u003eB-ORG\u003c/code\u003e\u003c/th\u003e\n\u003cth\u003e\u003ccode\u003eI-ORG\u003c/code\u003e\u003c/th\u003e\n\u003cth\u003e\u003ccode\u003eO\u003c/code\u003e\u003c/th\u003e\n\u003cth\u003e\u003ccode\u003eEND\u003c/code\u003e\u003c/th\u003e\n\u003c/tr\u003e\n\u003c/thead\u003e\n\u003ctbody\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eSTART\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003ctd\u003e0.8\u003c/td\u003e\n\u003ctd\u003e0.007\u003c/td\u003e\n\u003ctd\u003e0.7\u003c/td\u003e\n\u003ctd\u003e0.0008\u003c/td\u003e\n\u003ctd\u003e0.9\u003c/td\u003e\n\u003ctd\u003e0.08\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eB_PER\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003ctd\u003e0.6\u003c/td\u003e\n\u003ctd\u003e0.9\u003c/td\u003e\n\u003ctd\u003e0.2\u003c/td\u003e\n\u003ctd\u003e0.0006\u003c/td\u003e\n\u003ctd\u003e0.6\u003c/td\u003e\n\u003ctd\u003e0.009\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eI_PER\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e-1\u003c/td\u003e\n\u003ctd\u003e0.5\u003c/td\u003e\n\u003ctd\u003e0.53\u003c/td\u003e\n\u003ctd\u003e0.55\u003c/td\u003e\n\u003ctd\u003e0.0003\u003c/td\u003e\n\u003ctd\u003e0.85\u003c/td\u003e\n\u003ctd\u003e0.008\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eB_ORG\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e0.9\u003c/td\u003e\n\u003ctd\u003e0.5\u003c/td\u003e\n\u003ctd\u003e0.0003\u003c/td\u003e\n\u003ctd\u003e0.25\u003c/td\u003e\n\u003ctd\u003e0.8\u003c/td\u003e\n\u003ctd\u003e0.77\u003c/td\u003e\n\u003ctd\u003e0.006\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eI_ORG\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e-0.9\u003c/td\u003e\n\u003ctd\u003e0.45\u003c/td\u003e\n\u003ctd\u003e0.007\u003c/td\u003e\n\u003ctd\u003e0.7\u003c/td\u003e\n\u003ctd\u003e0.65\u003c/td\u003e\n\u003ctd\u003e0.76\u003c/td\u003e\n\u003ctd\u003e0.2\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eO\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003ctd\u003e0.65\u003c/td\u003e\n\u003ctd\u003e0.0007\u003c/td\u003e\n\u003ctd\u003e0.7\u003c/td\u003e\n\u003ctd\u003e0.0008\u003c/td\u003e\n\u003ctd\u003e0.9\u003c/td\u003e\n\u003ctd\u003e0.08\u003c/td\u003e\n\u003c/tr\u003e\n\u003ctr\u003e\n\u003ctd\u003e\u003ccode\u003eEND\u003c/code\u003e\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003ctd\u003e0\u003c/td\u003e\n\u003c/tr\u003e\n\u003c/tbody\u003e\n\u003c/table\u003e\n\u003cp\u003e转移分数矩阵作为 BiLSTM-CRF 模型的一个参数，随机初始化并通过模型的训练不断更新，最终学习得到约束条件。\u003c/p\u003e\n\u003cp\u003eCRF 层的损失函数包含两个部分：真实路径分数和所有可能路径的总分数。假设每个可能的路径有一个分数 \u003ccode\u003e$P_i$\u003c/code\u003e，共 \u003ccode\u003e$N$\u003c/code\u003e 种可能的路径，所有路径的总分数为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ P_{\\text {total}}=P_{1}+P_{2}+\\ldots+P_{N}=e^{S_{1}}+e^{S_{2}}+\\ldots+e^{S_{N}} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e则损失函数定义为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\text{Loss} = \\dfrac{P_{\\text{RealPath}}}{\\sum_{i=1}^{N} P_i} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e对于 \u003ccode\u003e$S_i$\u003c/code\u003e，共包含两部分：发射分数和转移分数。以路径 \u003ccode\u003eSTART -\u0026gt; B_PER -\u0026gt; I_PER -\u0026gt; O -\u0026gt; B_ORG -\u0026gt; O -\u0026gt; END\u003c/code\u003e 为例，发射分数为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{aligned} \\text{EmissionScore} = \\ \u0026amp;x_{0, \\text{START}} + x_{1, \\text{B_PER}} + x_{2, \\text{I_PER}} \\\\ \u0026amp;+ x_{3, \\text{O}} + x_{4, \\text{B_ORG}} + x_{5, \\text{O}} + x_{6, \\text{END}} \\end{aligned} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中 \u003ccode\u003e$x_{i, y_j}$\u003c/code\u003e 表示第 \u003ccode\u003e$i$\u003c/code\u003e 个词标签为 \u003ccode\u003e$y_j$\u003c/code\u003e 的分数，为 BiLSTM 的输出，\u003ccode\u003e$x_{0, \\text{START}}$\u003c/code\u003e 和 \u003ccode\u003e$x_{6, \\text{END}}$\u003c/code\u003e 可以设置为 0。转换分数为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{aligned} \\text{TransitionScore} = \\ \u0026amp;t_{\\text{START}, \\text{B_PER}} + t_{\\text{B_PER}, \\text{I_PER}} + t_{\\text{I_PER}, \\text{O}} \\\\ \u0026amp;+ t_{\\text{O}, \\text{B_ORG}} + t_{\\text{B_ORG}, \\text{O}} + t_{\\text{O}, \\text{END}} \\end{aligned} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中 \u003ccode\u003e$t_{y_i, y_j}$\u003c/code\u003e 表示标注标签由 \u003ccode\u003e$y_i$\u003c/code\u003e 转移至 \u003ccode\u003e$y_j$\u003c/code\u003e 的分数。\u003c/p\u003e\n\u003cp\u003e对于所有路径的总分数的计算过程采用了类似 \u003ca href=\"/cn/2018/11/computational-complexity-and-dynamic-programming/\"\u003e动态规划\u003c/a\u003e 的思想，整个过程计算比较复杂，在此不再详细展开，详细请参见参考文章。\u003c/p\u003e\n\u003cp\u003e利用训练好的 BiLSTM-CRF 模型进行预测时，首先我们可以得到序列的发射分数和转移分数，其次用维特比算法可以得到最终的预测标注序列。\u003c/p\u003e\n\u003ch3 id=\"lattice-lstm-zhang2018chinese\"\u003eLattice LSTM \u003csup id=\"fnref:8\"\u003e\u003ca href=\"#fn:8\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e8\u003c/a\u003e\u003c/sup\u003e\u003c/h3\u003e\n\u003cp\u003eZhang 等人针对中文提出了一种基于 Lattice LSTM 的命名实体识别方法，Lattice LSTM 的结构如下图所示：\u003c/p\u003e\n\u003cfigure\u003e\n  \u003cimg class=\"lazyload\" data-src=\"/images/cn/2020-05-02-hmm-and-crf/lattice-lstm.png\" data-large-max-width=\"100%\" data-middle-max-width=\"100%\" data-small-max-width=\"100%\"/\u003e\n  \n\u003c/figure\u003e\n\u003cp\u003e模型的基本思想是将句子中的词汇（例如：南京，长江大桥等）信息融入到基于字符的 LSTM 模型中，从而可以显性地利用词汇信息。\u003c/p\u003e\n\u003cp\u003e模型的输入为一个字符序列 \u003ccode\u003e$c_1, c_2, \\cdots, c_m$\u003c/code\u003e 和词汇表 \u003ccode\u003e$\\mathbb{D}$\u003c/code\u003e 中所有匹配的字符子序列，其中词汇表 \u003ccode\u003e$\\mathbb{D}$\u003c/code\u003e 利用大量的原始文本通过分词构建。令 \u003ccode\u003e$w_{b, e}^d$\u003c/code\u003e 表示有以第 \u003ccode\u003e$b$\u003c/code\u003e 个字符起始，以第 \u003ccode\u003e$e$\u003c/code\u003e 个字符结尾的子序列，例如：\u003ccode\u003e$w_{1,2}^d$\u003c/code\u003e 表示“南京\n”，\u003ccode\u003e$w_{7,8}^d$\u003c/code\u003e 表示“大桥”。\u003c/p\u003e\n\u003cp\u003e不同于一般的字符级模型，LSTM 单元的状态考虑了句子中的子序列 \u003ccode\u003e$w_{b,e}^d$\u003c/code\u003e，每个子序列 \u003ccode\u003e$w_{b,e}^d$\u003c/code\u003e 表示为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\mathbf{x}_{b, e}^{w}=\\mathbf{e}^{w}\\left(w_{b, e}^{d}\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$\\mathbf{e}^{w}$\u003c/code\u003e 为词向量查询表。一个词单元 \u003ccode\u003e$\\mathbf{c}_{b,e}^w$\u003c/code\u003e 用于表示 \u003ccode\u003e$\\mathbf{x}_{b,e}^w$\u003c/code\u003e 的循环状态：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{aligned} \\left[\\begin{array}{c} \\mathbf{i}_{b, e}^{w} \\\\ \\mathbf{f}_{b, e}^{w} \\\\ \\widetilde{c}_{b, e}^{w} \\end{array}\\right] \u0026amp;=\\left[\\begin{array}{c} \\sigma \\\\ \\sigma \\\\ \\tanh \\end{array}\\right]\\left(\\mathbf{W}^{w \\top}\\left[\\begin{array}{c} \\mathbf{x}_{b, e}^{w} \\\\ \\mathbf{h}_{b}^{c} \\end{array}\\right]+\\mathbf{b}^{w}\\right) \\\\ \\mathbf{c}_{b, e}^{w} \u0026amp;=\\mathbf{f}_{b, e}^{w} \\odot \\mathbf{c}_{b}^{c}+\\mathbf{i}_{b, e}^{w} \\odot \\widetilde{c}_{b, e}^{w} \\end{aligned} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$\\mathbf{i}_{b, e}^{w}$\u003c/code\u003e 和 \u003ccode\u003e$\\mathbf{f}_{b, e}^{w}$\u003c/code\u003e 分别为输入门和遗忘门。由于仅在字符级别上进行标注，因此对于词单元来说没有输出门。\u003c/p\u003e\n\u003cp\u003e对于 \u003ccode\u003e$\\mathbf{c}_{j}^c$\u003c/code\u003e 来说可能有多条信息流，例如 \u003ccode\u003e$\\mathbf{c}_7^c$\u003c/code\u003e 的输入包括 \u003ccode\u003e$\\mathbf{x}_7^c$\u003c/code\u003e（桥），\u003ccode\u003e$\\mathbf{c}_{6,7}^w$\u003c/code\u003e（大桥）和 \u003ccode\u003e$\\mathbf{c}_{4,7}^w$\u003c/code\u003e（长江大桥）。论文采用了一个新的门 \u003ccode\u003e$\\mathbf{i}_{b,e}^c$\u003c/code\u003e 来控制所有子序列单元 \u003ccode\u003e$\\mathbf{c}_{b,e}^w$\u003c/code\u003e 对 \u003ccode\u003e$\\mathbf{c}_{j}^c$\u003c/code\u003e 的贡献：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\mathbf{i}_{b, e}^{c}=\\sigma\\left(\\mathbf{W}^{l \\top}\\left[\\begin{array}{c} \\mathbf{x}_{e}^{c} \\\\ \\mathbf{c}_{b, e}^{w} \\end{array}\\right]+\\mathbf{b}^{l}\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e则单元状态 \u003ccode\u003e$\\mathbf{c}_j^c$\u003c/code\u003e 的计算变为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\mathbf{c}_{j}^{c}=\\sum_{b \\in\\left\\{b^{\\prime} | w_{b^{\\prime}, j} \\in \\mathbb{D}\\right\\}} \\boldsymbol{\\alpha}_{b, j}^{c} \\odot \\boldsymbol{c}_{b, j}^{w}+\\boldsymbol{\\alpha}_{j}^{c} \\odot \\widetilde{\\boldsymbol{c}}_{j}^{c} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e在上式中，\u003ccode\u003e$\\mathbf{i}_{b,j}^c$\u003c/code\u003e 和 \u003ccode\u003e$\\mathbf{i}_j^c$\u003c/code\u003e 标准化为 \u003ccode\u003e$\\boldsymbol{\\alpha}_{b, j}^{c}$\u003c/code\u003e 和 \u003ccode\u003e$\\boldsymbol{\\alpha}_{j}^{c}$\u003c/code\u003e：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{aligned} \\boldsymbol{\\alpha}_{b, j}^{c} \u0026amp;=\\frac{\\exp \\left(\\mathbf{i}_{b, j}^{c}\\right)}{\\exp \\left(\\mathbf{i}_{j}^{c}\\right)+\\sum_{b^{\\prime} \\in\\left\\{b^{\\prime \\prime} | w_{b^{\\prime \\prime}, j}^{d} \\in \\mathbb{D}\\right\\}} \\exp \\left(\\mathbf{i}_{b^{\\prime}, j}^{c}\\right)} \\\\ \\boldsymbol{\\alpha}_{j}^{c} \u0026amp;=\\frac{\\exp \\left(\\mathbf{i}_{j}^{c}\\right)}{\\exp \\left(\\mathbf{i}_{j}^{c}\\right)+\\sum_{b^{\\prime} \\in\\left\\{b^{\\prime \\prime} | w_{b^{\\prime \\prime}, j}^{d} \\in \\mathbb{D}\\right\\}} \\exp \\left(\\mathbf{i}_{b^{\\prime}, j}^{c}\\right)} \\end{aligned} $$\u003c/code\u003e\u003c/p\u003e\n\u003ch1 id=\"开放资源\"\u003e开放资源\u003c/h1\u003e\n\u003ch2 id=\"标注工具\"\u003e标注工具\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/synyi/poplar\"\u003esynyi/poplar\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/nlplab/brat\"\u003enlplab/brat\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/doccano/doccano\"\u003edoccano/doccano\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/heartexlabs/label-studio\"\u003eheartexlabs/label-studio\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/deepwel/Chinese-Annotator\"\u003edeepwel/Chinese-Annotator\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/jiesutd/YEDDA\"\u003ejiesutd/YEDDA\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"开源模型-框架和代码\"\u003e开源模型，框架和代码\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/pytorch/text\"\u003epytorch/text\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/flairNLP/flair\"\u003eflairNLP/flair\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/PetrochukM/PyTorch-NLP\"\u003ePetrochukM/PyTorch-NLP\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/allenai/allennlp\"\u003eallenai/allennlp\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/fastnlp/fastNLP\"\u003efastnlp/fastNLP\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://stanfordnlp.github.io/CoreNLP/index.html\"\u003eStanford CoreNLP\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"http://neuroner.com/\"\u003eNeuroNER\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://spacy.io/\"\u003espaCy\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.nltk.org/\"\u003eNLTK\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/BrikerMan/Kashgari\"\u003eBrikerMan/Kashgari\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/Hironsan/anago\"\u003eHironsan/anago\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/crownpku/Information-Extraction-Chinese\"\u003ecrownpku/Information-Extraction-Chinese\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/thunlp/OpenNRE\"\u003ethunlp/OpenNRE\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/hankcs/HanLP\"\u003ehankcs/HanLP\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/jiesutd/NCRFpp\"\u003ejiesutd/NCRFpp\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch2 id=\"其他资源\"\u003e其他资源\u003c/h2\u003e\n\u003col\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/keon/awesome-nlp\"\u003ekeon/awesome-nlp\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/crownpku/Awesome-Chinese-NLP\"\u003ecrownpku/Awesome-Chinese-NLP\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/sebastianruder/NLP-progress\"\u003esebastianruder/NLP-progress\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/thunlp/NREPapers\"\u003ethunlp/NREPapers\u003c/a\u003e\u003c/li\u003e\n\u003c/ol\u003e\n\u003cdiv class=\"footnotes\" role=\"doc-endnotes\"\u003e\n\u003chr/\u003e\n\u003col\u003e\n\u003cli id=\"fn:1\"\u003e\n\u003cp\u003e李航. (2019). \u003cem\u003e统计学习方法（第二版）\u003c/em\u003e. 清华大学出版社. \u003ca href=\"#fnref:1\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e \u003ca href=\"#fnref1:1\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:2\"\u003e\n\u003cp\u003e俞士汶, 段慧明, 朱学锋, \u0026amp; 孙斌. (2002). 北京大学现代汉语语料库基本加工规范. \u003cem\u003e中文信息学报\u003c/em\u003e, 16(5), 51-66. \u003ca href=\"#fnref:2\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:3\"\u003e\n\u003cp\u003e俞士汶, 段慧明, 朱学锋, 孙斌, \u0026amp; 常宝宝. (2003). 北大语料库加工规范: 切分· 词性标注· 注音. \u003cem\u003e汉语语言与计算学报\u003c/em\u003e, 13(2), 121-158. \u003ca href=\"#fnref:3\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:4\"\u003e\n\u003cp\u003e\u003ca href=\"http://ictclas.nlpir.org/nlpir/html/readme.htm\"\u003ehttp://ictclas.nlpir.org/nlpir/html/readme.htm\u003c/a\u003e \u003ca href=\"#fnref:4\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:5\"\u003e\n\u003cp\u003eXia, F. (2000). The part-of-speech tagging guidelines for the Penn Chinese Treebank (3.0). \u003cem\u003eIRCS Technical Reports Series\u003c/em\u003e, 38. \u003ca href=\"#fnref:5\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:6\"\u003e\n\u003cp\u003eHuang, C. N., Li, Y., \u0026amp; Zhu, X. (2006). Tokenization guidelines of Chinese text (v5.0, in Chinese). \u003cem\u003eMicrosoft Research Asia\u003c/em\u003e. \u003ca href=\"#fnref:6\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:7\"\u003e\n\u003cp\u003eHuang, Z., Xu, W., \u0026amp; Yu, K. (2015). Bidirectional LSTM-CRF models for sequence tagging. \u003cem\u003earXiv preprint arXiv:1508.01991\u003c/em\u003e. \u003ca href=\"#fnref:7\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:8\"\u003e\n\u003cp\u003eZhang, Y., \u0026amp; Yang, J. (2018). Chinese NER Using Lattice LSTM. In \u003cem\u003eProceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\u003c/em\u003e (pp. 1554-1564). \u003ca href=\"#fnref:8\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/div\u003e\n\n\n\n\n\n\u003cdiv class=\"donate\"\u003e\n  \u003cdiv class=\"donate-header\"\u003e\u003c/div\u003e\n  \u003cdiv class=\"donate-slug\" id=\"donate-slug\"\u003ehmm-crf-and-sequence-labeling\u003c/div\u003e\n  \u003cbutton class=\"donate-button\"\u003e赞 赏\u003c/button\u003e\n  \u003cdiv class=\"donate-footer\"\u003e「真诚赞赏，手留余香」\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"donate-modal-wrapper\"\u003e\n  \u003cdiv class=\"donate-modal\"\u003e\n    \u003cdiv class=\"donate-box\"\u003e\n      \u003cdiv class=\"donate-box-content\"\u003e\n        \u003cdiv class=\"donate-box-content-inner\"\u003e\n          \u003cdiv class=\"donate-box-header\"\u003e「真诚赞赏，手留余香」\u003c/div\u003e\n          \u003cdiv class=\"donate-box-body\"\u003e\n            \u003cdiv class=\"donate-box-money\"\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-2\" data-v=\"2\" data-unchecked=\"￥ 2\" data-checked=\"2 元\"\u003e￥ 2\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-5\" data-v=\"5\" data-unchecked=\"￥ 5\" data-checked=\"5 元\"\u003e￥ 5\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-10\" data-v=\"10\" data-unchecked=\"￥ 10\" data-checked=\"10 元\"\u003e￥ 10\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-50\" data-v=\"50\" data-unchecked=\"￥ 50\" data-checked=\"50 元\"\u003e￥ 50\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-100\" data-v=\"100\" data-unchecked=\"￥ 100\" data-checked=\"100 元\"\u003e￥ 100\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-custom\" data-v=\"custom\" data-unchecked=\"任意金额\" data-checked=\"任意金额\"\u003e任意金额\u003c/button\u003e\n            \u003c/div\u003e\n            \u003cdiv class=\"donate-box-pay\"\u003e\n              \u003cimg class=\"donate-box-pay-qrcode\" id=\"donate-box-pay-qrcode\" src=\"\"/\u003e\n            \u003c/div\u003e\n          \u003c/div\u003e\n          \u003cdiv class=\"donate-box-footer\"\u003e\n            \u003cdiv class=\"donate-box-pay-method donate-box-pay-method-checked\" data-v=\"wechat-pay\"\u003e\n              \u003cimg class=\"donate-box-pay-method-image\" id=\"donate-box-pay-method-image-wechat-pay\" src=\"\"/\u003e\n            \u003c/div\u003e\n            \u003cdiv class=\"donate-box-pay-method\" data-v=\"alipay\"\u003e\n              \u003cimg class=\"donate-box-pay-method-image\" id=\"donate-box-pay-method-image-alipay\" src=\"\"/\u003e\n            \u003c/div\u003e\n          \u003c/div\u003e\n        \u003c/div\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n    \u003cbutton type=\"button\" class=\"donate-box-close-button\"\u003e\n      \u003csvg class=\"donate-box-close-button-icon\" fill=\"#fff\" viewBox=\"0 0 24 24\" width=\"24\" height=\"24\"\u003e\u003cpath d=\"M13.486 12l5.208-5.207a1.048 1.048 0 0 0-.006-1.483 1.046 1.046 0 0 0-1.482-.005L12 10.514 6.793 5.305a1.048 1.048 0 0 0-1.483.005 1.046 1.046 0 0 0-.005 1.483L10.514 12l-5.208 5.207a1.048 1.048 0 0 0 .006 1.483 1.046 1.046 0 0 0 1.482.005L12 13.486l5.207 5.208a1.048 1.048 0 0 0 1.483-.006 1.046 1.046 0 0 0 .005-1.482L13.486 12z\" fill-rule=\"evenodd\"\u003e\u003c/path\u003e\u003c/svg\u003e\n    \u003c/button\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\u003cscript type=\"text/javascript\" src=\"/js/donate.js\"\u003e\u003c/script\u003e\n\n\n  \u003cfooter\u003e\n  \n\u003cnav class=\"post-nav\"\u003e\n  \u003cspan class=\"nav-prev\"\u003e← \u003ca href=\"/cn/2020/04/graph-embedding-and-gnn/\"\u003e图嵌入 (Graph Embedding) 和图神经网络 (Graph Neural Network)\u003c/a\u003e\u003c/span\u003e\n  \u003cspan class=\"nav-next\"\u003e\u003ca href=\"/cn/2020/05/compile-and-install-tmux-on-synology-nas/\"\u003e在群晖 NAS 上编译安装 tmux\u003c/a\u003e →\u003c/span\u003e\n\u003c/nav\u003e\n\n\n\n\n\u003cins class=\"adsbygoogle\" style=\"display:block; text-align:center;\" data-ad-layout=\"in-article\" data-ad-format=\"fluid\" data-ad-client=\"ca-pub-2608165017777396\" data-ad-slot=\"8302038603\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n  (adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/js-cookie@3.0.5/dist/js.cookie.min.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"/js/toggle-theme.js\"\u003e\u003c/script\u003e\n\n\n\u003cscript src=\"/js/no-highlight.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"/js/math-code.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"/js/heading-anchor.js\"\u003e\u003c/script\u003e\n\n\n\n\u003csection class=\"comments\"\u003e\n\u003cscript src=\"https://giscus.app/client.js\" data-repo=\"leovan/leovan.me\" data-repo-id=\"MDEwOlJlcG9zaXRvcnkxMTMxOTY0Mjc=\" data-category=\"Comments\" data-category-id=\"DIC_kwDOBr89i84CT-R7\" data-mapping=\"pathname\" data-strict=\"1\" data-reactions-enabled=\"1\" data-emit-metadata=\"0\" data-input-position=\"top\" data-theme=\"preferred_color_scheme\" data-lang=\"zh-CN\" data-loading=\"lazy\" crossorigin=\"anonymous\" defer=\"\"\u003e\n\u003c/script\u003e\n\u003c/section\u003e\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cscript async=\"\" src=\"/js/center-img.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/right-quote.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/external-link.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/alt-title.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/figure.js\"\u003e\u003c/script\u003e\n\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js\"\u003e\u003c/script\u003e\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/vanilla-back-to-top@latest/dist/vanilla-back-to-top.min.js\"\u003e\u003c/script\u003e\n\u003cscript\u003e\naddBackToTop({\n  diameter: 48\n});\n\u003c/script\u003e\n\n  \u003chr/\u003e\n  \u003cdiv class=\"copyright no-border-bottom\"\u003e\n    \u003cdiv class=\"copyright-author-year\"\u003e\n      \u003cspan\u003eCopyright © 2017-2024 \u003ca href=\"/\"\u003e范叶亮 | Leo Van\u003c/a\u003e\u003c/span\u003e\n    \u003c/div\u003e\n  \u003c/div\u003e\n  \u003c/footer\u003e\n  \u003c/article\u003e",
  "Date": "2020-05-02T00:00:00Z",
  "Author": "范叶亮"
}