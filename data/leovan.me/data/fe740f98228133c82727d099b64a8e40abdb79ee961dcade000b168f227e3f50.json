{
  "Source": "leovan.me",
  "Title": "卷积神经网络 (Convolutional Neural Network, CNN)",
  "Link": "https://leovan.me/cn/2018/08/cnn/",
  "Content": "\u003carticle class=\"main\"\u003e\n    \u003cheader class=\"content-title\"\u003e\n    \n\u003ch1 class=\"title\"\u003e\n  \n  卷积神经网络 (Convolutional Neural Network, CNN)\n  \n\u003c/h1\u003e\n\n\n\n\n\n\n\n\u003ch2 class=\"author-date\"\u003e范叶亮 / \n2018-08-25\u003c/h2\u003e\n\n\n\n\u003ch3 class=\"post-meta\"\u003e\n\n\n\u003cstrong\u003e分类: \u003c/strong\u003e\n\u003ca href=\"/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0\"\u003e深度学习\u003c/a\u003e\n\n\n\n\n/\n\n\n\n\n\u003cstrong\u003e标签: \u003c/strong\u003e\n\u003cspan\u003e卷积神经网络\u003c/span\u003e, \u003cspan\u003eConvolutional Neural Network\u003c/span\u003e, \u003cspan\u003eCNN\u003c/span\u003e\n\n\n\n\n/\n\n\n\u003cstrong\u003e字数: \u003c/strong\u003e\n9048\n\u003c/h3\u003e\n\n\n\n\u003chr/\u003e\n\n\n\n    \n    \n    \u003cins class=\"adsbygoogle\" style=\"display:block; text-align:center;\" data-ad-layout=\"in-article\" data-ad-format=\"fluid\" data-ad-client=\"ca-pub-2608165017777396\" data-ad-slot=\"1261604535\"\u003e\u003c/ins\u003e\n    \u003cscript\u003e\n    (adsbygoogle = window.adsbygoogle || []).push({});\n    \u003c/script\u003e\n    \n    \n    \u003c/header\u003e\n\n\n\n\n\u003ch1 id=\"发展史\"\u003e发展史\u003c/h1\u003e\n\u003cp\u003e卷积神经网络 (Convolutional Neural Network, CNN) 是一种目前广泛用于图像，自然语言处理等领域的深度神经网络模型。1998 年，Lecun 等人 \u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e1\u003c/a\u003e\u003c/sup\u003e 提出了一种基于梯度的反向传播算法用于文档的识别。在这个神经网络中，卷积层 (Convolutional Layer) 扮演着至关重要的角色。\u003c/p\u003e\n\u003cp\u003e随着运算能力的不断增强，一些大型的 CNN 网络开始在图像领域中展现出巨大的优势，2012 年，Krizhevsky 等人 \u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e 提出了 AlexNet 网络结构，并在 ImageNet 图像分类竞赛 \u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e3\u003c/a\u003e\u003c/sup\u003e 中以超过之前 11% 的优势取得了冠军。随后不同的学者提出了一系列的网络结构并不断刷新 ImageNet 的成绩，其中比较经典的网络包括：VGG (Visual Geometry  Group) \u003csup id=\"fnref:4\"\u003e\u003ca href=\"#fn:4\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e4\u003c/a\u003e\u003c/sup\u003e，GoogLeNet \u003csup id=\"fnref:5\"\u003e\u003ca href=\"#fn:5\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e5\u003c/a\u003e\u003c/sup\u003e 和 ResNet \u003csup id=\"fnref:6\"\u003e\u003ca href=\"#fn:6\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e6\u003c/a\u003e\u003c/sup\u003e。\u003c/p\u003e\n\u003cp\u003eCNN 在图像分类问题上取得了不凡的成绩，同时一些学者也尝试将其应用在图像的其他领域，例如：物体检测 \u003csup id=\"fnref:7\"\u003e\u003ca href=\"#fn:7\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e7\u003c/a\u003e\u003c/sup\u003e\u003csup id=\"fnref:8\"\u003e\u003ca href=\"#fn:8\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e8\u003c/a\u003e\u003c/sup\u003e\u003csup id=\"fnref:9\"\u003e\u003ca href=\"#fn:9\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e9\u003c/a\u003e\u003c/sup\u003e，语义分割 \u003csup id=\"fnref:10\"\u003e\u003ca href=\"#fn:10\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e10\u003c/a\u003e\u003c/sup\u003e，图像摘要 \u003csup id=\"fnref:11\"\u003e\u003ca href=\"#fn:11\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e11\u003c/a\u003e\u003c/sup\u003e，行为识别 \u003csup id=\"fnref:12\"\u003e\u003ca href=\"#fn:12\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e12\u003c/a\u003e\u003c/sup\u003e 等。除此之外，在非图像领域 CNN 也取得了一定的成绩 \u003csup id=\"fnref:13\"\u003e\u003ca href=\"#fn:13\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e13\u003c/a\u003e\u003c/sup\u003e。\u003c/p\u003e\n\u003ch1 id=\"模型原理\"\u003e模型原理\u003c/h1\u003e\n\u003cp\u003e下图为 Lecun 等人提出的 LeNet-5 的网络架构：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-08-25-cnn/lenet-5.png\" alt=\"LeNet-5\"/\u003e\u003c/p\u003e\n\u003cp\u003e下面我们针对 CNN 网络中的不同类型的网络层逐一进行介绍。\u003c/p\u003e\n\u003ch2 id=\"输入层\"\u003e输入层\u003c/h2\u003e\n\u003cp\u003eLeNet-5 解决的手写数字分类问题的输入为一张 32x32 像素的灰度图像 (Gray Scale)。日常生活中计算机常用的图像的表示方式为 RGB，即将一张图片分为红色通道 (Red Channel)，绿色通道 (Green Channel) 和蓝色通道 (Blue Channel)，其中每个通道的每个像素点的数值范围为 \u003ccode\u003e$\\left[0, 255\\right]$\u003c/code\u003e。灰度图像表示该图片仅包含一个通道，也就是不具备彩色信息，每个像素点的数值范围同 RGB 图像的取值范围相同。\u003c/p\u003e\n\u003cp\u003e因此，一张图片在计算机的眼里就是一个如下图所示的数字矩阵 (示例图片来自于 MNIST 数据集 \u003csup id=\"fnref:14\"\u003e\u003ca href=\"#fn:14\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e14\u003c/a\u003e\u003c/sup\u003e)：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-08-25-cnn/digit-pixels.png\" alt=\"Digit-Pixels\"/\u003e\u003c/p\u003e\n\u003cp\u003e在将图像输入到 CNN 网络之前，通常我们会对其进行预处理，因为每个像素点的最大取值为 \u003ccode\u003e$255$\u003c/code\u003e，因此将每个像素点的值除以 \u003ccode\u003e$255$\u003c/code\u003e 则可以将其归一化到 \u003ccode\u003e$\\left[0, 1\\right]$\u003c/code\u003e 的范围。\u003c/p\u003e\n\u003ch2 id=\"卷积层\"\u003e卷积层\u003c/h2\u003e\n\u003cp\u003e在了解卷积层之前，让我们先来了解一下什么是卷积？设 \u003ccode\u003e$f\\left(x\\right), g\\left(x\\right)$\u003c/code\u003e 是 \u003ccode\u003e$\\mathbb{R}$\u003c/code\u003e 上的两个可积函数，则卷积定义为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\left(f * g\\right) \\left(x\\right) = \\int_{- \\infty}^{\\infty}{f \\left(\\tau\\right) g \\left(x - \\tau\\right) d \\tau} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e离散形式定义为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\left(f * g\\right) \\left(x\\right) = \\sum_{\\tau = - \\infty}^{\\infty}{f \\left(\\tau\\right) g \\left(x - \\tau\\right)} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e我们用一个示例来形象的理解一下卷积的含义，以离散的形式为例，假设我们有两个骰子，\u003ccode\u003e$f\\left(x\\right), g\\left(x\\right)$\u003c/code\u003e 分别表示投两个骰子，\u003ccode\u003e$x$\u003c/code\u003e 面朝上的概率。\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ f \\left(x\\right) = g \\left(x\\right) = \\begin{cases} 1/6 \u0026amp; x = 1, 2, 3, 4, 5, 6 \\\\ 0 \u0026amp; \\text{otherwise} \\end{cases} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e卷积 \u003ccode\u003e$\\left(f * g\\right) \\left(x\\right)$\u003c/code\u003e 表示投两个骰子，朝上数字之和为 \u003ccode\u003e$x$\u003c/code\u003e 的概率。则和为 \u003ccode\u003e$4$\u003c/code\u003e 的概率为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{equation} \\begin{split} \\left(f * g\\right) \\left(4\\right) \u0026amp;= \\sum_{\\tau = 1}^{6}{f \\left(\\tau\\right) g \\left(4 - \\tau\\right)} \\\\ \u0026amp;= f \\left(1\\right) g \\left(4 - 1\\right) + f \\left(2\\right) g \\left(4 - 2\\right) + f \\left(3\\right) g \\left(4 - 3\\right) \\\\ \u0026amp;= 1/6 \\times 1/6 + 1/6 \\times 1/6 + 1/6 \\times 1/6 \\\\ \u0026amp;= 1/12 \\end{split} \\end{equation} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e这是一维的情况，我们处理的图像为一个二维的矩阵，因此类似的有：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\left(f * g\\right) \\left(x, y\\right) = \\sum_{v = - \\infty}^{\\infty}{\\sum_{h = - \\infty}^{\\infty}{f \\left(h, v\\right) g \\left(x - h, y - v\\right)}} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e这次我们用一个抽象的例子解释二维情况下卷积的计算，设 \u003ccode\u003e$f, g$\u003c/code\u003e 对应的概率矩阵如下：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ f = \\left[ \\begin{array}{ccc} \\color{red}{a_{0, 0}} \u0026amp; \\color{orange}{a_{0, 1}} \u0026amp; \\color{yellow}{a_{0, 2}} \\\\ \\color{green}{a_{1, 0}} \u0026amp; \\color{cyan}{a_{1, 1}} \u0026amp; \\color{blue}{a_{1, 2}} \\\\ \\color{purple}{a_{2, 0}} \u0026amp; \\color{black}{a_{2, 1}} \u0026amp; \\color{gray}{a_{2, 2}} \\end{array} \\right] , g = \\left[ \\begin{array}{ccc} \\color{gray}{b_{-1, -1}} \u0026amp; \\color{black}{b_{-1, 0}} \u0026amp; \\color{purple}{b_{-1, 1}} \\\\ \\color{blue}{b_{0, -1}} \u0026amp; \\color{cyan}{b_{0, 0}} \u0026amp; \\color{green}{b_{0, 1}} \\\\ \\color{yellow}{b_{1, -1}} \u0026amp; \\color{orange}{b_{1, 0}} \u0026amp; \\color{red}{b_{1, 1}} \\end{array} \\right] $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e则 \u003ccode\u003e$\\left(f * g\\right) \\left(1, 1\\right)$\u003c/code\u003e 计算方式如下：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\left(f * g\\right) \\left(1, 1\\right) = \\sum_{v = 0}^{2}{\\sum_{h = 0}^{2}{f \\left(h, v\\right) g \\left(1 - h, 1 - v\\right)}} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e从这个计算公式中我们就不难看出为什么上面的 \u003ccode\u003e$f, g$\u003c/code\u003e 两个概率矩阵的角标会写成上述形式，即两个矩阵相同位置的角标之和均为 \u003ccode\u003e$1$\u003c/code\u003e。\u003ccode\u003e$\\left(f * g\\right) \\left(1, 1\\right)$\u003c/code\u003e 即为 \u003ccode\u003e$f, g$\u003c/code\u003e 两个矩阵中对应颜色的元素乘积之和。\u003c/p\u003e\n\u003cp\u003e在上例中，\u003ccode\u003e$f, g$\u003c/code\u003e 两个概率矩阵的大小相同，而在 CNN 中，\u003ccode\u003e$f$\u003c/code\u003e 为输入的图像，\u003ccode\u003e$g$\u003c/code\u003e 一般是一个相对较小的矩阵，我们称之为卷积核。这种情况下，卷积的计算方式是类似的，只是会将 \u003ccode\u003e$g$\u003c/code\u003e 矩阵旋转 \u003ccode\u003e$180^{\\circ}$\u003c/code\u003e 使得相乘的元素的位置也相同，同时需要 \u003ccode\u003e$g$\u003c/code\u003e 在 \u003ccode\u003e$f$\u003c/code\u003e 上进行滑动并计算对应位置的卷积值。下图 \u003csup id=\"fnref:15\"\u003e\u003ca href=\"#fn:15\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e15\u003c/a\u003e\u003c/sup\u003e 展示了一步计算的具体过程：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-08-25-cnn/conv-example.png\" alt=\"Conv-Example\"/\u003e\u003c/p\u003e\n\u003cp\u003e下图 \u003csup id=\"fnref1:15\"\u003e\u003ca href=\"#fn:15\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e15\u003c/a\u003e\u003c/sup\u003e 形象的刻画了利用一个 3x3 大小的卷积核的整个卷积计算过程：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-08-25-cnn/conv-sobel.gif\" alt=\"Conv-Sobel\"/\u003e\u003c/p\u003e\n\u003cp\u003e一些预设的卷积核对于图片可以起到不同的滤波器效果，例如下面 4 个卷积核分别会对图像产生不同的效果：不改变，边缘检测，锐化和高斯模糊。\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\left[ \\begin{array}{ccc} 0 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 0 \\end{array} \\right] , \\left[ \\begin{array}{ccc} -1 \u0026amp; -1 \u0026amp; -1 \\\\ -1 \u0026amp;  8 \u0026amp; -1 \\\\ -1 \u0026amp; -1 \u0026amp; -1 \\end{array} \\right] , \\left[ \\begin{array}{ccc} 0  \u0026amp; -1 \u0026amp; 0 \\\\ -1 \u0026amp;  5 \u0026amp; -1 \\\\ 0  \u0026amp; -1 \u0026amp; 0 \\end{array} \\right] , \\dfrac{1}{16} \\left[ \\begin{array}{ccc} 1 \u0026amp; 2 \u0026amp; 1 \\\\ 2 \u0026amp; 4 \u0026amp; 2 \\\\ 1 \u0026amp; 2 \u0026amp; 1 \\end{array} \\right] $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e对 lena 图片应用这 4 个卷积核，变换后的效果如下 (从左到右，从上到下)：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-08-25-cnn/lena-filters.png\" alt=\"Lena-Filters\"/\u003e\u003c/p\u003e\n\u003cp\u003e在上面整个计算卷积的动图中，我们不难发现，利用 3x3 大小 (我们一般将这个参数称之为 \u003ccode\u003ekernel_size\u003c/code\u003e，即\u003cstrong\u003e卷积核的大小\u003c/strong\u003e，其可以为一个整数表示长宽大小相同，也可以为两个不同的整数) 的卷积核对 5x5 大小的原始矩阵进行卷积操作后，结果矩阵并没有保持原来的大小，而是变为了 (5-(3-1))x(5-(3-1)) (即 3x3) 大小的矩阵。这就需要引入 CNN 网络中卷积层的两个常用参数 \u003ccode\u003epadding\u003c/code\u003e 和 \u003ccode\u003estrides\u003c/code\u003e。\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003epadding\u003c/code\u003e 是指是否对图像的外侧进行\u003cstrong\u003e补零操作\u003c/strong\u003e，其取值一般为 \u003ccode\u003eVALID\u003c/code\u003e 和 \u003ccode\u003eSAME\u003c/code\u003e 两种。\u003ccode\u003eVALID\u003c/code\u003e 表示\u003cstrong\u003e不进行补零\u003c/strong\u003e操作，对于输入形状为 \u003ccode\u003e$\\left(x, y\\right)$\u003c/code\u003e 的矩阵，利用形状为 \u003ccode\u003e$\\left(m, n\\right)$\u003c/code\u003e 的卷积核进行卷积，得到的结果矩阵的形状则为 \u003ccode\u003e$\\left(x-m+1, y-n+1\\right)$\u003c/code\u003e。\u003ccode\u003eSAME\u003c/code\u003e 表示\u003cstrong\u003e进行补零\u003c/strong\u003e操作，在进行卷积操作前，会对图像的四个边缘分别向左右补充 \u003ccode\u003e$\\left(m \\mid 2 \\right) + 1$\u003c/code\u003e 个零，向上下补充 \u003ccode\u003e$\\left(n \\mid 2 \\right) + 1$\u003c/code\u003e 个零 (\u003ccode\u003e$\\mid$\u003c/code\u003e 表示整除)，从而保证进行卷积操作后，结果的形状与原图像的形状保持相同，如下图 \u003csup id=\"fnref2:15\"\u003e\u003ca href=\"#fn:15\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e15\u003c/a\u003e\u003c/sup\u003e 所示：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-08-25-cnn/conv-zero-padding.png\" alt=\"Conv2d-Zero-Padding\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003estrides\u003c/code\u003e 是指进行卷积操作时，每次卷积核移动的步长。示例中，卷积核在横轴和纵轴方向上的移动步长均为 \u003ccode\u003e$1$\u003c/code\u003e，除此之外用于也可以指定不同的步长。移动的步长同样会对卷积后的结果的形状产生影响。\u003c/p\u003e\n\u003cp\u003e除此之外，还有另一个重要的参数 \u003ccode\u003efilters\u003c/code\u003e，其表示在一个卷积层中使用的\u003cstrong\u003e卷积核的个数\u003c/strong\u003e。在一个卷积层中，一个卷积核可以学习并提取图像的一种特征，但往往图片中包含多种不同的特征信息，因此我们需要多个不同的卷积核提取不同的特征。下图 \u003csup id=\"fnref3:15\"\u003e\u003ca href=\"#fn:15\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e15\u003c/a\u003e\u003c/sup\u003e 是一个利用 4 个不同的卷积核对一张图像进行卷积操作的示意图：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-08-25-cnn/conv2d-kernels.png\" alt=\"Conv2d-Kernels\"/\u003e\u003c/p\u003e\n\u003cp\u003e上面我们都是以一个灰度图像 (仅包含 1 个通道) 为示例进行的讨论，那么对于一个 RGB 图像 (包含 3 个通道)，相应的，卷积核也是一个 3 维的形状，如下图 \u003csup id=\"fnref4:15\"\u003e\u003ca href=\"#fn:15\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e15\u003c/a\u003e\u003c/sup\u003e 所示：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-08-25-cnn/conv3d-kernels.png\" alt=\"Conv3d-Kernels\"/\u003e\u003c/p\u003e\n\u003cp\u003e卷积层对于我们的神经网络的模型带来的改进主要包括如下三个方面：\u003cstrong\u003e稀疏交互 (sparse interactions)\u003c/strong\u003e，\u003cstrong\u003e参数共享 (parameter sharing)\u003c/strong\u003e 和\u003cstrong\u003e等变表示 (equivariant representations)\u003c/strong\u003e。\u003c/p\u003e\n\u003cp\u003e在全连接的神经网络中，隐含层中的每一个节点都和上一层的所有节点相连，同时有被连接到下一层的全部节点。而卷积层不同，节点之间的连接性受到卷积核大小的制约。下图 \u003csup id=\"fnref:16\"\u003e\u003ca href=\"#fn:16\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e16\u003c/a\u003e\u003c/sup\u003e 分别以自下而上 (左) 和自上而下 (右) 两个角度对比了卷积层和全连接层节点之间连接性的差异。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-08-25-cnn/sparse-interactions.png\" alt=\"Sparse-Interactions\"/\u003e\u003c/p\u003e\n\u003cp\u003e在上图 (右) 中，我们可以看出节点 \u003ccode\u003e$s_3$\u003c/code\u003e 受到节点 \u003ccode\u003e$x_2$\u003c/code\u003e，\u003ccode\u003e$x_3$\u003c/code\u003e 和 \u003ccode\u003e$x_4$\u003c/code\u003e 的影响，这些节点被称之为 \u003ccode\u003e$s_3$\u003c/code\u003e 的\u003cstrong\u003e接受域 (receptive field)\u003c/strong\u003e。稀疏交互使得在 \u003ccode\u003e$m$\u003c/code\u003e 个输入和 \u003ccode\u003e$n$\u003c/code\u003e 个输出的情况下，参数的个数由 \u003ccode\u003e$m \\times n$\u003c/code\u003e 个减少至 \u003ccode\u003e$k \\times n$\u003c/code\u003e 个，其中 \u003ccode\u003e$k$\u003c/code\u003e 为卷积核的大小。尽管一个节点在一个层级之间仅与其接受域内的节点相关联，但是对于深层中的节点，其与绝大部分输入之间却存在这\u003cstrong\u003e间接交互\u003c/strong\u003e，如下图 \u003csup id=\"fnref1:16\"\u003e\u003ca href=\"#fn:16\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e16\u003c/a\u003e\u003c/sup\u003e 所示：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-08-25-cnn/indirect-interactions.png\" alt=\"Indirect-Interactions\"/\u003e\u003c/p\u003e\n\u003cp\u003e节点 \u003ccode\u003e$g_3$\u003c/code\u003e 尽管\u003cstrong\u003e直接\u003c/strong\u003e的连接是稀疏的，但处于更深的层中可以\u003cstrong\u003e间接\u003c/strong\u003e的连接到全部或者大部分的输入节点。这就使得网络可以仅通过这种稀疏交互来高效的描述多个输入变量之间的复杂关系。\u003c/p\u003e\n\u003cp\u003e除了稀疏交互带来的参数个数减少外，\u003cstrong\u003e参数共享\u003c/strong\u003e也起到了类似的作用。所谓参数共享就是指在进行不同操作时使用相同的参数，具体而言也就是在我们利用卷积核在图像上滑动计算卷积时，每一步使用的卷积核都是相同的。同全连接网络的情况对比如下图 \u003csup id=\"fnref2:16\"\u003e\u003ca href=\"#fn:16\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e16\u003c/a\u003e\u003c/sup\u003e 所示：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-08-25-cnn/parameter-sharing.png\" alt=\"Parameter-Sharing\"/\u003e\u003c/p\u003e\n\u003cp\u003e在全连接网络 (上图 - 下) 中，任意两个节点之间的连接 (权重) 仅用于这两个节点之间，而在卷积层中，如上图所示，其对卷积核中间节点 (黑色箭头) 的使用方式 (权重) 是相同的。参数共享虽然对于计算的时间复杂度没有带来改进，仍然是 \u003ccode\u003e$O \\left(k \\times n\\right)$\u003c/code\u003e，但其却将参数个数降低至 \u003ccode\u003e$k$\u003c/code\u003e 个。\u003c/p\u003e\n\u003cp\u003e正是由于参数共享机制，使得卷积层具有平移 \u003cstrong\u003e等变 (equivariance)\u003c/strong\u003e 的性质。对于函数 \u003ccode\u003e$f\\left(x\\right)$\u003c/code\u003e 和 \u003ccode\u003e$g\\left(x\\right)$\u003c/code\u003e，如果满足 \u003ccode\u003e$f\\left(g\\left(x\\right)\\right) = g\\left(f\\left(x\\right)\\right)$\u003c/code\u003e，我们就称 \u003ccode\u003e$f\\left(x\\right)$\u003c/code\u003e 对于变换 \u003ccode\u003e$g$\u003c/code\u003e 具有等变性。简言之，对于图像如果我们将所有的像素点进行移动，则卷积后的输出表示也会移动同样的量。\u003c/p\u003e\n\u003ch2 id=\"非线性层\"\u003e非线性层\u003c/h2\u003e\n\u003cp\u003e非线性层并不是 CNN 特有的网络层，在此我们不再详细介绍，一般情况下我们会使用 ReLU 作为我们的激活函数。\u003c/p\u003e\n\u003ch2 id=\"池化层\"\u003e池化层\u003c/h2\u003e\n\u003cp\u003e\u003cstrong\u003e池化层\u003c/strong\u003e 是一个利用 \u003cstrong\u003e池化函数 (pooling function)\u003c/strong\u003e 对网络输出进行进一步调整的网络层。池化函数使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出。常用的池化函数包括最大池化 (max pooling) 函数 (即给出邻域内的最大值) 和平均池化 (average pooling) 函数 (即给出邻域内的平均值) 等。但无论选择何种池化函数，当对输入做出少量平移时，池化对输入的表示都近似 \u003cstrong\u003e不变 (invariant)\u003c/strong\u003e。\u003cstrong\u003e局部平移不变性\u003c/strong\u003e 是一个很重要的性质，尤其是当我们关心某个特征是否出现而不关心它出现的位置时。\u003c/p\u003e\n\u003cp\u003e池化层同卷积层类似，具有三个比较重要的参数：\u003ccode\u003epool_size\u003c/code\u003e，\u003ccode\u003estrides\u003c/code\u003e 和 \u003ccode\u003epadding\u003c/code\u003e，分别表示池化窗口的大小，步长以及是否对图像的外侧进行补零操作。下图 \u003csup id=\"fnref3:16\"\u003e\u003ca href=\"#fn:16\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e16\u003c/a\u003e\u003c/sup\u003e 是一个 \u003ccode\u003epool_size=3\u003c/code\u003e，\u003ccode\u003estrides=3\u003c/code\u003e，\u003ccode\u003epadding=\u0026#39;valid\u0026#39;\u003c/code\u003e 的最大池化过程示例：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-08-25-cnn/max-pooling.gif\" alt=\"Max-Pooling\"/\u003e\u003c/p\u003e\n\u003cp\u003e池化层同时也能够提高网络的计算效率，例如上图中在横轴和纵轴的步长均为 \u003ccode\u003e$3$\u003c/code\u003e，经过池化后，下一层网络节点的个数降低至前一层的 \u003ccode\u003e$\\frac{1}{3 \\times 3} = \\frac{1}{9}$\u003c/code\u003e。\u003c/p\u003e\n\u003ch2 id=\"全连接层\"\u003e全连接层\u003c/h2\u003e\n\u003cp\u003e全链接层 (Fully-connected or Dense Layer) 的目的就是将我们最后一个池化层的输出连接到最终的输出节点上。例如，最后一个池化层的输出大小为 \u003ccode\u003e$\\left[5 \\times 5 \\times 16\\right]$\u003c/code\u003e，也就是有 \u003ccode\u003e$5 \\times 5 \\times 16 = 400$\u003c/code\u003e 个节点，对于手写数字识别的问题，我们的输出为 0 至 9 共 10 个数字，采用 one-hot 编码的话，输出层共 10 个节点。例如在 LeNet 中有 2 个全连接层，每层的节点数分别为 120 和 84，在实际应用中，通常全连接层的节点数会逐层递减。需要注意的是，在进行编码的时候，第一个全连接层并不是直接与最后一个池化层相连，而是先对池化层进行 flatten 操作，使其变成一个一维向量后再与全连接层相连。\u003c/p\u003e\n\u003ch2 id=\"输出层\"\u003e输出层\u003c/h2\u003e\n\u003cp\u003e输出层根据具体问题的不同会略有不同，例如对于手写数字识别问题，采用 one-hot 编码的话，输出层则包含 10 个节点。对于回归或二分类问题，输出层则仅包含 1 个节点。当然对于二分类问题，我们也可以像多分类问题一样将其利用 one-hot 进行编码，例如 \u003ccode\u003e$\\left[1, 0\\right]$\u003c/code\u003e 表示类型 0，\u003ccode\u003e$\\left[0, 1\\right]$\u003c/code\u003e 表示类型 1。\u003c/p\u003e\n\u003ch1 id=\"扩展与应用\"\u003e扩展与应用\u003c/h1\u003e\n\u003cp\u003e本节我们将介绍一些经典的 CNN 网络架构及其相关的改进。\u003c/p\u003e\n\u003ch2 id=\"alexnet-krizhevsky2012imagenet\"\u003eAlexNet \u003csup id=\"fnref1:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e2\u003c/a\u003e\u003c/sup\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-08-25-cnn/alexnet.png\" alt=\"AlexNet\"/\u003e\u003c/p\u003e\n\u003cp\u003eAlexNet 在整体结构上同 LeNet-5 类似，其改进大致如下：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e网络包含了 5 个卷积层和 3 个全连接层，网络规模变大。\u003c/li\u003e\n\u003cli\u003e使用了 ReLU 非线性激活函数。\u003c/li\u003e\n\u003cli\u003e应用了 Data Augmentation，Dropout，Momentum，Weight Decay 等策略改进训练。\u003c/li\u003e\n\u003cli\u003e在算力有限的情况下，对模型进行划分为两部分并行计算。\u003c/li\u003e\n\u003cli\u003e增加局部响应归一化 (LRN, Local Response Normalization)。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eLRN 的思想来自与生物学中侧抑制 (Lateral Inhibition) 的概念，简单来说就是相近的神经元之间会发生抑制作用。在 AlexNet 中，给出的 LRN 计算公式如下：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ b_{x,y}^{i} = a_{x,y}^{i} / \\left(k + \\alpha \\sum_{j = \\max \\left(0, i - n/2\\right)}^{\\min \\left(N - 1, i + n/2\\right)}{\\left(a_{x,y}^{j}\\right)^2}\\right)^{\\beta} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中，\u003ccode\u003e$a_{x,y}^{i}$\u003c/code\u003e 表示第 \u003ccode\u003e$i$\u003c/code\u003e 个卷积核在位置 \u003ccode\u003e$\\left(x,y\\right)$\u003c/code\u003e 的输出，\u003ccode\u003e$N$\u003c/code\u003e 为卷积核的个数，\u003ccode\u003e$k, n, \\alpha, \\beta$\u003c/code\u003e 均为超参数，在原文中分别初值为：\u003ccode\u003e$k=2, n=5, \\alpha=10^{-4}, \\beta=0.75$\u003c/code\u003e。在上式中，分母为所有卷积核 (Feature Maps) 的加和，因此 LRN 可以简单理解为一个跨 Feature Maps 的像素级归一化。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e开源实现\u003c/strong\u003e：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ci class=\"icon icon-tensorflow\"\u003e\u003c/i\u003e \u003ca href=\"https://github.com/tensorflow/models/tree/master/research\"\u003etensorflow/models\u003c/a\u003e,  \u003ca href=\"https://github.com/tflearn/tflearn/blob/master/examples/images\"\u003etflearn/examples\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ci class=\"icon icon-pytorch\"\u003e\u003c/i\u003e \u003ca href=\"https://pytorch.org/docs/stable/torchvision/models.html\"\u003epytorch/torchvision/models\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ci class=\"icon icon-caffe2\"\u003e\u003c/i\u003e \u003ca href=\"https://github.com/caffe2/models\"\u003ecaffe2/models\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ci class=\"icon icon-mxnet\"\u003e\u003c/i\u003e \u003ca href=\"https://github.com/apache/incubator-mxnet/blob/master/example/image-classification/symbols\"\u003eincubator-mxnet/example\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"vgg-net-simonyan2014very\"\u003eVGG Net \u003csup id=\"fnref1:4\"\u003e\u003ca href=\"#fn:4\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e4\u003c/a\u003e\u003c/sup\u003e\u003c/h2\u003e\n\u003cimg src=\"/images/cn/2018-08-25-cnn/vgg-16.png\" width=\"180\" style=\"float:left; margin-right:3em;\"/\u003e\n\u003cp\u003e左图是 VGG-16 Net 的网络结构，原文中还有一个 VGG-19 Net，其差别在于后面三组卷积层中均多叠加了一个卷积层，使得网络层数由 16 增加至 19。\u003c/p\u003e\n\u003cp\u003eVGG Net 的主要改变如下：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e网络层级更深，从 AlexNet 的 8 层增加至 16 层和 19 层，更深的网络层级意味着更强的学习能力，但也需要更多的算力对模型进行优化。\u003c/li\u003e\n\u003cli\u003e仅使用 3x3 大小的卷积。在 AlexNet 中，浅层部分使用了较大的卷积核，而 VGG 使用了 3x3 的小卷积核进行串联叠加，减少了参数个数。\u003c/li\u003e\n\u003cli\u003e卷积采样的步长为 1x1，Max Pooling 的步长为 2x2。\u003c/li\u003e\n\u003cli\u003e去掉了效果提升不明显的但计算耗时的 LRN。\u003c/li\u003e\n\u003cli\u003e增加了 Feature Maps 的个数。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e开源实现\u003c/strong\u003e：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ci class=\"icon icon-tensorflow\"\u003e\u003c/i\u003e \u003ca href=\"https://github.com/tensorflow/models/tree/master/research\"\u003etensorflow/models\u003c/a\u003e,  \u003ca href=\"https://github.com/tflearn/tflearn/tree/master/examples\"\u003etflearn/examples\u003c/a\u003e, \u003ca href=\"https://github.com/tensorlayer/awesome-tensorlayer\"\u003etensorlayer/awesome-tensorlayer\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ci class=\"icon icon-keras\"\u003e\u003c/i\u003e \u003ca href=\"https://www.tensorflow.org/api_docs/python/tf/keras/applications\"\u003etf/keras/applications\u003c/a\u003e, \u003ca href=\"https://keras.io/applications\"\u003ekeras/applications\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ci class=\"icon icon-pytorch\"\u003e\u003c/i\u003e \u003ca href=\"https://pytorch.org/docs/stable/torchvision/models.html\"\u003epytorch/torchvision/models\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ci class=\"icon icon-caffe2\"\u003e\u003c/i\u003e \u003ca href=\"https://github.com/caffe2/models\"\u003ecaffe2/models\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ci class=\"icon icon-mxnet\"\u003e\u003c/i\u003e \u003ca href=\"https://github.com/apache/incubator-mxnet/tree/master/example/image-classification/symbols\"\u003eincubator-mxnet/example\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp style=\"clear:both;\"\u003e\u003c/p\u003e\n\u003ch2 id=\"network-in-network-nin-lin2013network\"\u003eNetwork in Network (NIN) \u003csup id=\"fnref:17\"\u003e\u003ca href=\"#fn:17\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e17\u003c/a\u003e\u003c/sup\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-08-25-cnn/network-in-network.png\" alt=\"NIN\"/\u003e\u003c/p\u003e\n\u003cp\u003eNIN 网络的主要改变如下：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e利用多层的全连接网络替换线性的卷积，即 mlpconv (Conv + MLP) 层。其中卷积层为线性的操作，而 MLP 为非线性的操作，因此具有更高的抽象能力。\u003c/li\u003e\n\u003cli\u003e去掉了全连接层，使用 Global Average Pooling，也就是将每个 Feature Maps 上所有的值求平均，直接作为输出节点，如下图所示：\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-08-25-cnn/global-average-pooling.png\" alt=\"Global-Average-Pooling\"/\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e相比 AlexNet 简化了网络结构，仅包含 4 个 NIN 单元和一个 Global Average Pooling，整个参数空间比 AlexNet 小了一个数量级。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e在 NIN 中，在跨通道的情况下，mlpconv 层又等价于传统的 Conv 层后接一个 1x1 大小的卷积层，因此 mlpconv 层有时也称为 cccp (cascaded cross channel parametric pooling) 层。1x1 大小的卷积核可以说实现了不同通道信息的交互和整合，同时对于输入通道为 \u003ccode\u003e$m$\u003c/code\u003e 和输出通道为 \u003ccode\u003e$n$\u003c/code\u003e，1x1 大小的卷积核在不改变分辨率的同时实现了降维 (\u003ccode\u003e$m \u0026gt; n$\u003c/code\u003e 情况下) 或升维 (\u003ccode\u003e$m \u0026lt; n$\u003c/code\u003e 情况下) 操作。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e开源实现\u003c/strong\u003e：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ci class=\"icon icon-tensorflow\"\u003e\u003c/i\u003e \u003ca href=\"https://github.com/tflearn/tflearn/tree/master/examples\"\u003etflearn/examples\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"googlenet-inception-v1-szegedy2015going-inception-v3-szegedy2016rethinking-inception-v4-szegedy2016inception\"\u003eGoogLeNet (Inception V1) \u003csup id=\"fnref1:5\"\u003e\u003ca href=\"#fn:5\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e5\u003c/a\u003e\u003c/sup\u003e, Inception V3 \u003csup id=\"fnref:18\"\u003e\u003ca href=\"#fn:18\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e18\u003c/a\u003e\u003c/sup\u003e, Inception V4 \u003csup id=\"fnref:19\"\u003e\u003ca href=\"#fn:19\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e19\u003c/a\u003e\u003c/sup\u003e\u003c/h2\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-08-25-cnn/googlenet.png\" alt=\"GoogLeNet\"/\u003e\u003c/p\u003e\n\u003cp\u003e除了 VGG 这种从网络深度方向进行优化的策略以外，Google 还提出了在同一层利用不同大小的卷积核同时提取不同特征的思路，对于这样的结构我们称之为 Inception。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-08-25-cnn/inception-v1-naive-dim-reduction.png\" alt=\"Inception-V1\"/\u003e\u003c/p\u003e\n\u003cp\u003e上图 (左) 为原始的 Inception 结构，在这样一层中分别包括了 1x1 卷积，3x3 卷积，5x5 卷积和 3x3 Max Polling，使得网络在每一层都能学到不同尺度的特征。最后通过 Filter Concat 将其拼接为多个 Feature Maps。\u003c/p\u003e\n\u003cp\u003e这种方式虽然能够带来性能的提升，但同时也增加了计算量，因此为了进一步改善，其选择利用 1x1 大小的卷积进行降维操作，改进后的 Inception 模块如上图 (右) 所示。我们以 GoogLeNet 中的 inception (3a) 模块为例 (输入大小为 28x28x192)，解释 1x1 卷积的降维效果。\u003c/p\u003e\n\u003cp\u003e对于原始 Inception 模块，1x1 卷积的通道为 64，3x3 卷积的通道为 128，5x5 卷积的通道为 32，卷积层的参数个数为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{equation} \\begin{split} \\# w_{\\text{3a_conv_without_1x1}} =\u0026amp; 1 \\times 1 \\times 192 \\times 64 \\\\ \u0026amp; + 3 \\times 3 \\times 192 \\times 128 \\\\ \u0026amp; + 5 \\times 5 \\times 192 \\times 32 \\\\ =\u0026amp; 387072 \\end{split} \\end{equation} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e对于加上 1x1 卷积后的 Inception 模块 (通道数分别为 96 和 16) 后，卷积层的参数个数为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{equation} \\begin{split} \\# w_{\\text{3a_conv_with_1x1}} =\u0026amp; 1 \\times 1 \\times 192 \\times 64 \\\\ \u0026amp; + 1 \\times 1 \\times 192 \\times 96 + 3 \\times 3 \\times 96 \\times 128 \\\\ \u0026amp; + 1 \\times 1 \\times 192 \\times 16 + 5 \\times 5 \\times 16 \\times 32 \\\\ =\u0026amp; 157184 \\end{split} \\end{equation} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e可以看出，在添加 1x1 大小的卷积后，参数的个数减少了 2 倍多。通过 1x1 卷积对特征进行降维的层称之为 Bottleneck Layer 或 Bottleneck Block。\u003c/p\u003e\n\u003cp\u003e在 GoogLeNet 中，作者还提出了 Auxiliary Classifiers (AC)，用于辅助训练。AC 通过增加浅层的梯度来减轻深度梯度弥散的问题，从而加速整个网络的收敛。\u003c/p\u003e\n\u003cp\u003e随后 Google 在此对 Inception 进行了改进，同时提出了卷积神经网络的 4 项设计原则，概括如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e避免表示瓶颈，尤其是在网络的浅层部分。一般来说，在到达任务的最终表示之前，表示的大小应该从输入到输出缓慢减小。\u003c/li\u003e\n\u003cli\u003e高维特征在网络的局部更容易处理。在网络中增加更多的非线性有助于获得更多的解耦特征，同时网络训练也会加快。\u003c/li\u003e\n\u003cli\u003e空间聚合可以在低维嵌入中进行，同时也不会对表征能力带来太多影响。例如，再进行尺寸较大的卷积操作之前可以先对输入进行降维处理。\u003c/li\u003e\n\u003cli\u003e在网络的宽度和深度之间进行权衡。通过增加网络的深度和宽度均能够带来性能的提升，在同时增加其深度和宽度时，需要综合考虑算力的分配。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eInception V3 的主要改进包括：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e增加了 Batch Normalized 层。\u003c/li\u003e\n\u003cli\u003e将一个 5x5 的卷积替换为两个串联的 3x3 的卷积 (基于原则 3)，减少了网络参数，如下图所示：\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-08-25-cnn/inception-v3-v1-3x3.png\" alt=\"Inception-V3-3x3\"/\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e利用串联的 1xn 和 nx1 的非对称卷积 (Asymmetric Convolutions) 替代 nxn 的卷积 (基于原则 3)，减少了网络参数，如下图 (左) 所示：\u003c/li\u003e\n\u003cli\u003e增加带有滤波器组 (filter bank) 的 Inception 模块 (基于原则 2)，用于提升高维空间的表示能力，如下图 (右) 所示：\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-08-25-cnn/inception-v3-1xn-nx1.png\" alt=\"Inception-V3-1xn-nx1\"/\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e重新探讨了 Auxiliary Classifiers 的作用，发现其在训练初期并没有有效的提高收敛速度，尽在训练快结束时会略微提高网络的精度。\u003c/li\u003e\n\u003cli\u003e新的下采样方案。在传统的做法中，如果先进行 Pooling，在利用 Inception 模块进行操作，如下图 (左) 所示，会造成表示瓶颈 (原则 1)；而先利用 Inception 模块进行操作，再进行 Pooling，则会增加参数数量。\n\u003cimg src=\"/images/cn/2018-08-25-cnn/inception-v3-reducing-gird-size-old.png\" alt=\"Inception-V3-reducing-grid-size-old\"/\u003e\u003cbr/\u003e\n因此，借助 Inception 结构的思想，作者提出了一种新的下采样方案。下图 (左) 是利用 Inception 的思想进行下采样的内部逻辑，下图 (右) 为同时利用 Inception 思想和 Pooling 进行下采样的整体框架。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-08-25-cnn/inception-v3-reducing-gird-size-new.png\" alt=\"Inception-V3-reducing-grid-size-new\"/\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eLabel Smoothing 机制。假设标签的真实分布为 \u003ccode\u003e$q\\left(k\\right)$\u003c/code\u003e，则对于一个真实标签 \u003ccode\u003e$y$\u003c/code\u003e 而言，有 \u003ccode\u003e$q\\left(y\\right) = 1$\u003c/code\u003e，对于 \u003ccode\u003e$k \\neq y$\u003c/code\u003e，有 \u003ccode\u003e$q\\left(k\\right) = 0$\u003c/code\u003e。这种情况会导致两个问题：一个是当模型对于每个训练样本的真实标签赋予全部的概率时，模型将会发生过拟合；另一个是其鼓励拉大最大概率标签同其他标签之间的概率差距，从而降低网络的适应性。也就是说这种情况的发生是由于网络对于其预测结果过于自信。因此，对于一个真实标签 \u003ccode\u003e$y$\u003c/code\u003e，我们将其标签的分布 \u003ccode\u003e$q\\left(k | x\\right) = \\delta_{k, y}$\u003c/code\u003e 替换为：\n\u003ccode\u003e$$ q\u0026#39; \\left(k | x\\right) = \\left(1 - \\epsilon\\right) \\delta_{k, y} + \\epsilon u \\left(k\\right) $$\u003c/code\u003e\n其中，\u003ccode\u003e$u \\left(k\\right)$\u003c/code\u003e 是一个固定的分布，文中采用了均匀分布，即 \u003ccode\u003e$u \\left(k\\right) = 1 / K$\u003c/code\u003e；\u003ccode\u003e$\\epsilon$\u003c/code\u003e 为权重项，试验中取为 \u003ccode\u003e$0.1$\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eInception V4 对于 Inception 网络做了进一步细致的调整，其主要是将 Inception V3 中的前几层网络替换为了 stem 模块，具体的 stem 模块结构就不在此详细介绍了。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e开源实现\u003c/strong\u003e：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ci class=\"icon icon-tensorflow\"\u003e\u003c/i\u003e \u003ca href=\"https://github.com/tensorflow/models/tree/master/research\"\u003etensorflow/models\u003c/a\u003e,  \u003ca href=\"https://github.com/tflearn/tflearn/tree/master/examples\"\u003etflearn/examples\u003c/a\u003e, \u003ca href=\"https://github.com/tensorlayer/awesome-tensorlayer\"\u003etensorlayer/awesome-tensorlayer\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ci class=\"icon icon-keras\"\u003e\u003c/i\u003e \u003ca href=\"https://www.tensorflow.org/api_docs/python/tf/keras/applications\"\u003etf/keras/applications\u003c/a\u003e, \u003ca href=\"https://keras.io/applications\"\u003ekeras/applications\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ci class=\"icon icon-pytorch\"\u003e\u003c/i\u003e \u003ca href=\"https://pytorch.org/docs/stable/torchvision/models.html\"\u003epytorch/torchvision/models\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ci class=\"icon icon-caffe2\"\u003e\u003c/i\u003e \u003ca href=\"https://github.com/caffe2/models\"\u003ecaffe2/models\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ci class=\"icon icon-mxnet\"\u003e\u003c/i\u003e \u003ca href=\"https://github.com/apache/incubator-mxnet/tree/master/example/image-classification/symbols\"\u003eincubator-mxnet/example\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"deep-residual-net-he2016deep-identity-mapping-residual-net-he2016ientity-densenet-huang2016densely\"\u003eDeep Residual Net \u003csup id=\"fnref1:6\"\u003e\u003ca href=\"#fn:6\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e6\u003c/a\u003e\u003c/sup\u003e, Identity Mapping Residual Net \u003csup id=\"fnref:20\"\u003e\u003ca href=\"#fn:20\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e20\u003c/a\u003e\u003c/sup\u003e, DenseNet \u003csup id=\"fnref:21\"\u003e\u003ca href=\"#fn:21\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e21\u003c/a\u003e\u003c/sup\u003e\u003c/h2\u003e\n\u003cimg src=\"/images/cn/2018-08-25-cnn/residual-network.png\" width=\"300\" style=\"float:left; margin-right:3em;\"/\u003e\n\u003cp\u003e随着网络深度的不断增加啊，其效果并未如想象一般提升，甚至发生了退化，He 等人 \u003csup id=\"fnref2:6\"\u003e\u003ca href=\"#fn:6\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e6\u003c/a\u003e\u003c/sup\u003e 发现在 CIFAR-10 数据集上，一个 56 层的神经网络的性能要比一个 20 层的神经网络要差。网络层级的不断增加，不仅导致参数的增加，同时也可能导致梯度弥散问题 (vanishing gradients)。\u003c/p\u003e\n\u003cp\u003e这对这些问题，He 等人提出了一种 Deep Residual Net，在这个网络结构中，残差 (residual) 的思想可以理解为：假设原始潜在的映射关系为 \u003ccode\u003e$\\mathcal{H} \\left(\\mathbf{x}\\right)$\u003c/code\u003e，对于新的网络层我们不再拟合原始的映射关系，而是拟合 \u003ccode\u003e$\\mathcal{F} \\left(\\mathbf{x}\\right) = \\mathcal{H} \\left(\\mathbf{x}\\right) - \\mathbf{x}$\u003c/code\u003e，也就是说原始潜在的映射关系变为 \u003ccode\u003e$\\mathcal{F} \\left(\\mathbf{x}\\right) + \\mathbf{x}$\u003c/code\u003e。新的映射关系可以理解为在网络前向传播中添加了一条捷径 (shortcut connections)，如下图所示：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-08-25-cnn/residual-block.png\" alt=\"Residual-Block\"/\u003e\u003c/p\u003e\n\u003cp style=\"clear:both;\"\u003e\u003c/p\u003e\n\u003cp\u003e增加 Short Connections 并没有增加参数个数，也没有增加计算量，与此同时模型依旧可以利用 SGD 等算法进行优化。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-08-25-cnn/residual-results.png\" alt=\"Residual-Results\"/\u003e\u003c/p\u003e\n\u003cp\u003e从 Deep Residual Net 的实验结果 (如上图) 可以看出，在没有加入残差模块的网络中 (上图 - 左) 出现了上文中描述的问题：更多层级的网络的效果反而较差；在加入了残差模块的网络中 (上图 - 右)，其整体性能均比未加入残差模块的网络要好，同时具有更多层级的网络的效果也更好。\u003c/p\u003e\n\u003cp\u003e随后 He 等人 \u003csup id=\"fnref1:20\"\u003e\u003ca href=\"#fn:20\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e20\u003c/a\u003e\u003c/sup\u003e 又提出了 Identity Mapping Residual Net，在原始的 ResNet 中，一个残差单元可以表示为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{equation} \\begin{split} \\mathbb{y}_{\\ell} = \u0026amp; h \\left(\\mathbb{x}_{\\ell}\\right) + \\mathcal{F} \\left(\\mathbb{x}_{\\ell}, \\mathcal{W}_l\\right) \\\\ \\mathbb{x}_{\\ell+1} = \u0026amp; f \\left(\\mathbb{y}_{\\ell}\\right) \\end{split} \\end{equation} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中 \u003ccode\u003e$\\mathbb{x}_{\\ell}$\u003c/code\u003e 和 \u003ccode\u003e$\\mathbb{x}_{\\ell+1}$\u003c/code\u003e 为第 \u003ccode\u003e$\\ell$\u003c/code\u003e 个单元的输入和输出，\u003ccode\u003e$\\mathcal{F}$\u003c/code\u003e 为残差函数，\u003ccode\u003e$h \\left(\\mathbb{x}_{\\ell}\\right) = \\mathbb{x}_{\\ell}$\u003c/code\u003e 为一个恒等映射，\u003ccode\u003e$f$\u003c/code\u003e 为 ReLU 函数。在 Identity Mapping Residual Net，作者将 \u003ccode\u003e$f$\u003c/code\u003e 由原来的 ReLU 函数也替换成一个恒定映射，即 \u003ccode\u003e$\\mathbb{x}_{\\ell+1} \\equiv \\mathbb{y}_{\\ell}$\u003c/code\u003e，则上式可以改写为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\mathbb{x}_{\\ell+1} = \\mathbb{x}_{\\ell} + \\mathcal{F} \\left(\\mathbb{x}_{\\ell}, \\mathcal{W}_{\\ell}\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e则对于任意深度的单元 \u003ccode\u003e$L$\u003c/code\u003e，有如下表示：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\mathbb{x}_L = \\mathbb{x}_{\\ell} + \\sum_{i=\\ell}^{L-1}{\\mathcal{F} \\left(\\mathbb{x}_i, \\mathcal{W}_i\\right)} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e上式形式的表示使得其在反向传播中具有一个很好的性质，假设损失函数为 \u003ccode\u003e$\\mathcal{E}$\u003c/code\u003e，根据链式法则，对于单元 \u003ccode\u003e$\\ell$\u003c/code\u003e，梯度为：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\dfrac{\\partial \\mathcal{E}}{\\partial \\mathbb{x}_{\\ell}} = \\dfrac{\\partial \\mathcal{E}}{\\partial \\mathbb{x}_L} \\dfrac{\\partial \\mathbb{x}_L}{\\partial \\mathbb{x}_{\\ell}} = \\dfrac{\\partial \\mathcal{E}}{\\partial \\mathbb{x}_{\\ell}} \\left(1 + \\dfrac{\\partial}{\\partial \\mathbb{x}_{\\ell}} \\sum_{i=\\ell}^{L-1}{\\mathcal{F} \\left(\\mathbb{x}_i, \\mathcal{W}_i\\right)}\\right) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e对于上式形式的梯度，我们可以将其拆解为两部分：\u003ccode\u003e$\\frac{\\partial \\mathcal{E}}{\\partial \\mathbb{x}_{\\ell}}$\u003c/code\u003e 为不通过任何权重层的直接梯度传递，\u003ccode\u003e$\\frac{\\partial \\mathcal{E}}{\\partial \\mathbb{x}_{\\ell}} \\left(\\frac{\\partial}{\\partial \\mathbb{x}_{\\ell}} \\sum_{i=\\ell}^{L-1}{\\mathcal{F} \\left(\\mathbb{x}_i, \\mathcal{W}_i\\right)}\\right)$\u003c/code\u003e 为通过权重层的梯度传递。前一项保证了梯度能够直接传回任意浅层 \u003ccode\u003e$\\ell$\u003c/code\u003e，同时对于任意一个 mini-batch 的所有样本，\u003ccode\u003e$\\frac{\\partial}{\\partial \\mathbb{x}_{\\ell}} \\sum_{i=\\ell}^{L-1}\\mathcal{F}$\u003c/code\u003e 不可能永远为 \u003ccode\u003e$-1$\u003c/code\u003e，所以保证了即使权重很小的情况下也不会出现梯度弥散。下图展示了原始的 ResNet 和 Identity Mapping Residual Net 之间残差单元的区别和网络的性能差异：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-08-25-cnn/identity-mapping-residual-network-unit.png\" alt=\"Identity-Mapping-Residual-Net-Unit\"/\u003e\u003c/p\u003e\n\u003cp\u003eHuang 等人 \u003csup id=\"fnref1:21\"\u003e\u003ca href=\"#fn:21\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e21\u003c/a\u003e\u003c/sup\u003e 在 ResNet 的基础上又提出了 DenseNet 网络，其网络结构如下所示：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-08-25-cnn/densenet.png\" alt=\"DenseNet\"/\u003e\u003c/p\u003e\n\u003cp\u003eDenseNet 的主要改进如下：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eDense Connectivity：将网络中每一层都与其后续层进行直接连接。\u003c/li\u003e\n\u003cli\u003eGrowth Rate：\u003ccode\u003e$H_{\\ell}$\u003c/code\u003e 将产生 \u003ccode\u003e$k$\u003c/code\u003e 个 Feature Maps，因此第 \u003ccode\u003e$\\ell$\u003c/code\u003e 层将包含 \u003ccode\u003e$k_0 + k \\times \\left(\\ell - 1\\right)$\u003c/code\u003e 个 Feature Maps，其中 \u003ccode\u003e$k_0$\u003c/code\u003e 为输入层的通道数。DenseNet 与现有框架的不同之处就是将网络限定的比较窄，例如：\u003ccode\u003e$k = 12$\u003c/code\u003e，并将该超参数称之为网络的增长率 (Growth Rate)。\u003c/li\u003e\n\u003cli\u003eBottleneck Layers：在 3x3 的卷积之前增加 1x1 的卷积进行降维操作。\u003c/li\u003e\n\u003cli\u003eCompression：在两个 Dense Block 之间增加过渡层 (Transition Layer)，进一步减少 Feature Maps 个数。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cstrong\u003e开源实现\u003c/strong\u003e：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ci class=\"icon icon-tensorflow\"\u003e\u003c/i\u003e \u003ca href=\"https://github.com/tensorflow/models/tree/master/research\"\u003etensorflow/models\u003c/a\u003e,  \u003ca href=\"https://github.com/tflearn/tflearn/tree/master/examples\"\u003etflearn/examples\u003c/a\u003e, \u003ca href=\"https://github.com/tensorlayer/awesome-tensorlayer\"\u003etensorlayer/awesome-tensorlayer\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ci class=\"icon icon-keras\"\u003e\u003c/i\u003e \u003ca href=\"https://www.tensorflow.org/api_docs/python/tf/keras/applications\"\u003etf/keras/applications\u003c/a\u003e, \u003ca href=\"https://keras.io/applications\"\u003ekeras/applications\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ci class=\"icon icon-pytorch\"\u003e\u003c/i\u003e \u003ca href=\"https://pytorch.org/docs/stable/torchvision/models.html\"\u003epytorch/torchvision/models\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ci class=\"icon icon-caffe2\"\u003e\u003c/i\u003e \u003ca href=\"https://github.com/caffe2/models\"\u003ecaffe2/models\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ci class=\"icon icon-mxnet\"\u003e\u003c/i\u003e \u003ca href=\"https://github.com/apache/incubator-mxnet/tree/master/example/image-classification/symbols\"\u003eincubator-mxnet/example\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"综合比较\"\u003e综合比较\u003c/h2\u003e\n\u003cp\u003eCanziani 等人 \u003csup id=\"fnref:22\"\u003e\u003ca href=\"#fn:22\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e22\u003c/a\u003e\u003c/sup\u003e 综合了模型的准确率，参数大小，内存占用，推理时间等多个角度对现有的 CNN 模型进行了对比分析。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"/images/cn/2018-08-25-cnn/cnn-accuracy-and-parameters.png\" alt=\"CNN-Accuracy-and-Parameters\"/\u003e\u003c/p\u003e\n\u003cp\u003e上图 (左) 展示了在 ImageNet 挑战赛中不同 CNN 网络模型的 Top-1 的准确率。可以看出近期的 ResNet 和 Inception 架构以至少 7% 的显著优势超过了其他架构。上图 (右) 以另一种形式展现了除了准确率以外的更多信息，包括计算成本和网络的参数个数，其中横轴为计算成本，纵轴为 Top-1 的准确率，气泡的大小为网络的参数个数。可以看出 ResNet 和 Inception 架构相比 AlexNet 和 VGG 不仅有更高的准确率，其在计算成本和网络的参数个数 (模型大小) 方面也具有一定优势。\u003c/p\u003e\n\u003cp\u003e文章部分内容参考了 \u003cstrong\u003e刘昕\u003c/strong\u003e 的 \u003ca href=\"http://valser.org/2016/dl/%E5%88%98%E6%98%95.pdf\"\u003e\u003cstrong\u003eCNN近期进展与实用技巧\u003c/strong\u003e\u003c/a\u003e。CNN 除了在图像分类问题上取得很大的进展外，在例如：物体检测：R-CNN \u003csup id=\"fnref:23\"\u003e\u003ca href=\"#fn:23\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e23\u003c/a\u003e\u003c/sup\u003e, SPP-Net \u003csup id=\"fnref:24\"\u003e\u003ca href=\"#fn:24\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e24\u003c/a\u003e\u003c/sup\u003e, Fast R-CNN \u003csup id=\"fnref:25\"\u003e\u003ca href=\"#fn:25\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e25\u003c/a\u003e\u003c/sup\u003e, Faster R-CNN \u003csup id=\"fnref:26\"\u003e\u003ca href=\"#fn:26\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e26\u003c/a\u003e\u003c/sup\u003e，语义分割：FCN \u003csup id=\"fnref:27\"\u003e\u003ca href=\"#fn:27\" class=\"footnote-ref\" role=\"doc-noteref\"\u003e27\u003c/a\u003e\u003c/sup\u003e 等多个领域也取得了不俗的成绩。针对不同的应用场景，网络模型和处理方法均有一定的差异，本文就不再对其他场景一一展开说明，不同场景将在后续进行单独整理。\u003c/p\u003e\n\u003cdiv class=\"footnotes\" role=\"doc-endnotes\"\u003e\n\u003chr/\u003e\n\u003col\u003e\n\u003cli id=\"fn:1\"\u003e\n\u003cp\u003eLeCun, Y., Bottou, L., Bengio, Y., \u0026amp; Haffner, P. (1998). Gradient-based learning applied to document recognition. \u003cem\u003eProceedings of the IEEE, 86\u003c/em\u003e(11), 2278-2324. \u003ca href=\"#fnref:1\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:2\"\u003e\n\u003cp\u003eKrizhevsky, A., Sutskever, I., \u0026amp; Hinton, G. E. (2012). Imagenet classification with deep convolutional neural networks. In \u003cem\u003eAdvances in neural information processing systems\u003c/em\u003e (pp. 1097-1105). \u003ca href=\"#fnref:2\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e \u003ca href=\"#fnref1:2\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:3\"\u003e\n\u003cp\u003e\u003ca href=\"http://www.image-net.org/\"\u003ehttp://www.image-net.org/\u003c/a\u003e \u003ca href=\"#fnref:3\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:4\"\u003e\n\u003cp\u003eSimonyan, K., \u0026amp; Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. \u003cem\u003earXiv preprint arXiv:1409.1556.\u003c/em\u003e \u003ca href=\"#fnref:4\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e \u003ca href=\"#fnref1:4\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:5\"\u003e\n\u003cp\u003eSzegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., … \u0026amp; Rabinovich, A. (2015). Going deeper with convolutions. In \u003cem\u003eProceedings of the IEEE conference on computer vision and pattern recognition\u003c/em\u003e (pp. 1-9). \u003ca href=\"#fnref:5\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e \u003ca href=\"#fnref1:5\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:6\"\u003e\n\u003cp\u003eHe, K., Zhang, X., Ren, S., \u0026amp; Sun, J. (2016). Deep residual learning for image recognition. In \u003cem\u003eProceedings of the IEEE conference on computer vision and pattern recognition\u003c/em\u003e (pp. 770-778). \u003ca href=\"#fnref:6\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e \u003ca href=\"#fnref1:6\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e \u003ca href=\"#fnref2:6\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:7\"\u003e\n\u003cp\u003eGirshick, R., Donahue, J., Darrell, T., \u0026amp; Malik, J. (2014). Rich feature hierarchies for accurate object detection and semantic segmentation. In \u003cem\u003eProceedings of the IEEE conference on computer vision and pattern recognition\u003c/em\u003e (pp. 580-587). \u003ca href=\"#fnref:7\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:8\"\u003e\n\u003cp\u003eGirshick, R. (2015). Fast r-cnn. In \u003cem\u003eProceedings of the IEEE international conference on computer vision\u003c/em\u003e (pp. 1440-1448). \u003ca href=\"#fnref:8\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:9\"\u003e\n\u003cp\u003eRen, S., He, K., Girshick, R., \u0026amp; Sun, J. (2015). Faster r-cnn: Towards real-time object detection with region proposal networks. In \u003cem\u003eAdvances in neural information processing systems\u003c/em\u003e (pp. 91-99). \u003ca href=\"#fnref:9\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:10\"\u003e\n\u003cp\u003eLong, J., Shelhamer, E., \u0026amp; Darrell, T. (2015). Fully convolutional networks for semantic segmentation. In \u003cem\u003eProceedings of the IEEE conference on computer vision and pattern recognition\u003c/em\u003e (pp. 3431-3440). \u003ca href=\"#fnref:10\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:11\"\u003e\n\u003cp\u003eVinyals, O., Toshev, A., Bengio, S., \u0026amp; Erhan, D. (2015). Show and tell: A neural image caption generator. In \u003cem\u003eProceedings of the IEEE conference on computer vision and pattern recognition\u003c/em\u003e (pp. 3156-3164). \u003ca href=\"#fnref:11\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:12\"\u003e\n\u003cp\u003eJi, S., Xu, W., Yang, M., \u0026amp; Yu, K. (2013). 3D convolutional neural networks for human action recognition. \u003cem\u003eIEEE transactions on pattern analysis and machine intelligence, 35\u003c/em\u003e(1), 221-231. \u003ca href=\"#fnref:12\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:13\"\u003e\n\u003cp\u003eKim, Y. (2014). Convolutional neural networks for sentence classification. \u003cem\u003earXiv preprint arXiv:1408.5882.\u003c/em\u003e \u003ca href=\"#fnref:13\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:14\"\u003e\n\u003cp\u003e\u003ca href=\"http://yann.lecun.com/exdb/mnist\"\u003ehttp://yann.lecun.com/exdb/mnist\u003c/a\u003e \u003ca href=\"#fnref:14\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:15\"\u003e\n\u003cp\u003e\u003ca href=\"https://mlnotebook.github.io/post/CNN1/\"\u003ehttps://mlnotebook.github.io/post/CNN1/\u003c/a\u003e \u003ca href=\"#fnref:15\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e \u003ca href=\"#fnref1:15\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e \u003ca href=\"#fnref2:15\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e \u003ca href=\"#fnref3:15\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e \u003ca href=\"#fnref4:15\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:16\"\u003e\n\u003cp\u003eGoodfellow, I., Bengio, Y., Courville, A., \u0026amp; Bengio, Y. (2016). \u003cem\u003eDeep learning\u003c/em\u003e (Vol. 1). Cambridge: MIT press. \u003ca href=\"#fnref:16\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e \u003ca href=\"#fnref1:16\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e \u003ca href=\"#fnref2:16\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e \u003ca href=\"#fnref3:16\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:17\"\u003e\n\u003cp\u003eLin, M., Chen, Q., \u0026amp; Yan, S. (2013). Network In Network. \u003cem\u003earXiv preprint arXiv:1312.4400.\u003c/em\u003e \u003ca href=\"#fnref:17\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:18\"\u003e\n\u003cp\u003eSzegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., \u0026amp; Wojna, Z. (2016). Rethinking the Inception Architecture for Computer Vision. In \u003cem\u003eProceedings of the IEEE Conference on Computer Vision and Pattern Recognition\u003c/em\u003e (pp. 2818–2826). \u003ca href=\"#fnref:18\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:19\"\u003e\n\u003cp\u003eSzegedy, C., Ioffe, S., Vanhoucke, V., \u0026amp; Alemi, A. (2016). Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning. \u003cem\u003earXiv preprint arXiv:1602.07261.\u003c/em\u003e \u003ca href=\"#fnref:19\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:20\"\u003e\n\u003cp\u003eHe, K., Zhang, X., Ren, S., \u0026amp; Sun, J. (2016). Identity Mappings in Deep Residual Networks. \u003cem\u003earXiv preprint arXiv:1603.05027.\u003c/em\u003e \u003ca href=\"#fnref:20\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e \u003ca href=\"#fnref1:20\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:21\"\u003e\n\u003cp\u003eHuang, G., Liu, Z., van der Maaten, L., \u0026amp; Weinberger, K. Q. (2016). Densely Connected Convolutional Networks. \u003cem\u003earXiv preprint arXiv:1608.06993\u003c/em\u003e \u003ca href=\"#fnref:21\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e \u003ca href=\"#fnref1:21\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:22\"\u003e\n\u003cp\u003eCanziani, A., Paszke, A., \u0026amp; Culurciello, E. (2016). An Analysis of Deep Neural Network Models for Practical Applications. \u003cem\u003earXiv preprint arXiv:1605.07678\u003c/em\u003e \u003ca href=\"#fnref:22\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:23\"\u003e\n\u003cp\u003eGirshick, R., Donahue, J., Darrell, T., \u0026amp; Malik, J. (2014). Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation. In \u003cem\u003eProceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition\u003c/em\u003e (pp. 580–587). \u003ca href=\"#fnref:23\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:24\"\u003e\n\u003cp\u003eHe, K., Zhang, X., Ren, S., \u0026amp; Sun, J. (2015). Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition. \u003cem\u003eIEEE Transactions on Pattern Analysis and Machine Intelligence, 37(9)\u003c/em\u003e, 1904–1916. \u003ca href=\"#fnref:24\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:25\"\u003e\n\u003cp\u003eGirshick, R. (2015). Fast R-CNN. \u003cem\u003earXiv preprint arXiv:1504.08083.\u003c/em\u003e \u003ca href=\"#fnref:25\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:26\"\u003e\n\u003cp\u003eRen, S., He, K., Girshick, R., \u0026amp; Sun, J. (2017). Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks. \u003cem\u003eIEEE Transactions on Pattern Analysis and Machine Intelligence, 39(6),\u003c/em\u003e 1137–1149. \u003ca href=\"#fnref:26\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003cli id=\"fn:27\"\u003e\n\u003cp\u003eShelhamer, E., Long, J., \u0026amp; Darrell, T. (2017). Fully Convolutional Networks for Semantic Segmentation. \u003cem\u003eIEEE Transactions on Pattern Analysis and Machine Intelligence, 39(4),\u003c/em\u003e 640–651. \u003ca href=\"#fnref:27\" class=\"footnote-backref\" role=\"doc-backlink\"\u003e↩︎\u003c/a\u003e\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ol\u003e\n\u003c/div\u003e\n\n\n\n\n\n\u003cdiv class=\"donate\"\u003e\n  \u003cdiv class=\"donate-header\"\u003e\u003c/div\u003e\n  \u003cdiv class=\"donate-slug\" id=\"donate-slug\"\u003ecnn\u003c/div\u003e\n  \u003cbutton class=\"donate-button\"\u003e赞 赏\u003c/button\u003e\n  \u003cdiv class=\"donate-footer\"\u003e「真诚赞赏，手留余香」\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"donate-modal-wrapper\"\u003e\n  \u003cdiv class=\"donate-modal\"\u003e\n    \u003cdiv class=\"donate-box\"\u003e\n      \u003cdiv class=\"donate-box-content\"\u003e\n        \u003cdiv class=\"donate-box-content-inner\"\u003e\n          \u003cdiv class=\"donate-box-header\"\u003e「真诚赞赏，手留余香」\u003c/div\u003e\n          \u003cdiv class=\"donate-box-body\"\u003e\n            \u003cdiv class=\"donate-box-money\"\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-2\" data-v=\"2\" data-unchecked=\"￥ 2\" data-checked=\"2 元\"\u003e￥ 2\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-5\" data-v=\"5\" data-unchecked=\"￥ 5\" data-checked=\"5 元\"\u003e￥ 5\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-10\" data-v=\"10\" data-unchecked=\"￥ 10\" data-checked=\"10 元\"\u003e￥ 10\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-50\" data-v=\"50\" data-unchecked=\"￥ 50\" data-checked=\"50 元\"\u003e￥ 50\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-100\" data-v=\"100\" data-unchecked=\"￥ 100\" data-checked=\"100 元\"\u003e￥ 100\u003c/button\u003e\n              \u003cbutton class=\"donate-box-money-button donate-box-money-button-unchecked\" id=\"donate-box-money-button-custom\" data-v=\"custom\" data-unchecked=\"任意金额\" data-checked=\"任意金额\"\u003e任意金额\u003c/button\u003e\n            \u003c/div\u003e\n            \u003cdiv class=\"donate-box-pay\"\u003e\n              \u003cimg class=\"donate-box-pay-qrcode\" id=\"donate-box-pay-qrcode\" src=\"\"/\u003e\n            \u003c/div\u003e\n          \u003c/div\u003e\n          \u003cdiv class=\"donate-box-footer\"\u003e\n            \u003cdiv class=\"donate-box-pay-method donate-box-pay-method-checked\" data-v=\"wechat-pay\"\u003e\n              \u003cimg class=\"donate-box-pay-method-image\" id=\"donate-box-pay-method-image-wechat-pay\" src=\"\"/\u003e\n            \u003c/div\u003e\n            \u003cdiv class=\"donate-box-pay-method\" data-v=\"alipay\"\u003e\n              \u003cimg class=\"donate-box-pay-method-image\" id=\"donate-box-pay-method-image-alipay\" src=\"\"/\u003e\n            \u003c/div\u003e\n          \u003c/div\u003e\n        \u003c/div\u003e\n      \u003c/div\u003e\n    \u003c/div\u003e\n    \u003cbutton type=\"button\" class=\"donate-box-close-button\"\u003e\n      \u003csvg class=\"donate-box-close-button-icon\" fill=\"#fff\" viewBox=\"0 0 24 24\" width=\"24\" height=\"24\"\u003e\u003cpath d=\"M13.486 12l5.208-5.207a1.048 1.048 0 0 0-.006-1.483 1.046 1.046 0 0 0-1.482-.005L12 10.514 6.793 5.305a1.048 1.048 0 0 0-1.483.005 1.046 1.046 0 0 0-.005 1.483L10.514 12l-5.208 5.207a1.048 1.048 0 0 0 .006 1.483 1.046 1.046 0 0 0 1.482.005L12 13.486l5.207 5.208a1.048 1.048 0 0 0 1.483-.006 1.046 1.046 0 0 0 .005-1.482L13.486 12z\" fill-rule=\"evenodd\"\u003e\u003c/path\u003e\u003c/svg\u003e\n    \u003c/button\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\n\u003cscript type=\"text/javascript\" src=\"/js/donate.js\"\u003e\u003c/script\u003e\n\n\n  \u003cfooter\u003e\n  \n\u003cnav class=\"post-nav\"\u003e\n  \u003cspan class=\"nav-prev\"\u003e← \u003ca href=\"/cn/2018/07/buy-books-hoard-books-and-read-books/\"\u003e买书，囤书，看书 (Buy Books, Hoard Books and Read Books)\u003c/a\u003e\u003c/span\u003e\n  \u003cspan class=\"nav-next\"\u003e\u003ca href=\"/cn/2018/09/war-of-medias/\"\u003e媒介之战 (War of Medias)\u003c/a\u003e →\u003c/span\u003e\n\u003c/nav\u003e\n\n\n\n\n\u003cins class=\"adsbygoogle\" style=\"display:block; text-align:center;\" data-ad-layout=\"in-article\" data-ad-format=\"fluid\" data-ad-client=\"ca-pub-2608165017777396\" data-ad-slot=\"8302038603\"\u003e\u003c/ins\u003e\n\u003cscript\u003e\n  (adsbygoogle = window.adsbygoogle || []).push({});\n\u003c/script\u003e\n\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/js-cookie@3.0.5/dist/js.cookie.min.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"/js/toggle-theme.js\"\u003e\u003c/script\u003e\n\n\n\u003cscript src=\"/js/no-highlight.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"/js/math-code.js\"\u003e\u003c/script\u003e\n\u003cscript src=\"/js/heading-anchor.js\"\u003e\u003c/script\u003e\n\n\n\n\u003csection class=\"comments\"\u003e\n\u003cscript src=\"https://giscus.app/client.js\" data-repo=\"leovan/leovan.me\" data-repo-id=\"MDEwOlJlcG9zaXRvcnkxMTMxOTY0Mjc=\" data-category=\"Comments\" data-category-id=\"DIC_kwDOBr89i84CT-R7\" data-mapping=\"pathname\" data-strict=\"1\" data-reactions-enabled=\"1\" data-emit-metadata=\"0\" data-input-position=\"top\" data-theme=\"preferred_color_scheme\" data-lang=\"zh-CN\" data-loading=\"lazy\" crossorigin=\"anonymous\" defer=\"\"\u003e\n\u003c/script\u003e\n\u003c/section\u003e\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\u003cscript async=\"\" src=\"/js/center-img.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/right-quote.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/external-link.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/alt-title.js\"\u003e\u003c/script\u003e\n\u003cscript async=\"\" src=\"/js/figure.js\"\u003e\u003c/script\u003e\n\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/lazysizes@5.3.2/lazysizes.min.js\"\u003e\u003c/script\u003e\n\n\n\u003cscript src=\"//cdn.jsdelivr.net/npm/vanilla-back-to-top@latest/dist/vanilla-back-to-top.min.js\"\u003e\u003c/script\u003e\n\u003cscript\u003e\naddBackToTop({\n  diameter: 48\n});\n\u003c/script\u003e\n\n  \u003chr/\u003e\n  \u003cdiv class=\"copyright no-border-bottom\"\u003e\n    \u003cdiv class=\"copyright-author-year\"\u003e\n      \u003cspan\u003eCopyright © 2017-2024 \u003ca href=\"/\"\u003e范叶亮 | Leo Van\u003c/a\u003e\u003c/span\u003e\n    \u003c/div\u003e\n  \u003c/div\u003e\n  \u003c/footer\u003e\n  \u003c/article\u003e",
  "Date": "2018-08-25T00:00:00Z",
  "Author": "范叶亮"
}