{
  "Source": "liam.page",
  "Title": "XGBoost 在计算 NDCG 时的特殊处理",
  "Link": "https://liam.page/2016/12/21/XGBoost-idealDCG-and-NDCG/",
  "Content": "\u003cdiv class=\"post-body\" itemprop=\"articleBody\"\u003e\n\n      \n        \u003cp\u003eXGBoost 是陈天奇（怪）领衔开发的一套 Gradient Boost 算法实现，比如我会用到它做 LambdaMART 的实验。如果要给它一个评价，那应该是：好用、耐操。\u003c/p\u003e\n\u003cp\u003e不过，也有甜蜜的烦恼。XGBoost 在每轮迭代后，能够贴心地给出模型在数据集上的指标。比如我会关心 NDCG 指标。然而，这里列印出来的指标，会比事后用标准算法计算出来的值要高不少。\u003c/p\u003e\n\u003cspan id=\"more\"\u003e\u003c/span\u003e\n\n\u003cp\u003e最后，经检查（这里包含了血与泪），是 XGBoost 对 idealDCG 为 0 时的默认处理方式与标准算法不同导致的。标准算法，当 idealDCG 为 0 时，当前 NDCG 给 0；而 XGBoost 给 1，于是 XGBoost 计算出的最终 NDCG 要比标准算法计算得到的大不少。这件事情在 XGBoost 的文档里没有说明，所以称其为一个坑。\u003c/p\u003e\n\u003cp\u003e解决方法也很简单，在传参的时候，修改一下即可。\u003c/p\u003e\n\u003cfigure class=\"highlight python\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e2\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e3\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e4\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"comment\"\u003e# before\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"string\"\u003e\u0026#39;eval_metric\u0026#39;\u003c/span\u003e: \u003cspan class=\"string\"\u003e\u0026#39;ndcg@10\u0026#39;\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"comment\"\u003e# after\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"string\"\u003e\u0026#39;eval_metric\u0026#39;\u003c/span\u003e: \u003cspan class=\"string\"\u003e\u0026#39;ndcg@10-\u0026#39;\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003cp\u003e没错，只需要加一个减号就行了……\u003c/p\u003e\n\u003cp\u003e并附链接：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eXGBoost：\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://github.com/dmlc/xgboost\"\u003ehttps://github.com/dmlc/xgboost\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e陈天奇怪：\u003ca target=\"_blank\" rel=\"noopener\" href=\"http://weibo.com/u/2397265244\"\u003ehttp://weibo.com/u/2397265244\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eLambdaMART: \u003ca href=\"/2016/07/10/a-not-so-simple-introduction-to-lambdamart/\"\u003eLambdaMART 不太简短之介绍\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\n    \u003c/div\u003e",
  "Date": "2016-12-21T12:51:23Z",
  "Author": "Liam Huang"
}