{
  "Source": "liam.page",
  "Title": "谈谈激活函数以零为中心的问题",
  "Link": "https://liam.page/2018/04/17/zero-centered-active-function/",
  "Content": "\u003cdiv class=\"post-body\" itemprop=\"articleBody\"\u003e\n\n      \n        \u003cp\u003e今天在讨论神经网络中的激活函数时，陆同学提出 Sigmoid 函数的输出不是以零为中心的（non-zero-centered），这会导致神经网络收敛较慢。关于这一点，过去我只是将其记下，却并未理解背后的原因。此篇谈谈背后的原因。\u003c/p\u003e\n\u003cspan id=\"more\"\u003e\u003c/span\u003e\n\n\u003ch2 id=\"神经元\"\u003e\u003ca href=\"#神经元\" class=\"headerlink\" title=\"神经元\"\u003e\u003c/a\u003e神经元\u003c/h2\u003e\u003cp\u003e\u003cimg data-src=\"/uploads/images/MachineLearning/cell.jpg\"/\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e图片来自：\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://zhuanlan.zhihu.com/p/25110450\"\u003ehttps://zhuanlan.zhihu.com/p/25110450\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e如图是神经网络中一个典型的神经元设计，它完全仿照人类大脑中神经元之间传递数据的模式设计。大脑中，神经元通过若干树突（dendrite）的突触（synapse），接受其他神经元的轴突（axon）或树突传递来的消息，而后经过处理再由轴突输出。\u003c/p\u003e\n\u003cp\u003e在这里，诸 \u003ccode\u003e$x_i$\u003c/code\u003e 是其他神经元的轴突传来的消息，诸 \u003ccode\u003e$w_i$\u003c/code\u003e 是突触对消息的影响，诸 \u003ccode\u003e$w_ix_i$\u003c/code\u003e 则是神经元树突上传递的消息。这些消息经由神经元整合后（\u003ccode\u003e$z(\\vec x; \\vec w, b) = \\sum_iw_ix_i + b$\u003c/code\u003e）再激活输出（\u003ccode\u003e$f(z)$\u003c/code\u003e）。这里，整合的过程是线性加权的过程，各输入特征 \u003ccode\u003e$x_i$\u003c/code\u003e 之间没有相互作用。激活函数（active function）一般来说则是非线性的，各输入特征 \u003ccode\u003e$x_i$\u003c/code\u003e 在此处相互作用。\u003c/p\u003e\n\u003ch2 id=\"Sigmoid-与-tanh\"\u003e\u003ca href=\"#Sigmoid-与-tanh\" class=\"headerlink\" title=\"Sigmoid 与 tanh\"\u003e\u003c/a\u003eSigmoid 与 tanh\u003c/h2\u003e\u003cp\u003e此篇集中讨论激活函数输出是否以零为中心的问题，因而不对激活函数做过多的介绍，而只讨论 Sigmoid 与 tanh 两个激活函数。\u003c/p\u003e\n\u003ch3 id=\"Sigmoid-函数\"\u003e\u003ca href=\"#Sigmoid-函数\" class=\"headerlink\" title=\"Sigmoid 函数\"\u003e\u003c/a\u003eSigmoid 函数\u003c/h3\u003e\u003cp\u003eSigmoid 函数的一般形式是\u003c/p\u003e\n\u003cp\u003e$$\\sigma(x; a) = \\frac{1}{1 + \\mathrm{e}^{-ax}}.$$\u003c/p\u003e\n\u003cp\u003e这里，参数 $a$ 控制 Sigmoid 函数的形状，对函数基本性质没有太大的影响。在神经网络中，一般设置 \u003ccode\u003e$a = 1$\u003c/code\u003e，直接省略。\u003c/p\u003e\n\u003cp\u003eSigmoid 函数的导数很好求\u003c/p\u003e\n\u003cp\u003e$$\\sigma\u0026#39;(x) = \\sigma(x)\\bigl(1 - \\sigma(x)\\bigr).$$\u003c/p\u003e\n\u003cp\u003e\u003cimg data-src=\"/uploads/images/MachineLearning/sigma-sigma-prime.jpg\"/\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e图片来自：\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://zhuanlan.zhihu.com/p/25110450\"\u003ehttps://zhuanlan.zhihu.com/p/25110450\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch3 id=\"tanh-函数\"\u003e\u003ca href=\"#tanh-函数\" class=\"headerlink\" title=\"tanh 函数\"\u003e\u003c/a\u003etanh 函数\u003c/h3\u003e\u003cp\u003etanh 函数全称 Hyperbolic Tangent，即双曲正切函数。它的表达式是\u003c/p\u003e\n\u003cp\u003e$$\\tanh(x) = 2\\sigma(2x) - 1 = \\frac{\\mathrm{e}^{x} - \\mathrm{e}^{-x}}{\\mathrm{e}^{x} + \\mathrm{e}^{-x}}.$$\u003c/p\u003e\n\u003cp\u003e双曲正切函数的导数也很好求\u003c/p\u003e\n\u003cp\u003e$$\\tanh\u0026#39;(x) = 1 - \\tanh^2(x).$$\u003c/p\u003e\n\u003cp\u003e\u003cimg data-src=\"/uploads/images/MachineLearning/tanh-tanh-prime.jpg\"/\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e图片来自：\u003ca target=\"_blank\" rel=\"noopener\" href=\"https://zhuanlan.zhihu.com/p/25110450\"\u003ehttps://zhuanlan.zhihu.com/p/25110450\u003c/a\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch3 id=\"一些性质\"\u003e\u003ca href=\"#一些性质\" class=\"headerlink\" title=\"一些性质\"\u003e\u003c/a\u003e一些性质\u003c/h3\u003e\u003cp\u003eSigmoid 和 tanh 两个函数非常相似，具有不少相同的性质。简单罗列如下\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e优点：平滑\u003c/li\u003e\n\u003cli\u003e优点：易于求导\u003c/li\u003e\n\u003cli\u003e缺点：幂运算相对耗时\u003c/li\u003e\n\u003cli\u003e缺点：导数值小于 $1$，反向传播易导致梯度消失（Gradient Vanishing）\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e对于 Sigmoid 函数来说，它的值域是 $(0, 1)$，因此又有如下特点\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e优点：可以作为概率，辅助模型解释\u003c/li\u003e\n\u003cli\u003e缺点：输出值不以零为中心，可能导致模型收敛速度慢\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e此篇重点讲 Sigmoid 函数输出值不以零为中心的这一缺点。\u003c/p\u003e\n\u003ch2 id=\"收敛速度\"\u003e\u003ca href=\"#收敛速度\" class=\"headerlink\" title=\"收敛速度\"\u003e\u003c/a\u003e收敛速度\u003c/h2\u003e\u003cp\u003e这里首先需要给收敛速度做一个诠释。模型的最优解即是模型参数的最优解。通过逐轮迭代，模型参数会被更新到接近其最优解。这一过程中，迭代轮次多，则我们说模型收敛速度慢；反之，迭代轮次少，则我们说模型收敛速度快。\u003c/p\u003e\n\u003ch3 id=\"参数更新\"\u003e\u003ca href=\"#参数更新\" class=\"headerlink\" title=\"参数更新\"\u003e\u003c/a\u003e参数更新\u003c/h3\u003e\u003cp\u003e深度学习一般的学习方法是反向传播。简单来说，就是通过链式法则，求解全局损失函数 $L(\\vec x)$ 对某一参数 \u003ccode\u003e$w$\u003c/code\u003e 的偏导数（梯度）；而后辅以学习率 $\\eta$，向梯度的反方向更新参数 \u003ccode\u003e$w$\u003c/code\u003e。\u003c/p\u003e\n\u003cp\u003e$$w \\gets w - \\eta\\cdot\\frac{\\partial L}{\\partial w}.$$\u003c/p\u003e\n\u003cp\u003e考虑学习率 $\\eta$ 是全局设置的超参数，参数更新的核心步骤即是计算 $\\frac{\\partial L}{\\partial w}$。再考虑到对于某个神经元来说，其输入与输出的关系是\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$f(\\vec x; \\vec w, b) = f(z) = f\\Bigl(\\sum_iw_ix_i + b\\Bigr).$$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e因此，对于参数 \u003ccode\u003e$w_i$\u003c/code\u003e 来说，\u003c/p\u003e\n\u003cp\u003e$$\\frac{\\partial L}{\\partial w_i} = \\frac{\\partial L}{\\partial f}\\frac{\\partial f}{\\partial z}\\frac{\\partial z}{\\partial w_i} = x_i \\cdot \\frac{\\partial L}{\\partial f}\\frac{\\partial f}{\\partial z}.$$\u003c/p\u003e\n\u003cp\u003e因此，参数的更新步骤变为\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$w_i \\gets w_i - \\eta x_i\\cdot \\frac{\\partial L}{\\partial f}\\frac{\\partial f}{\\partial z}.$$\u003c/code\u003e\u003c/p\u003e\n\u003ch3 id=\"更新方向\"\u003e\u003ca href=\"#更新方向\" class=\"headerlink\" title=\"更新方向\"\u003e\u003c/a\u003e更新方向\u003c/h3\u003e\u003cp\u003e由于 \u003ccode\u003e$w_i$\u003c/code\u003e 是上一轮迭代的结果，此处可视为常数，而 $\\eta$ 是模型超参数，参数 \u003ccode\u003e$w_i$\u003c/code\u003e 的更新方向实际上由 \u003ccode\u003e$x_i\\cdot \\frac{\\partial L}{\\partial f}\\frac{\\partial f}{\\partial z}$\u003c/code\u003e 的符号决定。\u003c/p\u003e\n\u003cp\u003e考虑下标不同的两个参数 \u003ccode\u003e$w_i$\u003c/code\u003e 和 \u003ccode\u003e$w_j$\u003c/code\u003e，它们的更新方向分别由以下两式的符号决定：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$\\begin{cases}x_i\\cdot \\frac{\\partial L}{\\partial f}\\frac{\\partial f}{\\partial z}, \\\\ x_j\\cdot \\frac{\\partial L}{\\partial f}\\frac{\\partial f}{\\partial z}.\\end{cases}$$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e又考虑到 \u003ccode\u003e$\\frac{\\partial L}{\\partial f}\\frac{\\partial f}{\\partial z}$\u003c/code\u003e 对于所有的 \u003ccode\u003e$w_i$\u003c/code\u003e 来说是常数，因此各个 \u003ccode\u003e$w_i$\u003c/code\u003e 更新方向之间的\u003cstrong\u003e差异\u003c/strong\u003e，完全由对应的输入值 \u003ccode\u003e$x_i$\u003c/code\u003e 的符号的差异决定。例如说，若 \u003ccode\u003e$x_i$\u003c/code\u003e 与 \u003ccode\u003e$x_j$\u003c/code\u003e 的符号相同，则 \u003ccode\u003e$w_i$\u003c/code\u003e 和 \u003ccode\u003e$w_j$\u003c/code\u003e 更新方向就相同；反之，若 \u003ccode\u003e$x_i$\u003c/code\u003e 与 \u003ccode\u003e$x_j$\u003c/code\u003e 的符号相反，则 \u003ccode\u003e$w_i$\u003c/code\u003e 和 \u003ccode\u003e$w_j$\u003c/code\u003e 更新方向就相反。\u003c/p\u003e\n\u003ch3 id=\"以零为中心的影响\"\u003e\u003ca href=\"#以零为中心的影响\" class=\"headerlink\" title=\"以零为中心的影响\"\u003e\u003c/a\u003e以零为中心的影响\u003c/h3\u003e\u003cp\u003e至此，为了描述方便，我们以二维的情况为例。亦即，神经元描述为\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$f(\\vec x; \\vec w, b) = f\\bigl(w_0x_0 + w_1x_1 + b\\bigr).$$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e现在假设，参数 \u003ccode\u003e$w_0$\u003c/code\u003e, \u003ccode\u003e$w_1$\u003c/code\u003e 的最优解 \u003ccode\u003e$w_0^{*}$\u003c/code\u003e, \u003ccode\u003e$w_1^{*}$\u003c/code\u003e 满足条件\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$\\begin{cases}w_0 \u0026lt; w_0^{*}, \\\\ w_1\\geqslant w_1^{*}.\\end{cases}$$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e这也就是说，我们希望 \u003ccode\u003e$w_0$\u003c/code\u003e 适当增大，但希望 \u003ccode\u003e$w_1$\u003c/code\u003e 适当减小。考虑到上一小节提到的更新方向的问题，如果想要「一次到位」这就必然要求 \u003ccode\u003e$x_0$\u003c/code\u003e 和 \u003ccode\u003e$x_1$\u003c/code\u003e 符号相反。\u003c/p\u003e\n\u003cp\u003e但在 Sigmoid 函数中，输出值恒为正。这也就是说，\u003cstrong\u003e如果上一级神经元采用 Sigmoid 函数作为激活函数，那么我们无法做到 \u003ccode\u003e$x_0$\u003c/code\u003e 和 \u003ccode\u003e$x_1$\u003c/code\u003e 符号相反\u003c/strong\u003e。此时，模型为了收敛，不得不向逆风前行的风助力帆船一样，走 Z 字形逼近最优解。\u003c/p\u003e\n\u003cp\u003e\u003cimg data-src=\"/uploads/images/MachineLearning/zig-zag-gradient.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e如图，模型参数走绿色箭头能够最快收敛，但由于输入值的符号总是为正，所以模型参数可能走类似红色折线的箭头。如此一来，使用 Sigmoid 函数作为激活函数的神经网络，收敛速度就会慢上不少了。\u003c/p\u003e\n\n    \u003c/div\u003e",
  "Date": "2018-04-17T03:25:59Z",
  "Author": "Liam Huang"
}