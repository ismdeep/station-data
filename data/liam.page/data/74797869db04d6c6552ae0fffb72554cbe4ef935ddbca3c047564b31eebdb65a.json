{
  "Source": "liam.page",
  "Title": "将 HDFS 上的目录作为 Hive 外表分区同时避免数据拷贝",
  "Link": "https://liam.page/2019/11/05/connect-HDFS-data-into-Hive-external-table-without-redundant-movement/",
  "Content": "\u003cdiv class=\"post-body\" itemprop=\"articleBody\"\u003e\n\n      \n        \u003cp\u003eHive 是个好东西，它能够把 SQL 查询自动转化为一系列 Map-Reduce 任务。但显然，如何将数据引入 Hive 也会是个问题。一个典型的场景是：你通过某种方式，生成了大量结构化的数据，保存在 HDFS 上。现在你希望 Hive 能够基于这些数据，建立数据库，从而能够使用 SQL 语句进行数据库操作。但与此同时，因为数据量十分庞大，你不希望产生数据拷贝、搬移，以免消耗无谓的存储资源和计算资源。\u003c/p\u003e\n\u003cp\u003e此篇介绍我近期的一个实践方案。\u003c/p\u003e\n\u003cspan id=\"more\"\u003e\u003c/span\u003e\n\n\u003ch2 id=\"数据产出\"\u003e\u003ca href=\"#数据产出\" class=\"headerlink\" title=\"数据产出\"\u003e\u003c/a\u003e数据产出\u003c/h2\u003e\u003cp\u003e首先，你需要将数据以特定的格式产出到 HDFS 上。\u003c/p\u003e\n\u003cp\u003e例如，这里我以 Spark Streaming 任务将制表符分隔的 4 列数据，以 GZip 的格式，输出到 HDFS 位置：\u003ccode\u003ehdfs://namenode/path/to/data/\u0026lt;date\u0026gt;/\u0026lt;hour\u0026gt;/\u0026lt;dstreamid\u0026gt;\u003c/code\u003e。其中 \u003ccode\u003e\u0026lt;date\u0026gt;\u003c/code\u003e 是数据产出的日期，\u003ccode\u003e\u0026lt;hour\u0026gt;\u003c/code\u003e 是数据产出的小时，\u003ccode\u003e\u0026lt;dstreamid\u0026gt;\u003c/code\u003e 是数据产出时，对应 Spark Streaming 的 Direct Stream 的 ID。于是有类似这样的目录结构：\u003c/p\u003e\n\u003cfigure class=\"highlight shell\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e2\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e3\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e4\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e5\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e6\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e7\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003ehdfs://namenode/path/to/data/2019-11-01/13/123456/_SUCCESS\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003ehdfs://namenode/path/to/data/2019-11-01/13/123456/part-00000.gz\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003ehdfs://namenode/path/to/data/2019-11-01/13/123456/part-00001.gz\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003ehdfs://namenode/path/to/data/2019-11-01/13/123456/part-00002.gz\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003ehdfs://namenode/path/to/data/2019-11-01/13/123456/part-00003.gz\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003ehdfs://namenode/path/to/data/2019-11-01/13/123456/part-00004.gz\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003ehdfs://namenode/path/to/data/2019-11-01/13/123456/part-00005.gz\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003ch2 id=\"建立-Hive-表\"\u003e\u003ca href=\"#建立-Hive-表\" class=\"headerlink\" title=\"建立 Hive 表\"\u003e\u003c/a\u003e建立 Hive 表\u003c/h2\u003e\u003cp\u003e有了数据之后，我们需要建立与数据格式相对应的 Hive 表。注意，由于我们不希望对数据进行额外的搬移操作，所以这里需要建立一张外表（EXTERNAL TABLE）。例如，\u003c/p\u003e\n\u003cfigure class=\"highlight sql\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e2\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e3\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e4\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e5\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e6\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e7\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e8\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e9\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e10\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e11\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e12\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e13\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e14\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e15\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e16\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e17\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e18\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e19\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e20\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"keyword\"\u003eCREATE\u003c/span\u003e \u003cspan class=\"keyword\"\u003eEXTERNAL\u003c/span\u003e \u003cspan class=\"keyword\"\u003eTABLE\u003c/span\u003e `table_name` (\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  `field_1` string,\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  `field_2` string,\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  `field_3` string,\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  `field_4` string)\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003ePARTITIONED \u003cspan class=\"keyword\"\u003eBY\u003c/span\u003e (\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  `\u003cspan class=\"type\"\u003edate\u003c/span\u003e` string,\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  `\u003cspan class=\"keyword\"\u003ehour\u003c/span\u003e` string,\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  `dstreamid` string)\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"type\"\u003eROW\u003c/span\u003e FORMAT SERDE\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  \u003cspan class=\"string\"\u003e\u0026#39;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe\u0026#39;\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"keyword\"\u003eWITH\u003c/span\u003e SERDEPROPERTIES (\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  \u003cspan class=\"string\"\u003e\u0026#39;field.delim\u0026#39;\u003c/span\u003e\u003cspan class=\"operator\"\u003e=\u003c/span\u003e\u003cspan class=\"string\"\u003e\u0026#39;\\t\u0026#39;\u003c/span\u003e,\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  \u003cspan class=\"string\"\u003e\u0026#39;serialization.format\u0026#39;\u003c/span\u003e\u003cspan class=\"operator\"\u003e=\u003c/span\u003e\u003cspan class=\"string\"\u003e\u0026#39;\\t\u0026#39;\u003c/span\u003e)\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003eSTORED \u003cspan class=\"keyword\"\u003eAS\u003c/span\u003e INPUTFORMAT\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  \u003cspan class=\"string\"\u003e\u0026#39;org.apache.hadoop.mapred.TextInputFormat\u0026#39;\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003eOUTPUTFORMAT\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  \u003cspan class=\"string\"\u003e\u0026#39;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat\u0026#39;\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003eLOCATION\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  \u003cspan class=\"string\"\u003e\u0026#39;hdfs://namenode/path/to/data/\u0026#39;\u003c/span\u003e;\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003cp\u003e这里，\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e数据共有 4 个域，名字分别是 \u003ccode\u003efield_1\u003c/code\u003e 至 \u003ccode\u003efield_4\u003c/code\u003e（你可以根据实际情况设置恰当的域名字）。\u003c/li\u003e\n\u003cli\u003e分区字段有三个，分别是 \u003ccode\u003edate\u003c/code\u003e/\u003ccode\u003ehour\u003c/code\u003e/\u003ccode\u003edstreamid\u003c/code\u003e，与数据保存时的子路径名保持一致。\u003c/li\u003e\n\u003cli\u003e域分隔符是 \u003ccode\u003e\\t\u003c/code\u003e，即制表符。\u003c/li\u003e\n\u003cli\u003e输入格式是 \u003ccode\u003eorg.apache.hadoop.mapred.TextInputFormat\u003c/code\u003e，即文本输入。\u003c/li\u003e\n\u003cli\u003e数据位于 \u003ccode\u003ehdfs://namenode/path/to/data/\u003c/code\u003e，这是我们所有数据的完整路径。\u003c/li\u003e\n\u003cli\u003e表名字是 \u003ccode\u003etable_name\u003c/code\u003e，你可以根据实际情况设置恰当的表名字。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"将数据接入-Hive-表\"\u003e\u003ca href=\"#将数据接入-Hive-表\" class=\"headerlink\" title=\"将数据接入 Hive 表\"\u003e\u003c/a\u003e将数据接入 Hive 表\u003c/h2\u003e\u003cp\u003e有了数据并创建好 Hive 表之后，我们就可以将数据接入 Hive 表了。这里，我们需要用到 \u003ccode\u003eALTER TABLE\u003c/code\u003e 语句。例如：\u003c/p\u003e\n\u003cfigure class=\"highlight sql\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e2\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e3\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e4\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e5\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e6\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"keyword\"\u003eALTER\u003c/span\u003e \u003cspan class=\"keyword\"\u003eTABLE\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  table_name\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"keyword\"\u003eADD\u003c/span\u003e IF \u003cspan class=\"keyword\"\u003eNOT\u003c/span\u003e \u003cspan class=\"keyword\"\u003eEXISTS\u003c/span\u003e \u003cspan class=\"keyword\"\u003ePARTITION\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  (dt\u003cspan class=\"operator\"\u003e=\u003c/span\u003e\u003cspan class=\"string\"\u003e\u0026#39;2019-11-01\u0026#39;\u003c/span\u003e, \u003cspan class=\"keyword\"\u003ehour\u003c/span\u003e\u003cspan class=\"operator\"\u003e=\u003c/span\u003e\u003cspan class=\"string\"\u003e\u0026#39;13\u0026#39;\u003c/span\u003e, dstreamid\u003cspan class=\"operator\"\u003e=\u003c/span\u003e\u003cspan class=\"string\"\u003e\u0026#39;123456\u0026#39;\u003c/span\u003e)\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003eLOCATION\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e  \u003cspan class=\"string\"\u003e\u0026#39;hdfs://namenode/path/to/data/2019-11-01/13/123456\u0026#39;\u003c/span\u003e;\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003cp\u003e这个语句表示：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e更改名为 \u003ccode\u003etable_name\u003c/code\u003e 的表；\u003c/li\u003e\n\u003cli\u003e具体的动作是 \u003ccode\u003eADD IF NOT EXISTS PARTITION\u003c/code\u003e，即当表中不存在相应分区时，添加该分区；\u003c/li\u003e\n\u003cli\u003e添加的数据来自的路径是 \u003ccode\u003ehdfs://namenode/path/to/data/2019-11-01/13/123456\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e执行成功之后，即可在不进行数据搬移的前提下，将 HDFS 上目录中的数据作为 Hive 外表的分区了。\u003c/p\u003e\n\n    \u003c/div\u003e",
  "Date": "2019-11-04T23:38:07Z",
  "Author": "Liam Huang"
}