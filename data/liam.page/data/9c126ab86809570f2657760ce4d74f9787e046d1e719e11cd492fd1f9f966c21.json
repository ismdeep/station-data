{
  "Source": "liam.page",
  "Title": "FTRL 不太简短之介绍",
  "Link": "https://liam.page/2019/08/31/a-not-so-simple-introduction-to-FTRL/",
  "Content": "\u003cdiv class=\"post-body\" itemprop=\"articleBody\"\u003e\n\n      \n        \u003cp\u003eFTRL 是 Follow The Regularized Leader 的缩写，它是 Google 在 2010 -- 2013 年三年时间内，从理论研究到实际工程化实现的在线优化算法框架。FTRL 在处理带 \u003ccode\u003e$L_1$\u003c/code\u003e 正则化的\u003ca href=\"/2018/10/10/logistic-regression/\"\u003e逻辑回归\u003c/a\u003e类模型时，效果非常出色：能够得到性能较好的稀疏解。\u003c/p\u003e\n\u003cp\u003e中文网络上，已有一些关于 FTRL 的介绍。比较详细和出名的是新浪微博的冯扬撰写的「在线最优化求解」。但在我看来，已有的关于 FTRL 的介绍，都或多或少有些值得调整和改进的地方。这促成了这篇文章。\u003c/p\u003e\n\u003cp\u003e这篇文章讲 FTRL 的理论部分，大致会按照这样的路径来阐述：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e我们想要解决什么问题？\u003c/li\u003e\n\u003cli\u003eFTRL 的前辈们是怎么尝试解决问题的？\u003c/li\u003e\n\u003cli\u003e前辈们之间是什么关系？又留下了哪些尚未解决的问题？FTRL 是如何解决这些遗留问题的？\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e而后，在下一篇工程部分的文章中，我们会讨论一下 FTRL 的工程实现有哪些值得谈一谈的问题。\u003c/p\u003e\n\u003cspan id=\"more\"\u003e\u003c/span\u003e\n\n\u003ch2 id=\"我们面临的问题\"\u003e\u003ca href=\"#我们面临的问题\" class=\"headerlink\" title=\"我们面临的问题\"\u003e\u003c/a\u003e我们面临的问题\u003c/h2\u003e\u003cp\u003e传统的运用机器学习解决实际问题的步骤如下：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e数据融合，获取数据样本的标签。\u003c/li\u003e\n\u003cli\u003e特征工程及其 ETL，获取每个样本的特征。\u003c/li\u003e\n\u003cli\u003e样本处理，处理正负样本比例、无效或作弊样本等问题，输出用于训练、验证、测试的样本集。\u003c/li\u003e\n\u003cli\u003e构建模型，根据业务特点和数据特点，选取恰当的模型；比如 LR、FM、GBDT、DNN 等。\u003c/li\u003e\n\u003cli\u003e训练模型，在训练集上训练模型，在验证集上调参。\u003c/li\u003e\n\u003cli\u003e模型评估，在测试机上评估模型。\u003c/li\u003e\n\u003cli\u003e在线预测，将有效模型上线，进行在线预测。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e这样的流程能够解决很多问题，但存在至少两方面的瓶颈：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e整套流程在样本维度是「批量」的，在特征高维数据大量的情况下，这导致模型更新周期较长。在工程能力强的团队手上，模型的更新周期最好能做到小时级别；在工程能力差的团队手上，这个周期可能是天级甚至是周级别的。\u003c/li\u003e\n\u003cli\u003e模型的复杂度和线上预测性能之间难以权衡：模型复杂度低，线上预测效果差；模型复杂度高，线上预测效果好，但需要的存储、时间资源也随之升高，无法保证 RT 和 QPS。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e为了解决这里的问题 (1)，在线学习（Online Learning）逐渐兴起；为了解决问题 (2)，人们从各种正则、剪枝开始，尝试用各种手段，在保证模型精度的前提下，尽可能获得稀疏的模型。\u003c/p\u003e\n\u003ch3 id=\"在线学习的兴起\"\u003e\u003ca href=\"#在线学习的兴起\" class=\"headerlink\" title=\"在线学习的兴起\"\u003e\u003c/a\u003e在线学习的兴起\u003c/h3\u003e\u003cp\u003e我曾经在多个场合谈到，机器学习模型的三要素是：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e模型结构；\u003c/li\u003e\n\u003cli\u003e优化目标；\u003c/li\u003e\n\u003cli\u003e求解方法。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e在这里，模型结构通常会需要根据实际问题的特点进行调整。例如，对于具有稠密特征样本的分类问题，GBDT 类的树模型往往效果良好。又例如，对于具有高维稀疏特征的大规模样本，\u003ca href=\"/2018/10/10/logistic-regression/\"\u003e逻辑回归\u003c/a\u003e和\u003ca href=\"https://liam.page/2019/03/25/Factorization-Machine/\"\u003e因子分解机\u003c/a\u003e（及其\u003ca href=\"https://liam.page/2019/06/28/variants-of-FM/\"\u003e变体\u003c/a\u003e）就会是不错的选择。\u003c/p\u003e\n\u003cp\u003e优化目标往往会以目标函数这一数学形式来表达。目标函数中的损失函数，则是用来描述「模型对经验数据拟合程度好坏」的方法。目标函数（或损失函数）的选择，通常也是和实际问题的特点相关的。例如对于回归问题和分类问题，通常就会选择不同的损失函数。\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e对于样本集合 \u003ccode\u003e$\\mathcal{D}$\u003c/code\u003e 中编号为 \u003ccode\u003e$i$\u003c/code\u003e 的样本 \u003ccode\u003e$\\{\\vec x_i, y_i\\}$\u003c/code\u003e 来说，在确定好模型结构 \u003ccode\u003e$h(\\cdot; \\vec\\omega)$\u003c/code\u003e 的基础上，损失函数记为\u003cbr/\u003e\u003ccode\u003e$$\\ell\\bigl(h(\\vec x_i; \\vec\\omega), y_i\\bigr).$$\u003c/code\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e求解方法则是解决如何在有限的时间内，求得一个既简单（模型复杂度低，不易过拟合）性能又好（对经验数据拟合程度较高）的模型。在模型结构确定的基础上，机器学习模型的学习，往往会化归为带参数目标函数的最优化求解问题。如何解决这些最优化问题，或者说，采用何种求解方法，往往要根据问题特点、模型结构、目标函数等等各种因素的不同，综合考虑。\u003c/p\u003e\n\u003ch4 id=\"批量\"\u003e\u003ca href=\"#批量\" class=\"headerlink\" title=\"批量\"\u003e\u003c/a\u003e批量\u003c/h4\u003e\u003cp\u003e在机器学习兴起的早期，由于数据规模较小，计算性能较低与求解复杂度较高的矛盾尚不明显，人们很自然地选择与直觉相符合的批量求解方式来优化模型。具体来说，人们通常会随机给定模型参数 \u003ccode\u003e$\\vec\\omega$\u003c/code\u003e 的初值 \u003ccode\u003e$\\vec\\omega_0$\u003c/code\u003e，通过迭代，不断更新来调整 \u003ccode\u003e$\\vec\\omega$\u003c/code\u003e 的取值，使得目标函数在样本集合 \u003ccode\u003e$\\mathcal{D}$\u003c/code\u003e 上的加和取得或接近最小值：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{aligned}   L(\\vec\\omega\\mid \\mathcal{D}) \u0026amp;{} = \\sum_{\\{\\vec x_i, y_i\\} \\in \\mathcal{D}}\\ell\\bigl(h(\\vec x_i; \\vec\\omega), y_i\\bigr) \\\\   \\vec\\omega^{*} \u0026amp;{} = \\mathop{\\arg\\,\\min}_{\\vec\\omega} L(\\vec\\omega\\mid \\mathcal{D}) \\end{aligned} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e对于这种解法，典型的方式是梯度下降（Gradient Descend）和牛顿法、拟牛顿法等。以梯度下降法为例，其 \u003ccode\u003e$t$\u003c/code\u003e 轮迭代的更新如下所示：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\vec\\omega^{(t + 1)} \\gets \\vec\\omega^{(t)} - \\eta^{(t)}\\cdot\\nabla_{\\vec\\omega^{(t)}}L(\\vec\\omega^{(t)}\\mid \\mathcal{D}). $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e在这种做法当中，每一次迭代，都需要扫描整个样本集合 \u003ccode\u003e$\\mathcal{D}$\u003c/code\u003e 以计算全局损失 \u003ccode\u003e$L$\u003c/code\u003e，而后才能更新参数 \u003ccode\u003e$\\vec\\omega$\u003c/code\u003e。对于数据规模较小的情况，这样做的好处是能够准确计算每一次迭代时的梯度，避免「跑偏」。但对于随着数据规模的增大，每一次计算全局梯度的代价变得过高，完成训练的时间就会变得很长。为了解决这个问题，人们引入了随机（小批量）的解法。\u003c/p\u003e\n\u003ch4 id=\"随机小批量\"\u003e\u003ca href=\"#随机小批量\" class=\"headerlink\" title=\"随机小批量\"\u003e\u003c/a\u003e随机小批量\u003c/h4\u003e\u003cp\u003e我在\u003ca href=\"/2019/06/18/OCD-needs-stochastic-gradient-descent/\"\u003e强迫症患者也需要随机梯度下降\u003c/a\u003e一文中介绍了随机（小批量）梯度下降（Stochastic Gradient Descend）的方法和它的好处。按照本文的记号约定，随机梯度下降第 \u003ccode\u003e$t$\u003c/code\u003e 轮迭代的更新如下所示：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\vec\\omega^{(t + 1)} \\gets \\vec\\omega^{(t)} - \\eta^{(t)}\\cdot\\nabla_{\\vec\\omega^{(t)}}L(\\vec\\omega^{(t)}\\mid \\mathcal{D}^{(t)}). $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e这里描述的是随机小批量梯度下降。其中 \u003ccode\u003e$\\mathcal{D}^{(t)}$\u003c/code\u003e 是当前轮次的迭代从全部样本集 \u003ccode\u003e$\\mathcal{D}$\u003c/code\u003e 中随机选取的子集。当子集 \u003ccode\u003e$\\mathcal{D}^{(t)}$\u003c/code\u003e 当中只有 1 个元素时，算法退化为纯粹的随机梯度下降。\u003c/p\u003e\n\u003cp\u003e在这种做法当中，每一次迭代，无需扫描整个样本集合 \u003ccode\u003e$\\mathcal{D}$\u003c/code\u003e 以计算全局损失 \u003ccode\u003e$L$\u003c/code\u003e。取而代之的是，计算一个随机选取的小集合 \u003ccode\u003e$\\mathcal{D}^{(t)}$\u003c/code\u003e 中的局部损失，即可更新参数 \u003ccode\u003e$\\vec\\omega$\u003c/code\u003e。对于数据规模较大的情况，这样的做法节省了每次迭代的计算量，虽然代价是需要迭代更多轮次，但是总体来说极大地降低了整体的训练时间；与此同时，如\u003ca href=\"/2019/06/18/OCD-needs-stochastic-gradient-descent/\"\u003e强迫症患者也需要随机梯度下降\u003c/a\u003e一文中介绍的那样，随机梯度下降还能带来其它一些好处。\u003c/p\u003e\n\u003ch4 id=\"在线学习\"\u003e\u003ca href=\"#在线学习\" class=\"headerlink\" title=\"在线学习\"\u003e\u003c/a\u003e在线学习\u003c/h4\u003e\u003cp\u003e随机（小批量）的优化方法解决了一部分问题，但做到极限，模型的更新周期也只能缩短到小时级。因此，在线学习逐渐走上了舞台。\u003c/p\u003e\n\u003cp\u003e和前辈们相比，在线学习最大的特点（或者说需求）有两个：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e每次只处理少数几个样本，甚至每次只处理一个样本；\u003c/li\u003e\n\u003cli\u003e处理过的样本对于优化过程来说会被「丢弃」，再也看不到了，因此在线学习需要一种「不吃后悔药」的优化方法。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e回过头来看随机（小批量）梯度下降，我们发现它恰好能满足在线学习的这两方面需求。对于第一个需求来说，这是显然的。对于第二个需求来说，在线接收的样本某种意义上就可以理解为是一种随机，只要这些随机送到优化器的样本的梯度在统计期望上与总体样本是一致的（而这是在线学习的基本假设），那就适用随机（小批量）梯度下降。\u003c/p\u003e\n\u003cp\u003e事情看起来很美妙，只需要把随机（小批量）梯度下降整合进在线学习的工程框架当中就可以了。但是事情没有那么美妙，因为这依然无法解决我们面临的第二个问题——对模型稀疏性的追求。\u003c/p\u003e\n\u003ch3 id=\"对模型稀疏性的追求\"\u003e\u003ca href=\"#对模型稀疏性的追求\" class=\"headerlink\" title=\"对模型稀疏性的追求\"\u003e\u003c/a\u003e对模型稀疏性的追求\u003c/h3\u003e\u003ch4 id=\"模型稀疏的好处\"\u003e\u003ca href=\"#模型稀疏的好处\" class=\"headerlink\" title=\"模型稀疏的好处\"\u003e\u003c/a\u003e模型稀疏的好处\u003c/h4\u003e\u003cp\u003e模型稀疏的好处有几个方面。\u003c/p\u003e\n\u003cp\u003e一是能解决之前提到的「模型复杂度低，线上预测效果差；模型复杂度高，线上预测效果好，但需要的存储、时间资源也随之升高，无法保证 RT 和 QPS」之问题。这是比较显然的。稀疏的模型会大大减少预测时的内存和复杂度。以 LR 为例，若已知输入向量的维度是 \u003ccode\u003e$d$\u003c/code\u003e 而 LR 中不为 0 的参数的数量是 \u003ccode\u003e$w$\u003c/code\u003e，若 \u003ccode\u003e$d \\gg w$\u003c/code\u003e，那么绝大多数特征甚至不需要去采集。这样一来，从特征采集到预测运算整个步骤都能省下很多内存和计算复杂度。\u003c/p\u003e\n\u003cp\u003e二是模型的稀疏与 \u003ccode\u003e$L_1$\u003c/code\u003e 正则化不谋而合（见\u003ca href=\"/2017/03/30/L1-and-L2-regularizer/\"\u003e谈谈 L1 与 L2-正则项\u003c/a\u003e一文），这意味着运用 \u003ccode\u003e$L_1$\u003c/code\u003e 正则化一方面可以使得模型变得稀疏，另一方面还能够降低模型过拟合的风险。\u003c/p\u003e\n\u003cp\u003e三是稀疏性较好的模型，相对来说可解释性更好。这对于我们来说，特别是在实际应用当中，是很有好处的。以那个经典的例子来解释，假设你现在需要训练一个模型，解释人的某些特征和罹患某种疾病之间的关系。如果模型稀疏，那么意味着，罹患某种疾病只与少数一些特征有关。这种模型，对于医生来说，是很友好的。因为当医生拿到一个人的指标数据（特征），他就能根据模型，很容易地告诉来访的就医者说：「你的 XX 指标比较高，而 YY 指标比较低，这是罹患 ZZ 疾病的高危因素。因此你需要在日常生活中注意某些方面，同时定期进行身体检查。」\u003c/p\u003e\n\u003ch4 id=\"在批量梯度下降中，追求模型稀疏性\"\u003e\u003ca href=\"#在批量梯度下降中，追求模型稀疏性\" class=\"headerlink\" title=\"在批量梯度下降中，追求模型稀疏性\"\u003e\u003c/a\u003e在批量梯度下降中，追求模型稀疏性\u003c/h4\u003e\u003cp\u003e我们从最基本的批量梯度下降开始，逐步探寻如何解得一个稀疏的模型。\u003c/p\u003e\n\u003cp\u003e如\u003ca href=\"/2017/03/30/L1-and-L2-regularizer/\"\u003e谈谈 L1 与 L2-正则项\u003c/a\u003e一文所说的那样，我们只需将 \u003ccode\u003e$L_1$\u003c/code\u003e 范数引入模型求解过程中的目标函数，即可获得相对稀疏的模型。注意，由于我们的终极目标是「稀疏」，这意味着要有尽可能多的权重项为 0。这样看起来，使用 \u003ccode\u003e$L_0$\u003c/code\u003e 范数可能更好（向量 \u003ccode\u003e$\\vec x$\u003c/code\u003e 的 \u003ccode\u003e$L_0$\u003c/code\u003e 范数 \u003ccode\u003e$\\lVert \\vec x\\rVert_0$\u003c/code\u003e 是向量 \u003ccode\u003e$\\vec x$\u003c/code\u003e 各维度中不为 0 的维度的数量）。但由于 \u003ccode\u003e$L_0$\u003c/code\u003e 范数是非凸的，在求解优化上比较困难，故而采用 \u003ccode\u003e$L_0$\u003c/code\u003e 范数的最紧凸放松，即 \u003ccode\u003e$L_1$\u003c/code\u003e 范数作为替代。\u003c/p\u003e\n\u003cp\u003e这样一来，模型优化时需要最小化的目标函数变更为如下形式：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\text{Obj}(\\vec\\omega\\mid \\mathcal{D}) = L(\\vec\\omega\\mid \\mathcal{D}) + \\lambda_1\\frac{\\lVert \\vec\\omega\\rVert_1}{n}, \\quad\\lambda_1 \u0026gt; 0. $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e这里，等式右边的第一项表示模型在训练集 \u003ccode\u003e$\\mathcal{D}$\u003c/code\u003e 上经验损失，第二项则表示模型的 \u003ccode\u003e$L_1$\u003c/code\u003e 正则项。其中 \u003ccode\u003e$\\lVert \\vec\\omega\\rVert_1$\u003c/code\u003e 表示向量 \u003ccode\u003e$\\vec\\omega$\u003c/code\u003e 的 \u003ccode\u003e$L_1$\u003c/code\u003e 范数，\u003ccode\u003e$n$\u003c/code\u003e 表示向量 \u003ccode\u003e$\\vec\\omega$\u003c/code\u003e 的维度。\u003c/p\u003e\n\u003cp\u003e那么为什么加入 \u003ccode\u003e$L_1$\u003c/code\u003e 正则项，有助于产出稀疏解呢？\u003c/p\u003e\n\u003cp\u003e我们假设对于某个 \u003ccode\u003e$i \\in \\{1, 2, \\ldots, n\\}$\u003c/code\u003e 来说，\u003ccode\u003e$\\omega_i = 0$\u003c/code\u003e。然后，在接下来的迭代中，\u003ccode\u003e$\\omega_i$\u003c/code\u003e 被更新为 \u003ccode\u003e$\\omega_i \\gets 0 - \\eta\\frac{\\partial \\text{Obj}}{\\partial \\omega_i}$\u003c/code\u003e 而其它参数保持不变。这意味着，对于 \u003ccode\u003e$L_1$\u003c/code\u003e 正则项来说，在这一轮迭代中增加了 \u003ccode\u003e$\\Delta\\Omega = \\eta\\frac{\\lambda_1}{n}\\Bigl\\lvert \\frac{\\partial \\text{Obj}}{\\partial \\omega_i}\\Bigr\\rvert$\u003c/code\u003e；对于损失函数来说，在这一轮迭代中大约下降了 \u003ccode\u003e$\\Delta L = \\eta\\Bigl\\lvert \\frac{\\partial \\text{Obj}}{\\partial \\omega_i}\\Bigr\\rvert\\Bigl\\lvert \\frac{\\partial L}{\\partial \\omega_i}\\Bigr\\rvert$\u003c/code\u003e。而如果 \u003ccode\u003e$\\Delta L \u0026lt; \\Delta\\Omega$\u003c/code\u003e，即 \u003ccode\u003e$\\Bigl\\lvert \\frac{\\partial L}{\\partial \\omega_i}\\Bigr\\rvert \u0026lt; \\frac{\\lambda_1}{n}$\u003c/code\u003e，那么目标函数整体是变大了（而不是变小了）。因此，对于这种情况，优化器会拒绝更新 \u003ccode\u003e$\\omega_i$\u003c/code\u003e，也就是拒绝将 \u003ccode\u003e$\\omega_i$\u003c/code\u003e 更新为非 0 值。由此就得到了相对稀疏的模型。\u003c/p\u003e\n\u003ch4 id=\"L-1-正则在-SGD-中\"\u003e\u003ca href=\"#L-1-正则在-SGD-中\" class=\"headerlink\" title=\"$L_1$ 正则在 SGD 中\"\u003e\u003c/a\u003e\u003ccode\u003e$L_1$\u003c/code\u003e 正则在 SGD 中\u003c/h4\u003e\u003cp\u003e注意，在批量梯度下降中，\u003ccode\u003e$L_1$\u003c/code\u003e 正则项能有效的原因在于下式的成立：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$\\biggl\\lvert \\frac{\\partial L}{\\partial \\omega_i}\\biggr\\rvert \u0026lt; \\frac{\\lambda_1}{n}.$$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e但是，SGD 的假设（随机梯度的期望等于全局梯度）并不能保证在全局梯度满足上式的情况下，随机梯度总能使上式成立。这意味着，在 SGD 的场景中，使用 \u003ccode\u003e$L_1$\u003c/code\u003e 正则化有助于提升模型的稀疏性，但并不能很好地保证有在批量梯度下降中的那种稀疏化效果。\u003c/p\u003e\n\u003cp\u003e那么问题就来了：按之前的说法，在线学习中，我们必然要依赖类似 SGD 的算法；但 \u003ccode\u003e$L_1$\u003c/code\u003e 正则化并不能在 SGD 中确保模型是足够稀疏的。于是，\u003cstrong\u003e我们迫切需要找到一种能够满足在线学习的需要，同时又能保证模型稀疏性的优化方法\u003c/strong\u003e。\u003c/p\u003e\n\u003ch2 id=\"FTRL-的前辈们\"\u003e\u003ca href=\"#FTRL-的前辈们\" class=\"headerlink\" title=\"FTRL 的前辈们\"\u003e\u003c/a\u003eFTRL 的前辈们\u003c/h2\u003e\u003cp\u003e前面提到，加入 \u003ccode\u003e$L_1$\u003c/code\u003e 正则项，是获得稀疏模型的主要手段；但由于 SGD 的原因，\u003ccode\u003e$L_1$\u003c/code\u003e 正则项又很难发挥作用。因此，我们需要新的手段——或者在 \u003ccode\u003e$L_1$\u003c/code\u003e 正则化的基础上改进，或者有全新的手段——来解决模型稀疏化的问题。完全创新总是比较困难的。事实上，目前也没有发现完全独立于 \u003ccode\u003e$L_1$\u003c/code\u003e 范数同时又十分有效的稀疏化方法。因此，人们的目光还是更多地会聚焦在，如何基于 \u003ccode\u003e$L_1$\u003c/code\u003e 正则项进行改进之上。\u003c/p\u003e\n\u003cp\u003e一个粗暴有简单的想法是：基于 \u003ccode\u003e$L_1$\u003c/code\u003e 正则项，对模型参数进行截断。具体是这样做的，以 \u003ccode\u003e$k$\u003c/code\u003e 轮迭代为一组：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e按带 \u003ccode\u003e$L_1$\u003c/code\u003e 正则项的 SGD 的方法训练 \u003ccode\u003e$k - 1$\u003c/code\u003e 轮\u003c/li\u003e\n\u003cli\u003e在第 \u003ccode\u003e$k$\u003c/code\u003e 轮迭代中，先按通常的 SGD 进行更新，得到 \u003ccode\u003e$\\vec\\omega^{(k\u0026#39;)}$\u003c/code\u003e，然后对所有参数进行考察，以超参数 \u003ccode\u003e$\\theta$\u003c/code\u003e 进行截断置零：\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ccode\u003e$$\\omega_i^{(k)} \\gets \\begin{cases} 0 \u0026amp; \\text{if $\\Bigl\\lvert\\omega_{i}^{(k\u0026#39;)}\\Bigr\\rvert \u0026lt; \\theta$,} \\\\ \\omega_{i}^{(k\u0026#39;)} \u0026amp; \\text{otherwise.} \\end{cases}$$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e显然，这种做法太过粗暴，存在很多问题；但它是所有类似方法的祖师爷，反映的是「不等式约束下的凸优化」的思路。在这种思路下，求到的梯度 \u003ccode\u003e$g^{(t)} = \\frac{\\partial \\text{Obj}}{\\partial \\omega_i}$\u003c/code\u003e 被视作是次梯度（subgradient）。根据次梯度更新的结果，可能落在不等式约束的范围之外。此时，就要取该梯度在不等式约束范围内的投影作为真正的迭代结果。\u003c/p\u003e\n\u003cp\u003e简单截断法采取的投影方式，是直接截断。接下来，我们看看 FTRL 的其他前辈们是怎么做的。\u003c/p\u003e\n\u003ch3 id=\"Truncated-Gradient\"\u003e\u003ca href=\"#Truncated-Gradient\" class=\"headerlink\" title=\"Truncated Gradient\"\u003e\u003c/a\u003eTruncated Gradient\u003c/h3\u003e\u003cp\u003e既然简单地截断过于粗暴，那么我们就让截断温和一点。这就是 09 年提出的截断梯度法。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e按带 \u003ccode\u003e$L_1$\u003c/code\u003e 正则项的 SGD 的方法训练 \u003ccode\u003e$k - 1$\u003c/code\u003e 轮\u003c/li\u003e\n\u003cli\u003e在第 \u003ccode\u003e$k$\u003c/code\u003e 轮迭代中，先按通常的 SGD 进行更新，得到 \u003ccode\u003e$\\vec\\omega^{(k\u0026#39;)}$\u003c/code\u003e，然后对所有参数进行考察，以超参数 \u003ccode\u003e$\\theta$\u003c/code\u003e 和 \u003ccode\u003e$\\alpha$\u003c/code\u003e 进行截断：\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ccode\u003e$$\\omega_i^{(k)} \\gets \\begin{cases} 0\u0026amp; \\text{if $\\Bigl\\lvert\\omega_{i}^{(k\u0026#39;)}\\Bigr\\rvert \\leqslant \\alpha$,} \\\\ \\omega_{i}^{(k\u0026#39;)} - \\alpha\\,\\text{sgn}\\Bigl(\\omega_{i}^{(k\u0026#39;)}\\Bigr) \u0026amp; \\text{if $\\alpha \u0026lt; \\Bigl\\lvert\\omega_{i}^{(k\u0026#39;)}\\Bigr\\rvert \\leqslant \\theta$,} \\\\ \\omega_{i}^{(k\u0026#39;)} \u0026amp; \\text{otherwise.} \\end{cases}$$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e这里 \u003ccode\u003e$\\alpha$\u003c/code\u003e 通常取学习率 \u003ccode\u003e$\\eta^{(k)}$\u003c/code\u003e 的倍数，例如 \u003ccode\u003e$\\alpha^{(k)} = \\eta^{(k)}\\lambda$\u003c/code\u003e。截断梯度法采用的投影方式，是以分段函数的方式，对参数进行截断。\u003c/p\u003e\n\u003cp\u003e显然，\u003ccode\u003e$\\alpha$\u003c/code\u003e 或 \u003ccode\u003e$\\theta$\u003c/code\u003e 越大，模型越容易求得稀疏解。当 \u003ccode\u003e$\\alpha = \\theta$\u003c/code\u003e，TG 退化为简单截断法；当 \u003ccode\u003e$\\theta = \\infty$\u003c/code\u003e 且 \u003ccode\u003e$k = 1$\u003c/code\u003e，在截断区域之外，TG 继续退化为 SGD-\u003ccode\u003e$L_1$\u003c/code\u003e，此时 \u003ccode\u003e$\\omega_i$\u003c/code\u003e 的更新是：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$\\omega_{i}^{(t + 1)} \\gets \\omega_{i}^{(t)} - \\eta^{(t)}g_{i}^{(t)} - \\eta^{(t)}\\lambda\\,\\text{sgn}\\Bigl(\\omega_{i}^{(t)}\\Bigr).$$\u003c/code\u003e\u003c/p\u003e\n\u003ch3 id=\"FOBOS-Forward-Backward-Splitting\"\u003e\u003ca href=\"#FOBOS-Forward-Backward-Splitting\" class=\"headerlink\" title=\"FOBOS (Forward-Backward Splitting)\"\u003e\u003c/a\u003eFOBOS (Forward-Backward Splitting)\u003c/h3\u003e\u003cp\u003eFOBOS 最开始的名字叫做 Forward Looking Subgradients，简写叫做 FOLOS；后来改名叫做 Forward-Backward Splitting，按说应该简写成 FOBAS。但作者为了减少可能的困扰，就只修改了一个字母，变成了 FOBOS。\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e吐槽：但实际上，变得更加困惑了好不好……\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eFOBOS 可以看做是 TG 的改进。\u003c/p\u003e\n\u003cp\u003e首先，FOBOS 将 \u003ccode\u003e$k$\u003c/code\u003e 设置为 1。如此一来，每一轮迭代都一样了：先根据次梯度做梯度下降，再做一步投影操作。\u003c/p\u003e\n\u003cp\u003e其次，FOBOS 将投影操作改进如下：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$\\vec\\omega^{(t + 1)} \\gets \\mathop{\\arg\\,\\min}_{\\vec\\omega}\\biggl\\{\\frac{1}{2}\\Bigl\\lVert\\vec\\omega - \\vec\\omega^{t\u0026#39;}\\Bigr\\rVert^{2} + \\eta^{(t\u0026#39;)}\\Omega(\\vec\\omega)\\biggr\\}.$$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e这里，优化符号中的第一项保证了投影之后的结果距离梯度下降的结果不太远，第二项是正则项，用于产生稀疏性。我们将它转换为无约束优化的形式：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{aligned} \\vec\\omega^{(t + 1)} \\gets{}\u0026amp; \\mathop{\\arg\\,\\min}_{\\vec\\omega}\\biggl\\{\\frac{1}{2}\\Bigl\\lVert\\vec\\omega - \\vec\\omega^{(t)} + \\eta^{(t)}\\nabla\\text{Obj}(\\vec\\omega^{(t)})\\Bigr\\rVert^{2} + \\eta^{(t\u0026#39;)}\\Omega(\\vec\\omega)\\biggr\\},  \\\\ ={}\u0026amp; \\vec\\omega^{(t)} - \\eta^{(t)}\\nabla\\text{Obj}(\\vec\\omega^{(t)}) - \\eta^{(t\u0026#39;)}\\nabla\\Omega(\\vec\\omega^{(t + 1)}). \\end{aligned} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e可见，更新结果不仅与上一轮迭代的结果有关（梯度下降），还与迭代之后的状态有关（正则约束），这就是所谓的 Forward-Backword Splitting。\u003c/p\u003e\n\u003cp\u003e当 \u003ccode\u003e$\\Omega(\\cdot) = \\eta^{(t\u0026#39;)}\\lambda\\lVert\\cdot\\rVert_1 = \\tilde\\lambda\\lVert\\cdot\\rVert_1$\u003c/code\u003e 时，我们将向量形式再化简到具体某一维度的更新：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{aligned} \\omega_{i}^{(t + 1)} \\gets{}\u0026amp; \\text{sgn}\\bigl(\\omega_{i}^{(t)} - \\eta^{(t)}g_{i}^{(t)}\\bigr)\\cdot\\max\\Bigl\\{0, \\bigl\\lvert \\omega_{i}^{(t)} - \\eta^{(t)}g_{i}^{(t)} \\bigr\\rvert - \\tilde\\lambda\\Bigr\\}, \\\\ ={}\u0026amp; \\begin{cases} 0 \u0026amp; \\text{if $\\lvert\\omega_{i}^{(t\u0026#39;)}\\rvert \u0026lt; \\tilde\\lambda$,} \\\\ \\omega_{i}^{(t)} - \\eta^{(t)}g_{i}^{(t)} - \\tilde\\lambda\\,\\text{sgn}\\Bigl(\\omega_{i}^{(t)} - \\eta^{(t)}g_{i}^{(t)}\\Bigr) \u0026amp; \\text{otherwise.} \\end{cases} \\end{aligned} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e不难发现，它与 TG 的形式非常接近。当 TG 中的 \u003ccode\u003e$\\theta = \\infty$\u003c/code\u003e, \u003ccode\u003e$\\alpha = \\tilde\\lambda$\u003c/code\u003e, \u003ccode\u003e$k = 1$\u003c/code\u003e 时，TG 与 FOBOS 的唯一差别就在于惩罚项上。TG 是惩罚在迭代前的项上，FOBOS 是惩罚在经过次梯度迭代后的项上。\u003c/p\u003e\n\u003ch3 id=\"RDA-Regularized-Dual-Averaging\"\u003e\u003ca href=\"#RDA-Regularized-Dual-Averaging\" class=\"headerlink\" title=\"RDA (Regularized Dual Averaging)\"\u003e\u003c/a\u003eRDA (Regularized Dual Averaging)\u003c/h3\u003e\u003cp\u003eRDA 是微软 10 年发表的研究成果，其权重更新策略如下：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$\\vec\\omega^{(t + 1)} \\gets \\mathop{\\arg\\,\\min}_{\\vec\\omega}\\biggl\\{\\frac{1}{t}\\sum_{r = 1}^{t}\\Bigl\\langle \\nabla\\text{Obj}\\bigl(\\vec\\omega^{(r)}\\bigr), \\vec\\omega\\Bigr\\rangle + \\Omega(\\vec\\omega) + \\frac{\\beta^{(t)}}{t}h(\\vec\\omega)\\biggr\\}.$$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e这里，\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003e$\\Bigl\\langle \\nabla\\text{Obj}\\bigl(\\vec\\omega^{(r)}\\bigr), \\vec\\omega\\Bigr\\rangle$\u003c/code\u003e 表示梯度 \u003ccode\u003e$\\nabla\\text{Obj}\\bigl(\\vec\\omega^{(r)}\\bigr)$\u003c/code\u003e 对参数 \u003ccode\u003e$\\vec\\omega$\u003c/code\u003e 的积分中值，即：第 \u003ccode\u003e$r$\u003c/code\u003e 轮迭代中的梯度对参数 \u003ccode\u003e$\\vec\\omega$\u003c/code\u003e 产生的变动在所有样本上产生的平均影响。\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e$\\frac{1}{t}\\sum_{r = 1}^{t}\\Bigl\\langle \\nabla\\text{Obj}\\bigl(\\vec\\omega^{(r)}\\bigr), \\vec\\omega\\Bigr\\rangle$\u003c/code\u003e 则是前 \u003ccode\u003e$r$\u003c/code\u003e 轮迭代上述平均影响的平均值（Dual Average）。\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e$\\Omega(\\vec\\omega)$\u003c/code\u003e 是正则项。\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e$\\frac{\\beta^{(t)}}{t}h(\\vec\\omega)$\u003c/code\u003e 是额外的正则项。\u003cul\u003e\n\u003cli\u003e\u003ccode\u003e$\\bigl\\{\\beta^{(t)}\\mid t \\geqslant 1\\bigr\\}$\u003c/code\u003e 是一个非负的非降序列。\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e$h(\\vec\\omega)$\u003c/code\u003e 是一个严格的凸函数。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e除开正则项的变化，和 FOBOS 及之前的截断方法比较，RDA 最大的差别在于丢弃了梯度下降的那一项，换成了梯度的二次平均值。接下来，我们取\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ccode\u003e$\\Omega(\\vec\\omega) = \\lambda\\lVert\\vec\\omega\\rVert_1$\u003c/code\u003e，其中 \u003ccode\u003e$\\lambda \u0026gt; 0$\u003c/code\u003e；\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e$h(\\vec\\omega) = \\frac{1}{2}\\lVert\\vec\\omega\\rVert_2^2$\u003c/code\u003e；\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003e$\\beta^{(t)} = \\gamma\\sqrt{t}$\u003c/code\u003e，其中 \u003ccode\u003e$\\gamma \u0026gt; 0$\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e记 \u003ccode\u003e$g_i^{(1:t)} = \\frac{1}{t}\\sum_{r = 1}^{t} g_i^{(r)}$\u003c/code\u003e，于是得到第 \u003ccode\u003e$i$\u003c/code\u003e 维权重的更新：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\omega_{i}^{(t + 1)} \\gets \\begin{cases} 0\u0026amp; \\text{if $\\bigl\\lvert g_i^{(1:t)}\\bigr\\rvert \u0026lt; \\lambda$,} \\\\ -\\frac{\\sqrt{t}}{\\gamma}\\Bigl(g_i^{(1:t)} - \\lambda\\,\\text{sgn}\\bigl(g_i^{(1:t)}\\bigr)\\Bigr) \u0026amp; \\text{otherwise.} \\end{cases} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e可见，当某一维度参数的二次平均梯度小于阈值 \u003ccode\u003e$\\lambda$\u003c/code\u003e 时，这一维度被截断，产生稀疏性。\u003c/p\u003e\n\u003ch2 id=\"FTRL-Follow-The-Regularized-Leader\"\u003e\u003ca href=\"#FTRL-Follow-The-Regularized-Leader\" class=\"headerlink\" title=\"FTRL (Follow The Regularized Leader)\"\u003e\u003c/a\u003eFTRL (Follow The Regularized Leader)\u003c/h2\u003e\u003cp\u003e接下来介绍 FTRL。\u003c/p\u003e\n\u003ch3 id=\"FOBOS-和-RDA-的区别\"\u003e\u003ca href=\"#FOBOS-和-RDA-的区别\" class=\"headerlink\" title=\"FOBOS 和 RDA 的区别\"\u003e\u003c/a\u003eFOBOS 和 RDA 的区别\u003c/h3\u003e\u003cp\u003e为便于比较，这里把 FOBOS 和 RDA 在单一维度上的更新策略再次抄录如下。\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e\\begin{equation} \\omega_{i}^{(t + 1)} \\gets \\begin{cases} 0 \u0026amp; \\text{if $\\lvert\\omega_{i}^{(t\u0026#39;)}\\rvert \u0026lt; \\tilde\\lambda$,} \\\\ \\omega_{i}^{(t)} - \\eta^{(t)}g_{i}^{(t)} - \\tilde\\lambda\\,\\text{sgn}\\Bigl(\\omega_{i}^{(t)} - \\eta^{(t)}g_{i}^{(t)}\\Bigr) \u0026amp; \\text{otherwise.} \\end{cases} \\tag{FOBOS}\\label{eq:FOBOS} \\end{equation}\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e\\begin{equation} \\omega_{i}^{(t + 1)} \\gets \\begin{cases} 0\u0026amp; \\text{if $\\bigl\\lvert g_i^{(1:t)}\\bigr\\rvert \u0026lt; \\lambda$,} \\\\ -\\frac{\\sqrt{t}}{\\gamma}\\Bigl(g_i^{(1:t)} - \\lambda\\,\\text{sgn}\\bigl(g_i^{(1:t)}\\bigr)\\Bigr) \u0026amp; \\text{otherwise.} \\end{cases} \\tag{RDA}\\label{eq:RDA} \\end{equation}\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e首先我们看 FOBOS 和 RDA 的截断部分的差异。\u003c/p\u003e\n\u003cp\u003eFOBOS 的截断判断取的是单次梯度下降的结果，而 RDA 的截断判断取的是往期所有梯度的二次平均。考虑到我们面临的是「在线学习」，样本在局部抖动的几率比较大。因此 FOBOS 的做法容易因为某些异常、离群样本的出现而错误地截断；RDA 的做法则稳妥许多，参考了过去所有样本的梯度结果。\u003c/p\u003e\n\u003cp\u003eFOBOS 的截断阈值是 \u003ccode\u003e$\\tilde\\lambda = \\eta^{(t\u0026#39;)}\\lambda$\u003c/code\u003e。考虑到学习率 \u003ccode\u003e$\\eta^{(t\u0026#39;)}$\u003c/code\u003e 往往会随着 \u003ccode\u003e$t$\u003c/code\u003e 的增加而减小。故而 FOBOS 的截断阈值是不断减小的；与之相对，RDA 的截断阈值是固定的 \u003ccode\u003e$\\lambda$\u003c/code\u003e。这说明，随着训练的进程，FOBOS 对截断的要求越放越松，因而 RDA 相对更容易得到稀疏解。\u003c/p\u003e\n\u003cp\u003e接下来我们看 FOBOS 和 RDA 截断之外部分的差异。\u003c/p\u003e\n\u003cp\u003eFOBOS 的取值主体是 \u003ccode\u003e$\\omega_{i}^{(t)} - \\eta^{(t)}g_{i}^{(t)}$\u003c/code\u003e，即梯度下降的结果，在此基础上做微调——向 0 的方向微调 \u003ccode\u003e$\\tilde\\lambda$\u003c/code\u003e 步长。按「下山」的比喻，FOBOS 的取值，是在梯度反方向上下山，每次做一定的微调。RDA 的取值，主体是往期所有梯度的二次平均的缩放（\u003ccode\u003e$-\\frac{\\sqrt{t}}{\\gamma}$\u003c/code\u003e），在此基础上做微调——向 0 的方向微调 \u003ccode\u003e$\\lambda$\u003c/code\u003e。按同样的比喻，RDA 的取值，是在山顶上试探很多步，平均之后只走出一小步。从感性的认知来说，FOBOS 的准确度显然会更高一些。\u003c/p\u003e\n\u003cp\u003e这也就是说，FOBOS 的精度较高，但解的稀疏性相对较差；RDA 的解的稀疏性好，但精度较差。于是，很自然地，我们会问：\u003cstrong\u003e是否有办法，将二者的优点合在一起呢\u003c/strong\u003e？\u003c/p\u003e\n\u003ch3 id=\"统一-FOBOS-和-RDA-的形式\"\u003e\u003ca href=\"#统一-FOBOS-和-RDA-的形式\" class=\"headerlink\" title=\"统一 FOBOS 和 RDA 的形式\"\u003e\u003c/a\u003e统一 FOBOS 和 RDA 的形式\u003c/h3\u003e\u003cp\u003e想要取长补短，就要想办法将 FOBOS 和 RDA 的形式统一起来。这样才方便拆墙补墙。\u003c/p\u003e\n\u003cp\u003e首先看 FOBOS 的无约束优化形式：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\vec\\omega^{(t + 1)} \\gets \\mathop{\\arg\\,\\min}_{\\vec\\omega}\\biggl\\{\\frac{1}{2}\\Bigl\\lVert\\vec\\omega - \\vec\\omega^{(t)} + \\eta^{(t)}\\vec g^{(t)}\\Bigr\\rVert^{2} + \\eta^{(t)}\\lambda\\lVert\\vec\\omega\\rVert_1\\biggr\\}. $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e注意，这里 \u003ccode\u003e$\\vec g^{(t)} = \\nabla\\text{Obj}(\\vec\\omega^{(t)})$\u003c/code\u003e，并且令 \u003ccode\u003e$\\eta^{(t\u0026#39;)} = \\eta^{(t)} = \\frac{\\gamma}{\\sqrt{t}}$\u003c/code\u003e。我们将之按维度拆开：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{aligned} \u0026amp; \\min_{\\omega_i\\in\\mathbb{R}}\\biggl\\{\\frac{1}{2}\\Bigl\\lVert\\omega_i - \\omega_i^{(t)} + \\eta^{(t)}g_i^{(t)}\\Bigr\\rVert^{2} + \\eta^{(t)}\\lambda\\lvert\\omega_i\\rvert\\biggr\\} \\\\ ={}\u0026amp; \\min_{\\omega_i\\in\\mathbb{R}}\\biggl\\{\\omega_ig_i^{(t)} + \\lambda\\lvert\\omega_i\\rvert + \\frac{1}{2\\eta^{(t)}}\\bigl(\\omega_i - \\omega_i^{(t)}\\bigr)_2^2 + \\biggl[ \\frac{\\eta^{(t)}}{2}\\bigl(g_i^{(t)}\\bigr)_2^2 + \\omega_i^{(t)}g_i^{(t)} \\biggr]\\biggr\\} \\\\ ={}\u0026amp; \\min_{\\omega_i\\in\\mathbb{R}}\\biggl\\{\\omega_ig_i^{(t)} + \\lambda\\lvert\\omega_i\\rvert + \\frac{1}{2\\eta^{(t)}}\\bigl(\\omega_i - \\omega_i^{(t)}\\bigr)_2^2\\biggr\\}. \\end{aligned} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e再合并起来有，\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{aligned} \\vec\\omega^{(t + 1)} \\gets\u0026amp;{} \\mathop{\\arg\\,\\min}_{\\vec\\omega}\\biggl\\{\\vec g^{(t)}\\cdot\\vec\\omega + \\lambda\\lVert\\vec\\omega\\rVert_1 + \\frac{1}{2\\eta^{(t)}}\\bigl\\lVert\\vec\\omega - \\vec\\omega^{(t)}\\bigr\\rVert_2^2\\biggr\\} \\\\ =\u0026amp;{} \\mathop{\\arg\\,\\min}_{\\vec\\omega}\\biggl\\{\\vec g^{(t)}\\cdot\\vec\\omega + \\lambda\\lVert\\vec\\omega\\rVert_1 + \\frac{1}{2}\\sigma^{(1:t)}\\bigl\\lVert\\vec\\omega - \\vec\\omega^{(t)}\\bigr\\rVert_2^2\\biggr\\}. \\end{aligned} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中 \u003ccode\u003e$\\sigma^{(t)} = \\frac{1}{\\eta^{(t)}} - \\frac{1}{\\eta^{(t - 1)}}$\u003c/code\u003e，以及 \u003ccode\u003e$\\sigma^{(1:t)} = \\sum_{r = 1}^{t}\\sigma^{(r)} = \\frac{1}{\\eta^{(t)}}$\u003c/code\u003e（注意与 \u003ccode\u003e$\\vec g^{(1:t)}$\u003c/code\u003e 不同，\u003ccode\u003e$\\sigma^{(1:t)}$\u003c/code\u003e 在求和符号外没有 \u003ccode\u003e$\\frac{1}{t}$\u003c/code\u003e）。类似地，对于 RDA 有：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\vec\\omega^{(t + 1)} \\gets \\mathop{\\arg\\,\\min}_{\\vec\\omega}\\biggl\\{\\vec G^{(1:t)}\\cdot\\vec\\omega + t\\lambda\\lVert\\vec\\omega\\rVert_1 + \\frac{1}{2}\\sigma^{(1:t)}\\bigl\\lVert\\vec\\omega - \\vec 0\\bigr\\rVert_2^2\\biggr\\}. $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e这里 \u003ccode\u003e$\\vec G^{(t)} = \\vec g^{(t)}$\u003c/code\u003e，而 \u003ccode\u003e$\\vec G^{(1:t)} = \\sum_{r = 1}^{t}\\vec G^{(t)} = t\\cdot\\vec g^{(1:t)}$\u003c/code\u003e。\u003c/p\u003e\n\u003ch3 id=\"拆墙补墙得到-FTRL\"\u003e\u003ca href=\"#拆墙补墙得到-FTRL\" class=\"headerlink\" title=\"拆墙补墙得到 FTRL\"\u003e\u003c/a\u003e拆墙补墙得到 FTRL\u003c/h3\u003e\u003cp\u003e统一了 FOBOS 和 RDA 的形式之后，我们就可以将它们各自的优点拿出来了。\u003c/p\u003e\n\u003cp\u003e对于 FOBOS，它的优点体现在 \u003ccode\u003e$ \\frac{1}{2}\\sigma^{(1:t)}\\bigl\\lVert\\vec\\omega - \\vec\\omega^{(t)}\\bigr\\rVert_2^2$\u003c/code\u003e 这一项上；对于 RDA，它的优点体现在 \u003ccode\u003e$\\vec G^{(1:t)}\\cdot\\vec\\omega$\u003c/code\u003e 这一项上。于是，我们将这两项组合起来，得到的就是标准的 FTRL 了（11 年的论文中的原始版本）：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\vec\\omega^{(t + 1)} \\gets \\mathop{\\arg\\,\\min}_{\\vec\\omega}\\biggl\\{\\vec G^{(1:t)}\\cdot\\vec\\omega + \\lambda\\lVert\\vec\\omega\\rVert_1 + \\frac{1}{2}\\sum_{r = 1}^{t}\\sigma^{(r)}\\bigl\\lVert\\vec\\omega - \\vec\\omega^{(r)}\\bigr\\rVert_2^2\\biggr\\}. $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e注意这里式中第 3 项与 FOBOS 的第三项稍有区别。我们还可以为它加上 \u003ccode\u003e$L_2$\u003c/code\u003e 正则项，变成：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e\\begin{equation} \\vec\\omega^{(t + 1)} \\gets \\mathop{\\arg\\,\\min}_{\\vec\\omega}\\biggl\\{\\vec G^{(1:t)}\\cdot\\vec\\omega + \\lambda_1\\lVert\\vec\\omega\\rVert_1 + \\frac{1}{2}\\lambda_2\\lVert\\vec\\omega\\rVert_2^2 + \\frac{1}{2}\\sum_{r = 1}^{t}\\sigma^{(r)}\\bigl\\lVert\\vec\\omega - \\vec\\omega^{(r)}\\bigr\\rVert_2^2\\biggr\\}. \\tag{FTRL}\\label{eq:FTRL} \\end{equation}\u003c/code\u003e\u003c/p\u003e\n\u003ch3 id=\"FTRL-更新公式的推导\"\u003e\u003ca href=\"#FTRL-更新公式的推导\" class=\"headerlink\" title=\"FTRL 更新公式的推导\"\u003e\u003c/a\u003eFTRL 更新公式的推导\u003c/h3\u003e\u003cp\u003e我们将 \\ref{eq:FTRL} 展开，得到\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\begin{aligned} \\vec\\omega^{(t + 1)} \\gets{}\u0026amp; \\mathop{\\arg\\,\\min}_{\\vec\\omega}\\biggl\\{\\vec G^{(1:t)}\\cdot\\vec\\omega + \\lambda_1\\lVert\\vec\\omega\\rVert_1 + \\frac{1}{2}\\lambda_2\\lVert\\vec\\omega\\rVert_2^2 + \\frac{1}{2}\\sum_{r = 1}^{t}\\sigma^{(r)}\\bigl\\lVert\\vec\\omega - \\vec\\omega^{(r)}\\bigr\\rVert_2^2\\biggr\\} \\\\ ={}\u0026amp; \\mathop{\\arg\\,\\min}_{\\vec\\omega}\\biggl\\{\\vec z^{(1:t)}\\vec\\omega + \\lambda_1\\lVert\\vec\\omega\\rVert_1 + \\frac{1}{2}\\Bigl(\\lambda_2 + \\sum_{r = 1}^{t}\\sigma^{(r)}\\Bigr)\\lVert\\vec\\omega\\rVert_2^2 + \\frac{1}{2}\\sum_{r = 1}^{t}\\sigma^{(r)}\\lVert\\vec\\omega^{(r)}\\rVert_2^2\\biggr\\} \\\\ ={}\u0026amp; \\mathop{\\arg\\,\\min}_{\\vec\\omega}\\biggl\\{\\vec z^{(1:t)}\\vec\\omega + \\lambda_1\\lVert\\vec\\omega\\rVert_1 + \\frac{1}{2}\\Bigl(\\lambda_2 + \\sum_{r = 1}^{t}\\sigma^{(r)}\\Bigr)\\lVert\\vec\\omega\\rVert_2^2\\biggr\\}. \\end{aligned} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e其中 \u003ccode\u003e$\\vec z^{(t)} = \\vec g^{(t)} - \\sigma^{(t)}\\cdot\\vec\\omega^{(t)}$\u003c/code\u003e，而 \u003ccode\u003e$\\vec z^{(1:t)} = \\sum_{r = 1}^{t}\\vec z^{(r)}$\u003c/code\u003e。我们将之按维度拆开，有\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e\\begin{equation} \\min_{\\omega_{i}\\in\\mathbb{R}}\\biggl\\{z_i^{(t)}\\omega_{i} + \\lambda_1\\lvert\\omega_i\\rvert + \\frac{1}{2}\\Bigl(\\lambda_2 + \\sigma^{(1:t)}\\Bigr)\\omega_{i}^{2}\\biggr\\}. \\label{eq:ftrl-one-dim} \\end{equation}\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e式 \\ref{eq:ftrl-one-dim} 是一个无约束的非平滑参数优化问题，其中第二项 \u003ccode\u003e$\\lambda_1\\lvert\\omega_i\\rvert$\u003c/code\u003e 在 \u003ccode\u003e$\\omega_i = 0$\u003c/code\u003e 处不可导。假设 \u003ccode\u003e$\\omega_i^*$\u003c/code\u003e 是使式 \\ref{eq:ftrl-one-dim} 得到最优解的 \u003ccode\u003e$\\omega_i$\u003c/code\u003e 的取值；定义 \u003ccode\u003e$\\xi\\in\\partial\\lvert\\omega_i^*\\rvert$\u003c/code\u003e 是 \u003ccode\u003e$\\lvert\\omega_i\\rvert$\u003c/code\u003e 在 \u003ccode\u003e$\\omega_i^*$\u003c/code\u003e 处的次导数，于是有\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e\\begin{equation} \\partial\\lvert\\omega_i^*\\rvert = \\begin{cases} 1  \u0026amp; \\text{if $\\omega_i^* \u0026gt; 0$}, \\\\ {-1 \u0026lt; \\xi \u0026lt; 1} \u0026amp; \\text{if $\\omega_i^* = 0$}, \\\\ -1 \u0026amp; \\text{if $\\omega_i^* \u0026lt; 0$}. \\end{cases} \\label{eq:ftrl-subgradient} \\end{equation}\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e根据式 \\ref{eq:ftrl-subgradient} 定义的次导数，对式 \\ref{eq:ftrl-one-dim} 待优化的部分求导，令其为零，得到方程：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e\\begin{equation} z_i^{(t)} + \\lambda_1\\cdot\\xi + \\bigl(\\lambda_2 + \\sigma^{(1:t)}\\bigr)\\omega_{i}^* = 0. \\label{eq:ftrl-equation} \\end{equation}\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e式 \\ref{eq:ftrl-equation} 中，\u003ccode\u003e$\\lambda_1 \u0026gt; 0$\u003c/code\u003e 且 \u003ccode\u003e$\\bigl(\\lambda_2 + \\sigma^{(1:t)}\\bigr) \u0026gt; 0$\u003c/code\u003e。对 \u003ccode\u003e$z_i^{(t)}$\u003c/code\u003e 的取值进行分类讨论：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e当 \u003ccode\u003e$\\bigl\\lvert z_i^{(t)}\\bigr\\rvert \u0026lt; \\lambda_1$\u003c/code\u003e 时，有 \u003ccode\u003e$\\omega_i^{*} = 0$\u003c/code\u003e。因为若不然：\u003cul\u003e\n\u003cli\u003e当 \u003ccode\u003e$\\omega_i^{*} \u0026lt; 0$\u003c/code\u003e，有 \u003ccode\u003e$\\xi = -1$\u003c/code\u003e。式 \\ref{eq:ftrl-equation} 左边有 \u003ccode\u003e$z_i^{(t)} - \\lambda_1 + \\bigl(\\lambda_2 + \\sigma^{(1:t)}\\bigr)\\omega_{i}^* \u0026lt; z_i^{(t)} - \\lambda_1 \u0026lt; 0$\u003c/code\u003e，与式 \\ref{eq:ftrl-equation} 矛盾。\u003c/li\u003e\n\u003cli\u003e当 \u003ccode\u003e$\\omega_i^{*} \u0026gt; 0$\u003c/code\u003e，有 \u003ccode\u003e$\\xi = 1$\u003c/code\u003e。式 \\ref{eq:ftrl-equation} 左边有 \u003ccode\u003e$z_i^{(t)} + \\lambda_1 + \\bigl(\\lambda_2 + \\sigma^{(1:t)}\\bigr)\\omega_{i}^* \u0026gt; z_i^{(t)} + \\lambda_1 \u0026gt; 0$\u003c/code\u003e，与式 \\ref{eq:ftrl-equation} 矛盾。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e当 \u003ccode\u003e$z_i^{(t)} \u0026gt; \\lambda_1$\u003c/code\u003e 时，有 \u003ccode\u003e$\\omega_i^{*} = -\\frac{1}{\\lambda_2 + \\sigma^{(1:t)}}\\bigl(z_i^{(t)} - \\lambda_1\\bigr) \u0026lt; 0$\u003c/code\u003e。因为若不然：\u003cul\u003e\n\u003cli\u003e当 \u003ccode\u003e$\\omega_i^{*} = 0$\u003c/code\u003e，由式 \\ref{eq:ftrl-equation} 知 \u003ccode\u003e$\\xi = -\\frac{z_i^{(t)}}{\\lambda_1} \u0026lt; -1$\u003c/code\u003e，与式 \\ref{eq:ftrl-subgradient} 矛盾。\u003c/li\u003e\n\u003cli\u003e当 \u003ccode\u003e$\\omega_i^{*} \u0026gt; 0$\u003c/code\u003e，与 \u003ccode\u003e$\\bigl\\lvert z_i^{(t)}\\bigr\\rvert \u0026lt; \\lambda_1$\u003c/code\u003e 的情况类似，与式 \\ref{eq:ftrl-equation} 矛盾。\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e当 \u003ccode\u003e$z_i^{(t)} \u0026lt; -\\lambda_1$\u003c/code\u003e，类似分析，有 \u003ccode\u003e$\\omega_i^{*} = -\\frac{1}{\\lambda_2 + \\sigma^{(1:t)}}\\bigl(z_i^{(t)} + \\lambda_1\\bigr) \u0026gt; 0$\u003c/code\u003e。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e如此一来，我们得到 FTRL 的更新公式：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e\\begin{equation} \\omega_i^{(t + 1)} = \\begin{cases} 0 \u0026amp; \\text{if $\\lvert z_i^{(t)}\\rvert \u0026lt; \\lambda_1$}, \\\\ -\\frac{1}{\\lambda_2 + \\sigma^{(1:t)}}\\bigl(z_i^{(t)} - \\text{sgn}(z_i^{(t)})\\lambda_1\\bigr) \u0026amp; \\text{otherwise}. \\end{cases} \\label{eq:ftrl-updates} \\end{equation}\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e从式 \\ref{eq:ftrl-updates} 来看，加入 \u003ccode\u003e$L_2$\u003c/code\u003e 正则，没有影响模型的稀疏性，而只是使得参数的取值趋向零。\u003c/p\u003e\n\u003ch3 id=\"FTRL-为什么是有效的\"\u003e\u003ca href=\"#FTRL-为什么是有效的\" class=\"headerlink\" title=\"FTRL 为什么是有效的\"\u003e\u003c/a\u003eFTRL 为什么是有效的\u003c/h3\u003e\u003cp\u003e我们引出 FTRL 是按「稀疏性」的路径，从 FOBOS 和 RDA 拆借出来的。从上面的推导，我们能看出 FTRL 能够较好地获得稀疏解。但是，我们仍未能说明，FTRL 能够获得较好的稀疏解。（大家来找茬，笑）这一小节里，我们来说明 FTRL 是有效的。\u003c/p\u003e\n\u003cp\u003e首先回顾一下 SGD 的更新公式：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e\\begin{equation} \\vec\\omega_i^{(t + 1)} \\gets \\vec\\omega_i^{(t)} - \\eta^{(t)}\\vec g^{(t)}. \\label{eq:sgd} \\end{equation}\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e我们丢掉式 \\ref{eq:FTRL} 中有关 \u003ccode\u003e$L_1$\u003c/code\u003e 和 \u003ccode\u003e$L_2$\u003c/code\u003e 正则相关的部分，有\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e\\begin{equation} \\vec\\omega^{(t + 1)} \\gets \\mathop{\\arg\\,\\min}_{\\vec\\omega}\\biggl\\{\\vec G^{(1:t)}\\cdot\\vec\\omega + \\frac{1}{2}\\sum_{r = 1}^{t}\\sigma^{(r)}\\bigl\\lVert\\vec\\omega - \\vec\\omega^{(r)}\\bigr\\rVert_2^2\\biggr\\}. \\label{eq:ftrl-pure} \\end{equation}\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e记式 \\ref{eq:ftrl-pure} 中待优化的部分为 \u003ccode\u003e$f(\\vec\\omega)$\u003c/code\u003e。对其求导，有：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e\\begin{equation} \\frac{\\partial f(\\vec\\omega)}{\\partial\\vec\\omega} = \\vec G^{(1:t)} + \\sum_{r = 1}^{t}\\sigma^{(r)}\\bigl(\\vec\\omega - \\vec\\omega^{(r)}\\bigr). \\label{eq:ftrl-pure-gradient} \\end{equation}\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e当式 \\ref{eq:ftrl-pure-gradient} 为 0 时的 \u003ccode\u003e$\\vec\\omega$\u003c/code\u003e，式 \\ref{eq:ftrl-pure} 取得极值。此即有\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e\\begin{equation} \\begin{aligned} \\vec G^{(1:t)} + \\sum_{r = 1}^{t}\\sigma^{(r)}\\bigl(\\vec\\omega^{(t + 1)} - \\vec\\omega^{(r)}\\bigr) ={}\u0026amp; 0 \\\\ \\sigma^{(1:t)} \\vec\\omega^{(t + 1)} ={}\u0026amp; \\sum_{r = 1}^{t}\\sigma^{(r)} \\vec\\omega^{(r)} - \\vec G^{(1:t)} \\end{aligned} \\label{eq:ftrl-pure-gradient-equation} \\end{equation}\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e在式 \\ref{eq:ftrl-pure-gradient-equation} 中，以 \u003ccode\u003e$t - 1$\u003c/code\u003e 替换 \u003ccode\u003e$t$\u003c/code\u003e，得到\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e\\begin{equation} \\sigma^{(1:t - 1)} \\vec\\omega^{(t)} = \\sum_{r = 1}^{t - 1}\\sigma^{(r)} \\vec\\omega^{(r)} - \\vec G^{(1:t - 1)} \\label{eq:ftrl-pure-gradient-equation-minus} \\end{equation}\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e用式 \\ref{eq:ftrl-pure-gradient-equation} 减去式 \\ref{eq:ftrl-pure-gradient-equation-minus} 得到\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e\\begin{equation} \\begin{aligned} \\sigma^{(1:t)} \\vec\\omega^{(t + 1)} - \\sigma^{(1:t - 1)} \\vec\\omega^{(t)} ={}\u0026amp; \\sigma^{(t)}\\vec\\omega^{(t)} - \\vec g^{(t)} \\\\ \\sigma^{(1:t)} \\vec\\omega^{(t + 1)} ={}\u0026amp; \\sigma^{(1:t)} \\vec\\omega^{(t)} - \\vec g^{(t)} \\end{aligned} \\label{eq:ftrl-sgd-equiv} \\end{equation}\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e考虑 \u003ccode\u003e$\\sigma^{(1:t)} = \\frac{1}{\\eta^{(t)}}$\u003c/code\u003e，化简式 \\ref{eq:ftrl-sgd-equiv} 即得到式 \\ref{eq:sgd}。这也就是说，FTRL 去掉 \u003ccode\u003e$L_1$\u003c/code\u003e 和 \u003ccode\u003e$L_2$\u003c/code\u003e 部分后，和 SGD 是等价的。这说明 FTRL 能够较好地获得稀疏解并且能够获得较好的稀疏解。\u003c/p\u003e\n\n    \u003c/div\u003e",
  "Date": "2019-08-31T12:01:23Z",
  "Author": "Liam Huang"
}