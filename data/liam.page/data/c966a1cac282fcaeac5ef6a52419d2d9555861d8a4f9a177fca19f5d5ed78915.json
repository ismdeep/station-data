{
  "Source": "liam.page",
  "Title": "LambdaMART 不太简短之介绍",
  "Link": "https://liam.page/2016/07/10/a-not-so-simple-introduction-to-lambdamart/",
  "Content": "\u003cdiv class=\"post-body\" itemprop=\"articleBody\"\u003e\n\n      \n        \u003cp\u003e传统的搜索引擎排序（Ranking）问题，通常会涉及到很多的排序策略。这些策略根据不同的特征，在不同的适用范围中起作用。因此，一个传统的排序算法，至少涉及到两方面的内容：策略的制定，以及不同策略的组合。策略的组合需要考虑策略分析适用的特征，以及相应策略的适用情况。根据这些内容，通过人工或者半机器半人工的方式组合起来，才能组成一个可堪使用的排序算法。\u003c/p\u003e\n\u003cp\u003e和\u003ca href=\"/2015/07/25/mathematics-theory-of-natural-language-processing-1/\"\u003e自然语言处理\u003c/a\u003e中遇到的情况一样，随着数据量的增加，使用人工的方式做策略的组合，会变得越来越困难。因此，将机器学习引入搜索引擎排序问题，也就是相当自然的事情了。在排序问题中使用的机器学习算法，被称为 Learning to Rank (LTR) 算法，或者 Machine-Learning Rank (MLR) 算法。\u003c/p\u003e\n\u003cp\u003eLTR 算法通常有三种手段，分别是：Pointwise、Pairwise 和 Listwise。Pointwise 和 Pairwise 类型的 LTR 算法，将排序问题转化为\u003cstrong\u003e回归\u003c/strong\u003e、\u003cstrong\u003e分类\u003c/strong\u003e或者\u003cstrong\u003e有序分类\u003c/strong\u003e问题。Listwise 类型的 LTR 算法则另辟蹊径，将用户查询（Query）所得的结果作为整体，作为训练用的实例（Instance）。\u003c/p\u003e\n\u003cp\u003eLambdaMART 是一种 Listwise 类型的 LTR 算法，它基于 LambdaRank 算法和 MART (Multiple Additive Regression Tree) 算法，将搜索引擎结果排序问题转化为回归决策树问题。MART 实际就是梯度提升决策树（GBDT, Gradient Boosting Decision Tree）算法。GBDT 的核心思想是在不断的迭代中，新一轮迭代产生的回归决策树模型拟合损失函数的梯度，最终将所有的回归决策树叠加得到最终的模型。LambdaMART 使用一个特殊的 Lambda 值来代替上述梯度，也就是将 LambdaRank 算法与 MART 算法加和起来。考虑到 LambdaRank 是基于 RankNet 算法的，所以在搞清楚 LambdaMART 算法之前，我们首先需要了解 MART、RankNet 和 LambdaRank 是怎么回事。\u003c/p\u003e\n\u003cspan id=\"more\"\u003e\u003c/span\u003e\n\n\u003ch2 id=\"MART-算法\"\u003e\u003ca href=\"#MART-算法\" class=\"headerlink\" title=\"MART 算法\"\u003e\u003c/a\u003eMART 算法\u003c/h2\u003e\u003cp\u003eMART，即多重增量回归树（Multiple Additive Regression Tree）有许多名字：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMART - 多重增量回归树（Multiple Additive Regression Tree）\u003c/li\u003e\n\u003cli\u003eGBDT - 梯度渐进决策树（Gradient Boosting Decision Tree）\u003c/li\u003e\n\u003cli\u003eGBRT - 梯度渐进回归树（Gradient Boosting Regression Tree）\u003c/li\u003e\n\u003cli\u003eTreeNet - 决策树网络（Tree Net）\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e这些名字的含义都一样，都是一个意思。\u003c/p\u003e\n\u003cp\u003e从这些名字，我们可以看出 MART 的一些特征：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e使用决策树来预测结果；\u003c/li\u003e\n\u003cli\u003e用到的决策树有很多个；\u003c/li\u003e\n\u003cli\u003e每个树都比之前的树改进一点点，逐渐回归、拟合到真实结果。\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e实际上，这三点就是 Boosting 思想的精髓了。Boosting 思想源自 Kearns 和 Valiant 的提问，并最终从 Robert Schapire 在 1990 年的论文 \u003ca target=\"_blank\" rel=\"noopener\" href=\"http://www.cs.princeton.edu/~schapire/papers/strengthofweak.pdf\"\u003e\u003cem\u003eThe Strength of Weak Learnablity\u003c/em\u003e\u003c/a\u003e 中对上述问题明确的回答发展起来。\u003c/p\u003e\n\u003ch3 id=\"Boosting（渐进）思想\"\u003e\u003ca href=\"#Boosting（渐进）思想\" class=\"headerlink\" title=\"Boosting（渐进）思想\"\u003e\u003c/a\u003eBoosting（渐进）思想\u003c/h3\u003e\u003cp\u003eBoosting 思想，尝试通过不断迭代弱模型（Weak Learner），通过叠加弱模型的方式，渐进地逼近真实情况，起到足以预测真实值的强模型的作用。显而易见，Boosting 思想至少需要解决两个问题：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e如何保证每一次迭代都对解决问题有所帮助，或者说如何确定迭代步骤中拟合的方向？\u003c/li\u003e\n\u003cli\u003e如何将每一次迭代产生的弱模型有效地叠加起来？\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e下面，我们通过 AdaBoost（Adaptive Boosting，自适应渐进法）来回答这两个问题。\u003c/p\u003e\n\u003ch3 id=\"AdaBoost\"\u003e\u003ca href=\"#AdaBoost\" class=\"headerlink\" title=\"AdaBoost\"\u003e\u003c/a\u003eAdaBoost\u003c/h3\u003e\u003cp\u003eAdaBoost 是 Yoav Freund 和 Robert Schapire 提出的机器学习算法。两人因为该算法获得了 2003 年的哥德尔奖。\u003c/p\u003e\n\u003cp\u003eAdaBoost 是一种用于分类的算法，它的运行过程大致可以理解如下：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e制作一个弱分类器（实际是一个决策树），去拟合实际情况，我们将它记录为 \u003ccode\u003eWL1\u003c/code\u003e。\u003c/li\u003e\n\u003cli\u003e运行 \u003ccode\u003eWL1\u003c/code\u003e，记录分类错误的那些样本。接下来，赋予这些被错误分类的样本比较高的权重，进行第二次拟合，得到新的弱分类器 \u003ccode\u003eWL2\u003c/code\u003e。\u003c/li\u003e\n\u003cli\u003e依次运行 \u003ccode\u003eWL1\u003c/code\u003e - \u003ccode\u003eWL2\u003c/code\u003e，记录分类错误的那些样本。接下来，赋予这些被错误分类的样本比较高的权重，进行第三次拟合，得到新的弱分类器 \u003ccode\u003eWL3\u003c/code\u003e。\u003c/li\u003e\n\u003cli\u003e依次运行 \u003ccode\u003eWL1\u003c/code\u003e - \u003ccode\u003eWL2\u003c/code\u003e - \u003ccode\u003eWL3\u003c/code\u003e，如此迭代……\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e\u003cimg data-src=\"//liam.page/attachment/attachment/images/LTR/boosting.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e上图来自 Pattern Recognition and Machine Learning 一书的 660 页，讲述的是运用 Boosting 思想进行分类的过程。\u003c/p\u003e\n\u003cp\u003e蓝色和红色的圆圈，代表两类样本，圆圈的大小代表当前该点的权重；绿色的线条代表训练既得分类器模型；虚线表示当前训练新增的分类器模型。\u003c/p\u003e\n\u003cp\u003e可以看到，在不断的迭代过程中，每一次迭代，分类器都会关注之前区分错误的那些样本点，进行有针对性的处理。因此，在进行到 150 次迭代之后（$m = 150$），最终叠加起来的模型已经非常细致，足够将红蓝两种类别细致地区分开来了。\u003c/p\u003e\n\u003cp\u003e整个过程，用数学符号表达如下（PRML Page 658）：\u003c/p\u003e\n\u003cp\u003e\u003cimg data-src=\"//liam.page/attachment/attachment/images/LTR/boosting-illustration.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e在这里：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$w^{(k)}_n$ 表示一个 $n$ 元向量，向量中的每一个元素都对应训练集中的一个样本点；向量中元素的值，就是所对应样本点在第 $k$ 次迭代中的权重。\u003c/li\u003e\n\u003cli\u003e$y_{k}(\\mathbf{x})$ 是根据上述权重计算出来的第 $k$ 次迭代的模型。\u003c/li\u003e\n\u003cli\u003e$\\alpha_{k}$ 是第 $k$ 次迭代的模型的权重。\u003c/li\u003e\n\u003cli\u003e$Y_M(\\mathbf{x})$ 是根据上述 $M$ 个弱分类器加权求和得到的最终分类器。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e最开始的时候，每个样本点的权重都一致。随着算法不断迭代，被错误分类的样本，权重不断加强，与此同时被正确分类的样本，权重不断减弱。可以想象，越往后，算法越关注那些容易被分错的样本点，从而最终解决整个问题。\u003c/p\u003e\n\u003cp\u003e现在，我们至少可以从 AdaBoost 的角度回答上一小节的两个问题了：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003eAdaBoost 通过调整样本的权值，来确定下一轮迭代中弱模型的拟合方向：提升分类错误的样本的权值，降低分类正确的样本的权值。\u003c/li\u003e\n\u003cli\u003eAdaBoost 用一个「加法模型」，将每一轮迭代得到的弱模型组合叠加起来，得到一个有效的强模型。\u003c/li\u003e\n\u003c/ol\u003e\n\u003ch3 id=\"MART-的数学原理\"\u003e\u003ca href=\"#MART-的数学原理\" class=\"headerlink\" title=\"MART 的数学原理\"\u003e\u003c/a\u003eMART 的数学原理\u003c/h3\u003e\u003cp\u003eMART 是一种 Boosting 思想下的算法框架，它的目标是寻找强模型 $f(x)$ 满足：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$\\hat f(x) = \\text{arg min}_{f(x)} E\\Bigl[L\\bigl(y, f(x)\\bigr) \\big| x\\Bigr]$$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e和 AdaBoost 一样，训练之后的 MART 模型也是一个加法模型，形式如下：\u003cbr/\u003e\u003ccode\u003e$$ \\label{eq:MART-result} \\hat f(x) = \\hat f_M(x) = \\sum_{m = 1}^M f_m(x) $$\u003c/code\u003e\u003cbr/\u003e这里：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$\\hat f = \\hat f_M: \\mathbb{R}^d \\mapsto \\mathbb{R}$ 是模型的目标函数；\u003c/li\u003e\n\u003cli\u003e$x \\in \\mathbb{R}^d$ 是样本点，它包含 $d$ 个特征的值；\u003c/li\u003e\n\u003cli\u003e$\\hat f(x) \\in \\mathbb{R}$ 是预测值；\u003c/li\u003e\n\u003cli\u003e$M$ 是训练过程中迭代的次数，也就是模型中回归决策树的数量；\u003c/li\u003e\n\u003cli\u003e$f_m: \\mathbb{R}^d \\mapsto \\mathbb{R}$ 是模型训练过程中得到的弱模型（也就是回归决策树）。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e那么，关于 MART 的 Boosting，我们还剩下一个回答，即：如何保证每一次迭代都对解决问题有所帮助，或者说如何确定迭代步骤中拟合的方向。接下来的分析，我们就来解决这个问题。\u003c/p\u003e\n\u003cp\u003e假设我们已经迭代了 $m$ 次，得到了 $m$ 颗决策树。我们将这 $m$ 颗决策树的和记作 \u003ccode\u003e$\\hat f_m(x) = \\sum_{i = 1}^m  f_i(x)$\u003c/code\u003e，于是，第 $m + 1$ 轮拟合的目标\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e\\begin{equation} \\delta \\hat f_{m + 1} = \\hat f_{m + 1} - \\hat f_m = f_{m + 1} \\end{equation}\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e现在我们要求这个 \u003ccode\u003e$\\delta \\hat f_{m + 1}$\u003c/code\u003e。我们引入损失函数\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ L = L\\bigl((x, y), f\\bigr) = L \\bigl(y, f(x) \\big| x\\bigr) $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e来描述预测函数 $f$ 的预测结果 $y^{*} = f(x)$ 与真实值 $y$ 的差距。那么，我们的目的是要在进行第 $m + 1$ 轮拟合之后，预测值与真实情况的差距会减小，即\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\delta L_{m + 1} = L\\bigl((x, y), \\hat f_{m + 1}\\bigr) - L\\bigl((x, y), \\hat f_{m}\\bigr) \\lt 0 $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e考虑到\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\delta L_{m + 1} \\approx \\frac{\\partial L\\bigl((x, y), \\hat f_m\\bigr)}{\\partial \\hat f_m}\\cdot \\delta \\hat f_{m + 1}, $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e若取\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e\\begin{equation} \\label{eq:gradient} \\delta \\hat f_{m + 1} = -g_{im} = -\\frac{\\partial L\\bigl((x, y), \\hat f_m\\bigr)}{\\partial \\hat f_m}, \\end{equation}\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e则必有 \u003ccode\u003e$\\delta L_{m + 1} \\lt 0$\u003c/code\u003e。因此，这个 \u003ccode\u003e$\\delta \\hat f_{m + 1}$\u003c/code\u003e 就是 \u003ccode\u003e$f_{m + 1}$\u003c/code\u003e 拟合的目标；也就是说，\u003ccode\u003e$f_{m + 1}$\u003c/code\u003e 要拟合这样一个训练集：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$ \\Biggl\\{i = 1, 2, \\ldots, N \\Bigg\\vert \\biggl(x_i, -\\frac{\\partial L\\bigl((x_i, y_i), \\hat f_m(x_i)\\bigr)}{\\partial \\hat f_m(x_i)}\\biggr)\\Biggr\\} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e式 \\ref{eq:gradient} 描述的正是损失函数（泛函） $L$ 的梯度。亦即，循环迭代中，模型每次拟合的目标都是损失函数的梯度。这就是算法被称为「梯度渐进」的原因。\u003c/p\u003e\n\u003cp\u003e决策树实际上将样本空间划分成了若干区域，并对每个划分区域赋上预测值。假设 $f_m$ 划分而成的区域是 $R_m$，预测值则是 $\\gamma_m$。那么，我们有\u003cbr/\u003e\u003ccode\u003e$$     f_m(x) = h_m(x; R_m, \\gamma_m), $$\u003c/code\u003e\u003cbr/\u003e也就是\u003cbr/\u003e\u003ccode\u003e$$     \\hat{f}(x) = \\sum_{m=1}^{M}f_m(x) = \\sum_{m=1}^{M}h_m(x; R_m, \\gamma_m). $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e那么，MART 的每一步也就是要解优化问题：\u003cbr/\u003e\u003ccode\u003e\\begin{equation}     \\label{eq:argmin}     h_m(x; R_m, \\gamma_m) = \\text{arg min}_{R, \\gamma}\\sum_{i = 1}^{N} \\bigl(-g_{im} - F(x_i;R,\\gamma)\\bigr)^2,\\tag{$ \\star $} \\end{equation}\u003c/code\u003e\u003cbr/\u003e其中 $F$ 是一棵回归决策树。\u003c/p\u003e\n\u003cp\u003e现在我们引入一个非常小的正数 $ \\eta $，称为「学习度」或者「收缩系数」。如果，我们在每轮迭代中的预测结果前，乘上这么一个学习度；亦即我们将第 $m + 1$ 轮拟合的目标，从 \u003ccode\u003e$-g_{im}$\u003c/code\u003e 调整为 \u003ccode\u003e$-\\eta\\cdot g_{im}$\u003c/code\u003e。这样一来，我们每次拟合的目标，就变成了损失函数梯度的一部分。由于 $\\delta L \\lt 0$ 仍然成立，经过多次迭代之后，模型依然可以得到一个很好的结果。但是，与引入学习度之前的情况相比较，每次拟合像是「朝着正确的方向只迈出了一小步」。这种 Shrinkage（缩减），主要是为了防止「过拟合」现象。\u003c/p\u003e\n\u003ch3 id=\"小结\"\u003e\u003ca href=\"#小结\" class=\"headerlink\" title=\"小结\"\u003e\u003c/a\u003e小结\u003c/h3\u003e\u003cp\u003eMART 是一种 Boosting 思想下的算法框架。它通过加法模型，将每次迭代得到的子模型叠加起来；而每次迭代拟合的对象都是学习率与损失函数梯度的乘积。这两点保证了 MART 是一个正确而有效的算法。\u003c/p\u003e\n\u003cp\u003eMART 中最重要的思想，就是每次拟合的对象是损失函数的梯度。值得注意的是，在这里，MART 并不对损失函数的形式做具体规定。实际上，\u003cstrong\u003e损失函数几乎只需要满足可导这一条件就可以了\u003c/strong\u003e。这一点非常重要，意味着我们可以把任何合理的可导函数安插在 MART 模型中。LambdaMART 就是用一个 $\\lambda$ 值代替了损失函数的梯度，将 $\\lambda$ 和 MART 结合起来罢了。\u003c/p\u003e\n\u003ch2 id=\"Lambda\"\u003e\u003ca href=\"#Lambda\" class=\"headerlink\" title=\"Lambda\"\u003e\u003c/a\u003eLambda\u003c/h2\u003e\u003cp\u003eLambda 的设计，最早是由 LambdaRank 从 RankNet 继承而来。因此，我们先要从 RankNet 讲起。\u003c/p\u003e\n\u003ch3 id=\"RankNet-的创新\"\u003e\u003ca href=\"#RankNet-的创新\" class=\"headerlink\" title=\"RankNet 的创新\"\u003e\u003c/a\u003eRankNet 的创新\u003c/h3\u003e\u003cp\u003eRanking 常见的评价指标都无法求梯度，因此没法直接对评价指标做梯度下降。\u003c/p\u003e\n\u003cp\u003eRankNet 的创新之处在于，它将不适宜用梯度下降求解的 Ranking 问题，转化为对概率的交叉熵损失函数的优化问题，从而适用梯度下降方法。\u003c/p\u003e\n\u003cp\u003eRankNet 的终极目标是得到一个带参的算分函数：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$s = f(x; w).$$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e于是，根据这个算分函数，我们可以计算文档 \u003ccode\u003e$x_i$\u003c/code\u003e 和 \u003ccode\u003e$x_j$\u003c/code\u003e 的得分 \u003ccode\u003e$s_i$\u003c/code\u003e 和 \u003ccode\u003e$s_j$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$s_i = f(x_i; w)\\quad s_j = f(x_j; w),$$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e然后根据得分计算二者的偏序概率\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$P_{ij} = P(x_i \\rhd x_j) = \\frac{\\exp\\bigl(\\sigma\\cdot(s_i - s_j)\\bigr)}{1 + \\exp\\bigl(\\sigma\\cdot(s_i - s_j)\\bigr)} = \\frac{1}{1 + \\exp\\bigl(-\\sigma\\cdot(s_i - s_j)\\bigr)},$$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e再定义交叉熵为损失函数\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$L_{ij} =     -\\bar{P}_{ij}\\log P_{ij} - (1 - \\bar{P}_{ij})\\log (1 - P_{ij}) =     \\frac12 (1 - S_{ij})\\sigma\\cdot(s_i - s_j) + \\log\\Bigl\\{1 + \\exp\\bigl(-\\sigma\\cdot(s_i - s_j)\\bigr)\\Bigr\\},$$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e进行梯度下降\u003cbr/\u003e\u003ccode\u003e$$w_k \\to w_k - \\eta\\frac{\\partial L}{\\partial w_k}.$$\u003c/code\u003e\u003c/p\u003e\n\u003ch3 id=\"再探梯度\"\u003e\u003ca href=\"#再探梯度\" class=\"headerlink\" title=\"再探梯度\"\u003e\u003c/a\u003e再探梯度\u003c/h3\u003e\u003cp\u003e\u003cimg data-src=\"/uploads/images/LTR/gradient.png\"/\u003e\u003c/p\u003e\n\u003cp\u003e这里每条横线代表一条文档，其中蓝色的表示相关的文档，灰色的则表示不相关的文档。在某次迭代中，RankNet 将文档的顺序从左边调整到了右边。于是我们可以看到：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRankNet 的梯度下降表现在结果的整体变化中是逆序对的下降：13 → 11\u003c/li\u003e\n\u003cli\u003eRankNet 的梯度下降表现在单条结果的变化中，是结果在列表中的移动趋势（图中黑色箭头）\u003c/li\u003e\n\u003cli\u003e我们通常更关注前几条文档的排序情况，因此我们会期待真正的移动趋势如图中红色箭头所示\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e那么问题就来了：我们能不能直接定义梯度呢？\u003c/p\u003e\n\u003ch3 id=\"LambdaRank\"\u003e\u003ca href=\"#LambdaRank\" class=\"headerlink\" title=\"LambdaRank\"\u003e\u003c/a\u003eLambdaRank\u003c/h3\u003e\u003cp\u003e现在的情况是这样：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eRankNet 告诉我们如何绕开 NDCG 等无法求导的评价指标得到一个可用的梯度；\u003c/li\u003e\n\u003cli\u003e上一节我们明确了我们需要怎样的梯度；\u003c/li\u003e\n\u003cli\u003e梯度（红色箭头）反应的是某条结果排序变化的趋势和强度；\u003c/li\u003e\n\u003cli\u003e结果排序最终由模型得分 $s$ 确定。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e于是，\u003cstrong\u003e我要扼住 \u003ccode\u003e$\\frac{\\partial L_{ij}}{\\partial s_i}$\u003c/code\u003e 的咽喉\u003c/strong\u003e!\u003c/p\u003e\n\u003cp\u003e先看看 RankNet 的梯度\u003cbr/\u003e\u003ccode\u003e$$     \\frac{\\partial L}{\\partial w_k} =     \\sum_{(i, j) \\in P}\\frac{\\partial L_{ij}}{\\partial w_k} =     \\sum_{(i, j) \\in P}         \\frac{\\partial L_{ij}}{\\partial s_i}\\frac{\\partial s_i}{\\partial w_k}             +         \\frac{\\partial L_{ij}}{\\partial s_j}\\frac{\\partial s_j}{\\partial w_k}, $$\u003c/code\u003e\u003cbr/\u003e注意有下面对称性\u003cbr/\u003e\u003ccode\u003e$$     \\begin{aligned}         \\frac{\\partial L_{ij}}{\\partial s_i} \u0026amp; {} = \\frac{\\partial \\biggl\\{\\frac12 (1 - S_{ij})\\sigma\\cdot(s_i - s_j) + \\log\\Bigl\\{1 + \\exp\\bigl(-\\sigma\\cdot(s_i - s_j)\\bigr)\\Bigr\\}\\biggr\\}}{\\partial s_i}\\\\          \u0026amp; {}= \\frac12 (1 - S_{ij})\\sigma - \\frac{\\sigma}{1 + \\exp\\bigl(\\sigma\\cdot(s_i - s_j)\\bigr)} \\\\          \u0026amp; {}= \\sigma\\Biggl[\\frac12(1 - S_{ij}) - \\frac{1}{1 + \\exp\\bigl(\\sigma\\cdot(s_i - s_j)\\bigr)}\\Biggr] \\\\          \u0026amp; {}= -\\frac{\\partial L_{ij}}{\\partial s_j}.     \\end{aligned} $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e于是我们定义\u003cbr/\u003e\u003ccode\u003e$$     \\lambda_{ij}\\mathrel{\\stackrel{\\mbox{def}}{=}} \\frac{\\partial L_{ij}}{\\partial s_i} = -\\frac{\\partial L_{ij}}{\\partial s_j}, $$\u003c/code\u003e\u003cbr/\u003e考虑有序对 $(i, j)$，有 \u003ccode\u003e$S_{ij} = 1$\u003c/code\u003e，于是有简化\u003cbr/\u003e\u003ccode\u003e$$     \\lambda_{ij}\\mathrel{\\stackrel{\\mbox{def}}{=}} - \\frac{\\sigma}{1 + \\exp\\bigl(\\sigma\\cdot(s_i - s_j)\\bigr)}, $$\u003c/code\u003e\u003cbr/\u003e在此基础上，考虑评价指标 $Z$（比如 NDCG）的变化\u003cbr/\u003e\u003ccode\u003e$$     \\lambda_{ij}\\mathrel{\\stackrel{\\mbox{def}}{=}} - \\frac{\\sigma}{1 + \\exp\\bigl(\\sigma\\cdot(s_i - s_j)\\bigr)}\\cdot\\lvert\\Delta Z_{ij}\\rvert. $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e对于具体的文档 \u003ccode\u003e$x_i$\u003c/code\u003e，有\u003cbr/\u003e\u003ccode\u003e$$     \\lambda_{i} = \\sum_{(i, j) \\in P}\\lambda_{ij} - \\sum_{(j, i) \\in P}\\lambda_{ij}. $$\u003c/code\u003e\u003c/p\u003e\n\u003cp\u003e也就是说：\u003cstrong\u003e每条文档移动的方向和趋势取决于其他所有与之 label 不同的文档。\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e现在回过头来，看看我们做了什么？\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e分析了梯度的物理意义；\u003c/li\u003e\n\u003cli\u003e绕开损失函数，直接定义梯度。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e当然，我们可以反推一下 LambdaRank 的损失函数：\u003c/p\u003e\n\u003cp\u003e\u003ccode\u003e$$L_{ij} = \\log\\Bigl\\{1 + \\exp\\bigl(-\\sigma\\cdot(s_i - s_j)\\bigr)\\Bigr\\}\\cdot\\lvert\\Delta Z_{ij}\\rvert.$$\u003c/code\u003e\u003c/p\u003e\n\u003ch2 id=\"LambdaMART\"\u003e\u003ca href=\"#LambdaMART\" class=\"headerlink\" title=\"LambdaMART\"\u003e\u003c/a\u003eLambdaMART\u003c/h2\u003e\u003cp\u003e现在的情况变成了这样：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eMART 是一个框架，缺一个「梯度」；\u003c/li\u003e\n\u003cli\u003eLambdaRank 定义了一个「梯度」。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e让他们在一起吧！于是，就有了 LambdaMART。\u003c/p\u003e\n\u003cp\u003e\u003cimg data-src=\"/uploads/images/LTR/LambdaMART.png\"/\u003e\u003c/p\u003e\n\u003ch3 id=\"LambdaMART-的优点\"\u003e\u003ca href=\"#LambdaMART-的优点\" class=\"headerlink\" title=\"LambdaMART 的优点\"\u003e\u003c/a\u003eLambdaMART 的优点\u003c/h3\u003e\u003cp\u003eLambdaMART 有很多优点，取一些列举如下：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e直接求解排序问题，而不是用分类或者回归的方法；\u003c/li\u003e\n\u003cli\u003e可以将 NDCG 之类的不可求导的 IR 指标转换为可导的损失函数，具有明确的物理意义；\u003c/li\u003e\n\u003cli\u003e可以在已有模型的基础上进行 Continue Training；\u003c/li\u003e\n\u003cli\u003e每次迭代选取 gain 最大的特征进行梯度下降，因此可以学到不同特征的组合情况，并体现出特征的重要程度（特征选择）；\u003c/li\u003e\n\u003cli\u003e对正例和负例的数量比例不敏感。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003ca href=\"/uploads/slides/lambdamart.pdf\"\u003e幻灯片下载\u003c/a\u003e\u003c/p\u003e\n\n    \u003c/div\u003e",
  "Date": "2016-07-10T10:47:31Z",
  "Author": "Liam Huang"
}