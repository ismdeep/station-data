{
  "Source": "qiwsir.github.io",
  "Title": "cost function 笔记",
  "Link": "https://qiwsir.github.io/2020/04/26/cost-function-notebook/",
  "Content": "\u003carticle class=\"post-article\"\u003e\n    \u003ch2\u003ecost function 笔记\u003c/h2\u003e\n    \u003cp class=\"post-date\"\u003e2020-04-26\u003c/p\u003e\n    \u003csection class=\"markdown-content\"\u003e\u003cp\u003eCost Function，翻译为：代价函数、成本函数\u003c/p\u003e\n\u003cp\u003e与之同样的词语：损失函数（Loss function)\u003c/p\u003e\n\u003cp\u003e有资料认为二者不同。通常可以认为是一回事。维基百科：\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003eIn mathematical optimization, statistics, decision theory and machine learning, a loss function or cost function  is a function that maps an event or values of one or more variables onto a real number intuitively representing some “cost” associated with the  event. An optimization problem seeks to minimize a loss function.\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e损失函数用于衡量预测值与实际值的偏离程度，如果预测是完全精确的，则损失函数值为0；如果损失函数值不为0，则其表示的是预测的错误有多糟糕。使得损失函数值最小的那些待求参数值，就是“最优”的参数值。\u003c/p\u003e\n\u003cp\u003e常用的损失函数：\u003c/p\u003e\n\u003cp\u003e（1）0-1损失函数：可用于分类问题，即该函数用于衡量分类错误的数量，但由于此损失函数是非凸（non-convex）的，因此在做最优化计算时，难以求解，所以，正因为如此，0-1损失函数不是那么“实用”（如果这句话有误，请指正）。\u003cbr/\u003e（2）平方损失函数（Square Loss）：常用于线性回归（Linear Regression）。\u003cbr/\u003e（3）对数损失（Log Loss）函数：常用于其模型输出每一类概率的分类器（classifier），例如逻辑回归。\u003cbr/\u003e（4）Hinge损失函数：常用于SVM（Support Vector  Machine，支持向量机，一种机器学习算法）。中文名叫“合页损失函数”，因为hinge有“合页”之意。这个翻译虽然直白，但是你会发现，99％的文章都不会用它的中文名来称呼它，而是用“Hinge损失”之类的说法。\u003c/p\u003e\n\u003cp\u003e假设有训练样本$(x, y)$，模型为 $h$, 参数为 $\\theta$ 。$h(\\theta)=\\theta^Tx$ （$\\theta^T$ 表示 $\\theta$ 的转置）。\u003c/p\u003e\n\u003cp\u003e任何能够衡量模型预测出来的值 $h(\\theta)$ 与真实值 $y$ 之间的差异的函数都可以叫做代价函数，记作：$C(\\theta)$。\u003c/p\u003e\n\u003cp\u003e如果有多个样本，则可以将所有代价函数的取值求均值，记作：$J(\\theta)$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e选择代价函数时，最好挑选对参数$\\theta$ 可微的函数（全微分存在，偏导数一定存在）\u003c/li\u003e\n\u003cli\u003e对于每种算法来说，代价函数不是唯一的；\u003c/li\u003e\n\u003cli\u003e代价函数是参数 $\\theta$ 的函数；\u003c/li\u003e\n\u003cli\u003e总的代价函数 $J(\\theta)$ 可以用来评价模型的好坏，代价函数的值越小说明模型和参数越符合训练样本$(x, y)$ ；\u003c/li\u003e\n\u003cli\u003e$J(\\theta)$ 是一个标量；\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e训练过程，就是不断选择参数的过程，即优化参数θ的过程，最常用的方法是梯度下降，这里的梯度就是代价函数$J(\\theta)$ 对 $\\theta_1, \\theta_2, \\cdots, \\theta_n$ 的偏导数。\u003c/p\u003e\n\u003cp\u003e一个好的代价函数需要满足两个最基本的要求：能够评价模型的准确性，对参数 $\\theta$ 可微。\u003c/p\u003e\n\u003cp\u003e在线性回归中，最常用的是\u003cstrong\u003e均方误差\u003c/strong\u003e(Mean squared error)，即：\u003c/p\u003e\n\u003cp\u003e$$J(\\theta_0, \\theta_1)=\\frac{1}{2m}\\sum_{i=1}^{m}(\\hat y^{(i)}-y^{i})^2=\\frac{1}{2m}\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)})^2$$\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e$m$：训练样本的个数\u003c/li\u003e\n\u003cli\u003e$h_{\\theta}(x)$：用参数$\\theta$ 和 $x$ 预测出来的值\u003c/li\u003e\n\u003cli\u003e$y$：原训练样本中的标签\u003c/li\u003e\n\u003cli\u003e上角标 $(i)$：第$i$个样本。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e在逻辑回归中，最常用的是损失函数是\u003cstrong\u003e交叉熵\u003c/strong\u003e(Cross Entropy)（或者说是对数损失函数（Log Loss））。\u003c/p\u003e\n\u003cp\u003e在信息理论中，熵用于衡量某种事件的“不可预测性”，而交叉熵=事件的真实分布+不可预测性，所以交叉熵可以用于度量两个概率分布（真实分布\u0026amp;预测分布）之间的差异性，即：交叉熵损失函数（对数损失函数）可以衡量一个模型对真实值带来的额外噪音，通过最小化交叉熵损失函数（对数损失函数），我们就可以最大化分类器（模型）的精度。\u003c/p\u003e\n\u003cp\u003e交叉熵是对「出乎意料」（译者注：原文使用suprise）的度量。神经元的目标是去计算函数x→y=y(x)。但是我们让它取而代之计算函数x→a=a(x)。假设我们把a当作y等于1的概率，1−a是y等于0的概率。那么，交叉熵衡量的是我们在知道y的真实值时的平均「出乎意料」程度。当输出是我们期望的值，我们的「出乎意料」程度比较低；当输出不是我们期望的，我们的「出乎意料」程度就比较高。\u003c/p\u003e\n\u003cp\u003e在1948年，克劳德·艾尔伍德·香农将热力学的熵，引入到信息论，因此它又被称为香农熵(Shannon  Entropy)，它是香农信息量(Shannon Information Content,  SIC)的期望。香农信息量用来度量不确定性的大小：一个事件的香农信息量等于0，表示该事件的发生不会给我们提供任何新的信息，例如确定性的事件，发生的概率是1，发生了也不会引起任何惊讶；当不可能事件发生时，香农信息量为无穷大，这表示给我们提供了无穷多的新信息，并且使我们无限的惊讶。\u003c/p\u003e\n\u003cp\u003e$J(\\theta)=-\\frac 1 m [\\sum_{i=1}^m(y^{(i)}logh_{\\theta}(x^{(i)})+(1-y^{(i)})log(1-h_{\\theta}(x^{(i)}))]$\u003c/p\u003e\n\u003cp\u003e从单页样本角度，理解如此定义Logistic的代价函数的理由：\u003c/p\u003e\n\u003cp\u003e$C(\\theta)=ylogh_\\theta(x)+(1-y)log(1-h_{\\theta}(x))$\u003c/p\u003e\n\u003cp\u003e上述表达式等价于：\u003c/p\u003e\n\u003cp\u003e$$C(\\theta)=\\begin{cases}-log(h_{\\theta}(x), y=1\\-log(1-h_{\\theta}(x)), y=0\\end{cases}$$\u003c/p\u003e\n\u003cp\u003e其中：$h_{\\theta}(x)=\\frac{1}{1+e^{-\\theta^Tx}}$\u003c/p\u003e\n\u003cp\u003e参阅：\u003ca href=\"https://zhuanlan.zhihu.com/p/28415991\" target=\"_blank\" rel=\"noopener\"\u003ehttps://zhuanlan.zhihu.com/p/28415991\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e以下链接，从概率的角度解释为什么选择Sigmoid函数作为Logistic回归的函数：\u003c/p\u003e\n\u003cp\u003e\u003ca href=\"https://zhuanlan.zhihu.com/p/55321901\" target=\"_blank\" rel=\"noopener\"\u003ehttps://zhuanlan.zhihu.com/p/55321901\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003elogistic回归值表示所属类的后验概率，无论是二分类还是多分类，分类结果都是后验概率最大所对应的类。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e参考资料\u003c/strong\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://zhuanlan.zhihu.com/p/28408516\" target=\"_blank\" rel=\"noopener\"\u003ehttps://zhuanlan.zhihu.com/p/28408516\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/section\u003e\n    \u003c!-- Tags START --\u003e\n    \n    \u003c!-- Tags END --\u003e\n    \u003c!-- NAV START --\u003e\n    \n  \u003cdiv class=\"nav-container\"\u003e\n    \u003c!-- reverse left and right to put prev and next in a more logic postition --\u003e\n    \n      \u003ca class=\"nav-left\" href=\"/2020/04/25/about-official-account/\"\u003e\n        \u003cspan class=\"nav-arrow\"\u003e← \u003c/span\u003e\n        \n          关于【老齐教室】\n        \n      \u003c/a\u003e\n    \n    \n      \u003ca class=\"nav-right\" href=\"/2020/04/28/web-stack-second/\"\u003e\n        \n          剖析Web技术栈（二）\n        \n        \u003cspan class=\"nav-arrow\"\u003e →\u003c/span\u003e\n      \u003c/a\u003e\n    \n  \u003c/div\u003e\n\n    \u003c!-- NAV END --\u003e\n    \u003c!-- 打赏 START --\u003e\n    \n      \u003cdiv class=\"money-like\"\u003e\n        \u003cdiv class=\"reward-btn\"\u003e\n          赏\n          \u003cspan class=\"money-code\"\u003e\n            \u003cspan class=\"alipay-code\"\u003e\n              \u003cdiv class=\"code-image\"\u003e\u003c/div\u003e\n              \u003cb\u003e使用支付宝打赏\u003c/b\u003e\n            \u003c/span\u003e\n            \u003cspan class=\"wechat-code\"\u003e\n              \u003cdiv class=\"code-image\"\u003e\u003c/div\u003e\n              \u003cb\u003e使用微信打赏\u003c/b\u003e\n            \u003c/span\u003e\n          \u003c/span\u003e\n        \u003c/div\u003e\n        \u003cp class=\"notice\"\u003e若你觉得我的文章对你有帮助，欢迎点击上方按钮对我打赏\u003c/p\u003e\n      \u003c/div\u003e\n    \n    \u003c!-- 打赏 END --\u003e\n    \u003c!-- 二维码 START --\u003e\n    \u003c!--% if (theme.qrcode) { %--\u003e\n      \u003cdiv class=\"qrcode\"\u003e\n        \u003c!--canvas id=\"share-qrcode\"\u003e\u003c/!--canvas--\u003e\n        \u003cimg src=\"https://public-tuchuang.oss-cn-hangzhou.aliyuncs.com/WechatIMG6_20200109154827.jpeg\" width=\"400\"/\u003e\n        \u003cp class=\"notice\"\u003e关注微信公众号，读文章、听课程，提升技能\u003c/p\u003e\n      \u003c/div\u003e\n    \u003c!--% } %--\u003e\n    \u003c!-- 二维码 END --\u003e\n    \n      \u003c!-- No Comment --\u003e\n    \n  \u003c/article\u003e",
  "Date": "2020-04-26T00:00:00Z",
  "Author": "老齐教室"
}