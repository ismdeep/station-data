{
  "Source": "qiwsir.github.io",
  "Title": "必须懂：深度学习中的信息论概念",
  "Link": "https://qiwsir.github.io/2020/04/03/ml-information-theory/",
  "Content": "\u003carticle class=\"post-article\"\u003e\n    \u003ch2\u003e必须懂：深度学习中的信息论概念\u003c/h2\u003e\n    \u003cp class=\"post-date\"\u003e2020-04-03\u003c/p\u003e\n    \u003csection class=\"markdown-content\"\u003e\u003cp\u003e作者：Abhishek Parbhakar\u003c/p\u003e\n\u003cp\u003e翻译：老齐\u003c/p\u003e\n\u003cp\u003e与本文相关的图书推荐：《数据准备和特征工程》\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://public-tuchuang.oss-cn-hangzhou.aliyuncs.com/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B2_20200114135935.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cp\u003e本书已经发售，购买：【电子工业出版社天猫旗舰店】\u003c/p\u003e\n\u003chr/\u003e\n\u003cp\u003e信息论是对深度学习和AI有重大贡献的一个重要领域，当然，很多人对它知之甚少。如你所知，深度学习的基石是微积分、概率论和统计学，信息论可以视为是它们之间的复杂的融合。AI中的一些概念就来自于信息论或相关领域，例如：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e常见的交叉熵和损失函数\u003c/li\u003e\n\u003cli\u003e基于最大信息熵的决策树\u003c/li\u003e\n\u003cli\u003eNLP和语音处理中的Viterbi算法\u003c/li\u003e\n\u003cli\u003e循环神经网络和其他模型中的编码器概念\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"信息论简史\"\u003e\u003ca href=\"#信息论简史\" class=\"headerlink\" title=\"信息论简史\"\u003e\u003c/a\u003e信息论简史\u003c/h2\u003e\u003cp\u003e\u003cimg src=\"https://public-tuchuang.oss-cn-hangzhou.aliyuncs.com/ai2_20200402131322.jpeg\" alt=\"克劳德 香农，信息论之父\"/\u003e\u003c/p\u003e\n\u003cp\u003e20世纪早期，科学家和工程师困惑于这样的问题：如何量化信息？有没有某个数学化的方法能够测量信息量？例如，以下两句话：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eBruno是一只狗。\u003c/li\u003e\n\u003cli\u003eBruno是一只大个的有着棕色皮毛的狗。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e不难看出，第二句告诉了我们更多的信息，狗是大的，毛是棕色的，而不仅仅告诉我们是一只狗。我们如何量化这两句话的差异？我们能用数学化的方法测量第二句比第一句有更多得信息吗？\u003c/p\u003e\n\u003cp\u003e科学家困惑于此问题。用语义或者语句的数量来衡量信息，只能让问题更麻烦。后来，数学家和工程师克劳德·香农提出了“熵”的思想，这种思想永远改变了我们的世界，标志着“数字信息时代”的开始。\u003c/p\u003e\n\u003cp\u003e香农提出“数据的语义相彼此无关”，即数据的类型和含义在涉及信息内容时则无关紧要,相反，他根据概率分布和“不确定性”对信息进行了量化。香农还引入了“位”（“bit”），并谦虚地归功于他的同事John Tukey。 这一革命性的思想不仅奠定了信息理论的基础，而且还为人工智能等领域的发展开辟了新的途径。\u003c/p\u003e\n\u003cp\u003e下面我们讨论深度学习和数据科学中4个流行的且广泛应用的、必须要知道的信息论概念：\u003c/p\u003e\n\u003ch2 id=\"熵\"\u003e\u003ca href=\"#熵\" class=\"headerlink\" title=\"熵\"\u003e\u003c/a\u003e熵\u003c/h2\u003e\u003cp\u003e也称为信息熵或者香农熵。\u003c/p\u003e\n\u003ch3 id=\"初步理解\"\u003e\u003ca href=\"#初步理解\" class=\"headerlink\" title=\"初步理解\"\u003e\u003c/a\u003e初步理解\u003c/h3\u003e\u003cp\u003e熵是度量不确定性的量，让我们设想两个实验：\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e抛一枚均匀的硬币(P(H)=0.5)，观察它的输出，假设H\u003c/li\u003e\n\u003cli\u003e投掷一枚有偏差的硬币(P(H)=0.99)，观察它的输出，假设H\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003e比较这两个实验，相对于实验1，实验2更容易预测到它的结果。那么，我们说实验1比实验2具有更强的不确定性，实验中的这种不确定性就用熵来度量。\u003c/p\u003e\n\u003cp\u003e因此，如果实验具有更多不确定性，熵的值越大，或者说，实验结果的可预测性越强，熵越小。实验的概率分布常常用熵计算。\u003c/p\u003e\n\u003cp\u003e实验结果确定，即完全可预测，就相当于抛出一门P(H)=1的硬币，此时熵为0。如果实验完全随机，例如掷骰子，可预测性最低，具有最大的不确定，其实验的熵也最高。\u003c/p\u003e\n\u003cp\u003e另一种对熵的理解是通过观测随机实验输出的平均获得信息。从一个实验结果所获得的信息可以定义为一个概率函数，输出越少，获得信息越多。\u003c/p\u003e\n\u003cp\u003e例如，一个确定性实验，我们都知道其结果，所以，就没有获得新信息，熵即为0。\u003c/p\u003e\n\u003ch3 id=\"数学表示\"\u003e\u003ca href=\"#数学表示\" class=\"headerlink\" title=\"数学表示\"\u003e\u003c/a\u003e数学表示\u003c/h3\u003e\u003cp\u003eX为离散星随机变量，其状态分别是$x_1, …, x_n$，其熵定义为：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://public-tuchuang.oss-cn-hangzhou.aliyuncs.com/ai3_20200402131445.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cp\u003e其中$p(x_i)$是X的第i个输出（状态）\u003c/p\u003e\n\u003ch3 id=\"应用\"\u003e\u003ca href=\"#应用\" class=\"headerlink\" title=\"应用\"\u003e\u003c/a\u003e应用\u003c/h3\u003e\u003cul\u003e\n\u003cli\u003e熵可用于决策树模型，构建树的每一步，利用熵对特征进行选择。\u003c/li\u003e\n\u003cli\u003e依据最大熵原则选择模型，即在众多候选模型中，熵最大的模型最好。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"交叉熵\"\u003e\u003ca href=\"#交叉熵\" class=\"headerlink\" title=\"交叉熵\"\u003e\u003c/a\u003e交叉熵\u003c/h2\u003e\u003ch3 id=\"初步理解-1\"\u003e\u003ca href=\"#初步理解-1\" class=\"headerlink\" title=\"初步理解\"\u003e\u003c/a\u003e初步理解\u003c/h3\u003e\u003cp\u003e交叉熵用于比较两个概率分布，通过它能够知道两个分布的相似度。\u003c/p\u003e\n\u003ch3 id=\"数学表示-1\"\u003e\u003ca href=\"#数学表示-1\" class=\"headerlink\" title=\"数学表示\"\u003e\u003c/a\u003e数学表示\u003c/h3\u003e\u003cp\u003e假设p和q两个概率分布，用如下方式定义交叉熵：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://public-tuchuang.oss-cn-hangzhou.aliyuncs.com/ai4_20200402131548.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003ch3 id=\"应用-1\"\u003e\u003ca href=\"#应用-1\" class=\"headerlink\" title=\"应用\"\u003e\u003c/a\u003e应用\u003c/h3\u003e\u003cp\u003e\u003cimg src=\"https://public-tuchuang.oss-cn-hangzhou.aliyuncs.com/ai5_20200402131634.png\" alt=\"基于卷积神经网络的分类器通常使用softmax层作为最后一层，并使用交叉熵损失函数进行训练。\"/\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e交叉熵损失函数广泛用于分类模型，例如logistic回归。交叉熵损失函数随着预测与真实输出的偏差而增大。\u003c/li\u003e\n\u003cli\u003e在深度学习中，如卷积神经网络，最后输出softmax层经常使用交叉熵损失函数。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"互信息\"\u003e\u003ca href=\"#互信息\" class=\"headerlink\" title=\"互信息\"\u003e\u003c/a\u003e互信息\u003c/h2\u003e\u003ch3 id=\"初步理解-2\"\u003e\u003ca href=\"#初步理解-2\" class=\"headerlink\" title=\"初步理解\"\u003e\u003c/a\u003e初步理解\u003c/h3\u003e\u003cp\u003e互信息用于度量两个概率分布或随机变量间的相互独立性，通过它可以知道一个变量的信息有多少与另一个相关。\u003c/p\u003e\n\u003cp\u003e互信息显示了随机变量之间的相关性，比单纯的线性相关系数更一般化。\u003c/p\u003e\n\u003ch3 id=\"数学公式\"\u003e\u003ca href=\"#数学公式\" class=\"headerlink\" title=\"数学公式\"\u003e\u003c/a\u003e数学公式\u003c/h3\u003e\u003cp\u003e两个离散型随机变量X和Y的互信息定义为：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://public-tuchuang.oss-cn-hangzhou.aliyuncs.com/ai6_20200402131727.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cp\u003e其中p(x,y)是X和Y的联合概率分布，p(x)和p(y)分别是X和Y的边际概率分布。\u003c/p\u003e\n\u003ch3 id=\"应用-2\"\u003e\u003ca href=\"#应用-2\" class=\"headerlink\" title=\"应用\"\u003e\u003c/a\u003e应用\u003c/h3\u003e\u003cp\u003e\u003cimg src=\"https://public-tuchuang.oss-cn-hangzhou.aliyuncs.com/ai7_20200402131808.png\" alt=\"在贝叶斯网络中，变量之间的关系结构可以通过互信息来确定。\"/\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e特征选择：除了相关系数，还可以用互信息。相关系数仅限于线性相关，对非线性相关不适用，但是互信息则不然。零的相互独立性保证了随机变量是独立的，而零相关性则不是。\u003c/li\u003e\n\u003cli\u003e在贝叶斯网络总，互信息用于学习两个随机变量的之间的关系结构，并且定义这种关系的强度。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"KL散度\"\u003e\u003ca href=\"#KL散度\" class=\"headerlink\" title=\"KL散度\"\u003e\u003c/a\u003eKL散度\u003c/h2\u003e\u003cp\u003e也叫做相对熵\u003c/p\u003e\n\u003ch3 id=\"初步理解-3\"\u003e\u003ca href=\"#初步理解-3\" class=\"headerlink\" title=\"初步理解\"\u003e\u003c/a\u003e初步理解\u003c/h3\u003e\u003cp\u003eKL散度是另外一种度量两个随机分布相似度的方式，它度量的是一个分布相对另外一个分布的偏差。\u003c/p\u003e\n\u003cp\u003e假设有一些数据，其真实分布是P，但P是未知的，因此我们选择一个新的分布Q，来近似该数据。 由于Q只是一个近似值，因此它严格等于P，会发生一些信息丢失，此处的信息丢失就是由KL散度度量的。\u003c/p\u003e\n\u003cp\u003e当选择的P近似于Q时，P和Q之间的KL散度显示有多少信息丢失。\u003c/p\u003e\n\u003ch3 id=\"数学公式-1\"\u003e\u003ca href=\"#数学公式-1\" class=\"headerlink\" title=\"数学公式\"\u003e\u003c/a\u003e数学公式\u003c/h3\u003e\u003cp\u003e随机分布Q相对于随机分布的KL散度定义如下：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://public-tuchuang.oss-cn-hangzhou.aliyuncs.com/ai8_20200402131858.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003ch3 id=\"应用-3\"\u003e\u003ca href=\"#应用-3\" class=\"headerlink\" title=\"应用\"\u003e\u003c/a\u003e应用\u003c/h3\u003e\u003cp\u003eKL散度常用于无监督机器学习技术变分自编码器。\u003c/p\u003e\n\u003chr/\u003e\n\u003cp\u003e信息理论最初是由数学家和电气工程师克劳德·香农（Claude Shannon）在1948年发表的开创性论文《通信的数学理论》中提出的。\u003c/p\u003e\n\u003cp\u003e原文链接：\u003ca href=\"https://towardsdatascience.com/must-know-information-theory-concepts-in-deep-learning-ai-e54a5da9769d\" target=\"_blank\" rel=\"noopener\"\u003ehttps://towardsdatascience.com/must-know-information-theory-concepts-in-deep-learning-ai-e54a5da9769d\u003c/a\u003e\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e搜索技术问答的公众号：老齐教室\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cblockquote\u003e\n\u003cp\u003e在公众号中回复：\u003cstrong\u003e老齐\u003c/strong\u003e，可查看所有文章、书籍、课程。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp style=\"text-align:right\"\u003e\u003cstrong style=\"max-width: 100%;color: rgb(255, 255, 255);background-color: rgb(234, 6, 13);box-sizing: border-box !important;overflow-wrap: break-word !important;\"\u003e觉得好看，就点赞转发\u003c/strong\u003e\u003c/p\u003e\u003c/section\u003e\n    \u003c!-- Tags START --\u003e\n    \n      \u003cdiv class=\"tags\"\u003e\n        \u003cspan\u003eTags:\u003c/span\u003e\n        \n  \u003ca href=\"/tags#深度学习 熵\"\u003e\n    \u003cspan class=\"tag-code\"\u003e深度学习 熵\u003c/span\u003e\n  \u003c/a\u003e\n\n      \u003c/div\u003e\n    \n    \u003c!-- Tags END --\u003e\n    \u003c!-- NAV START --\u003e\n    \n  \u003cdiv class=\"nav-container\"\u003e\n    \u003c!-- reverse left and right to put prev and next in a more logic postition --\u003e\n    \n      \u003ca class=\"nav-left\" href=\"/2020/04/01/ml-pandas-read-html/\"\u003e\n        \u003cspan class=\"nav-arrow\"\u003e← \u003c/span\u003e\n        \n          用Pandas从HTML网页中读取数据\n        \n      \u003c/a\u003e\n    \n    \n      \u003ca class=\"nav-right\" href=\"/2020/04/04/ml-data-science-start/\"\u003e\n        \n          开启数据科学之旅\n        \n        \u003cspan class=\"nav-arrow\"\u003e →\u003c/span\u003e\n      \u003c/a\u003e\n    \n  \u003c/div\u003e\n\n    \u003c!-- NAV END --\u003e\n    \u003c!-- 打赏 START --\u003e\n    \n      \u003cdiv class=\"money-like\"\u003e\n        \u003cdiv class=\"reward-btn\"\u003e\n          赏\n          \u003cspan class=\"money-code\"\u003e\n            \u003cspan class=\"alipay-code\"\u003e\n              \u003cdiv class=\"code-image\"\u003e\u003c/div\u003e\n              \u003cb\u003e使用支付宝打赏\u003c/b\u003e\n            \u003c/span\u003e\n            \u003cspan class=\"wechat-code\"\u003e\n              \u003cdiv class=\"code-image\"\u003e\u003c/div\u003e\n              \u003cb\u003e使用微信打赏\u003c/b\u003e\n            \u003c/span\u003e\n          \u003c/span\u003e\n        \u003c/div\u003e\n        \u003cp class=\"notice\"\u003e若你觉得我的文章对你有帮助，欢迎点击上方按钮对我打赏\u003c/p\u003e\n      \u003c/div\u003e\n    \n    \u003c!-- 打赏 END --\u003e\n    \u003c!-- 二维码 START --\u003e\n    \u003c!--% if (theme.qrcode) { %--\u003e\n      \u003cdiv class=\"qrcode\"\u003e\n        \u003c!--canvas id=\"share-qrcode\"\u003e\u003c/!--canvas--\u003e\n        \u003cimg src=\"https://public-tuchuang.oss-cn-hangzhou.aliyuncs.com/WechatIMG6_20200109154827.jpeg\" width=\"400\"/\u003e\n        \u003cp class=\"notice\"\u003e关注微信公众号，读文章、听课程，提升技能\u003c/p\u003e\n      \u003c/div\u003e\n    \u003c!--% } %--\u003e\n    \u003c!-- 二维码 END --\u003e\n    \n      \u003c!-- No Comment --\u003e\n    \n  \u003c/article\u003e",
  "Date": "2020-04-03T00:00:00Z",
  "Author": "老齐教室"
}