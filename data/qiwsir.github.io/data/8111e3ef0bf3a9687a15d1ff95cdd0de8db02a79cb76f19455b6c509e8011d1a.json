{
  "Source": "qiwsir.github.io",
  "Title": "根据 2D 图片构建 3D 模型",
  "Link": "https://qiwsir.github.io/2021/09/29/create-3d-model-from-2d-image/",
  "Content": "\u003carticle class=\"post-article\"\u003e\n    \u003ch2\u003e根据 2D 图片构建 3D 模型\u003c/h2\u003e\n    \u003cp class=\"post-date\"\u003e2021-09-29\u003c/p\u003e\n    \u003csection class=\"markdown-content\"\u003e\u003cp\u003e近年来，深度学习（Deep Learning，DL）在解决图像分类、目标检测、语义分割等 2D 图像任务方面表现出了卓越的能力。在 3D 图形问题方面，DL 的应用也取得了巨大的进展。在这篇文章中，我们将探讨一个最新尝试：将 DL 应用于单个图像的 3D 建模上，这是 3D 计算机图形学领域最重要和最严峻的挑战之一。\u003c/p\u003e\n\u003ch2 id=\"任务\"\u003e\u003ca href=\"#任务\" class=\"headerlink\" title=\"任务\"\u003e\u003c/a\u003e任务\u003c/h2\u003e\u003cp\u003e\u003cimg src=\"https://gitee.com/qiwsir/images/raw/master/2021-9-29/1632907850633-d1.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cp\u003e一张图片，就是 3D 物体的 2D 平面投影，所以，从高维空间向低维空间转换过程中，必然会丢失一些数据。因此，从单一视图的 2D 图像中，永远不会有足够的数据来构建其 3D 模型。\u003c/p\u003e\n\u003cp\u003e所以，要实现从 2D 图像到 3D 模型的创建，必须对原来的 3D 物体本身有\u003cstrong\u003e先验知识\u003c/strong\u003e。\u003c/p\u003e\n\u003cp\u003e在 2D 深度学习中，卷积自动编码器是学习输入图像的压缩表”的非常有效的方法。将此架构扩展为学习紧凑的形状知识是将深度学习应用于 3D 数据的最有前途的方法。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://gitee.com/qiwsir/images/raw/master/2021-9-29/1632907914605-d2.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003ch2 id=\"3D数据表达\"\u003e\u003ca href=\"#3D数据表达\" class=\"headerlink\" title=\"3D数据表达\"\u003e\u003c/a\u003e3D数据表达\u003c/h2\u003e\u003cp\u003e\u003cimg src=\"https://gitee.com/qiwsir/images/raw/master/2021-9-29/1632907983869-d3.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cp\u003e2D 图像在计算机中只有一种通用格式（像素），与之不同的是：3D 数据可以用许多不同的数字格式来表示。这些表示方法各有优缺点，因此数据呈现方式的选择直接影响到可使用的方法。  \u003c/p\u003e\n\u003ch3 id=\"栅格化形式（体积网格，Voxel）：可以直接应用-CNN\"\u003e\u003ca href=\"#栅格化形式（体积网格，Voxel）：可以直接应用-CNN\" class=\"headerlink\" title=\"栅格化形式（体积网格，Voxel）：可以直接应用 CNN\"\u003e\u003c/a\u003e栅格化形式（体积网格，Voxel）：可以直接应用 CNN\u003c/h3\u003e\u003cp\u003e\u003cimg src=\"https://gitee.com/qiwsir/images/raw/master/2021-9-29/1632908050371-d4.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e立体像素\u003c/strong\u003e的英文 voxel 是 volumetric pixel 的缩写，是空间网格像素到体积网格立体像素的直接扩展。每个立体像素的局部性共同定义了该体积数据的独特结构，因此卷积神经网络（CNN）的局部性假设在体积格式中仍然成立。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://gitee.com/qiwsir/images/raw/master/2021-9-29/1632908118714-d5.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cp\u003e然而，这种表示是稀疏和浪费的。随着分辨率的增加，有用的立体像素的密度会降低。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cstrong\u003e优点：\u003c/strong\u003e可以直接将 2D 表达中的 CNN 应用到 3D 表达。\u003c/li\u003e\n\u003cli\u003e\u003cstrong\u003e缺点\u003c/strong\u003e：表达方式浪费，必须要权衡计算资源。\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"几何形式：不能直接应用-CNN\"\u003e\u003ca href=\"#几何形式：不能直接应用-CNN\" class=\"headerlink\" title=\"几何形式：不能直接应用 CNN\"\u003e\u003c/a\u003e几何形式：不能直接应用 CNN\u003c/h3\u003e\u003cp\u003e\u003cimg src=\"https://gitee.com/qiwsir/images/raw/master/2021-9-29/1632908190962-d6.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e多边形网格\u003c/strong\u003e：是顶点、边和面的集合，可用于定义三维表面。它可以相当紧凑颗粒形式表达中细节。\u003c/p\u003e\n\u003cp\u003e\u003cstrong\u003e点云\u003c/strong\u003e：3D 坐标（x、y、z）中的点的集合，这些点共同形成一个类似于 3D 对象形状的云。点的集合越大，得到的细节就越多。不同顺序的相同点集仍然表示相同的 3D 对象。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e优点：\u003c/strong\u003e紧凑的表现形式，关注 3D 对象的表面细节。\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e\u003cstrong\u003e缺点：\u003c/strong\u003e无法直接应用 CNN。\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cfigure class=\"highlight python\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e2\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e3\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e4\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"comment\"\u003e# point_cloud1 and point_cloud2 represent the same 3D structure\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"comment\"\u003e# even though they are represented differently in memory\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003epoint_cloud1 = [(x1, y1, z1), (x2, y2, z2),..., (xn, yn, zn)]\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003epoint_cloud2 = [(x2, y2, z2), (x1, y1, z1),..., (xn, yn, zn)]\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003ch2 id=\"方法\"\u003e\u003ca href=\"#方法\" class=\"headerlink\" title=\"方法\"\u003e\u003c/a\u003e方法\u003c/h2\u003e\u003cp\u003e以下方法参考论文：\u003ca href=\"https://arxiv.org/abs/1706.07036\" target=\"_blank\" rel=\"noopener\"\u003eLearning Efficient Point Cloud Generation for Dense 3D Object Reconstruction\u003c/a\u003e，论文网址：\u003ca href=\"https://arxiv.org/abs/1706.07036\" target=\"_blank\" rel=\"noopener\"\u003ehttps://arxiv.org/abs/1706.07036\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e下面的方法结合了点云的优点，但使用传统的 2D 卷积神经网络来学习先验的形状知识。  \u003c/p\u003e\n\u003ch3 id=\"2D-结构生成器\"\u003e\u003ca href=\"#2D-结构生成器\" class=\"headerlink\" title=\"2D 结构生成器\"\u003e\u003c/a\u003e2D 结构生成器\u003c/h3\u003e\u003cp\u003e\u003cimg src=\"https://gitee.com/qiwsir/images/raw/master/2021-9-29/1632908284941-d7.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cp\u003e建立一个标准的 2D CNN 结构生成器，用于学习对象的先验形状知识。“立体像素方法”是不可取的，因为它效率低下，而且不可能用 CNN 直接学习点云。因此，我们将学习从单个图像到一个点云的 2D 投影的映射，并定义为： \u003ccode\u003e2D projection == 3D coordinates (x,y,z) + binary mask (m)\u003c/code\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cp\u003e输入：单个 RGB 图像\u003c/p\u003e\n\u003c/li\u003e\n\u003cli\u003e\u003cp\u003e输出：\u003cstrong\u003e预测\u003c/strong\u003e视点处的 2D 投影。\u003c/p\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003cfigure class=\"highlight python\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e2\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e3\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e4\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e5\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e6\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e7\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e8\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e9\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e10\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e11\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e12\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e13\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e14\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e15\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"comment\"\u003e#--------- Pytorch pseudo-code for Structure Generator ---------#\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"class\"\u003e\u003cspan class=\"keyword\"\u003eclass\u003c/span\u003e \u003cspan class=\"title\"\u003eStructure_Generator\u003c/span\u003e\u003cspan class=\"params\"\u003e(nn.Module)\u003c/span\u003e:\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e    \u003cspan class=\"comment\"\u003e# contains two module in sequence, an encoder and a decoder\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e    \u003cspan class=\"function\"\u003e\u003cspan class=\"keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"title\"\u003e__init__\u003c/span\u003e\u003cspan class=\"params\"\u003e(self)\u003c/span\u003e:\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        self.encoder = Encoder()\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        self.decoder = Decoder()    \u003cspan class=\"function\"\u003e\u003cspan class=\"keyword\"\u003edef\u003c/span\u003e \u003cspan class=\"title\"\u003eforward\u003c/span\u003e\u003cspan class=\"params\"\u003e(self, RGB_image)\u003c/span\u003e:\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        \u003cspan class=\"comment\"\u003e# Encoder takes in one RGB image and \u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        \u003cspan class=\"comment\"\u003e# output an encoded deep shape-embedding\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        shape_embedding = self.encoder(RGB_image)\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        \u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        \u003cspan class=\"comment\"\u003e# Decoder takes the encoded values and output  \u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        \u003cspan class=\"comment\"\u003e# multiples 2D projection (XYZ + mask)\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e        XYZ, maskLogit = self.decoder(shape_embedding)\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e \u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e       \u003cspan class=\"keyword\"\u003ereturn\u003c/span\u003e XYZ, maskLogit\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003ch3 id=\"点云融合\"\u003e\u003ca href=\"#点云融合\" class=\"headerlink\" title=\"点云融合\"\u003e\u003c/a\u003e点云融合\u003c/h3\u003e\u003cp\u003e\u003cimg src=\"https://gitee.com/qiwsir/images/raw/master/2021-9-29/1632908403646-d8.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cp\u003e将预测的 2D 投影融合到 3D 点云数据中。这是可以实现的，因为这些预测的视点是固定的，并且是事先知道的。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e输入：\u003cstrong\u003e预测\u003c/strong\u003e视点处的 2D 投影\u003c/li\u003e\n\u003cli\u003e输出：点云\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"伪渲染器\"\u003e\u003ca href=\"#伪渲染器\" class=\"headerlink\" title=\"伪渲染器\"\u003e\u003c/a\u003e伪渲染器\u003c/h3\u003e\u003cp\u003e\u003cimg src=\"https://gitee.com/qiwsir/images/raw/master/2021-9-29/1632908503796-d9.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cp\u003e可以推断，应该有必要将预测的 2D 投影融合的点云中。那么，如果我们从新视点渲染不同的 2D 投影，它也应该类似于真实 3D 模型的投影。 \u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e输入：点云\u003c/li\u003e\n\u003cli\u003e输出：在\u003cstrong\u003e新\u003c/strong\u003e视点处的有深度的图像\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch3 id=\"动态训练\"\u003e\u003ca href=\"#动态训练\" class=\"headerlink\" title=\"动态训练\"\u003e\u003c/a\u003e动态训练\u003c/h3\u003e\u003cp\u003e\u003cimg src=\"https://gitee.com/qiwsir/images/raw/master/2021-9-29/1632908558207-d10.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cp\u003e将上述三部分结合在一起，我们获得了一个端到端的模型，此模型可以用 2D 卷积结构生成器，将单个 2D 图像生成紧凑的点云。\u003c/p\u003e\n\u003cp\u003e该模型的巧妙之处在于使 “融合+伪渲染”模块变成完全可微，其几何解释：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e几何代数（详细参考：\u003ca href=\"https://en.wikipedia.org/wiki/Geometric_algebra）意味着没有可学习的参数，使模型尺寸更小，更易于训练。\" target=\"_blank\" rel=\"noopener\"\u003ehttps://en.wikipedia.org/wiki/Geometric_algebra）意味着没有可学习的参数，使模型尺寸更小，更易于训练。\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e微分意味着可以计算反向传播的梯度，从而可以使用 2D 投影的损失来学习生成 3D 点云。\u003c/li\u003e\n\u003c/ul\u003e\n\u003cfigure class=\"highlight python\"\u003e\u003ctable\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd class=\"gutter\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e1\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e2\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e3\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e4\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e5\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e6\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e7\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e8\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e9\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e10\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e11\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e12\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e13\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e14\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003ctd class=\"code\"\u003e\u003cpre\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"comment\"\u003e# --------- Pytorch pseudo-code for training loop ----------## Create 2D Conv Structure generator\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003emodel = Structure_Generator()\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"comment\"\u003e# only need to learn the 2D structure optimizer\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003eoptimizer = optim.SGD(model.parameters())\u003cspan class=\"comment\"\u003e# 2D projections from predetermined viewpoints\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003eXYZ, maskLogit = model(RGB_images)\u003cspan class=\"comment\"\u003e# fused point cloud\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"comment\"\u003e#fuseTrans is predetermined viewpoints info\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003eXYZid, ML = fuse3D(XYZ, maskLogit, fuseTrans)\u003cspan class=\"comment\"\u003e# Render new depth images at novel viewpoints\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"comment\"\u003e# renderTrans is novel viewpoints info\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003enewDepth, newMaskLogit, collision = render2D(XYZid, ML, renderTrans)\u003cspan class=\"comment\"\u003e# Compute loss between novel view and ground truth\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003eloss_depth = L1Loss()(newDepth, GTDepth)\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003eloss_mask = BCEWithLogitLoss()(newMaskLogit, GTMask)\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003eloss_total = loss_depth + loss_mask\u003cspan class=\"comment\"\u003e# Back-propagation to update Structure Generator\u003c/span\u003e\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003eloss_total.backward()\u003c/span\u003e\u003cbr/\u003e\u003cspan class=\"line\"\u003eoptimizer.step()\u003c/span\u003e\u003cbr/\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003c/figure\u003e\n\n\u003ch2 id=\"结果\"\u003e\u003ca href=\"#结果\" class=\"headerlink\" title=\"结果\"\u003e\u003c/a\u003e结果\u003c/h2\u003e\u003cul\u003e\n\u003cli\u003e基于真实 3D 模型的深度图像与基于学习点云模型的渲染深度图像的比较。 \u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"./result.gif\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e最终结果：来自单个 RBG 图像→ 3D点云\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e\u003cimg src=\"https://gitee.com/qiwsir/images/raw/master/2021-9-29/1632908754242-d12.gif\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cp\u003e本文的源码、论文和项目地址：\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003ePytorch 源码：\u003ca href=\"https://github.com/lkhphuc/pytorch-3d-point-cloud-generation\" target=\"_blank\" rel=\"noopener\"\u003ehttps://github.com/lkhphuc/pytorch-3d-point-cloud-generation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eTensorflow 源码：\u003ca href=\"https://github.com/chenhsuanlin/3D-point-cloud-generation\" target=\"_blank\" rel=\"noopener\"\u003ehttps://github.com/chenhsuanlin/3D-point-cloud-generation\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e论文：\u003ca href=\"https://arxiv.org/abs/1706.07036\" target=\"_blank\" rel=\"noopener\"\u003ehttps://arxiv.org/abs/1706.07036\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e项目地址：\u003ca href=\"https://chenhsuanlin.bitbucket.io/3D-point-cloud-generation/\" target=\"_blank\" rel=\"noopener\"\u003ehttps://chenhsuanlin.bitbucket.io/3D-point-cloud-generation/\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"参考文献\"\u003e\u003ca href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"\u003e\u003c/a\u003e参考文献\u003c/h2\u003e\u003cp\u003e\u003ca href=\"https://medium.com/vitalify-asia/create-3d-model-from-a-single-2d-image-in-pytorch-917aca00bb07\" target=\"_blank\" rel=\"noopener\"\u003ehttps://medium.com/vitalify-asia/create-3d-model-from-a-single-2d-image-in-pytorch-917aca00bb07\u003c/a\u003e\u003c/p\u003e\n\u003c/section\u003e\n    \u003c!-- Tags START --\u003e\n    \n    \u003c!-- Tags END --\u003e\n    \u003c!-- NAV START --\u003e\n    \n  \u003cdiv class=\"nav-container\"\u003e\n    \u003c!-- reverse left and right to put prev and next in a more logic postition --\u003e\n    \n      \u003ca class=\"nav-left\" href=\"/2021/09/15/compressandarchivefile/\"\u003e\n        \u003cspan class=\"nav-arrow\"\u003e← \u003c/span\u003e\n        \n          用 Python 压缩文件方法汇总\n        \n      \u003c/a\u003e\n    \n    \n      \u003ca class=\"nav-right\" href=\"/2021/10/08/geometrical-matrices/\"\u003e\n        \n          从几何角度理解矩阵\n        \n        \u003cspan class=\"nav-arrow\"\u003e →\u003c/span\u003e\n      \u003c/a\u003e\n    \n  \u003c/div\u003e\n\n    \u003c!-- NAV END --\u003e\n    \u003c!-- 打赏 START --\u003e\n    \n      \u003cdiv class=\"money-like\"\u003e\n        \u003cdiv class=\"reward-btn\"\u003e\n          赏\n          \u003cspan class=\"money-code\"\u003e\n            \u003cspan class=\"alipay-code\"\u003e\n              \u003cdiv class=\"code-image\"\u003e\u003c/div\u003e\n              \u003cb\u003e使用支付宝打赏\u003c/b\u003e\n            \u003c/span\u003e\n            \u003cspan class=\"wechat-code\"\u003e\n              \u003cdiv class=\"code-image\"\u003e\u003c/div\u003e\n              \u003cb\u003e使用微信打赏\u003c/b\u003e\n            \u003c/span\u003e\n          \u003c/span\u003e\n        \u003c/div\u003e\n        \u003cp class=\"notice\"\u003e若你觉得我的文章对你有帮助，欢迎点击上方按钮对我打赏\u003c/p\u003e\n      \u003c/div\u003e\n    \n    \u003c!-- 打赏 END --\u003e\n    \u003c!-- 二维码 START --\u003e\n    \u003c!--% if (theme.qrcode) { %--\u003e\n      \u003cdiv class=\"qrcode\"\u003e\n        \u003c!--canvas id=\"share-qrcode\"\u003e\u003c/!--canvas--\u003e\n        \u003cimg src=\"https://public-tuchuang.oss-cn-hangzhou.aliyuncs.com/WechatIMG6_20200109154827.jpeg\" width=\"400\"/\u003e\n        \u003cp class=\"notice\"\u003e关注微信公众号，读文章、听课程，提升技能\u003c/p\u003e\n      \u003c/div\u003e\n    \u003c!--% } %--\u003e\n    \u003c!-- 二维码 END --\u003e\n    \n      \u003c!-- No Comment --\u003e\n    \n  \u003c/article\u003e",
  "Date": "2021-09-29T00:00:00Z",
  "Author": "老齐教室"
}