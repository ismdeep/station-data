{
  "Source": "qiwsir.github.io",
  "Title": "交叉熵损失函数",
  "Link": "https://qiwsir.github.io/2021/02/13/cross-entropy/",
  "Content": "\u003carticle class=\"post-article\"\u003e\n    \u003ch2\u003e交叉熵损失函数\u003c/h2\u003e\n    \u003cp class=\"post-date\"\u003e2021-02-13\u003c/p\u003e\n    \u003csection class=\"markdown-content\"\u003e\u003cp\u003e\u003cstrong\u003e注：\u003c/strong\u003e 本文内容是对《机器学习数学基础》一书有关内容的补充资料。《机器学习数学基础》即将由电子工业出版社于2021年5月出版。与本书相关的更多资料，请查阅微信公众号：\u003cstrong\u003e老齐教室\u003c/strong\u003e，或者：\u003ca href=\"https://qiwsir.gitee.io/mathmetics/\" target=\"_blank\" rel=\"noopener\"\u003ehttps://qiwsir.gitee.io/mathmetics/\u003c/a\u003e \u003c/p\u003e\n\u003chr/\u003e\n\u003cp\u003e在研究机器学习或深度学习问题时，损失函数或者代价函数——关于两者的区别，请参阅《机器学习数学基础》中的详细说明——是必不可少的，它们主要用以优化训练模型。目标就是让损失函数最小化，损失越小的模型越好。交叉熵损失函数，就是众多损失函数中重要一员，它主要用与对分类模型的优化。为了理解交叉熵损失函数，以及为什么同时用Softmax作为激活函数，特别撰写本文。\u003c/p\u003e\n\u003cp\u003e下面我们使用一个图像分类的示例，这个示例中包括狗、猫、马和豹。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://gitee.com/qiwsir/images/raw/master/2021-2-13/1613173338441-image1.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cp\u003e如上图所示，以Softmax函数作为激活函数，交叉熵损失函数旨在度量预测值（$P$）与真实值之间的差距，如下图所示。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://gitee.com/qiwsir/images/raw/master/2021-2-13/1613173585431-image2.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cp\u003e例如，如果输入图片是狗，其真实值为 $[1,0,0,0]$ ，但通过深度学习模型，得到的预测值为 $[0.775, 0.116, 0.039, 0.070]$ 。我们的目标就是要让输出的预测值与真实值之间尽可能地靠近。在模型训练过程中，将模型权重进行迭代调整，以最大程度地减少交叉熵损失。 权重的调整过程就是模型训练过程，并且随着模型的不断训练和损失的最小化，这就是机器学习中所说的学习过程。\u003c/p\u003e\n\u003cp\u003e交叉熵的概念起源于信息论，香农（\u003ca href=\"https://en.wikipedia.org/wiki/Claude_Shannon\" target=\"_blank\" rel=\"noopener\"\u003eClaude Shannon\u003c/a\u003e）在1948年创立了信息论，其中最重要的概念就是信息熵，所以，在学习交叉熵之前，要先了解信息熵——不过，下面仅仅是列出信息熵的基本概念，因为在《机器学习数学基础》一书中，有专门章节讨论信息熵的有关知识。\u003c/p\u003e\n\u003ch2 id=\"熵\"\u003e\u003ca href=\"#熵\" class=\"headerlink\" title=\"熵\"\u003e\u003c/a\u003e熵\u003c/h2\u003e\u003cp\u003e随机变量 $X$ 的熵定义：\u003c/p\u003e\n\u003cp\u003e$H(X)=\\begin{cases}-\\sum_xp(x)\\log(p(x)),\\quad \u0026amp;X是离散型随机变量\\-\\int_xp(x)\\log(p(x)),\u0026amp;X是连续型随机变量\\end{cases}$\u003c/p\u003e\n\u003cp\u003e关于熵的更多内容，请参阅《机器学习数学基础》（2021年5月，电子工业出版社出版）。\u003c/p\u003e\n\u003ch2 id=\"交叉熵损失函数\"\u003e\u003ca href=\"#交叉熵损失函数\" class=\"headerlink\" title=\"交叉熵损失函数\"\u003e\u003c/a\u003e交叉熵损失函数\u003c/h2\u003e\u003cp\u003e交叉熵损失函数，也称为对数损失或者logistic损失。当模型产生了预测值之后，将对类别的预测概率与真实值（由 $0$ 或 $1$ 组成）进行不比较，计算所产生的损失，然后基于此损失设置对数形式的惩罚项。\u003c/p\u003e\n\u003cp\u003e在训练模型的时候，使用交缠上损失函数，目的是最小化损失，即损失越小的模型越好。最理想的就是交叉熵损失函数为 $0$ 。\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e定义\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e$L_{CE}=-\\sum_{i=1}^nt_i\\log(p_i)$\u003c/p\u003e\n\u003cp\u003e其中，$n$ 是类别的数量，$t_i$ 是某个类别的真实值，$p_i$ 是该了别的预测概率。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e一般情况下，取以 $2$ 为底的对数进行计算。\u003c/p\u003e\n\u003ch3 id=\"二分类交叉熵损失函数\"\u003e\u003ca href=\"#二分类交叉熵损失函数\" class=\"headerlink\" title=\"二分类交叉熵损失函数\"\u003e\u003c/a\u003e二分类交叉熵损失函数\u003c/h3\u003e\u003cp\u003e对于二分类问题，由于分类结果服从伯努利分布（参阅《机器学习数学基础》），所以二分类交叉熵损失函数定义为：\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cstrong\u003e定义\u003c/strong\u003e\u003c/p\u003e\n\u003cp\u003e$L=-\\sum_{i=1}^nt_i\\log(p_i)=-[t\\log(p)+(1-t)\\log(1-p)]$\u003c/p\u003e\n\u003cp\u003e其中，$t_i$ 是某类别的真实值，取值为 $0$ 或 $1$ ；$p_i$ 为某类别的预测概率。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e在二分类问题中，通常计算所有样本的平均交叉熵损失：\u003c/p\u003e\n\u003cp\u003e$L=-\\frac{1}{N}\\left[\\sum_{j=1}^Nt_j\\log(p_j)+(1-t_j)\\log(1-p_j)\\right]$\u003c/p\u003e\n\u003cp\u003e其中，$N$ 为样本数量，$t_j$ 为第 $j$ 个样本的真实类别值，$p_j$ 为相应样本的预测概率。\u003c/p\u003e\n\u003cp\u003e以前面提到的图片识别为例，$S$ 表示预测结果，$T$ 表示真实标签，如下图所示。\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://gitee.com/qiwsir/images/raw/master/2021-2-13/1613173585431-image2.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cp\u003e根据上面的数据，计算两者之间的交叉熵：\u003c/p\u003e\n\u003cp\u003e$\\begin{split}L\u0026amp;=-\\sum_{i=1}^4T_i\\log(S_i)\\\u0026amp;=-[1\\log_2(0.775)+0\\log_2(0.116)+0\\log_2(0.039)+0\\log_2(0.070)]\\\u0026amp;=-\\log_2(0.775)\\\u0026amp;=0.3677\\end{split}$\u003c/p\u003e\n\u003cp\u003e在神经网络中，所使用的Softmax函数是连续可导函数，这使得可以计算出损失函数相对于神经网络中每个权重的导数（在《机器学习数学基础》中有对此的完整推导过程和案例，读者可以理解其深层含义，请参阅）。这样就可以相应地调整模型的权重以最小化损失函数（模型输出接近真实值）。\u003c/p\u003e\n\u003cp\u003e假设经过权重调整之后，其输出值变为：\u003c/p\u003e\n\u003cp\u003e\u003cimg src=\"https://gitee.com/qiwsir/images/raw/master/2021-2-13/1613176751441-image3.png\" alt=\"\"/\u003e\u003c/p\u003e\n\u003cp\u003e用上面方法，可以容易计算出，这次交叉熵损失比原来小了。\u003c/p\u003e\n\u003cp\u003e在(Keras)[\u003ca href=\"https://keras.io/zh/]（一种高级神经网络接口，Google的TensorFlow在其核心库中已经支持Keras[2]）中提供了多种交叉熵损失函数：\" target=\"_blank\" rel=\"noopener\"\u003ehttps://keras.io/zh/]（一种高级神经网络接口，Google的TensorFlow在其核心库中已经支持Keras[2]）中提供了多种交叉熵损失函数：\u003c/a\u003e\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e二分类\u003c/li\u003e\n\u003cli\u003e多分类\u003c/li\u003e\n\u003cli\u003e稀疏类别\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e关于交叉熵损失函数的更多内容，建议参阅《机器学习数学基础》中的详细说明，本书于2021年5月由电子工业出版社出版。\u003c/p\u003e\n\u003ch2 id=\"参考文献\"\u003e\u003ca href=\"#参考文献\" class=\"headerlink\" title=\"参考文献\"\u003e\u003c/a\u003e参考文献\u003c/h2\u003e\u003cp\u003e[1]. 齐伟.机器学习数学基础.北京：电子工业出版社，2021\u003c/p\u003e\n\u003cp\u003e[2]. \u003ca href=\"https://zh.wikipedia.org/wiki/Keras\" target=\"_blank\" rel=\"noopener\"\u003ehttps://zh.wikipedia.org/wiki/Keras\u003c/a\u003e\u003c/p\u003e\n\u003cp\u003e[3]. \u003ca href=\"https://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e\" target=\"_blank\" rel=\"noopener\"\u003ehttps://towardsdatascience.com/cross-entropy-loss-function-f38c4ec8643e\u003c/a\u003e\u003c/p\u003e\n\u003c/section\u003e\n    \u003c!-- Tags START --\u003e\n    \n    \u003c!-- Tags END --\u003e\n    \u003c!-- NAV START --\u003e\n    \n  \u003cdiv class=\"nav-container\"\u003e\n    \u003c!-- reverse left and right to put prev and next in a more logic postition --\u003e\n    \n      \u003ca class=\"nav-left\" href=\"/2021/02/12/trig-function/\"\u003e\n        \u003cspan class=\"nav-arrow\"\u003e← \u003c/span\u003e\n        \n          三角函数\n        \n      \u003c/a\u003e\n    \n    \n      \u003ca class=\"nav-right\" href=\"/2021/02/14/python-packages/\"\u003e\n        \n          Python第三方包的那些事\n        \n        \u003cspan class=\"nav-arrow\"\u003e →\u003c/span\u003e\n      \u003c/a\u003e\n    \n  \u003c/div\u003e\n\n    \u003c!-- NAV END --\u003e\n    \u003c!-- 打赏 START --\u003e\n    \n      \u003cdiv class=\"money-like\"\u003e\n        \u003cdiv class=\"reward-btn\"\u003e\n          赏\n          \u003cspan class=\"money-code\"\u003e\n            \u003cspan class=\"alipay-code\"\u003e\n              \u003cdiv class=\"code-image\"\u003e\u003c/div\u003e\n              \u003cb\u003e使用支付宝打赏\u003c/b\u003e\n            \u003c/span\u003e\n            \u003cspan class=\"wechat-code\"\u003e\n              \u003cdiv class=\"code-image\"\u003e\u003c/div\u003e\n              \u003cb\u003e使用微信打赏\u003c/b\u003e\n            \u003c/span\u003e\n          \u003c/span\u003e\n        \u003c/div\u003e\n        \u003cp class=\"notice\"\u003e若你觉得我的文章对你有帮助，欢迎点击上方按钮对我打赏\u003c/p\u003e\n      \u003c/div\u003e\n    \n    \u003c!-- 打赏 END --\u003e\n    \u003c!-- 二维码 START --\u003e\n    \u003c!--% if (theme.qrcode) { %--\u003e\n      \u003cdiv class=\"qrcode\"\u003e\n        \u003c!--canvas id=\"share-qrcode\"\u003e\u003c/!--canvas--\u003e\n        \u003cimg src=\"https://public-tuchuang.oss-cn-hangzhou.aliyuncs.com/WechatIMG6_20200109154827.jpeg\" width=\"400\"/\u003e\n        \u003cp class=\"notice\"\u003e关注微信公众号，读文章、听课程，提升技能\u003c/p\u003e\n      \u003c/div\u003e\n    \u003c!--% } %--\u003e\n    \u003c!-- 二维码 END --\u003e\n    \n      \u003c!-- No Comment --\u003e\n    \n  \u003c/article\u003e",
  "Date": "2021-02-13T00:00:00Z",
  "Author": "老齐教室"
}