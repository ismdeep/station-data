{
  "Source": "raphlinus.github.io",
  "Title": "An interactive review of Oklab",
  "Link": "https://raphlinus.github.io/color/2021/01/18/oklab-critique.html",
  "Content": "\u003cdiv class=\"post-content e-content\" itemprop=\"articleBody\"\u003e\n    \u003cscript type=\"text/x-mathjax-config\"\u003e\n\tMathJax.Hub.Config({\n\t\ttex2jax: {\n\t\t\tinlineMath: [['$', '$']]\n\t\t}\n\t});\n\u003c/script\u003e\n\n\u003cscript src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML\" type=\"text/javascript\"\u003e\u003c/script\u003e\n\n\u003cp\u003e\u003cstrong\u003eUpdate 2021-01-29:\u003c/strong\u003e See the \u003ca href=\"#update-2021-01-29\"\u003eupdate\u003c/a\u003e below for more color spaces and discussion.\u003c/p\u003e\n\n\u003cp\u003eBjörn Ottosson recenty published a blog post introducing \u003ca href=\"https://bottosson.github.io/posts/oklab/\"\u003eOklab\u003c/a\u003e. The blog claimed that Oklab is a better perceptual color space than what came before. It piqued my interest, and I wanted to see for myself.\u003c/p\u003e\n\n\u003cp\u003eIn exploring perceptual color spaces, I find an interactive gradient tool to be invaluable, so I’ve reproduced one here:\u003c/p\u003e\n\n\u003cstyle\u003e\n    .gradient {\n        display: flex;\n        align-items: center;\n        justify-content: center;\n        flex-wrap: wrap;\n    }\n    .gradname {\n        width: 5em;\n    }\n    .sliders {\n        display: flex;\n        flex-wrap: wrap;\n        justify-content: space-evenly;\n    }\n    .cluster {\n        margin: 10px;\n    }\n    .axis {\n        display: flex;\n        flex-wrap: wrap;\n    }\n    .axis div:nth-child(1) {\n        width: 2em;\n    }\n    .axis div:nth-child(2) {\n        width: 2em;\n    }\n    .buttonrow {\n        display: flex;\n        flex-wrap: wrap;\n        justify-content: center;\n        margin-top: 5px;\n        margin-bottom: 5px;\n    }\n    .buttonrow button {\n        margin: 0 5px;\n    }\n    .quantize {\n        display: flex;\n        flex-wrap: wrap;\n        margin-bottom: 10px;\n        justify-content: center;\n    }\n    .quantize div {\n        margin: 0 10px;\n    }\n    .gradients \u003e details[open] \u003e summary {\n        display: none;\n    }\n\u003c/style\u003e\n\n\u003cdiv class=\"gradients\"\u003e\n\n\u003cdiv class=\"gradient\"\u003e\n\u003cdiv class=\"gradname\"\u003esRGB\u003c/div\u003e\n\u003cdiv\u003e\u003ccanvas width=\"480\" height=\"50\" id=\"c0\"\u003e\u003c/canvas\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"gradient\"\u003e\n\u003cdiv class=\"gradname\"\u003eCIELAB\u003c/div\u003e\n\u003cdiv\u003e\u003ccanvas width=\"480\" height=\"50\" id=\"c1\"\u003e\u003c/canvas\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"gradient\"\u003e\n\u003cdiv class=\"gradname\"\u003eIPT\u003c/div\u003e\n\u003cdiv\u003e\u003ccanvas width=\"480\" height=\"50\" id=\"c2\"\u003e\u003c/canvas\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"gradient\"\u003e\n\u003cdiv class=\"gradname\"\u003eOklab\u003c/div\u003e\n\u003cdiv\u003e\u003ccanvas width=\"480\" height=\"50\" id=\"c3\"\u003e\u003c/canvas\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"gradient\"\u003e\n\u003cdiv class=\"gradname\"\u003eICtCp\u003c/div\u003e\n\u003cdiv\u003e\u003ccanvas width=\"480\" height=\"50\" id=\"c4\"\u003e\u003c/canvas\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003cdetails\u003e\n\u003csummary\u003eShow more color spaces (added after the initial publication of the article, for further comparisons)\u003c/summary\u003e\n\u003cdiv class=\"gradient\"\u003e\n\u003cdiv class=\"gradname\"\u003eXYB\u003c/div\u003e\n\u003cdiv\u003e\u003ccanvas width=\"480\" height=\"50\" id=\"c5\"\u003e\u003c/canvas\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"gradient\"\u003e\n\u003cdiv class=\"gradname\"\u003eSRLAB2\u003c/div\u003e\n\u003cdiv\u003e\u003ccanvas width=\"480\" height=\"50\" id=\"c6\"\u003e\u003c/canvas\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"gradient\"\u003e\n\u003cdiv class=\"gradname\"\u003eLinear\u003c/div\u003e\n\u003cdiv\u003e\u003ccanvas width=\"480\" height=\"50\" id=\"c7\"\u003e\u003c/canvas\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003c/details\u003e\n\u003c/div\u003e\n\n\u003c!--Jerry-rigged color picker--\u003e\n\u003cdiv class=\"buttonrow\"\u003e\n    \u003cbutton id=\"randomize\" type=\"button\"\u003eRandom\u003c/button\u003e\n    \u003cbutton id=\"q0\"\u003e\u003cspan style=\"background: #00f\"\u003e \u003c/span\u003e\u003cspan style=\"background: #fff\"\u003e \u003c/span\u003e\u003c/button\u003e\n    \u003cbutton id=\"q1\"\u003e\u003cspan style=\"background: #000\"\u003e \u003c/span\u003e\u003cspan style=\"background: #fff\"\u003e \u003c/span\u003e\u003c/button\u003e\n    \u003cbutton id=\"q2\"\u003e\u003cspan style=\"background: #001\"\u003e \u003c/span\u003e\u003cspan style=\"background: #fff\"\u003e \u003c/span\u003e\u003c/button\u003e\n    \u003cbutton id=\"q3\"\u003e\u003cspan style=\"background: #00f\"\u003e \u003c/span\u003e\u003cspan style=\"background: #ff0\"\u003e \u003c/span\u003e\u003c/button\u003e\n    \u003cbutton id=\"q4\"\u003e\u003cspan style=\"background: #f00\"\u003e \u003c/span\u003e\u003cspan style=\"background: #00f\"\u003e \u003c/span\u003e\u003c/button\u003e\n    \u003cbutton id=\"q5\"\u003e\u003cspan style=\"background: #f00\"\u003e \u003c/span\u003e\u003cspan style=\"background: #0f0\"\u003e \u003c/span\u003e\u003c/button\u003e\n\u003c/div\u003e\n\u003cdiv class=\"sliders\"\u003e\n\u003cdiv class=\"cluster\"\u003e\n\u003cdiv class=\"axis\"\u003e\n\u003cdiv\u003eR\u003c/div\u003e\n\u003cdiv id=\"ro1\"\u003e\u003c/div\u003e\n\u003cdiv\u003e\u003cinput id=\"r1\" type=\"range\" min=\"0\" max=\"255\" value=\"0\"/\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"axis\"\u003e\n\u003cdiv\u003eG\u003c/div\u003e\n\u003cdiv id=\"go1\"\u003e\u003c/div\u003e\n\u003cdiv\u003e\u003cinput id=\"g1\" type=\"range\" min=\"0\" max=\"255\" value=\"0\"/\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"axis\"\u003e\n\u003cdiv\u003eB\u003c/div\u003e\n\u003cdiv id=\"bo1\"\u003e\u003c/div\u003e\n\u003cdiv\u003e\u003cinput id=\"b1\" type=\"range\" min=\"0\" max=\"255\" value=\"255\"/\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003c/div\u003e\n\n\u003cdiv class=\"cluster\"\u003e\n\u003cdiv class=\"axis\"\u003e\n\u003cdiv\u003eR\u003c/div\u003e\n\u003cdiv id=\"ro2\"\u003e\u003c/div\u003e\n\u003cdiv\u003e\u003cinput id=\"r2\" type=\"range\" min=\"0\" max=\"255\" value=\"255\"/\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"axis\"\u003e\n\u003cdiv\u003eG\u003c/div\u003e\n\u003cdiv id=\"go2\"\u003e\u003c/div\u003e\n\u003cdiv\u003e\u003cinput id=\"g2\" type=\"range\" min=\"0\" max=\"255\" value=\"255\"/\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"axis\"\u003e\n\u003cdiv\u003eB\u003c/div\u003e\n\u003cdiv id=\"bo2\"\u003e\u003c/div\u003e\n\u003cdiv\u003e\u003cinput id=\"b2\" type=\"range\" min=\"0\" max=\"255\" value=\"255\"/\u003e\u003c/div\u003e\n\u003c/div\u003e\n\u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"quantize\"\u003e\n\u003cdiv\u003eQuantize\u003c/div\u003e\n\u003cdiv\u003e\n\u003cinput id=\"quant\" type=\"range\" min=\"0\" max=\"1\" value=\"1\" step=\"any\"/\u003e\n\u003c/div\u003e\n\u003c/div\u003e\n\n\u003ch2 id=\"why-and-when-a-perceptual-color-space\"\u003eWhy (and when) a perceptual color space?\u003c/h2\u003e\n\n\u003cp\u003eMost image processing is done using a device color space (most often \u003ca href=\"https://en.wikipedia.org/wiki/SRGB\"\u003esRGB\u003c/a\u003e), and most libraries and interfaces expose that color space. Even when an image editing tool or a standard (such as CSS) exposes other color spaces, it’s still most common to use the device color space. But for some use cases, a perceptual color space can give better results.\u003c/p\u003e\n\n\u003cp\u003eBasically \u003cem\u003ethe\u003c/em\u003e central question of color theory is how colors (in the physical sense) are perceived. The trichromacy assumption basically groups colors into equivalence classes in a three-dimensional space, and display devices generally produce colors by mixing three additive primaries: the familiar red, green, and blue.\u003c/p\u003e\n\n\u003cp\u003eIn an ideal perceptual color space, the distance of two points in the space would correlate strongly with the \u003cem\u003eperception\u003c/em\u003e of color difference. Put another way, all pairs separated by a “just noticeable difference” would be separated by an equal distance.\u003c/p\u003e\n\n\u003cp\u003eAs it turns out, such a thing is no more possible than flattening an orange peel, because color perception is inherently \u003ca href=\"https://www.researchgate.net/publication/2900785_Non-Euclidean_Structure_of_Spectral_Color_Space\"\u003enon-Euclidean\u003c/a\u003e. To put it another way, the ratio of perceptually distinct steps around a hue circle to those directly across through gray is greater than would be expected as a circle in an ordinary Euclidean space.\u003c/p\u003e\n\n\u003cp\u003eEven so, like map projections, it is possible to make a color space that approximates perceptual uniformity and is useful for various tasks. One of these, a primary focus of this blog post, is smoother gradients.\u003c/p\u003e\n\n\u003cp\u003eGradients are of course very similar to color scales, and a good perceptual color space can be used as the basis for those. An even better approach is to use a real color appearance model, as was done in \u003ca href=\"https://www.youtube.com/watch?v=xAoljeRJ3lU\"\u003eA Better Default Colormap for Matplotlib\u003c/a\u003e, and is well explained and motivated in that video (it contains a brief introduction to color science as well).\u003c/p\u003e\n\n\u003cp\u003eA major application of perceptual color spaces is image manipulation, especially changing the color saturation of an image. This is a particular place where hue uniformity is important, as you don’t want hues to shift. Also, prediction of lightness is especially important when transforming a color image to black and white.\u003c/p\u003e\n\n\u003cp\u003ePerceptual color spaces are also a good basis for programmatic manipulation of color palettes, for example to create sets of colors in particular relation to each other, or to derive one of dark and light mode from the other. For example, the normal/hover/active/disabled states of a button may be different lightness and saturation values of the same hue, so hue shift would be undesirable. In particular, I think a future evolution of the standard \u003ca href=\"https://en.wikipedia.org/wiki/Blend_modes\"\u003eBlend modes\u003c/a\u003e should have at least the option to do the blending in a high quality perceptual color space.\u003c/p\u003e\n\n\u003cp\u003eAnd, as mentioned by Björn, a color picker widget can benefit from a good perceptual space. It should be possible to adjust lightness and saturation without affecting hue, in particular.\u003c/p\u003e\n\n\u003cp\u003eMuch of the literature on perceptual color spaces is geared to image compression, with two primary motivations. First, as compression adds errors, you generally want those errors distributed evenly in perceptual space; it wouldn’t be good at all to have artifacts that appear more prominently in areas of a particular shade. Second, good compression depends on a clean separation of lightness and chroma information, as the latter can be compressed better.\u003c/p\u003e\n\n\u003cp\u003eAll that said, there are definitely cases where you do \u003cem\u003enot\u003c/em\u003e want to use a perceptual space. Generally for image filtering, antialiasing, and alpha compositing, you want to use a linear space (though there are subtleties here). And there are even some cases you want to use a device space, as the device gamut is usually nice cube there, while it has quite the complex shape in other color spaces.\u003c/p\u003e\n\n\u003ch2 id=\"focus-on-gradients\"\u003eFocus on gradients\u003c/h2\u003e\n\n\u003cp\u003eThis blog post will use gradients as the primary lens to study perceptual color spaces. They are a very sensitive instrument for certain flaws (especially lack of hue uniformity), and a useful goal in and of itself.\u003c/p\u003e\n\n\u003ch2 id=\"no-one-true-gamma\"\u003eNo one true gamma\u003c/h2\u003e\n\n\u003cp\u003eThe transfer function for neutral colors is the literal backbone of any color space. Commonly referred to as “gamma,” it is one of the most commonly misunderstood topics in computer graphics. \u003ca href=\"http://poynton.ca/PDFs/Poynton-2018-PhD.pdf\"\u003ePoynton’s thesis\u003c/a\u003e is a definitive account, and I refer the interested reader there, but will try to summarize the main points.\u003c/p\u003e\n\n\u003cp\u003eAn ideal transfer function for gradients will have perceptually equal steps from black to white. In \u003cem\u003egeneral,\u003c/em\u003e the transfer function in CIELAB is considered close to perceptually uniform, but as always in color perception, the truth is a bit more complicated.\u003c/p\u003e\n\n\u003cp\u003eIn particular, perception depends on viewing conditions. That includes the ambient light, but also the surround; the same gradient surrounded by white will appear darker than when surrounded by black. For an extremely compelling demonstration of the power of surround to affect the perception of lightness, see \u003ca href=\"http://www.ritsumei.ac.jp/~akitaoka/index-e.html\"\u003eAkiyoshi’s illusion pages\u003c/a\u003e, for example \u003ca href=\"http://www.psy.ritsumei.ac.jp/~akitaoka/light2e.html\"\u003ethis one\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eAnother complication is that the light received by the eye includes so-called “veiling glare,” a fraction of ambient light reflected by the monitor because its black is not a perfect absorber (veiling glare is much less of a problem in movie-like conditions).\u003c/p\u003e\n\n\u003cp\u003eThe actual CIELAB transfer function is not a perfect cube-root rule, but rather contains a linear segment in the near-black region (the sRGB transfer function is similar but has different parameters). This segment increases perceptual uniformity of ramps in the presence of veiling glare, and also makes the transform robustly invertible using lookup tables.\u003c/p\u003e\n\n\u003cp\u003eThere is good science on how perception varies with viewing conditions, and the \u003ca href=\"https://en.wikipedia.org/wiki/CIECAM02\"\u003eCIECAM\u003c/a\u003e color appearance model has parameters that can fine-tune it when these are known. But in practice, viewing conditions are not known, and the best approach is to adopt a best guess or compromise.\u003c/p\u003e\n\n\u003ch3 id=\"hdr\"\u003eHDR\u003c/h3\u003e\n\n\u003cp\u003eHDR is a different story, and I need to go into it to explain the \u003ca href=\"https://en.wikipedia.org/wiki/ICtCp\"\u003eICtCp\u003c/a\u003e color space.\u003c/p\u003e\n\n\u003cp\u003eIn standard dynamic range, you basically assume the visual system is adapted to a particular set of viewing conditions (in fact, \u003ca href=\"https://en.wikipedia.org/wiki/SRGB\"\u003esRGB\u003c/a\u003e specifies an exact set of viewing conditions, including monitor brightness, white point, and room lighting). A perceptually uniform gradient from black to white is also useful for image coding, because if you set number of steps so that each individual step is \u003cem\u003ejust\u003c/em\u003e imperceptible, it uses a minimum number of bits for each sample while faithfully rendering the image without artifacts. And in sRGB, 256 level is just barely enough for most uses, though steps are often visible when displaying gradients, the case where the eye is most sensitive to quantization errors.\u003c/p\u003e\n\n\u003cp\u003eIn HDR, however, this approach doesn’t quite work. Because of the wider range of brightness values from the display device, and also weaker assumptions about the viewing conditions (darkened rooms are common for movie viewing), the human visual system might at any time be adapted to quite light or quite dark viewing conditions. In the latter case, it would be sensitive to much finer gradations in near-black shades than when adapted to lighter conditions, and a similar situation is true the other way around. If tuned for any one single brightness level, results will be good when adaptation matches, but poor otherwise.\u003c/p\u003e\n\n\u003cp\u003eThus, HDR uses a different approach. It uses a model (known as the Barten model, and shown in Figure 4.6 of \u003ca href=\"http://poynton.ca/PDFs/Poynton-2018-PhD.pdf\"\u003ePoynton’s thesis\u003c/a\u003e) of the minimum contrast step perceptible at each brightness level, over all possible adaptation conditions. The goal is to determine a sequence of steps so that each step is just under the threshold of what’s perceptible under \u003cem\u003eany\u003c/em\u003e viewing conditions.\u003c/p\u003e\n\n\u003cp\u003eThe SMPTE ST 2084 transfer function is basically a mathematical curve-fit to the empirical Barten model, and has the property that with 12 bits of code words, each step is just under 0.9 of the minimum perceptual difference as predicted by the Barten model, across a range from 0.001 to 10,000 nits of brightness (7 orders of magnitude). There’s lots more detail and context in the presentation \u003ca href=\"https://www.avsforum.com/attachments/smpte-2014-05-06-eotf-miller-1-2-handout-pdf.1347114/\"\u003eA Perceptual EOTF for Extended\nDynamic Range Imagery\u003c/a\u003e (PDF).\u003c/p\u003e\n\n\u003cp\u003eThat said, though it’s sophisticated and an excellent fit to the empirical Barten curve, it is \u003cem\u003enot\u003c/em\u003e perceptually uniform at any one particular viewing condition. In particular, a ramp of the ST 2084 curve will dwell far too long near-black (representing a range that would be more visible in dark viewing conditions). To see this for yourself, try the black+white button in the interactive explorer above.\u003c/p\u003e\n\n\u003ch3 id=\"a-comparison-of-curves\"\u003eA comparison of curves\u003c/h3\u003e\n\n\u003cp\u003eWe can basically place curves on a scale from “way too dark” (ST 2084) to “way too light” (linear light), with all the others in between. CIELAB is a pretty good median (though this may express my personal preference), with IPT a bit lighter and Oklab a bit darker.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/assets/colorspace_transfer_functions.png\" width=\"575\" style=\"margin: auto; display: block;\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eI found Björn’s arguments in favor of pure cube root to be not entirely compelling, but this is perhaps an open question. Both CIELAB and sRGB use a finite-derivative region near black. Is it important to limit derivatives for more accurate LUT-based calculation? Perhaps in 2021, we will almost always prefer ALU to LUT. The conditional part is also not ideal, especially on GPUs, where branches can hurt performance. I personally would explore transfer functions of the form $f(x) = a + (b + cx)^\\gamma$, constrained so $f(0) = 0$ and $f(1) = 1$, as these are GPU-friendly and have smooth derivatives. The XYB color space used in JPEG XL apparently uses a bias rather than a piecewise linear region, as well. (Source: \u003ca href=\"https://news.ycombinator.com/item?id=25525726\"\u003eHN thread on Oklab\u003c/a\u003e, as I wasn’t easily able to find a document)\u003c/p\u003e\n\n\u003ch2 id=\"the-labipt-architecture\"\u003eThe Lab/IPT architecture\u003c/h2\u003e\n\n\u003cp\u003eCIELAB, IPT, ICtCp, and Oklab all share a simple architecture: a 3x3 matrix, a nonlinear function, and another 3x3 matrix. In addition to being simple, this architecture is easily invertible. Many other color spaces are considerably more complex, with CIECAM as an especially bad offender.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/assets/ipt_flow.svg\" alt=\"flow diagram of IPT architecture\" style=\"margin: auto; display: block;\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eThe main difference between the various color spaces in this architecture is the nonlinear function, which determines the black-to-white ramp as discussed above. Once that is in place, there is a relatively small number of remaining parameters. Those can be optimized, either by hand or using an automated optimizer trying to minimize a cost function.\u003c/p\u003e\n\n\u003cp\u003eIn particular, \u003ca href=\"https://en.wikipedia.org/wiki/ICtCp\"\u003eICtCp\u003c/a\u003e was derived from the ST 2084 transfer function, and then optimized for hue linearity and good lightness prediction. It’s important to note, good lightness prediction in an HDR context does \u003cem\u003enot\u003c/em\u003e mean that the lightness steps are perceptually uniform, but that colors with the same reported lightness have the same perceived lightness. ICtCp does well in the latter criterion, but not so much the former; it’s fundamentally in tension with a color space suitable for HDR.\u003c/p\u003e\n\n\u003ch2 id=\"comparisons\"\u003eComparisons\u003c/h2\u003e\n\n\u003ch3 id=\"hue-linearity\"\u003eHue linearity\u003c/h3\u003e\n\n\u003cp\u003eOn hue linearity, my evaluation is that IPT, ICtCp, and Oklab are all quite good, and a huge improvement over CIELAB. This is not terribly surprising, as they are all optimized for hue linearity with respect to the Ebner-Fairchild data set, which is of very high quality.\u003c/p\u003e\n\n\u003ch4 id=\"where-does-the-shift-in-blue-come-from\"\u003eWhere does the shift in blue come from?\u003c/h4\u003e\n\n\u003cp\u003eIt’s always been a bit of a mystery to me \u003cem\u003ewhy\u003c/em\u003e CIELAB has such a bad hue shift in the blue range. I think some of it is just bad tuning of the matrix parameters, but there is an even deeper issue. Additive mixing of deep blue and white creates a hue shift towards purple. James Clerk Maxwell would have been able to observe this in spinning top experiments, had he mixed a sufficiently strong blue; I’m not sure exactly when this was first noted.\u003c/p\u003e\n\n\u003cp\u003eMore recent research indicates that changing the \u003cem\u003ebandwidth\u003c/em\u003e of a gaussian spectrum, but retaining its peak wavelength, tends to change the perceived color saturation while retaining hue constancy. The authors of \u003ca href=\"https://doi.org/10.2352/J.Percept.Imaging.2020.3.2.020401\"\u003eUsing Gaussian Spectra to Derive a Hue-linear Color Space\u003c/a\u003e observe this and tries to use the fact to derive a hue-linear color space (very similar to IPT as well) from first principles, rather than experimental data. The results are mixed; the results are pretty good but not perfect, perhaps illustrating that an optimization process is always very sensitive to flaws in the optimization criterion. In any case, I found the paper to provide intuitive insight into why this problem is not so simple, and in particular why additive light mixing with neutral is not hue-preserving.\u003c/p\u003e\n\n\u003ch3 id=\"lightness\"\u003eLightness\u003c/h3\u003e\n\n\u003cp\u003eOn lightness, the Oklab blog argues more accurate predictions than IPT. I was initially skeptical of this claim (it’s not entirely consistent with similar claims in \u003ca href=\"https://www.researchgate.net/publication/221677980_Development_and_Testing_of_a_Color_Space_IPT_with_Improved_Hue_Uniformity\"\u003eEbner’s thesis\u003c/a\u003e, see Figures 69 and 71 in particular), but on closer examination I agree. The following images should help you verify this claim yourself:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/assets/iso_luma_ipt.png\" width=\"350\" alt=\"iso-luma patches in IPT\"/\u003e\n\u003cimg src=\"/assets/iso_luma_oklab.png\" width=\"350\" alt=\"iso-luma patches in Oklab\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eThe first is a collection of color patches with the same lightness (I) value in IPT, and the second with the same lightness (L) in Oklab. To my eyes, the second has more uniform lightness, while in IPT blues are too dark and yellow-greens are too light.\u003c/p\u003e\n\n\u003cp\u003eIt’s also possible to evaluate this claim objectively. The L* axis in CIELAB is widely agreed to predict lightness. Thus, deviations from it are a bad sign. The plots below show a scatterplot of random colors, with CIELAB L* on the horizontal axis, and the lightness axis of IPT and Oklab on the vertical axis:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/assets/ipt_l_scatter.png\" width=\"350\" alt=\"lightness scatterplot of IPT vs CIELAB\"/\u003e\n\u003cimg src=\"/assets/oklab_l_scatter.png\" width=\"350\" alt=\"lightness scatterplot of Oklab vs CIELAB\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eAs can be seen, Oklab correlates \u003cem\u003emuch\u003c/em\u003e more strongly with CIELAB on the lightness scale, while IPT has considerable variation. I was also interested to see that ICtCp correlates strongly with CIELAB as well, though there’s a pronounced nonlinearity due to the transfer function.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/assets/ictcp_l_scatter.png\" width=\"350\" alt=\"lightness scatterplot of ICtCp vs CIELAB\" style=\"margin: auto; display: block;\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eDifferences in lightness don’t have a huge effect on gradients, but they do affect image processing operations such as changing saturation. Thus, I wouldn’t recommend IPT as a color space for these operations, and am more comfortable recommending Oklab.\u003c/p\u003e\n\n\u003ch3 id=\"chroma\"\u003eChroma\u003c/h3\u003e\n\n\u003cp\u003eI didn’t evaluate this claim as carefully, in part because its relevance to high quality gradients is limited. \u003cem\u003eRelative\u003c/em\u003e changes in chroma are of course very important, but the \u003cem\u003eabsolute\u003c/em\u003e chroma value ascribed to highly saturated colors matters little to gradients. Even so, the quantitative data suggest that Oklab is a more accurate predictor, and it’s easy to believe, as IPT wasn’t carefully optimized for this criterion.\u003c/p\u003e\n\n\u003ch2 id=\"conclusions\"\u003eConclusions\u003c/h2\u003e\n\n\u003cp\u003eA good perceptual color space can make higher quality, more uniform gradients. While CIELAB is popular and well-known, its hue shift is a problem. The original IPT color space was a great advancement: it has tremendously better hue constancy, while retaining the same simple structure. That said, its predictions of lightness and chroma are less highly tuned.\u003c/p\u003e\n\n\u003cp\u003eThe desirable properties of IPT inspired a family of IPT-like color spaces, with the main difference being the choice of transfer function. Obviously the family includes ICtCp, which uses a transfer function optimized for HDR (but which, sadly, makes it less suitable for general purpose use). The modern recipe for an IPT-like color space is to choose a transfer function, then optimize the matrix parameters for hue linearity and accurate prediction of lightness and chroma. Oklab is basically the result of applying that recipe, starting at the cube-root transfer function.\u003c/p\u003e\n\n\u003cp\u003eFor most applications where CIELAB is used today, both IPT and Oklab are better alternatives. The choice between them is a judgment call, though Oklab does have better prediction of chroma and lightness.\u003c/p\u003e\n\n\u003cp\u003eIs an even better color space possible? I think so. I personally would like to see a transfer function with a little more contrast in the near-black region (closer to CIELAB), but this is something of a judgment call. I also think it’s possible to optimize on the basis of higher quality data; indexing off an existing color space retains any flaws in that space. Perhaps the biggest concern is that there is no one clear contender for a post-CIELAB standard. Another spin on Oklab with even higher quality data, and a consensus-building process, could be exactly that.\u003c/p\u003e\n\n\u003cp\u003eIn the meantime, I can highly recommend Oklab for tasks such as making better gradients.\u003c/p\u003e\n\n\u003cp\u003eThis blog post benefitted greatly from conversations with Björn Ottson and Jacob Rus, though of course my mistakes are my own.\u003c/p\u003e\n\n\u003cp\u003eDiscuss on \u003ca href=\"https://news.ycombinator.com/item?id=25830327\"\u003eHacker News\u003c/a\u003e.\u003c/p\u003e\n\n\u003ch2 id=\"update-2021-01-29\"\u003eUpdate 2021-01-29\u003c/h2\u003e\n\n\u003cp\u003eI’ve added several more color spaces to the widget above.\u003c/p\u003e\n\n\u003cp\u003eOne of the color spaces is linear mixing, which doesn’t have great hue linearity, and is perceptually \u003cem\u003eway\u003c/em\u003e too light in the white-black ramp. But I include it for comparison because some have suggested that it is appropriate for gradients.\u003c/p\u003e\n\n\u003cp\u003eAnother great resource for “better gradients” is Matt Deslaurier’s \u003ca href=\"https://observablehq.com/@mattdesl/perceptually-smooth-multi-color-linear-gradients\"\u003egradient Observable notebook\u003c/a\u003e. That now has an open source license and a correct implementation of XYB.\u003c/p\u003e\n\n\u003ch3 id=\"xyb\"\u003eXYB\u003c/h3\u003e\n\n\u003cp\u003eThe XYB color space is part of \u003ca href=\"https://jpeg.org/jpegxl/\"\u003eJPEG XL\u003c/a\u003e, and thus its main motivation is image compression. Its hue linearity is not great, about halfway between CIELAB and IPT. However, it has a nice transfer function - it’s a cube root with an offset, which has generally similar darkness/lightness as CIELAB, but with smoother derivatives throughout. If I were to make a new color space, I would choose this transfer function.\u003c/p\u003e\n\n\u003ch3 id=\"srlab2\"\u003eSRLAB2\u003c/h3\u003e\n\n\u003cp\u003eA very little known color space is \u003ca href=\"https://www.magnetkern.de/srlab2.html\"\u003eSRLAB2\u003c/a\u003e by Jan Behrens. It uses the CIELAB transfer function but updates the matrices for better hue linearity. Overall it performs very well; the only flaw I found is that the red-white ramp bends toward orange (as does CIELAB).\u003c/p\u003e\n\n\u003cp\u003eJuha Järvi has produced a \u003ca href=\"https://gist.github.com/jjrv/b27d0840b4438502f9cad2a0f9edeabc\"\u003egist analyzing SRLAB2\u003c/a\u003e in more detail, and there is some \u003ca href=\"https://www.reddit.com/r/GraphicsProgramming/comments/l2o8zu/a_new_perceptual_color_space/gl08lzq/?context=3\"\u003eReddit discussion\u003c/a\u003e as well. Thanks to Juha for bringing this to my attention.\u003c/p\u003e\n\n\u003cscript\u003e\n// The following code is licensed under Apache-2.0 license as indicated in\n// the \"About\" page of this blog.\nfunction lerp(a, b, t) {\n    if (Array.isArray(a)) {\n        var result = [];\n        for (let i = 0; i \u003c a.length; i++) {\n            result.push(lerp(a[i], b[i], t));\n        }\n        return result;\n    } else {\n        return a + (b - a) * t;\n    }\n}\nfunction mat_vec_mul(m, v) {\n    var result = [];\n    for (row of m) {\n        let sum = 0;\n        for (let i = 0; i \u003c row.length; i++) {\n            sum += row[i] * v[i];\n        }\n        result.push(sum);\n    }\n    return result;\n}\n// Argument is in range 0..1\nfunction srgb_eotf(u) {\n    if (u \u003c 0.04045) {\n        return u / 12.92;\n    } else {\n        return Math.pow((u + 0.055) / 1.055, 2.4);\n    }\n}\nfunction srgb_eotf_inv(u) {\n    if (u \u003c 0.0031308) {\n        return u * 12.92;\n    } else {\n        return 1.055 * Math.pow(u, 1/2.4) - 0.055;\n    }\n}\n// Source: Wikipedia sRGB article, rounded to 4 decimals\nconst SRGB_TO_XYZ = [\n    [0.4124, 0.3576, 0.1805],\n    [0.2126, 0.7152, 0.0722],\n    [0.0193, 0.1192, 0.9505]\n];\nconst XYZ_TO_SRGB = [\n    [3.2410, -1.5374, -0.4986],\n    [-0.9692, 1.8760, 0.0416],\n    [0.0556, -0.2040, 1.0570]\n];\n// Color is sRGB with 0..255 range. Result is in D65 white point.\nfunction sRGB_to_XYZ(rgb) {\n    const rgblin = rgb.map(x =\u003e srgb_eotf(x / 255));\n    return mat_vec_mul(SRGB_TO_XYZ, rgblin);\n}\nfunction XYZ_to_sRGB(xyz) {\n    const rgblin = mat_vec_mul(XYZ_TO_SRGB, xyz);\n    return rgblin.map(x =\u003e Math.round(255 * srgb_eotf_inv(x)));\n}\nconst SRGB = {\"to_xyz\": sRGB_to_XYZ, \"from_xyz\": XYZ_to_sRGB};\n\n\n// From Oklab article, then some numpy. Note these are transposed. I'm\n// not sure I have the conventions right, but it is giving the right\n// answer.\nconst OKLAB_M1 = [\n    [ 0.8189,  0.3619, -0.1289],\n    [ 0.033 ,  0.9293,  0.0361],\n    [ 0.0482,  0.2644,  0.6339]\n];\nconst OKLAB_M2 = [\n    [ 0.2105,  0.7936, -0.0041],\n    [ 1.978 , -2.4286,  0.4506],\n    [ 0.0259,  0.7828, -0.8087]\n];\nconst OKLAB_INV_M1 = [\n    [ 1.227 , -0.5578,  0.2813],\n    [-0.0406,  1.1123, -0.0717],\n    [-0.0764, -0.4215,  1.5862]\n];\nconst OKLAB_INV_M2 = [\n    [ 1.0   ,  0.3963,  0.2158],\n    [ 1.0   , -0.1056, -0.0639],\n    [ 1.0   , -0.0895, -1.2915]\n];\nfunction Oklab_to_XYZ(lab) {\n    const lms = mat_vec_mul(OKLAB_INV_M2, lab);\n    const lmslin = lms.map(x =\u003e x * x * x);\n    return mat_vec_mul(OKLAB_INV_M1, lmslin);\n}\nfunction XYZ_to_Oklab(xyz) {\n    const lmslin = mat_vec_mul(OKLAB_M1, xyz);\n    const lms = lmslin.map(Math.cbrt);\n    return mat_vec_mul(OKLAB_M2, lms);\n}\nconst OKLAB = {\"to_xyz\": Oklab_to_XYZ, \"from_xyz\": XYZ_to_Oklab};\n\nfunction cielab_f(t) {\n    const d = 6.0/29.0;\n    if (t \u003c d * d * d) {\n        return t / (3 * d * d) + 4.0/29.0;\n    } else {\n        return Math.cbrt(t);\n    }\n}\nfunction cielab_f_inv(t) {\n    const d = 6.0/29.0;\n    if (t \u003c d) {\n        return 3 * d * d * (t - 4.0/29.0);\n    } else {\n        return t * t * t;\n    }\n}\nfunction XYZ_to_Lab(xyz) {\n    // Just normalizing XYZ values to the white point is the \"wrong von Kries\"\n    // transformation, which is faithful to the spec.\n    const xyz_n = [xyz[0] / .9505, xyz[1], xyz[2] / 1.0888];\n    const fxyz = xyz_n.map(cielab_f);\n    const L = 116 * fxyz[1] - 16;\n    const a = 500 * (fxyz[0] - fxyz[1]);\n    const b = 200 * (fxyz[1] - fxyz[2]);\n    return [L, a, b];\n}\nfunction Lab_to_XYZ(lab) {\n    const l_ = (lab[0] + 16) / 116;\n    const x = 0.9505 * cielab_f_inv(l_ + lab[1] / 500);\n    const y = cielab_f_inv(l_);\n    const z = 1.0888 * cielab_f_inv(l_ - lab[2] / 200);\n    return [x, y, z];\n}\nconst CIELAB = {\"to_xyz\": Lab_to_XYZ, \"from_xyz\": XYZ_to_Lab};\n\n// https://professional.dolby.com/siteassets/pdfs/ictcp_dolbywhitepaper_v071.pdf\nconst ICTCP_XYZ_TO_LMS = [\n    [ 0.3593,  0.6976, -0.0359],\n    [-0.1921,  1.1005,  0.0754],\n    [ 0.0071,  0.0748,  0.8433]\n];\nconst ICTCP_LMS_TO_ITP = [\n    [ 0.5   ,  0.5   ,  0.0   ],\n    [ 1.6138, -3.3235,  1.7097],\n    [ 4.3782, -4.2456, -0.1326]\n];\nconst ICTCP_LMS_TO_XYZ = [\n    [ 2.0703, -1.3265,  0.2067],\n    [ 0.3647,  0.6806, -0.0453],\n    [-0.0498, -0.0492,  1.1881]\n];\nconst ICTCP_ITP_TO_LMS = [\n    [ 1.0   ,  0.0086,  0.111 ],\n    [ 1.0   , -0.0086, -0.111 ],\n    [ 1.0   ,  0.56  , -0.3206]\n];\nconst m1 = 2610/16384;\nconst m2 = 2523/4096 * 128;\nconst c2 = 2413/4096 * 32;\nconst c3 = 2392/4096 * 32;\nconst c1 = c3 - c2 + 1;\n// This peak luminance value is from the Dolby whitepaper but seems too high.\nconst L_p = 10000;\n// Note: 80 is what is specified by sRGB but seems too low; this value is chosen\n// to be typical for actual non-HDR displays.\nconst L_display = 200;\nfunction st_2084_eotf_inv(n) {\n    const fd = n * L_display;\n    const y = fd / L_p;\n    const ym1 = Math.pow(y, m1);\n    return Math.pow((c1 + c2 * ym1) / (1 + c3 * ym1), m2);\n}\nfunction st_2084_eotf(x) {\n    const V_p = Math.pow(x, 1 / m2);\n    const n = V_p - c1;\n    // maybe max with 0 here?\n    const L = Math.pow(n / (c2 - c3 * V_p), 1 / m1);\n    return L * L_p / L_display;\n}\nfunction ICtCp_to_XYZ(lab) {\n    const lms = mat_vec_mul(ICTCP_ITP_TO_LMS, lab);\n    const lmslin = lms.map(st_2084_eotf);\n    return mat_vec_mul(ICTCP_LMS_TO_XYZ, lmslin);\n}\nfunction XYZ_to_ICtCp(xyz) {\n    const lmslin = mat_vec_mul(ICTCP_XYZ_TO_LMS, xyz);\n    const lms = lmslin.map(st_2084_eotf_inv);\n    return mat_vec_mul(ICTCP_LMS_TO_ITP, lms);\n}\nconst ICTCP = {\"to_xyz\": ICtCp_to_XYZ, \"from_xyz\": XYZ_to_ICtCp};\n\n///\nconst IPT_XYZ_TO_LMS = [\n    [0.4002, 0.7075, -0.0807],\n    [-0.2280, 1.1500, 0.0612],\n    [0.0000, 0.0000, 0.9184]\n];\nconst IPT_LMS_TO_IPT = [\n    [0.4000, 0.4000, 0.2000],\n    [4.4550, -4.8510, 0.3960],\n    [0.8056, 0.3572, -1.1628],\n];\nconst IPT_LMS_TO_XYZ = [\n    [ 1.8502, -1.1383,  0.2384],\n    [ 0.3668,  0.6439, -0.0107],\n    [ 0.0   ,  0.0   ,  1.0889]\n];\nconst IPT_IPT_TO_LMS = [\n    [ 1.0   ,  0.0976,  0.2052],\n    [ 1.0   , -0.1139,  0.1332],\n    [ 1.0   ,  0.0326, -0.6769]\n];\nfunction IPT_to_XYZ(lab) {\n    const lms = mat_vec_mul(IPT_IPT_TO_LMS, lab);\n    const lmslin = lms.map(x =\u003e Math.pow(x, 1.0 / 0.43));\n    return mat_vec_mul(IPT_LMS_TO_XYZ, lmslin);\n}\nfunction XYZ_to_IPT(xyz) {\n    const lmslin = mat_vec_mul(IPT_XYZ_TO_LMS, xyz);\n    const lms = lmslin.map(x =\u003e Math.pow(x, 0.43));\n    return mat_vec_mul(IPT_LMS_TO_IPT, lms);\n}\nconst IPT = {\"to_xyz\": IPT_to_XYZ, \"from_xyz\": XYZ_to_IPT};\n\nconst XYB_XYZ_TO_LMS = [\n    [ 0.3739,  0.6896, -0.0413],\n    [ 0.0792,  0.9286, -0.0035],\n    [ 0.6212, -0.1027,  0.4704]\n];\nconst XYB_LMS_TO_XYB = [\n    [ 0.5, -0.5, 0.0],\n    [ 0.5, 0.5, 0.0],\n    [ 0.0, 0.0, 1.0],\n]\nconst XYB_LMS_TO_XYZ = [\n    [ 2.7253, -1.9993,  0.2245],\n    [-0.2462,  1.2585, -0.0122],\n    [-3.6527,  2.9148,  1.8268]\n];\nconst XYB_XYB_TO_LMS = [\n    [ 1.0, 1.0, 0.0],\n    [ -1.0, 1.0, 0.0],\n    [ 0.0, 0.0, 1.0],\n]\nconst XYB_BIAS = 0.00379307;\nconst XYB_BIAS_CBRT = Math.cbrt(XYB_BIAS);\nfunction XYB_to_XYZ(lab) {\n    const lms = mat_vec_mul(XYB_XYB_TO_LMS, lab);\n    const lmslin = lms.map(x =\u003e Math.pow(x + XYB_BIAS_CBRT, 3) - XYB_BIAS);\n    return mat_vec_mul(XYB_LMS_TO_XYZ, lmslin);\n}\nfunction XYZ_to_XYB(xyz) {\n    const lmslin = mat_vec_mul(XYB_XYZ_TO_LMS, xyz);\n    const lms = lmslin.map(x =\u003e Math.cbrt(x + XYB_BIAS) - XYB_BIAS_CBRT);\n    return mat_vec_mul(XYB_LMS_TO_XYB, lms);\n}\nconst XYB = {\"to_xyz\": XYB_to_XYZ, \"from_xyz\": XYZ_to_XYB};\n\nconst LINEAR = {\"to_xyz\": x =\u003e x, \"from_xyz\": x =\u003e x};\n\n// These are actually equivalent to the CIE curves and the code could be merged,\n// but we're keeping closer to the sources.\nfunction srlab2_f(t) {\n    if (t \u003c 216.0/24389.0) {\n        return t * (24389.0 / 2700.0);\n    } else {\n        return 1.16 * Math.cbrt(t) - 0.16;\n    }\n}\nfunction srlab2_f_inv(t) {\n    if (t \u003c 0.08) {\n        return t * (2700.0 / 24389.0);\n    } else {\n        return Math.pow((t + 0.16) / 1.16, 3.0);\n    }\n}\nconst SRLAB2_XYZ_TO_LMS = [\n    [ 0.424 ,  0.6933, -0.0884],\n    [-0.2037,  1.1537,  0.0367],\n    [-0.0008, -0.001 ,  0.9199]\n];\nconst SRLAB2_LMS_TO_LAB = [\n    [ 37.0950,   62.9054,   -0.0008],\n    [663.4684, -750.5078,   87.0328],\n    [ 63.9569,  108.4576, -172.4152],\n];\nconst SRLAB2_LMS_TO_XYZ = [\n    [ 1.8307, -1.1   ,  0.2198],\n    [ 0.3231,  0.6726,  0.0042],\n    [ 0.0019, -0.0002,  1.0873]\n];\nconst SRLAB2_LAB_TO_LMS = [\n    [0.01, +0.000904127, +0.000456344],\n    [0.01, -0.000533159, -0.000269178],\n    [0.01,  0.0        , -0.005800000]\n];\nfunction SRLAB2_to_XYZ(lab) {\n    const lms = mat_vec_mul(SRLAB2_LAB_TO_LMS, lab);\n    const lmslin = lms.map(srlab2_f_inv);\n    return mat_vec_mul(SRLAB2_LMS_TO_XYZ, lmslin);\n}\nfunction XYZ_to_SRLAB2(xyz) {\n    const lmslin = mat_vec_mul(SRLAB2_XYZ_TO_LMS, xyz);\n    const lms = lmslin.map(srlab2_f);\n    return mat_vec_mul(SRLAB2_LMS_TO_LAB, lms);\n}\nconst SRLAB2 = {\"to_xyz\": SRLAB2_to_XYZ, \"from_xyz\": XYZ_to_SRLAB2};\n\nfunction draw_gradient(id, c1, c2, cs, q) {\n    const n_steps = Math.round(2.0 / (1 - Math.cbrt(q)));\n    const a1 = cs[\"from_xyz\"](sRGB_to_XYZ(c1));\n    const a2 = cs[\"from_xyz\"](sRGB_to_XYZ(c2));\n    const element = document.getElementById(id);\n    const w = element.width;\n    const h = element.height;\n    const ctx = element.getContext(\"2d\");\n    const img = ctx.createImageData(w, h);\n    for (let x = 0; x \u003c w; x++) {\n        let t = x / (w - 1);\n        if (q \u003c 1) {\n            t = Math.min(Math.floor(t * (n_steps + 1)) / n_steps, 1.0);\n        }\n        const a = lerp(a1, a2, t);\n        const c = XYZ_to_sRGB(cs[\"to_xyz\"](a));\n        img.data[x * 4 + 0] = c[0];\n        img.data[x * 4 + 1] = c[1];\n        img.data[x * 4 + 2] = c[2];\n        img.data[x * 4 + 3] = 255;\n    }\n    for (let y = 1; y \u003c h; y++) {\n        img.data.copyWithin(y * w * 4, 0, w * 4);\n    }\n    ctx.putImageData(img, 0, 0);\n}\n\n// UI\nfunction getrgb(n) {\n    return ['r', 'g', 'b'].map(c =\u003e {\n        const v = document.getElementById(c + n).valueAsNumber;\n        document.getElementById(c + 'o' + n).innerText = `${v}`;\n        return v;\n    });\n}\nfunction update(e) {\n    rgb1 = getrgb(1);\n    rgb2 = getrgb(2);\n    q = document.getElementById('quant').valueAsNumber;\n    draw_gradient(\"c0\", rgb1, rgb2, SRGB, q);\n    draw_gradient(\"c1\", rgb1, rgb2, CIELAB, q);\n    draw_gradient(\"c2\", rgb1, rgb2, IPT, q);\n    draw_gradient(\"c3\", rgb1, rgb2, OKLAB, q);\n    draw_gradient(\"c4\", rgb1, rgb2, ICTCP, q);\n    draw_gradient(\"c5\", rgb1, rgb2, XYB, q);\n    draw_gradient(\"c6\", rgb1, rgb2, SRLAB2, q);\n    draw_gradient(\"c7\", rgb1, rgb2, LINEAR, q);\n}\nfunction setrgb(rgb1, rgb2) {\n    for (let i = 0; i \u003c 3; i++) {\n        const c = ['r', 'g', 'b'][i];\n        document.getElementById(c + 1).valueAsNumber = rgb1[i];\n        document.getElementById(c + 2).valueAsNumber = rgb2[i];\n    }\n    update();\n}\nfunction randomize(e) {\n    const rgb1 = [0, 1, 2].map(_ =\u003e Math.round(255 * Math.random()));\n    const rgb2 = [0, 1, 2].map(_ =\u003e Math.round(255 * Math.random()));\n    setrgb(rgb1, rgb2);\n}\nfunction install_ui() {\n    for (var c of ['r', 'g', 'b']) {\n        document.getElementById(c + 1).addEventListener('input', update);\n        document.getElementById(c + 2).addEventListener('input', update);\n    }\n    document.getElementById('quant').addEventListener('input', update);\n    document.getElementById('randomize').addEventListener('click', randomize);\n    const colors = [\n        [[0, 0, 255], [255, 255, 255]],\n        [[0, 0, 0], [255, 255, 255]],\n        [[0, 0, 17], [255, 255, 255]],\n        [[0, 0, 255], [255, 255, 0]],\n        [[255, 0, 0], [0, 0, 255]],\n        [[255, 0, 0], [0, 255, 0]]\n    ];\n    for (var i = 0; i \u003c colors.length; i++) {\n        const c = colors[i];\n        document.getElementById('q' + i).addEventListener('click', e =\u003e {\n            setrgb(c[0], c[1]);\n        });\n    }\n}\ninstall_ui();\nupdate();\n\u003c/script\u003e\n\n\n  \u003c/div\u003e",
  "Date": "2021-01-18T21:39:42Z",
  "Author": "raphlinus"
}