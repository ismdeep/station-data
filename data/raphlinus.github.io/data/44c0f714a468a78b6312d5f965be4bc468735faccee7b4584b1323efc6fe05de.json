{
  "Source": "raphlinus.github.io",
  "Title": "How long is that Bézier?",
  "Link": "https://raphlinus.github.io/curves/2018/12/28/bezier-arclength.html",
  "Content": "\u003cdiv class=\"post-content e-content\" itemprop=\"articleBody\"\u003e\n    \u003cscript type=\"text/x-mathjax-config\"\u003e\n        MathJax.Hub.Config({\n                tex2jax: {\n                        inlineMath: [['$', '$']]\n                }\n        });\n\u003c/script\u003e\n\n\u003cscript src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML\" type=\"text/javascript\"\u003e\u003c/script\u003e\n\n\u003cp\u003eOne of the fundamental curve algorithms is determining its arclength. For some curves, like lines and circular arcs, it’s simple enough, but it gets tricky for Bézier curves. I’ve implemented these algorithms for my new \u003ca href=\"http://github.com/linebender/kurbo\"\u003ekurbo\u003c/a\u003e curves library, and I think the work that went into getting it right makes a good story.\u003c/p\u003e\n\n\u003ch2 id=\"béziers-and-arclength\"\u003eBéziers and arclength\u003c/h2\u003e\n\n\u003cp\u003eFirst, if you haven’t read \u003ca href=\"https://pomax.github.io/bezierinfo/\"\u003eA Primer on Bézier Curves\u003c/a\u003e, go do that now. In particular, the section on \u003ca href=\"https://pomax.github.io/bezierinfo/#arclength\"\u003earclength\u003c/a\u003e motivates almost everything I’m writing about below.\u003c/p\u003e\n\n\u003cp\u003eWhy is arclength important? One important traditional application is rendering strokes with dashed patterns. My main interest is part of generating optimized Béziers to closely fit a given curve. My research has shown that the optimum Bézier tends to have an arclength very close to the original curve. Thus, searching only for Béziers with matching arclength is a good way to reduce the parameter space when searching for optimum fit. I use this technique in the \u003ca href=\"https://raphlinus.github.io/curves/2018/12/08/euler-spiral.html\"\u003eEuler explorer\u003c/a\u003e, as well as optimized conversion from Spiro to cubic Béziers in my \u003ca href=\"https://levien.com/phd/thesis.pdf\"\u003ePhD thesis\u003c/a\u003e. I’m embarrassed to admit, though, that the code I used for that is very slow and not all that precise — generating the full map for the Euler explorer took hours.\u003c/p\u003e\n\n\u003cp\u003eThe most obvious approach to computing arclength is to sample the curve at a sequence of points, then add up all the distances. This is equivalent to flattening the curve into lines and adding up all the line lengths. It’s simple and robust (it doesn’t care too much about the presence of kinks), so fairly widely implemented. The only problem is, it’s not very accurate. Or, to put it another way, it’s astonishingly slow when high accuracy is desired. The accuracy quadruples with every doubling of the number of samples, or another way of putting it, the number of samples is $O(\\sqrt{N})$ where $N$ is the reciprocal of the error tolerance.\u003c/p\u003e\n\n\u003cp\u003eLet’s try to find a better way. This \u003ca href=\"https://math.stackexchange.com/questions/12186/arc-length-of-b%C3%A9zier-curves\"\u003equestion\u003c/a\u003e was posed on Stack Overflow, specifically for quadratic Béziers, so to a large extent this post is an extended answer to that question, though we will also give a solution for cubic Béziers.\u003c/p\u003e\n\n\u003cp\u003eFor a parametric curve, expressed as $x(t)$ and $y(t)$, where $t$ ranges $(0..1)$, the arclength of a curve is this integral:\u003c/p\u003e\n\n\\[\\int_0^1 \\sqrt{(dx / dt)^2 + (dy / dt)^2}\\ dt\\]\n\n\u003cp\u003eFor quadratic Béziers, this integral has a closed form solution. I tried implementing the arsinh-based formula in the Stack Overflow post, and couldn’t get it to work. Also, I was both aware that cubics don’t have a closed form solution, and worried about numerical stability (rightfully so, as we’ll see below), so went down the path of adaptive subdivision algorithms.\u003c/p\u003e\n\n\u003ch2 id=\"the-control-polygon-length-approach\"\u003eThe control polygon length approach\u003c/h2\u003e\n\n\u003cp\u003eIn that Stack Overflow post was a reference to “\u003ca href=\"https://www.sciencedirect.com/science/article/pii/0925772195000542\"\u003eAdaptive subdivision and the length and energy of Bézier curves\u003c/a\u003e” by Jens Gravesen. My first step was to implement that. Long story short, it’s not terrible, but it is possible to do better.\u003c/p\u003e\n\n\u003cp\u003eThe insight of the Gravesen paper is that the actual length is always somewhere between the distance between the endpoints (the length of the chord) and the perimeter of the control polygon. And, for a quadratic Bézier, 2/3 the first + 1/3 the second is a reasonably good estimate.\u003c/p\u003e\n\n\u003csvg width=\"480\" height=\"240\"\u003e\n    \u003cpath d=\"m100 10 q100 0 200 200\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n    \u003cpath d=\"m100 10 l100 0 100 200\" stroke=\"black\" stroke-dasharray=\"4, 6\" fill=\"none\"\u003e\u003c/path\u003e\n    \u003ctext x=\"90\" y=\"110\"\u003eLc = 0.956 L\u003c/text\u003e\n    \u003cpath d=\"m100 10 l200 200\" stroke=\"black\" stroke-dasharray=\"5,5\" fill=\"none\"\u003e\u003c/path\u003e\n    \u003ctext x=\"210\" y=\"20\"\u003eLp =1.094 L\u003c/text\u003e\n    \u003ctext x=\"260\" y=\"80\"\u003e(2Lc + Lp)/3 = 1.002 L\u003c/text\u003e\n    \u003ccircle cx=\"100\" cy=\"10\" r=\"2\" fill=\"black\"\u003e\u003c/circle\u003e\n    \u003ccircle cx=\"200\" cy=\"10\" r=\"2\" fill=\"black\"\u003e\u003c/circle\u003e\n    \u003ccircle cx=\"300\" cy=\"210\" r=\"2\" fill=\"black\"\u003e\u003c/circle\u003e\n\u003c/svg\u003e\n\n\u003cp\u003eMore to the point, Gravesen’s approach gives hard error bounds, which is the basis of a subdivision approach. At each step, you test whether the estimate is within the desired tolerance. If so, you use the approximation. If not, you subdivide (using \u003ca href=\"https://en.wikipedia.org/wiki/De_Casteljau%27s_algorithm\"\u003ede Casteljau\u003c/a\u003e, of course), and run the algorithm on each half. This metric has 1/16 the error for each subdivision, so we’re at $O(N^\\frac{1}{4})$ worst case, decidedly better than above.\u003c/p\u003e\n\n\u003cp\u003eGravesen also observed that the approximation is often a lot better than the error bound would indicate. When this happens, successive refinements don’t change the estimate much. Why not use the amount by which the estimate changes on subdivision as a more realistic bound? Gravesen observes, “Error estimate no. 1 overestimates the error, and there are up to 400 times too many subdivisions. Error estimate no. 2 seems to be reliable…” And several examples are given in which it does the right thing. Much of the paper is taken with theorems and proofs that the error bound is accurate, so I thought I was on solid ground.\u003c/p\u003e\n\n\u003ch2 id=\"empirical-evaluation\"\u003eEmpirical evaluation\u003c/h2\u003e\n\n\u003cp\u003eThe section on arclength in the Primer suggests Legendre-Gauss quadrature. After implementing the Gravesen algorithm, of course I wondered, is that better? How much so?\u003c/p\u003e\n\n\u003cp\u003eThe best way to evaluate such questions is to visualize the data. It would be nice to plot an error metric for every possible quadratic Bézier. That might seem tricky, but it turns out that quadratic Bézier curves can be considered a 2-parameter curve family. For any arbitrary quad segment, translate, uniformly scale, and rotate the curve to bring the endpoints to fixed positions. These operations (known as conformal transformations) don’t affect the fundamental shape of the curve, and arclength measurement should be invariant to them.\u003c/p\u003e\n\n\u003cp\u003eFor this evaluation, it’s convenient to put the endpoints at (-1, 0) and (1, 0). Then, the two free parameters can be interpreted simply as the control point in the middle. The reason for these particular points is that the center point can be interpreted as the second derivative of the curve. When it’s at (0, 0) it’s a straight line, which is especially easy. Also, the more a curve is subdivided, the closer the subdivided curves come to a straight line.\u003c/p\u003e\n\n\u003cp\u003eHere’s what such a map looks like; smooth Béziers near the bottom left corner (which is 0,0), more curved ones farther out, up to 2 in each direction. Along the bottom edge, up to (1, 0) are straight lines just with the control point shifted, but beyond that are pathological curves that contain an infinitely sharp turn; not surprisingly, these will be tricky for some algorithms.\u003c/p\u003e\n\n\u003csvg width=\"320\" height=\"240\"\u003e\n\u003cpath d=\"M40.0 235.0q10.0 -0.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M62.0 235.0q12.0 -0.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M84.0 235.0q14.0 -0.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M106.0 235.0q16.0 -0.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M128.0 235.0q18.0 -0.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M150.0 235.0q20.0 -0.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M172.0 235.0q22.0 -0.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M194.0 235.0q24.0 -0.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M216.0 235.0q26.0 -0.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M238.0 235.0q28.0 -0.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M260.0 235.0q30.0 -0.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M40.0 213.0q10.0 -2.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M62.0 213.0q12.0 -2.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M84.0 213.0q14.0 -2.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M106.0 213.0q16.0 -2.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M128.0 213.0q18.0 -2.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M150.0 213.0q20.0 -2.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M172.0 213.0q22.0 -2.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M194.0 213.0q24.0 -2.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M216.0 213.0q26.0 -2.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M238.0 213.0q28.0 -2.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M260.0 213.0q30.0 -2.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M40.0 191.0q10.0 -4.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M62.0 191.0q12.0 -4.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M84.0 191.0q14.0 -4.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M106.0 191.0q16.0 -4.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M128.0 191.0q18.0 -4.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M150.0 191.0q20.0 -4.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M172.0 191.0q22.0 -4.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M194.0 191.0q24.0 -4.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M216.0 191.0q26.0 -4.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M238.0 191.0q28.0 -4.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M260.0 191.0q30.0 -4.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M40.0 169.0q10.0 -6.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M62.0 169.0q12.0 -6.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M84.0 169.0q14.0 -6.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M106.0 169.0q16.0 -6.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M128.0 169.0q18.0 -6.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M150.0 169.0q20.0 -6.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M172.0 169.0q22.0 -6.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M194.0 169.0q24.0 -6.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M216.0 169.0q26.0 -6.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M238.0 169.0q28.0 -6.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M260.0 169.0q30.0 -6.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M40.0 147.0q10.0 -8.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M62.0 147.0q12.0 -8.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M84.0 147.0q14.0 -8.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M106.0 147.0q16.0 -8.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M128.0 147.0q18.0 -8.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M150.0 147.0q20.0 -8.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M172.0 147.0q22.0 -8.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M194.0 147.0q24.0 -8.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M216.0 147.0q26.0 -8.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M238.0 147.0q28.0 -8.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M260.0 147.0q30.0 -8.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M40.0 125.0q10.0 -10.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M62.0 125.0q12.0 -10.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M84.0 125.0q14.0 -10.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M106.0 125.0q16.0 -10.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M128.0 125.0q18.0 -10.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M150.0 125.0q20.0 -10.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M172.0 125.0q22.0 -10.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M194.0 125.0q24.0 -10.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M216.0 125.0q26.0 -10.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M238.0 125.0q28.0 -10.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M260.0 125.0q30.0 -10.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M40.0 103.0q10.0 -12.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M62.0 103.0q12.0 -12.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M84.0 103.0q14.0 -12.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M106.0 103.0q16.0 -12.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M128.0 103.0q18.0 -12.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M150.0 103.0q20.0 -12.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M172.0 103.0q22.0 -12.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M194.0 103.0q24.0 -12.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M216.0 103.0q26.0 -12.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M238.0 103.0q28.0 -12.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M260.0 103.0q30.0 -12.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M40.0 81.00000000000001q10.0 -14.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M62.0 81.00000000000001q12.0 -14.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M84.0 81.00000000000001q14.0 -14.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M106.0 81.00000000000001q16.0 -14.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M128.0 81.00000000000001q18.0 -14.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M150.0 81.00000000000001q20.0 -14.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M172.0 81.00000000000001q22.0 -14.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M194.0 81.00000000000001q24.0 -14.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M216.0 81.00000000000001q26.0 -14.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M238.0 81.00000000000001q28.0 -14.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M260.0 81.00000000000001q30.0 -14.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M40.0 58.99999999999999q10.0 -16.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M62.0 58.99999999999999q12.0 -16.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M84.0 58.99999999999999q14.0 -16.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M106.0 58.99999999999999q16.0 -16.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M128.0 58.99999999999999q18.0 -16.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M150.0 58.99999999999999q20.0 -16.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M172.0 58.99999999999999q22.0 -16.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M194.0 58.99999999999999q24.0 -16.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M216.0 58.99999999999999q26.0 -16.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M238.0 58.99999999999999q28.0 -16.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M260.0 58.99999999999999q30.0 -16.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M40.0 37.0q10.0 -18.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M62.0 37.0q12.0 -18.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M84.0 37.0q14.0 -18.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M106.0 37.0q16.0 -18.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M128.0 37.0q18.0 -18.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M150.0 37.0q20.0 -18.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M172.0 37.0q22.0 -18.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M194.0 37.0q24.0 -18.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M216.0 37.0q26.0 -18.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M238.0 37.0q28.0 -18.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M260.0 37.0q30.0 -18.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M40.0 15.0q10.0 -20.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M62.0 15.0q12.0 -20.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M84.0 15.0q14.0 -20.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M106.0 15.0q16.0 -20.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M128.0 15.0q18.0 -20.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M150.0 15.0q20.0 -20.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M172.0 15.0q22.0 -20.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M194.0 15.0q24.0 -20.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M216.0 15.0q26.0 -20.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M238.0 15.0q28.0 -20.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003cpath d=\"M260.0 15.0q30.0 -20.0 20 0\" stroke=\"black\" fill=\"none\"\u003e\u003c/path\u003e\n\u003c/svg\u003e\n\n\u003cp\u003eGiven such a map of all possible quadratic Béziers, we can now plot the accuracy of various approximate algorithms. Here’s the Gravesen one:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/assets/gravesen_error_plot.png\" width=\"570\" height=\"520\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eThis is plotted as error on a log-scale. Black and blue are the most accurate (10 and 8 digits of precision), 0 the least. Not surprisingly, we see it do well for nearly straight curves. There’s also a line in the middle, but there’s nothing special about it; it’s just a visual indication that the approximation overshoots on one side and undershoots on the other, so the error happens to be 0 between those two regions.\u003c/p\u003e\n\n\u003cp\u003eNow to compare to Legendre-Gauss quadrature. Fortunately there’s code for that in \u003ca href=\"https://github.com/Pomax/BezierInfo-2/issues/77\"\u003ePomax/BezierInfo-2#77\u003c/a\u003e by \u003ca href=\"http://behdad.org/\"\u003eBehdad\u003c/a\u003e so it was easy enough to test.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/assets/3rd_order_quadrature_error_plot.png\" width=\"570\" height=\"520\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eIt’s quite a bit better; the region where it’s very accurate is bigger. Interestingly enough, though, it doesn’t do a lot better for extreme cases. Intuitively, it should be possible to get accurate results with fewer subdivisions. The problem is: how do you compute a bound on the error? The advantage of the Gravesen approach is that it has the error metric built-in.\u003c/p\u003e\n\n\u003cp\u003eOr does it? Let’s verify that. I implemented the adaptive subdivision from the Gravesen paper and then made this plot, with the accuracy threshold set to 1e-4:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/assets/gravesen_subdivided_error_plot.png\" width=\"570\" height=\"520\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eIf it’s working as intended, then no part of the map should go beyond red, the color for 1e-4. But we can see that for some stripes (generally the regions where the approximation is crossing from overshoot to undershoot), the error is underestimated as well, and there are bits of orange where that happens. It’s a fairly small fraction of the area of the map, and these get even thinner as the threshold is set lower. But even so, as a way to guarantee measurements of a given accuracy, it’s a failure. We need a better approach.\u003c/p\u003e\n\n\u003cp\u003e(I’m using the \u003ca href=\"https://github.com/linebender/kurbo/blob/master/examples/arclen_accuracy.rs\"\u003earclen_accuracy\u003c/a\u003e example program in kurbo to make the raw data for these plots, then gnuplot with mostly default settings to visualize them.)\u003c/p\u003e\n\n\u003cp\u003eIncidentally, this map also lets us visualize the subdivisions; they’re sharp lines with the choice to subdivide or not subdivide on either side.\u003c/p\u003e\n\n\u003ch2 id=\"quadrature-overkill\"\u003eQuadrature overkill\u003c/h2\u003e\n\n\u003cp\u003eHow does Pomax’s \u003ca href=\"https://pomax.github.io/bezierjs/\"\u003eBezier.js\u003c/a\u003e solve this problem? Looking at the code, it uses 24-order Legendre-Gauss quadrature. This seems like it should be massive overkill and is not that expensive to compute (it’s basically 24 “hypot” operations plus some linear math). Can we just do that? Let’s take a look.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/assets/24th_order_quadrature_error_plot.png\" width=\"570\" height=\"520\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eLooking at this, reasonably smooth quadratic Béziers get measured very precisely, but more extreme ones do not. In fact, for the ones that have sharp kinks, it only does a little better than the simpler techniques. So it’s overkill for part of the range, and undershoot for other parts. These pathological Béziers can and do happen, especially during interactive editing. For completely general use, the technique in Bezier.js doesn’t solve our problem.\u003c/p\u003e\n\n\u003cp\u003eIncidentally, if I ever start a punk band, it will be called “quadrature overkill.”\u003c/p\u003e\n\n\u003ch2 id=\"an-error-metric\"\u003eAn error metric\u003c/h2\u003e\n\n\u003cp\u003eOne way to move forward is to cook up an error metric that absolutely bounds the error of some approximation. Ideally this metric is fairly tight, otherwise it’ll tell us we need to subdivide when we don’t. I basically came up with one by “painting with math”. It’s just a 2-parameter function, and we know the general shape it needs to have just by looking at the images. For the 3rd order quadrature above, I came up with this function:\u003c/p\u003e\n\n\u003cdiv class=\"language-rust highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e    \u003cspan class=\"k\"\u003elet\u003c/span\u003e \u003cspan class=\"n\"\u003eest_err\u003c/span\u003e \u003cspan class=\"o\"\u003e=\u003c/span\u003e \u003cspan class=\"mf\"\u003e0.06\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003elp\u003c/span\u003e \u003cspan class=\"o\"\u003e-\u003c/span\u003e \u003cspan class=\"n\"\u003elc\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"n\"\u003ex\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"n\"\u003ex\u003c/span\u003e \u003cspan class=\"o\"\u003e+\u003c/span\u003e \u003cspan class=\"n\"\u003ey\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"n\"\u003ey\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e\u003cspan class=\"nf\"\u003e.powi\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"mi\"\u003e2\u003c/span\u003e\u003cspan class=\"p\"\u003e);\u003c/span\u003e\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eHere the \u003ccode class=\"language-plaintext highlighter-rouge\"\u003elp\u003c/code\u003e and \u003ccode class=\"language-plaintext highlighter-rouge\"\u003elc\u003c/code\u003e variables represent the length of the perimeter and chord; these ideas are borrowed from the Gravesen paper, and including that gives us a tighter bound for values near the bottom edge of our plot. Other than that, it’s basically the norm of the second derivative raised to the power that matches the scaling of our quadrature.\u003c/p\u003e\n\n\u003cp\u003eOne good way to validate such a function is scatter plots; for each point we plot the estimated error on the x axis, and the actual error on the y axis. No point is allowed to be above the x=y line, and ideally every point is pretty close to it. Let’s see how we did:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/assets/quadrature_error_est.png\" width=\"603\" height=\"571\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eThat’s a good error metric. And let’s see how it performs:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/assets/quadrature_subdivided.png\" width=\"570\" height=\"520\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eNote that the colors are rescaled; it only goes up to 1e-4 so we can see more clearly (and because it’s prettier that way). And we see that the metric is doing well; precision much better than the threshold is a sign we’re wasting computation. We also see that it’s subdividing less than the Gravesen metric, which means computation is faster.\u003c/p\u003e\n\n\u003ch2 id=\"back-to-analytics\"\u003eBack to analytics\u003c/h2\u003e\n\n\u003cp\u003eI sent some of this to Pomax, who reminded me that the arclength of a quadratic Bézier has a closed-form analytical solution. If I’m going for the best possible solution, shouldn’t I give that another try? And honestly, by this time my competitive spirit had kicked in. If there was a better solution for this problem than what I had in kurbo, I wouldn’t be happy.\u003c/p\u003e\n\n\u003cp\u003eI’m not going to go over the solution of the integral, fortunately \u003ca href=\"https://web.archive.org/web/20180418075534/http://www.malczak.linuxpl.com/blog/quadratic-bezier-curve-length/\"\u003eMateusz Malczak\u003c/a\u003e has written up a good explanation, and it comes with working code as well. And who integrates things by hand any more? If the integral is at all tractable, just put it into Mathematica and it’s almost certain to find it. This is what Mateusz came up with:\u003c/p\u003e\n\n\\[\\frac{1}{8a^\\frac{3}{2}}{\\Large[}4a^\\frac{3}{2}\\sqrt{a+b+c}+2\\sqrt{a}b(\\sqrt{a+b+c} - \\sqrt{c})\n- (b^2 - 4ac)\\ln{\\large |}\\frac{2\\sqrt{a} + \\frac{b}{\\sqrt{a}} + 2\\sqrt{a+b+c}} {\\frac{b}{\\sqrt{a}} + 2\\sqrt{c}}{\\large|}\n{\\Large]}\\]\n\n\u003cp\u003eEasy! In this formula, $a$, $b$, and $c$ represent squared norms of the second and first derivatives at $t=0$, and $c$ is the squared norm of chord length. Details are of course on the linked page.\u003c/p\u003e\n\n\u003cp\u003eI had several concerns about this approach. One is numerical stability; the formula has several divide operations, which mostly are mostly over powers of the second derivative norm. Given that, it’s likely that accuracy will degrade as the curve gets closer to a straight line. And inded, for an exact straight line this code gives \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eNaN\u003c/code\u003e. Zooming in, we can see the problem:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/assets/analytical_error.png\" width=\"570\" height=\"520\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eNote that the colors are rescaled again; the worst error on this map (other than the \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eNaN\u003c/code\u003e at the origin which is not plotted) is 1e-11. That’s not bad, but let’s do the best we can while fixing the singularity at the origin. Fortunately, already have a function which is good in that range, the quadrature approach. The actual code in kurbo compares the second derivative norm against a threshold, and switches to quadrature inside that:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/assets/analytical_repaired.png\" width=\"570\" height=\"520\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eThere’s another numerical instability for curves with a sharp kink (surprise, surprise); internal to the math this happens when $b^2 - 4ac$ becomes zero. It’s fixed in a similar way. Also, in addition to the visualizations in this blog post, kurbo has tests for these cases. I believe this algorithm has over 13 digits of precision over the entire map.\u003c/p\u003e\n\n\u003ch2 id=\"performance\"\u003ePerformance\u003c/h2\u003e\n\n\u003cp\u003eDoes the extra accuracy of the analytical approach (with the fixes in place for numerical stability) come at a cost? Let’s benchmark:\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003etest bench_quad_arclen             ... bench:          46 ns/iter (+/- 17)\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eI’m impressed. It’s quite a bit faster (21ns) without the numerical stability fixes and with \u003ccode class=\"language-plaintext highlighter-rouge\"\u003etarget-cpu=native\u003c/code\u003e, but for a library I think it’s much more important to be robust than absolutely at the edge of performance.\u003c/p\u003e\n\n\u003ch2 id=\"onward-to-cubics\"\u003eOnward to cubics\u003c/h2\u003e\n\n\u003cp\u003eOf course, the analytical solution is only applicable to quadratics. The \u003ca href=\"https://en.wikipedia.org/wiki/Abel%E2%80%93Ruffini_theorem\"\u003eAbel–Ruffini theorem\u003c/a\u003e proves that it can’t be solved in closed form for higher polynomials. So for cubics we go back to the adaptive subdivision approach.\u003c/p\u003e\n\n\u003cp\u003eAgain, the tricky part is the error metric. The error metric for a quadratic Bézier is based on the norm of the second derivative. For a quadratic, the second derivative is constant, so it’s easy to write expressions in terms of it. For a cubic, the second derivative is linear in $t$, so we want to somehow capture the fact that it varies across the parameter space.\u003c/p\u003e\n\n\u003cp\u003eUnlike quadratics, it’s hard to visualize the space of all possible cubics; it’s a four-parameter space, and my ability to visualize fields in four dimensions is limited. Thus, instead of 2d maps I mostly used randomly generated cubics, and scatterplots of whatever I wanted to measure from those. Cubics are also trickier, it’s not going to be easy to get error bounds as tight.\u003c/p\u003e\n\n\u003cp\u003eAfter some experimentation, mostly iterating on those scatter plots and trying things that either improved the tightness of the error bound or made it worse, I found that working with the \u003cem\u003eintegral\u003c/em\u003e of the second derivative norm was both tractable and gave a decent error bound.\u003c/p\u003e\n\n\u003cp\u003eLet’s write the second derivative as a simple linear equation:\u003c/p\u003e\n\n\\[cubic\u0026#39;\u0026#39;(t) = at + b\\]\n\n\u003cp\u003eThen we want the integral of its square across the parameter range:\u003c/p\u003e\n\n\\[\\int_0^1 (at + b)^2 dt\\]\n\n\u003cp\u003eSince this is just a polynomial, and doesn’t have a square root in it, the integral is nearly trivial:\nit’s $|a|^2/3 + a \\cdot b + |b|^2$. I even did this without using Mathematica :).\u003c/p\u003e\n\n\u003cp\u003eIt’s also nice that this is easy to compute; I tried approaches based on numerically integrating a more sophisticated error metric, but then the time spent computing the metric dominates the time actually approximating arc length.\u003c/p\u003e\n\n\u003cp\u003eAll this work is based on heuristics. Going to higher order quadrature helps up to a point, but as long as we’re able to estimate the error metric accurately enough. I found a good compromise with a 9th order quadrature.\u003c/p\u003e\n\n\u003cp\u003eThere are a few ways to evaluate performance. One is a scatterplot of the number of subdivisions required and the actual accuracy. Below is a sampling of random cubic Béziers with an error tolerance of 1e-4:\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/assets/cubic_arclen_performance.png\" width=\"630\" height=\"478\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eThe vertical axis has the number of subdivisions, and the horizontal axis the actual error. Obviously we don’t want any points to the right of the specified tolerance. The majority of cases are handled with 4 or fewer subdivisions (in fact the mean is about 3.4 over my sample). It would be nicer to have them bunched closer to the right, but that would require a better error bound. Perhaps some enterprising reader will take up this work :).\u003c/p\u003e\n\n\u003cp\u003eWe can benchmark the time taken as well:\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003etest bench_cubic_arclen_1e_4 ... bench:         231 ns/iter (+/- 98)\ntest bench_cubic_arclen_1e_5 ... bench:         206 ns/iter (+/- 93)\ntest bench_cubic_arclen_1e_6 ... bench:         423 ns/iter (+/- 114)\ntest bench_cubic_arclen_1e_7 ... bench:         424 ns/iter (+/- 160)\ntest bench_cubic_arclen_1e_8 ... bench:         634 ns/iter (+/- 267)\ntest bench_cubic_arclen_1e_9 ... bench:         631 ns/iter (+/- 161)\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eWhile not quite as impressive as the quadratic case, these are still good showings and should be quite fine for production use. The Bézier benchmarked here is (0,0), (1/3, 0), (2/3, 1), (1,1), which has a nice “S” shape; computation time is dependent on the shape, and of course pathological Béziers with kinks will need more subdivision.\u003c/p\u003e\n\n\u003ch2 id=\"lessons-learned\"\u003eLessons learned\u003c/h2\u003e\n\n\u003cp\u003eI enjoyed this journey; it was a chance to dive deep into territory I find fun. I feel it’s likely that kurbo now has the finest arclength measurement code of any curves library on the planet. If anybody knows of better, please let me know! It’s likely that many other libraries have ranges where they give inaccurate results (especially for pathological Béziers) and are likely not as performant.\u003c/p\u003e\n\n\u003cp\u003eI also found Rust to be an excellent implementation language for this work. I enjoyed writing code using higher level concepts and being able to rely on the compiler to flatten out all the abstraction and generate excellent code (in a few cases I looked at the asm, it’s pretty sweet).\u003c/p\u003e\n\n\u003cp\u003eOne lesson is to \u003cem\u003ealways\u003c/em\u003e empirically measure what you’re doing. Guaranteed you will learn something, otherwise you’re flying blind. Visualizations are especially good because you can see lots of data points. Similarly for randomly generated data. Otherwise there’s a good chance you’ll miss something.\u003c/p\u003e\n\n\u003cp\u003eThis is especially true when working from academic papers. Having lots of theorems is not a reliable sign the algorithms translate directly to robust code.\u003c/p\u003e\n\n\u003cp\u003eAnother lesson is that simpler curves such as quadratics are easier to work with and ultimately give better results in spite of needing more of them to accurately represent a curve. I found this holds for the \u003ca href=\"https://docs.rs/kurbo/0.1.0/kurbo/trait.ParamCurveNearest.html#tymethod.nearest\"\u003enearest point\u003c/a\u003e method; for quadratics there’s an exact solution based on solving a cubic equation, but cubics require subdivision. I suspect the same will be true of other algorithms including offset curve.\u003c/p\u003e\n\n\u003ch2 id=\"other-resources\"\u003eOther resources\u003c/h2\u003e\n\n\u003cp\u003eBehdad has a great \u003ca href=\"https://www.youtube.com/watch?v=4_Dy3-_MyiA\u0026amp;feature=youtu.be\u0026amp;t=24m5s\"\u003eTYPO Labs 2017 presentation\u003c/a\u003e on the math used for variational fonts. The implementation of \u003ca href=\"https://github.com/fonttools/fonttools/blob/master/Lib/fontTools/misc/bezierTools.py#L98\"\u003earclength in FontTools\u003c/a\u003e uses the same basic analytical approach, and this is shown in the video along with low-order Legendre-Gauss Quadrature and recursive perimeter-chord subdivision. The presentation also shows exact calculation of area using Green’s theorem (also \u003ca href=\"https://docs.rs/kurbo/0.1.0/kurbo/trait.ParamCurveArea.html\"\u003eimplemented\u003c/a\u003e in kurbo) and the use of SymPy to compute curve properties symbolically.\u003c/p\u003e\n\n\u003cp\u003eJacob Rus has an interactive \u003ca href=\"https://beta.observablehq.com/@jrus/bezier-segment-arclength\"\u003eBézier Segment Arclength\u003c/a\u003e Observable notebook. It uses high-degree Chebyshev polynomials, which are closely related to Legendre polynomials, and there’s a sophisticated inverse solver (needed for dashing). The inverse solver in kurbo is bisection, which is robust but not as fast in smooth cases.\u003c/p\u003e\n\n\u003ch2 id=\"future-work\"\u003eFuture work\u003c/h2\u003e\n\n\u003cp\u003eI think arclength of quadratics is pretty much settled at this point. For cubics, the current solution is pretty good, but can likely be improved a bit more. Certainly I make no claims the error metric is perfect, and a tighter bound on that would unlock exploiting higher degree quadrature, which could converge a lot faster. One promising approach is to identify problematic inputs (ones for which the actual error is worse than what a simplistic error metric would predict). If, as seems likely, curves with cusps are an important category of those, then using the \u003ca href=\"https://pomax.github.io/bezierinfo/#canonical\"\u003eStone and DeRose geometric characterization\u003c/a\u003e could help find those (thanks Pomax for the idea). Another idea (thanks to Jacob Rus) is to search for curvature maxima and use that to guide the subdivision. Determining maximum curvature (at least approximately) should be fairly tractable, but as always there’s a tradeoff between the cost of computing the error metric vs the savings in subdivision.\u003c/p\u003e\n\n\u003cp\u003eApplying a Newton method is likely to speed up the inverse method. That shouldn’t be too hard.\u003c/p\u003e\n\n\u003cp\u003ePerhaps an enterprising math enthusiast will be inspired to take up these problems. If so, I’ll happily incorporate the work into kurbo.\u003c/p\u003e\n\n\u003cp\u003eI personally am inclined to declare victory and move on. There are other interesting curve problems to solve!\u003c/p\u003e\n\n\u003ch2 id=\"thanks\"\u003eThanks\u003c/h2\u003e\n\n\u003cp\u003eThanks to Pomax for the primer, Legendre-Gauss quadrature resources, and encouraging me to write this blog, as well as some feedback. Thanks to Behdad for the shared intellectual curiosity about Bézier math and the application to fonts, plus resource suggestions. Thanks to Mateusz Malczak for his derivation of the analytical quadratic Bézier arclength formula and permission to adapt his code. Thanks to Jacob Rus for feedback and suggestions.\u003c/p\u003e\n\n\u003cp\u003eAnd thanks to you for reading!\u003c/p\u003e\n\n\u003cp\u003eDiscuss on \u003ca href=\"https://lobste.rs/s/ysgy3e/how_long_is_bezier\"\u003elobste.rs\u003c/a\u003e and \u003ca href=\"https://news.ycombinator.com/item?id=18786583\"\u003eHacker News\u003c/a\u003e.\u003c/p\u003e\n\n\n  \u003c/div\u003e",
  "Date": "2018-12-28T17:23:42Z",
  "Author": "raphlinus"
}