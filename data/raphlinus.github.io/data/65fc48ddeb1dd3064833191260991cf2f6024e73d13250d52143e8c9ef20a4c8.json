{
  "Source": "raphlinus.github.io",
  "Title": "Towards GPGPU JSON parsing",
  "Link": "https://raphlinus.github.io/personal/2018/05/10/toward-gpu-json-parsing.html",
  "Content": "\u003cdiv class=\"post-content e-content\" itemprop=\"articleBody\"\u003e\n    \u003cp\u003e\u003cstrong\u003eUpdate 2020-09-06:\u003c/strong\u003e a followup to this post is \u003ca href=\"https://raphlinus.github.io/gpu/2020/09/05/stack-monoid.html\"\u003eThe stack monoid\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eThe amount of computing resources available on general purpose GPU hardware is vastly greater than in scalar CPUs. A continuing trend is to move computation from CPU to GPGPU. Some computations (most 3D graphics operations, many machine learning tasks) can be expressed efficiently in terms of primitives that GPUs offer. However, tasks such as JSON parsing are traditionally considered as serial algorithms and are not often implemented on GPU.\u003c/p\u003e\n\n\u003cp\u003eI’ve been thinking about how to apply rope science techniques to this problem, and now believe I have a practical solution. This post sketches my idea at a high level - I haven’t implemented it yet and have little hands-on experience with GPU, so who knows what could go wrong?\u003c/p\u003e\n\n\u003ch2 id=\"the-problem\"\u003eThe problem\u003c/h2\u003e\n\n\u003cp\u003eWe’ll choose a juicy fragment of general JSON: parsing \u003ca href=\"https://en.wikipedia.org/wiki/Dyck_language\"\u003eDyck languages\u003c/a\u003e. In this post, we’ll concentrate entirely on extracting the tree structure from the input.\u003c/p\u003e\n\n\u003cp\u003eThe output will be an array representation suitable for both random access and batch processing. Each node in the tree will be stored as a contiguous block in the output array. The first word in the block is the number of children, and the successive words will be the indices of the children (with the root at index 0). This representation is similar to Cap’n Proto, Flat buffers, and FIDL. For reasons that will become clear later, we’ll generate this result in \u003ca href=\"https://en.wikipedia.org/wiki/Breadth-first_search\"\u003eBFS\u003c/a\u003e order.\u003c/p\u003e\n\n\u003cp\u003eAs a running example, we’ll use the input \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e[[][[][][[]]][][]]\u003c/code\u003e. The result is:\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003eindex 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16\nvalue 4  5  6 10 11  0  3 12 13 14  0  0  0  0  1 16  0\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003ch2 id=\"the-primitives\"\u003eThe primitives\u003c/h2\u003e\n\n\u003cp\u003eI’m building my parser from three basic primitives: scan, scatter/gather, and sort. All three can be implemented reasonably efficiently on GPGPU.\u003c/p\u003e\n\n\u003ch3 id=\"scan\"\u003eScan\u003c/h3\u003e\n\n\u003cp\u003e\u003ca href=\"https://en.wikipedia.org/wiki/Prefix_sum\"\u003eScan\u003c/a\u003e is a generalization of prefix sum, potentially based on any binary associative operator. We’ll restrict ourselves to operators over small, fixed-size data structures\u003c/p\u003e\n\n\u003ch3 id=\"scattergather\"\u003eScatter/gather\u003c/h3\u003e\n\n\u003cp\u003eThe “gather” operation is basically texture read. For parsing, we’ll rely more heavily on “scatter,” which I’ll define as follows. There are three inputs (which can be arrays, or can be simple computations over arrays): condition, index, and value. The result is a buffer b, so that for any triple (true, index, value) in the input, b[index] = value. The operation is only well defined when the value for any index is unique, when we use the primitive we’ll make sure that there are no duplicate indexes.\u003c/p\u003e\n\n\u003cp\u003eScatter is not necessarily an efficient primitive on GPU, but good techniques exist, see [1] [2].\u003c/p\u003e\n\n\u003ch3 id=\"sort\"\u003eSort\u003c/h3\u003e\n\n\u003cp\u003eSorting is one of the most fundamental algorithms. There is extensive literature on efficient GPU implementation.\u003c/p\u003e\n\n\u003ch2 id=\"the-algorithm\"\u003eThe algorithm\u003c/h2\u003e\n\n\u003cp\u003eWe’ll present it in as a sequence of passes, using our running example.\u003c/p\u003e\n\n\u003ch3 id=\"count-nodes-compute-nesting-depth\"\u003eCount nodes, compute nesting depth\u003c/h3\u003e\n\n\u003cp\u003eThe first pass counts the number of nodes and the nesting depth of each node. To count nodes, map \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e[\u003c/code\u003e to 1 and \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e]\u003c/code\u003e to 0, and compute (exclusive) prefix sum. Nesting depth is the same, but map \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e]\u003c/code\u003e to -1.\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003einput:  [  [  ]  [  [  ]  [  ]  [  [  ]  ]  ]  [  ]  [  ]  ]\n node:  0  1  2  2  3  4  4  5  5  6  7  7  7  7  8  8  9  9\ndepth:  0  1  2  1  2  3  2  3  2  3  4  3  2  1  2  1  2  1\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eAt this point, we know the number of nodes (9). We can also validate that the brackets are balanced, the result of the depth sum should be 0. Note that for the node and depth calculations we’re using \u003cem\u003eexclusive\u003c/em\u003e prefix sum; we’ll be using both inclusive and exclusive variants depending on the exact needs.\u003c/p\u003e\n\n\u003ch3 id=\"reduce-to-nodes\"\u003eReduce to nodes\u003c/h3\u003e\n\n\u003cp\u003eWe only care about nodes, not symbols. Our first scatter will generate the depth of each node. The condition is that the input is \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e[\u003c/code\u003e, the index is the node count, and the value is the depth. The result:\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003eindex:  0  1  2  3  4  5  6  7  8\ndepth:  0  1  1  2  2  2  3  1  1\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eThis array fully captures the structure of our tree.\u003c/p\u003e\n\n\u003ch3 id=\"sort-by-depth\"\u003eSort by depth\u003c/h3\u003e\n\n\u003cp\u003eNow sort the tree into BFS order. This is basically a stable sort using depth as the primary key. However, instead of actually sorting the array, we record for each node its order in a BFS traversal. (This is equivalent to retaining the index of each element while sorting, then doing the inverse permutation, which can be represented as a scatter, but it’s likely that an actual implementation can be more direct).\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003eindex:  0  1  2  3  4  5  6  7  8\ndepth:  0  1  1  2  2  2  3  1  1\n  bfs:  0  1  2  5  6  7  8  3  4\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003ch3 id=\"determine-parents-of-first-children\"\u003eDetermine parents of first children\u003c/h3\u003e\n\n\u003cp\u003eNow we do a scatter operation, with the goal of associating first children with their parent nodes. The condition is: depth[i + 1] = depth[i] + 1 (this means that node i + 1 is the first child of node i). The key is bfs[i + 1], and the value is bfs[i]:\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003eindex:  0  1  2  3  4  5  6  7  8\n 1par:     0           2        7\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eAfter this pass, each element of 1par represents a node in BFS order, and holds the index of the parent node if it’s a first child.\u003c/p\u003e\n\n\u003ch3 id=\"propagate-parent-links\"\u003ePropagate parent links\u003c/h3\u003e\n\n\u003cp\u003eThe next pass is a scan that propagates the parent link to the right. In the same pass, count the number of children.\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e index:  0  1  2  3  4  5  6  7  8\nparent:     0  0  0  0  2  2  2  7\n count:     1  2  3  4  1  2  3  1\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eAfter this pass, each element of parent represents a node in BFS order, and holds the index of the parent node. This works because, in BFS order, all the siblings (ie all nodes sharing the same parent) are in a contiguous block.\u003c/p\u003e\n\n\u003ch3 id=\"scatter-child-counts\"\u003eScatter child counts\u003c/h3\u003e\n\n\u003cp\u003eAnother scatter. The condition is that parent[i] != parent[i + 1], the index is parent[i], and the value is count[i]. The default is 0 (the scatter only writes a count for nonempty nodes).\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e index:  0  1  2  3  4  5  6  7  8\nparent:     0  0  0  0  2  2  2  7\n count:     1  2  3  4  1  2  3  1\nnchild:  4  0  3  0  0  0  0  1  0\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003ch3 id=\"allocate\"\u003eAllocate\u003c/h3\u003e\n\n\u003cp\u003eNow we can assign each node an index in the final output. The number of cells of each node is 1 + the number of children. Do an (exclusive) prefix sum of 1 + nchild[i].\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e index:  0  1  2  3  4  5  6  7  8\nparent:     0  0  0  0  2  2  2  7\nnchild:  4  0  3  0  0  0  0  1  0\n alloc:  0  5  6 10 11 12 13 14 16\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003ch3 id=\"generate-the-result\"\u003eGenerate the result\u003c/h3\u003e\n\n\u003cp\u003eThe final result is two scatters into the same array. For each node i, store nchild[i] into alloc[i], which establishes the size fields. Also store alloc[i] into alloc[parent[i] + count[i]], which establishes the references to the children.\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003eindex 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16\nvalue 4  5  6 10 11  0  3 12 13 14  0  0  0  0  1 16  0\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003ch2 id=\"discussion\"\u003eDiscussion\u003c/h2\u003e\n\n\u003cp\u003eTraditional parsing of bracket-balanced languages uses a stack to keep track of parent-child relationships. A stack is inherently a sequential data structure that does not lend itself well to implementation on a GPU. We obviate the need for a stack by taking advantage of the fact that the parent/child relationships have a simple structure when the parse tree is sorted into BFS order.\u003c/p\u003e\n\n\u003cp\u003eThe overall algorithm is 3 scans, 4 scatters, and a sort. Each of these operations should be efficiently implementable on a GPGPU. The most expensive operation is likely the sort; the scans in particular should be quite efficient because work-efficient algorithms are known.\u003c/p\u003e\n\n\u003cp\u003eThe scan/scatter technique can quite easily be extended to support, for example, backslash unescaping for the strings in the JSON document. It’s likely that dictionaries don’t present any serious difficulties beyond arrays; the keys can be hashed entirely in parallel, and constructing hash tables in the output can be done with the same kind of scan/scatter.\u003c/p\u003e\n\n\u003cp\u003eIs this technique already known? There is literature on parallel parsing for more general grammars [4], but it’s not clear to me that these approaches are at all efficient for simple grammars such as those needed to parse JSON (here abstracted to Dyck languages).\u003c/p\u003e\n\n\u003cp\u003eIs it really practical? I’ve been thinking about this at an abstract level, considering the kinds of operations that could be parallelized, but don’t know how efficient the scatter and sort operations would be in practice.\u003c/p\u003e\n\n\u003cp\u003eThanks to Rif A. Sauros for comments on an earlier draft.\u003c/p\u003e\n\n\u003ch2 id=\"references\"\u003eReferences\u003c/h2\u003e\n\n\u003cp\u003e[0] \u003ca href=\"https://github.com/antonmks/nvParse\"\u003envParse\u003c/a\u003e: Parsing CSV files with GPU\u003c/p\u003e\n\n\u003cp\u003e[1] \u003ca href=\"https://developer.nvidia.com/gpugems/GPUGems2/gpugems2_chapter32.html\"\u003eImplementing Scatter\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003e[2] \u003ca href=\"http://www.cse.ust.hk/catalac/papers/scatter_sc07.pdf\"\u003eEfficient Gather and Scatter Operations on Graphics Processors\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003e[3] \u003ca href=\"https://news.ycombinator.com/item?id=13797797\"\u003eAMA: Explaining my 750 line compiler+runtime designed to GPU self-host APL\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003e[4] \u003ca href=\"http://www.aclweb.org/anthology/U11-1006\"\u003eParsing in Parallel on Multiple Cores and GPUs\u003c/a\u003e\u003c/p\u003e\n\n\u003cp\u003e[5] \u003ca href=\"https://www.microsoft.com/en-us/research/publication/mison-fast-json-parser-data-analytics/\"\u003eMison: A Fast JSON Parser for Data Analytics\u003c/a\u003e\u003c/p\u003e\n\n\n  \u003c/div\u003e",
  "Date": "2018-05-10T15:37:03Z",
  "Author": "raphlinus"
}