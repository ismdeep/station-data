{
  "Source": "raphlinus.github.io",
  "Title": "IIR filters can be evaluated in parallel",
  "Link": "https://raphlinus.github.io/audio/2019/02/14/parallel-iir.html",
  "Content": "\u003cdiv class=\"post-content e-content\" itemprop=\"articleBody\"\u003e\n    \u003cp\u003e(Author’s note: I got slightly stuck writing this, so am publishing a somewhat unfinished draft, largely so I can get to the many other items in my blogging queue (there’s a \u003ca href=\"https://xi.zulipchat.com/#narrow/stream/181284-blogging/topic/Raph\u0026#39;s.20backlog\"\u003ethread\u003c/a\u003e on the xi Zulip for those more interested). I can come back to this and do a more polished version if there’s interest.)\u003c/p\u003e\n\n\u003cp\u003eAt the risk of oversimplification, there are basically two types of digital filter: finite impulse response and infinite impulse response. The former is basically taking the dot product of the filter response with a slice of input samples, for each input sample. Analysis of the latter is trickier, and involves internal \u003cem\u003estate\u003c/em\u003e, which in general decays over time but never goes exactly to zero.\u003c/p\u003e\n\n\u003cp\u003eIt is trivial to see how to evaluate FIR filters in parallel; each dot product is independent of the others, so it’s basically \u003ca href=\"https://en.wikipedia.org/wiki/Embarrassingly_parallel\"\u003eembarrassingly parallel\u003c/a\u003e. There’s more to to the story, especially as the filter kernel becomes larger. Then the simple O(nm) approach yields to O(n log n) techniques based on FFT, and these techniques are why \u003ca href=\"https://en.wikipedia.org/wiki/Convolution_reverb\"\u003econvolutional reverb\u003c/a\u003e is practical. But we know how do FFT with high parallelism.\u003c/p\u003e\n\n\u003cp\u003eBy contrast, IIR filters at first glance look like they \u003cem\u003emust\u003c/em\u003e be evaluated in series. But the first glance is misleading. Their linear nature means they can be evaluated quite efficiently in parallel, but it’s not obvious. I’m not stating a new fact here, it’s in the literature, but I haven’t found a particularly clear statement of it, nor a clear discussion of whether it only applies to linear time-invariant filters or whether the filter parameters can be modulated (spoiler alert: they can).\u003c/p\u003e\n\n\u003cp\u003eTake the simplest IIR filter, the so-called “RC lowpass filter”, better known as a one-pole filter:\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003ey[i] = x[i] * c + y[i - 1] * (1 - c)\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eFrom this formulation, it’s not possible to start calculation of \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ey[i]\u003c/code\u003e until \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ey[i-1]\u003c/code\u003e is known. This is basically a serial chain of data dependencies. But there’s more we can do, because of the linear nature of the filter. Let’s see if we can unroll it to do two at a time:\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003ey[i] = x[i] * c + (x[i - 1] * c + y[i - 2] * (1 - c)) * (1 - c)\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eThis calculates the same value (subject to floating point roundoff), but the data dependency is twice as long. On the other hand, it seems to require more multiplications, so it’s not obvious it’s a win.\u003c/p\u003e\n\n\u003cp\u003eThis idea generalizes to more sophisticated filters. Rather than writing it it in \u003ca href=\"https://ccrma.stanford.edu/~jos/fp/Direct_Form_I.html\"\u003edirect form\u003c/a\u003e, which emphasizes the serial evaluation strategy, it’s better to use matrices. Basically, \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ey\u003c/code\u003e becomes a state vector, and \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ea\u003c/code\u003e becomes a matrix. This is known as the state space approach to filters and has many advantages. I’d go so far to say that direct form is essentially obsolete now, optimizing for the number of multiplies at the expense of parallelism, numerical stability, and good modulation properties.\u003c/p\u003e\n\n\u003cp\u003eWhen I implemented the filter in \u003ca href=\"https://github.com/google/music-synthesizer-for-android\"\u003emusic-synthesizer-for-android\u003c/a\u003e, I was looking for techniques to speed up the code using SIMD. I came across some papers, notably \u003ca href=\"http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.1.3729\u0026amp;rep=rep1\u0026amp;type=pdf\"\u003eImplementation of recursive digital filters into vector SIMD DSP architectures\u003c/a\u003e, that gave working recipes for more general filtering, optimized for SIMD. These techniques work, and I wrote about them a bit in a notebook entitled \u003ca href=\"https://github.com/google/music-synthesizer-for-android/blob/master/lab/Second%20order%20sections%20in%20matrix%20form.ipynb\"\u003eSecond order sections in matrix form\u003c/a\u003e. In that notebook, I also argue for the advantages for numerical stability and modulation, and I won’t go into that more here.\u003c/p\u003e\n\n\u003cp\u003eReviewing the literature again, a \u003cem\u003emuch\u003c/em\u003e more detailed reference is \u003ca href=\"https://hal.inria.fr/hal-01167185\"\u003eCompiling High Performance Recursive Filters\u003c/a\u003e, which emphasizes code generation techniques but does cover the underlying math, including good references. Likely the earliest reference showing that IIR filters can be evaluated in parallel is the Sung and Mitra 1986 reference, “Efficient multi-processor implementation of recursive digital filters.” (no direct link available, but of course sci-hub works). Indeed, Sung and Mitra state quite clearly that time-varying filters work, as long as they’re linear.\u003c/p\u003e\n\n\u003ch2 id=\"monoid-homomorphism-time\"\u003eMonoid homomorphism time\u003c/h2\u003e\n\n\u003cp\u003eOne of my favorite mathematical frameworks, monoid homomorphism, is powerful enough to accommodate parallel evaluation of IIR filters as well. The basic insight is that the target monoid is a function from input state to output state, and represents any integral number of samples. The monoid binary operator is function composition, which is by nature associative and has an identity (the identity function).\u003c/p\u003e\n\n\u003cp\u003eThis is the same fundamental trick as lifting a regular expression (or, equivalently, finite state machine) into a monoid. In \u003cem\u003egeneral,\u003c/em\u003e the amount of state required to represent such a function, as opposed to a single state value, is intractable, but in these two cases it works. In the case of a regular expression, it works because the domain of the regular expression is finite. In the case of an IIR, it works because the function is linear.\u003c/p\u003e\n\n\u003cp\u003eLet’s dig into more detail. The target monoid is a function of this form:\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003ey_out = a * y_in + b\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eThis can obviously be represented as two floats, we can write the representation as simply \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e(a, b)\u003c/code\u003e. The homomorphism then binds a single sample of input into a function. Given the filter above, an input of \u003ccode class=\"language-plaintext highlighter-rouge\"\u003ex\u003c/code\u003e maps to \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e(c, x * (1 - c))\u003c/code\u003e.\u003c/p\u003e\n\n\u003cp\u003eSimilarly, we can write out the effect of function composition in the two-floats representation space. Given \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e(a1, b1)\u003c/code\u003e and \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e(a2, b2)\u003c/code\u003e, their composition is \u003ccode class=\"language-plaintext highlighter-rouge\"\u003e(a1 * a2, a2 * b1 + b2)\u003c/code\u003e. Not especially complicated or difficult to compute.\u003c/p\u003e\n\n\u003cp\u003eIf we just evaluate this homomorphism, then we get the filter state at the end, which is nice, doesn’t really count as evaluating the filter. What we want is the filter state at the end of each prefix of the input. But fortunately, that’s possible too. It’s most generally called “scan,” and is often thought of as a generalization of \u003ca href=\"https://en.wikipedia.org/wiki/Prefix_sum\"\u003eprefix sum\u003c/a\u003e. Fortunately, there is a great literature on parallel evaluation of this primitive - one of the more sophisticated approaches has a depth of 2 * log\u003csub\u003e2\u003c/sub\u003e n), and a work factor of 2, meaning twice the number of primitive evaluations as the serial approach. A number of other intermediates are possible, especially including the SIMD-friendly variants we saw earlier. A good read on scans is the paper that (I believe) introduced them is \u003ca href=\"https://people.eecs.berkeley.edu/~driscoll/cs267/papers/scan_primitive.pdf\"\u003eScans as Primitive Parallel Operations\u003c/a\u003e.\u003c/p\u003e\n\n\u003ch2 id=\"time-varying-parameters\"\u003eTime-varying parameters\u003c/h2\u003e\n\n\u003cp\u003eNote that there’s nothing about the homomorphism above that requires the filter to be time-invariant. We can take both the input signal and the filter parameters as input to the map, and the math works out the same. Note that this is in stark contrast to convolutional reverb techniques, which do require that the filter kernel be linear time invariant.\u003c/p\u003e\n\n\u003ch2 id=\"possible-extension-to-nonlinearity\"\u003ePossible extension to nonlinearity\u003c/h2\u003e\n\n\u003cp\u003eThis technique is for linear filters, but nonlinear filters are also interesting in audio (and other) applications. For example, many subtractive synthesizers use a “virtual analog” technique, which is often based on a linear core but has nonlinearities, more faithfully capturing the actual performance of electronic components used in active filters. (For a comprehensive treatment of this topic, see \u003ca href=\"(https://www.native-instruments.com/fileadmin/ni_media/downloads/pdf/VAFilterDesign_1.1.1.pdf)\"\u003eThe art of VA filter design\u003c/a\u003e (PDF).)\u003c/p\u003e\n\n\u003cp\u003eI think it’s an interesting question whether the parallelization techniques described here can be extended to nonlinear filters as well. My guess is, probably not realistically for faithful simulation of analog circuits, but it might be possible to design a nonlinear response that can be represented with a relatively small number of parameters, and also composes (and respects associativity, the main requirement of monoid homomorphisms). I haven’t come up with one yet, but would be interested to explore the space more.\u003c/p\u003e\n\n\u003ch2 id=\"other-references\"\u003eOther references\u003c/h2\u003e\n\n\u003cp\u003eThere’s a great \u003ca href=\"http://www.rci.rutgers.edu/~shunsun/resource/IIR_History.pdf\"\u003ehistorical article comparing FIR and IIR\u003c/a\u003e that talks about how the relative advantages and disadvantages of each has evolved over time, and cites exploitation of parallelism as one of the reasons for FIR’s success.\u003c/p\u003e\n\n\u003cp\u003eFor an extremely in-depth presentation of digital filters, with solid mathematical foundations, see Julius Smith’s online book \u003ca href=\"https://ccrma.stanford.edu/~jos/filters/\"\u003eIntroduction to Digital Filters\u003c/a\u003e. In particular, it’s a great exposition of the state space approach.\u003c/p\u003e\n\n\n  \u003c/div\u003e",
  "Date": "2019-02-14T21:50:42Z",
  "Author": "raphlinus"
}