{
  "Source": "raphlinus.github.io",
  "Title": "The case of the curiously slow shader",
  "Link": "https://raphlinus.github.io/gpu/2021/04/28/slow-shader.html",
  "Content": "\u003cdiv class=\"post-content e-content\" itemprop=\"articleBody\"\u003e\n    \u003cp\u003eThese days, a significant amount of my time and energy is getting \u003ca href=\"https://github.com/linebender/piet-gpu\"\u003epiet-gpu\u003c/a\u003e, a compute-focused 2D rendering engine, to run well on mobile hardware. Not too long ago, I got it running on Pixel 4, and breathlessly waited for the performance numbers, which turned out to be… disappointing. I was able to figure out why, but therein lies a story.\u003c/p\u003e\n\n\u003cp\u003eTo track this down, I had to dive pretty deep into the lower levels of GPU infrastructure, and learned a lot in the process. And I’ll end with a look into the Vulkan memory model and why it could help with these performance portability concerns going forward.\u003c/p\u003e\n\n\u003ch2 id=\"initial-results\"\u003eInitial results\u003c/h2\u003e\n\n\u003cp\u003eTo recap, piet-gpu is an experimental rendering engine designed for vector graphics content (especially font rendering). It’s based on a pipeline of compute shaders, not the more familiar triangle rasterization pipeline with its vertex and fragment shaders. Compute shaders have more flexibility to process data structures representing 2D vector content, and compute high quality antialiasing without relying on hardware MSAA. Even so, the approach is speculative, and it is not obvious that it will work well on mobile GPU.\u003c/p\u003e\n\n\u003cp\u003eThe early results on desktop GPU where encouraging. On high-end discrete cards, performance is amazing, but that’s not surprising considering how much raw horsepower they have. Integrated graphics is perhaps a more useful baseline for comparison.\u003c/p\u003e\n\n\u003cp\u003eThe test is rendering a Ghostscript tiger at a resolution of 1088x2288 pixels. There are 7 pipeline stages, most of which are very quick to run, so the bulk is in the final stage, fine rasterization. On Intel HD Graphics 630, the first six pipelines take a total of 0.86 ms, and the fine rasterization 2.04ms at the best setting of CHUNK (about which more later; in any case performance is not very sensitive to this tuning parameter).\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/assets/gpu_intel_630_timings.png\" width=\"608\" alt=\"Timings of Intel 630\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eRunning the same workload on a Pixel 4 gave much worse results. The first six stages take a total of 2.45ms, but the fine rasterization stage is 11.7ms, which is \u003cem\u003emuch\u003c/em\u003e slower than the Intel reference. Further, it’s very dependent on this CHUNK parameter, which if nothing else is evidence that the performance characteristics are very different.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/assets/gpu_adreno_640_timings.png\" width=\"608\" alt=\"Timings of Adreno 640\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eThese numbers are disappointing. It’s barely capable of 60fps on the tiger, but that’s a simpler workload, and the display on this hardware actually has a refresh rate of 90fps.\u003c/p\u003e\n\n\u003cp\u003eBut already there are some clues. Most of the pipeline stages are reasonably fast, but the fine rasterization stage is disproportionately slow. That’s evidence that the GPU has raw horsepower roughly half of the Intel chip, but there’s something going wrong in this one shader.\u003c/p\u003e\n\n\u003ch2 id=\"reading-performance-counters\"\u003eReading performance counters\u003c/h2\u003e\n\n\u003cp\u003eGPU performance is something of a black box, but there are tools that can help. Most GPU hardware has performance counters, which measure total read/write memory bandwidth, ALU utilization, and other similar metrics. Traditionally, there are proprietary vendor-specific tools to read these, but in the Android world attention is moving to \u003ca href=\"https://gpuinspector.dev/\"\u003eAndroid GPU Inspector\u003c/a\u003e, an open source tool now in beta.\u003c/p\u003e\n\n\u003cp\u003eThese performance counters are especially useful for ruling out some hypotheses. One early hypothesis I had is that the Adreno 640 is seriously register-limited compared to desktop GPUs. That would result either in spilling (data ordinarily stored in registers would be written to memory instead) or low occupancy (fewer workgroups would be scheduled in each Streaming Processor, resulting in more stalls). If register spilling was the problem, then the performance counters would show significantly larger read/write traffic to memory.\u003c/p\u003e\n\n\u003cp\u003eAt first, I couldn’t get AGI to work, but \u003ca href=\"https://github.com/google/agi/issues/760\"\u003eupgrading to the 1.1 dev version\u003c/a\u003e fixed that problem.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/assets/gpu_agi_screenshot.png\" alt=\"Screenshot of Android GPU inspector\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eWhen I did get it running, initially the information didn’t seem very useful. The one thing that stood out was very low ALU utilization, which wasn’t a surprise considering the other things I was seeing.\u003c/p\u003e\n\n\u003cp\u003eTo me, performance counters are a fairly rough indicator. But I was also able to do some black box testing and instrumentation, and that helped rule out several hypotheses. One knob I could easily adjust is what’s called \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eCHUNK\u003c/code\u003e in the shader source. One of the optimizations available to a compute shader (but not a fragment shader) is that each thread can render and write multiple pixels, which especially has the effect of amortizing the memory traffic cost of reading the input scene description. On the flip side, increasing \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eCHUNK\u003c/code\u003e also makes the shader more register-hungry, as each thread has to maintain the RGBA color value and other state (area coverage buffer) per-pixel.\u003c/p\u003e\n\n\u003cp\u003eAGI has no direct way to indicate the number of workgroups running concurrently, but it was fairly easy to instrument the code to report that, using atomic counters. (Increment a counter at the beginning of the shader, decrement at the end, and accumulate that into a maximum occupancy value).\u003c/p\u003e\n\n\u003cp\u003eThis testing shows that the number of workgroups is 13, 24, 32, for a CHUNK size of 1, 2, and 4, respectively. Since the number of subgroups is 4/CHUNK, that’s actually saying that the number of subgroups scheduled is quite good even for low values of CHUNK. Additionally, these occupancy numbers are comparable to much simple shaders.\u003c/p\u003e\n\n\u003cp\u003eSo at this point, we know basically two things. First, we can essentially rule out any effect of “too few registers,” either spilling or low occupancy. Second, by adjusting \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eCHUNK\u003c/code\u003e it seems that the memory reads (of the vector path data in particular) are more expensive than they should be. On desktop GPUs, the effect of decreasing \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eCHUNK\u003c/code\u003e has some performance impact, but not very much, largely because the reads by different subgroups of the same path data are expected to hit in L1 cache. As we’ll see later, this is a real clue.\u003c/p\u003e\n\n\u003cp\u003eBy the way, another way in which performance counters are extremely useful is reporting clock frequency. Modern GPUs (not just mobile) have a wide range of clock frequencies, and clock down to save power when the workloads are lighter. A shader taking 10ms when clocked down means something very different than at full clock - the first is a good thing, the second much less so.\u003c/p\u003e\n\n\u003ch2 id=\"ablation-testing\"\u003eAblation testing\u003c/h2\u003e\n\n\u003cp\u003eFrom experimentation, a vague shape was beginning to form. Something about this shader was causing poor performance; simple workloads (even fairly ALU intensive) ran just fine, but the real thing did not. Given that, at this point, the shader compiler and hardware were pretty much a black box, I set to systematically create a series of workloads that explored the space between a simple fast shader and the actual piet-gpu fine rasterizer, to see where the cliff was.\u003c/p\u003e\n\n\u003cp\u003eAfter a fair amount of time exploring dead ends, I found that commenting out the BeginClip and EndClip operations improved performance signficicantly (of course at the expense of making clips not render correctly). This was especially interesting because the tiger workload doesn’t have any clip operations; it was the mere presence of these in the code that was causing the problem.\u003c/p\u003e\n\n\u003cp\u003eThe performance was much closer to what I was hoping - only about 2x slower than the Intel reference, in line with my expectations for the hardware capability and clock speed. In particular, fine rasterization was 4.22ms at the best CHUNK value, and, like the Intel reference, not hugely sensitive to the setting.\u003c/p\u003e\n\n\u003cp\u003e\u003cimg src=\"/assets/gpu_adreno_640_noclip_timings.png\" width=\"608\" alt=\"Timings of Adreno 640 with clip disabled\"/\u003e\u003c/p\u003e\n\n\u003cp\u003eContinuing to bisect, it was specifically the lines in BeginClip that were writing to memory. Further, in what I started to call the “happy path,” overall performance was only weakly affected by \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eCHUNK\u003c/code\u003e, pointing strongly to the hypothesis that whatever was happening was making memory reads slow, and in particular caching not effective.\u003c/p\u003e\n\n\u003cp\u003eThese were strong clues, and narrowed the focus considerably, but still not an explanation of what was really going on, and ultimately not very satisfying, as I wanted clips to work, and I was also worried that as I continued to build out the imaging model, some other enhancement might trigger a similar performance cliff.\u003c/p\u003e\n\n\u003cp\u003eI have a \u003ca href=\"https://github.com/raphlinus/raphlinus.github.io/issues/52\"\u003eblog post planned about clipping\u003c/a\u003e in piet-gpu, but it probably helps to explain a little what’s going on. An arbitrary 2D scene is potentially a tree of nested clips. Each clip node in the tree has an associated clip mask. When rendering the clip masks with antialiasing, one way to describe the desired result is to render the children of the clip node into an RGBA buffer, render the clip mask itself into an alpha buffer, and then alpha-composite that (multiplying the two alpha values together) to the render target surface.\u003c/p\u003e\n\n\u003cp\u003eThe way this works in piet-gpu is that the earlier stages of the pipeline (coarse rasterization) prepare a per-tile command list, which is then interpreted in the last stage (fine rasterization). Commands include \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eBeginClip\u003c/code\u003e and \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eEndClip\u003c/code\u003e, which push and pop these temporary pixel values onto a stack. Because the nesting depth of clips is not limited, this stack can be of arbitrary depth. We’ve spent some time exploring hybrid approaches where there’s a small window in registers and it spills to memory when that window is exceeded, but at present it always goes to memory.\u003c/p\u003e\n\n\u003ch2 id=\"reading-shader-assembly\"\u003eReading shader assembly\u003c/h2\u003e\n\n\u003cp\u003eAt this point, I had no desire to remain an ignoramus; I felt that we must know what’s really going on, and determined that we shall know. The next step was to \u003ca href=\"https://interplayoflight.wordpress.com/2021/04/18/how-to-read-shader-assembly/\"\u003eread shader assembly\u003c/a\u003e.\u003c/p\u003e\n\n\u003cp\u003eFor certain GPUs, especially Radeon, this is easy, as the shader compiler is open source and widely available. In fact, the Radeon shader analyzer is available on the \u003ca href=\"http://shader-playground.timjones.io/\"\u003eShader playground\u003c/a\u003e, a resource analogous to \u003ca href=\"https://godbolt.org/\"\u003eGodbolt\u003c/a\u003e for shaders. But for mobile GPUs, it’s a bit harder, as the vendor drivers are proprietary.\u003c/p\u003e\n\n\u003cp\u003eFortunately, for someone as determined as I was, it’s possible, largely thanks to the existence of free tools, especially \u003ca href=\"https://docs.mesa3d.org/drivers/freedreno.html\"\u003eFreedreno\u003c/a\u003e. I was prepared to go as deep as I needed, including getting Freedreno working on some hardware; if it performed well, that would point to the proprietary driver. But if the performance matched, I would easily be able to see the ISA it produced, and dig into that.\u003c/p\u003e\n\n\u003cp\u003eAs it turned out, I didn’t have to go that far. The \u003ca href=\"https://www.khronos.org/registry/OpenGL-Refpages/gl4/html/glGetProgramBinary.xhtml\"\u003eglGetProgramBinary\u003c/a\u003e function can be used to spit out a binary even from the vendor driver, and then the \u003ca href=\"https://github.com/mesa3d/mesa/blob/master/src/freedreno/decode/pgmdump2.c\"\u003edisassembly tools\u003c/a\u003e in Mesa could spit out disassembler for me to read. This way, I could gain more insight into the performance characteristics of the configuration I actually care about, which is vendor drivers. There’s a bit more complexity to this, as the drivers produce both a compressed and an uncompressed format. The corresponding Vulkan call is \u003ca href=\"https://www.khronos.org/registry/vulkan/specs/1.2-extensions/man/html/vkGetPipelineCacheData.html\"\u003evkGetPipelineCacheData\u003c/a\u003e, but I was only able to get the compressed form. I strongly suspect it’s possible to work out the compression format, but when I got readable results from the GL route, that was good enough for me. (It’s possible that the OpenGL and Vulkan paths would give different binaries, but I don’t have any reason to believe that)\u003c/p\u003e\n\n\u003cp\u003eLooking at the disassembly (the full traces are in the \u003ca href=\"https://github.com/linebender/piet-gpu/issues/83\"\u003epiet-gpu bug\u003c/a\u003e, and also in \u003ca href=\"https://gist.github.com/raphlinus/2bf7e8dcc2d2cb7a3eda3aff359f69e0\"\u003ea gist\u003c/a\u003e), one thing immediately stuck out. In the happy path, all reads from memory use the \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eisam\u003c/code\u003e (Image SAMple) instruction, like this:\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e    isam (s32)(x)r0.x, r0.x, s#0, t#1\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eBut in the sad path, the same memory buffer is being read with the \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eldib\u003c/code\u003e (LoaD Image Buffer) instruction instead:\u003c/p\u003e\n\n\u003cdiv class=\"language-plaintext highlighter-rouge\"\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre class=\"highlight\"\u003e\u003ccode\u003e    ldib.untyped.1d.u32.1.imm r0.y, r0.y, 1\n\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003c/div\u003e\n\n\u003cp\u003eChecking in with \u003ca href=\"https://www.phoronix.com/scan.php?page=news_item\u0026amp;px=Google-Graphics-Rob-Clark\"\u003eRob Clark\u003c/a\u003e, the author of Freedreno, who now works at Google also, yielded more insight. The \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eisam\u003c/code\u003e instruction goes through the texture cache (TPL1), while the \u003ccode class=\"language-plaintext highlighter-rouge\"\u003eldib\u003c/code\u003e instruction bypasses that and goes straight to memory.\u003c/p\u003e\n\n\u003cp\u003eThe last piece of the mystery was \u003cem\u003ewhy\u003c/em\u003e the compiler was doing that. Again Rob provided the needed insight. Because the shader is reading and writing to the same memory, it is being conservative about cache coherency; it wants to avoid situations where the shader writes to memory, then another thread reads from memory and gets stale data. Bypassing the cache avoids that risk.\u003c/p\u003e\n\n\u003cp\u003eIn this case, the actual memory locations for reading path data and for storing the clip stack are disjoint, so there’s no such risk. We use the same buffer partly for convenience and partly so there’s one free space pool; if there were multiple buffers, then one buffer might run out of memory while there’s plenty of free space in another, which would be unfortunate.\u003c/p\u003e\n\n\u003cp\u003eGiven this knowledge, a fix is fairly straightforward, and we also know how to avoid this particular problem in the future. Most likely, we’ll just use scratch memory for the clip stack, rather than sharing a buffer with the scene and path, so all access to that buffer is readonly. There are plenty of other solutions that would work, including a dedicated buffer for the clip stack.\u003c/p\u003e\n\n\u003ch2 id=\"why-we-need-the-vulkan-memory-model\"\u003eWhy we need the Vulkan memory model\u003c/h2\u003e\n\n\u003cp\u003eIn the middle of the investigation, I viewed the proprietary shader compiler with extreme suspicion: it’s making my code run slow, for no good reason that I could see, and also not matching the performance expectations set by other (desktop) GPUs. But now I understand much better why it’s doing that.\u003c/p\u003e\n\n\u003cp\u003eEven so, in the longer term I think it’s possible to do much better, and I believe the key to that is the \u003ca href=\"https://www.khronos.org/blog/comparing-the-vulkan-spir-v-memory-model-to-cs\"\u003eVulkan memory model\u003c/a\u003e. (I’ve written about this before in my \u003ca href=\"https://raphlinus.github.io/gpu/2020/04/30/prefix-sum.html\"\u003eprefix sum\u003c/a\u003e blog post, but at that time I did not observe a substantial performance difference and this time I do.)\u003c/p\u003e\n\n\u003cp\u003eAs in the \u003ca href=\"https://www.cs.utexas.edu/~bornholt/post/memory-models.html\"\u003eCPU case,\u003c/a\u003e the memory model is an abstraction over certain hardware and compiler mechanisms that can either uphold or break assumptions about memory coherence. And in both cases, it’s all about the program \u003cem\u003eexplicitly\u003c/em\u003e indicating what guarantees it needs, so the shader compiler and hardware are free to apply whatever optimizations they like, as long as the semantics are respected.\u003c/p\u003e\n\n\u003cp\u003eOn an Intel CPU, respecting the memory model is fairly simple; its total store order takes care of most of it, but in certain cases (sequential consistency semantics), the compiler needs to emit a \u003ccode class=\"language-plaintext highlighter-rouge\"\u003elock\u003c/code\u003e prefix. It also needs to avoid reordering memory accesses around atomic operations in certain cases. A major motivation for the memory model is that ARM (and most other chips) are a bit more demanding; for acquire and release semantics, the compiler needs to emit barrier instructions or special load-acquire and store-release versions of vanilla load and store instructions. The memory model lets you write your code \u003cem\u003eonce,\u003c/em\u003e then it is compiled to efficient and reliable assembly code as needed by the target architecture. In particular, when only relaxed semantics are needed, generally there is no additional cost.\u003c/p\u003e\n\n\u003cp\u003eOn a GPU, much more can go wrong, but the principles are the same. There are generally more levels of explicit caching, and more opportunities for parallelism, but at the end of the day, the compiler will convert explicit atomic operations with memory order semantics into code that respects that semantics. Depending on the details, it may emit barrier instructions, select load/store instructions that bypass cache (as would be the case here), or other similar mechanisms. Note that the pipeline barriers familiar to all Vulkan programmers are similar, but different in granularity - those generally cause a cache flush and barrier \u003cem\u003ebetween\u003c/em\u003e shader dispatches, while the memory model is about enforcing semantics \u003cem\u003ewithin\u003c/em\u003e a shader, but possibly between different workgroups, different subgroups, or even different threads (invocations) within a subgroup.\u003c/p\u003e\n\n\u003cp\u003eIn this particular case, the shader needs only the most relaxed possible memory semantics; each memory write is consumed only by a read on the same thread, which means no special semantics are necessary. Without taking the Vulkan memory model into account, the compiler has no way to know that. The proprietary compiler \u003cem\u003edoes\u003c/em\u003e \u003ca href=\"https://vulkan.gpuinfo.org/displayreport.php?id=9426#extensions\"\u003ereport\u003c/a\u003e that it respects the Vulkan memory model, but it’s not lying; if it always generates conservative code and then ticks the “support” bit, it’s satisfying the requirements. What is fair to say is that it’s missing an opportunity to optimize. Even so, I understand why they haven’t prioritized it. When I researched my prefix sum blog post, I didn’t find any evidence people were actually writing shaders that used the Vulkan memory model, and I haven’t heard of any since.\u003c/p\u003e\n\n\u003cp\u003eIn the above, I’m assuming that relaxed read/writes are as efficient as a memory traffic pattern consisting only of reads. I believe that’s true on the Adreno hardware, but might not be universally true. The safest bet is to segregate buffers so that if you have read-only traffic from one of them, it’s in a separate buffer from others you may be writing to.\u003c/p\u003e\n\n\u003cp\u003eI should also note that these differences in the way compilers handle memory coherence are not in any way specific to mobile vs desktop; it would not have been surprising to see this issue pop up on desktop GPU with very aggressive caching but not on a simpler mobile architecture. As an example of aggressive caching, \u003ca href=\"https://www.amd.com/system/files/documents/rdna-whitepaper.pdf\"\u003eRDNA\u003c/a\u003e has some cache attached to a SIMD execution unit, which is not necessarily coherent with caches of other SIMD execution units even in the same workgroup.\u003c/p\u003e\n\n\u003cp\u003eOverall, I’ve found the Adreno 640 to be generally similar to low-end desktop GPUs, at least in terms of the number of registers available per thread, scheduling of subgroups within a processing cluster, etc. There are fewer processing clusters, a slower clock speed, and less memory bandwidth overall, but those should scale fairly smoothly with existing code and are entirely to be expected. There’s more detail about the \u003ca href=\"https://gitlab.freedesktop.org/freedreno/freedreno/-/wikis/A6xx-SP\"\u003eA6xx Streaming Processor\u003c/a\u003e at the Freedreno wiki, and my empirical observations are consistent.\u003c/p\u003e\n\n\u003ch2 id=\"conclusion-and-thanks\"\u003eConclusion and thanks\u003c/h2\u003e\n\n\u003cp\u003eDeveloping GPU compute shaders risks mysterious performance slowdowns when deploying across a range of hardware. With some determination and tools, it’s possible to diagnose those. Fortunately, there’s a lot of activity on better open source tools, much of which is being done at Google, but of course the broader open source graphics community deserves credit too.\u003c/p\u003e\n\n\u003cp\u003eOne such slowdown is overly conservative memory coherency. In the future, the Vulkan memory model promises to make these performance concerns more explicit and predictable - a slowdown when you’re explicitly asking for barriers or coherent semantics would not be surprising. In the meantime, it’s something to be aware of and test. In particular, just because performance is good on one set of GPUs and drivers is not a guarantee it won’t be a problem on others.\u003c/p\u003e\n\n\u003cp\u003eThanks to Rob Clark for much valuable insight about Adreno ISA, Lei Zhang for pointing me to many useful Adreno (and mobile GPU resources), Hugues Evrard for help with AGI, and Elias Naur for going on this journey with me, as well as tool support in the form of making glGetProgramBinary easy to run.\u003c/p\u003e\n\n\n  \u003c/div\u003e",
  "Date": "2021-04-28T14:53:42Z",
  "Author": "raphlinus"
}