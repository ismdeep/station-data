{
  "Source": "solidot",
  "Title": "新数据中毒工具让艺术家能反击生成式 AI",
  "Link": "https://www.solidot.org/story?sid=76447",
  "Content": "\u003cdiv class=\"p_mainnew\"\u003e\n\t\t\t\t\t芝加哥大学计算机科学教授 Ben Zhao 领导的一个团队开发了一种数据中毒工具 Nightshade，允许艺术家在将作品上传到网上前使用该工具在像素中添加不可见的改动，如果作品被抓取到 AI 训练数据集中，它会导致模型以混乱且不可预测的方式崩溃。该工具旨在反击 AI 公司未经作者许可使用其作品训练 AI 模型的行为。训练数据中毒可能会破坏图像生成模型如 DALL-E、Midjourney 和 Stable Diffusion 的未来迭代，它会导致部分输出无用——狗变成猫，汽车变成牛，诸如此类。相关论文已递交到 Usenix 计算机安全会议接受同行评审。Zhao 表示，他希望数据中毒工具有助于将权力平衡从 AI 公司转向艺术家。他的团队还开发了另一种工具 Glaze，帮助艺术家隐藏个人艺术风格，防止被 AI 公司抓取。它的工作方式与 Nightshade 类似。研究团队计划将 Nightshade 整合到 Glaze 中，并将其开源，让其他人能开发自己的版本。大型 AI 模型使用的训练集可能包含数十亿幅图像，其中的有毒图像越多，造成的破坏就会越大。数据集一旦中毒是很难清理的，它需要公司努力找出并删除每一个损坏的样本。\n\u003cp\u003e\u003c/p\u003e\n\u003c!--more--\u003e\n\u003cp\u003e\u003c/p\u003e\n\u003cbr/\u003e\nhttps://slashdot.org/story/23/10/24/0057214/new-data-poisoning-tool-lets-artists-fight-back-against-generative-ai\u003cbr/\u003e\nhttp://people.cs.uchicago.edu/~ravenben/\t\t\t\t\t                \u003c/div\u003e",
  "Date": "2023-10-25T15:20:15Z",
  "Author": "Wilson"
}