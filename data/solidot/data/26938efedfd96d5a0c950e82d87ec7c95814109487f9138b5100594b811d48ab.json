{
  "Source": "solidot",
  "Title": "苹果放弃了扫描手机上儿童色情的 CSAM 计划",
  "Link": "https://www.solidot.org/story?sid=73591",
  "Content": "\u003cdiv class=\"p_mainnew\"\u003e\n\t\t\t\t\t去年八月，苹果宣布了一项受争议的决定：它将扫描美国用户 iPhone 手机上的已知儿童色情照片，利用来自 National Center for Missing and Exploited Children (NCMEC)的 CSAM（Child Sexual Abuse Material）图像哈希值去匹配用户手机上的图像哈希，如果发现至少 30 次匹配成功它将会在审核之后报告给相关机构。在引发广泛批评和反对之后，苹果暂停了该计划。现在它放弃了扫描手机上儿童色情的计划，改为加强“Communication Safety”功能，允许父母和看护者通过家庭 iCloud 账号选择加入保护功能，以确保儿童的通信安全，在儿童试图发送或接收含有裸体的照片时发出警告，阻止和减少新 CSAM 的产生。\n\u003cp\u003e\u003c/p\u003e\n\u003c!--more--\u003e\n\u003cp\u003e\u003c/p\u003e\n\u003cbr/\u003e\nhttps://www.wired.com/story/apple-photo-scanning-csam-communication-safety-messages/\t\t\t\t\t                \u003c/div\u003e",
  "Date": "2022-12-08T05:31:38Z",
  "Author": "wanwan"
}