{
  "Source": "solidot",
  "Title": "研究显示大模型在无损压缩上能超过 PNG 和 FLAC",
  "Link": "https://www.solidot.org/story?sid=76237",
  "Content": "\u003cdiv class=\"p_mainnew\"\u003e\n\t\t\t\t\tGoogle DeepMind 和 Meta 的研究人员在预印本平台 arXiv 上发表论文《Language Modeling Is Compression》，他们发现 DeepMind 的大语言模型 Chinchilla 70B 在图像和音频的无损压缩上超过了 PNG 和 FLAC。Chinchilla 70B 能将 ImageNet 图像数据库无损压缩到原始大小 43.4%，超过了 PNG 算法的 58.5%。Chinchilla 能将 LibriSpeech 音频数据集中的样本无损压缩到原始大小 16.4%，超过 FLAC 算法的 30.3%。Chinchilla 70B 主要是训练去处理文本，但它在压缩其它类型的数据集上的效果也表现优异，甚至优于专门的算法。\n\n\n\n\u003cp\u003e\u003c/p\u003e\n\u003c!--more--\u003e\n\u003cp\u003e\u003c/p\u003e\n\u003cbr/\u003e\nhttps://arxiv.org/pdf/2309.10668.pdf\u003cbr/\u003e\nhttps://arstechnica.com/information-technology/2023/09/ai-language-models-can-exceed-png-and-flac-in-lossless-compression-says-study/\t\t\t\t\t                \u003c/div\u003e",
  "Date": "2023-10-01T13:58:43Z",
  "Author": "Wilson"
}