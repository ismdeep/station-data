{
  "Source": "solidot",
  "Title": "测量人工智能的碳足迹",
  "Link": "https://www.solidot.org/story?sid=71960",
  "Content": "\u003cdiv class=\"p_mainnew\"\u003e\n\t\t\t\t\t机器学习模型正呈指数级增长。训练它们所需的能量也成倍增长——通过训练之后 AI 才能准确处理图像或文本或视频。随着人工智能社区努力应对其对环境的影响，一些会议现在要求论文提交者提供有关二氧化碳排放的信息。\u003ca href=\"https://spectrum.ieee.org/ai-carbon-footprint\"\u003e\u003cu\u003e新研究提供了一种更准确的方法计算排放量\u003c/u\u003e\u003c/a\u003e。它还比较了影响它们的因素，并测试了两种减少排放的方法。\n\n研究人员训练了 11 个规模不等的机器学习模型处理语言或图像。训练时间从单 GPU 上 1 小时到 256 个 GPU 上 8 天不等。他们记录每秒的能耗数据。还获得了 16 个地理区域 2020 年期间以五分钟为单位的每千瓦时能源碳排放量。然后他们可以比较在不同地区、不同时间运行不同模型的碳排放量。\n\n为训练最小模型的 GPU 供电的碳排放量大致相当于为手机充电。最大的模型包含了 60 亿个参数，参数是衡量模型大小的标准。虽然它的训练只完成了 13%，但是 GPU 的碳排放量几乎相当于一个美国家庭一年耗电的碳排放量。而一些已部署的模型，例如 OpenAI 的 GPT-3，包含的参数超过了 1000 亿个。\n\n减少碳排放的最大因素是地理区域：各地区每千瓦时的二氧化碳排放量从 200 克到 755 克不等。除了改变位置之外，研究人员还测试了两种减少二氧化碳排放的方法，他们能做到这一点得益于高时间粒度的数据。第一种方法是“灵活的开始”，这种方法可能会将训练延迟长达 24 个小时。对于需要几天时间训练的最大的模型，推迟一天通常只能将碳排放量减少不到 1%，但是对于小得多的模型，这样的延迟可以减少 10% 到 80% 的排放量。第二种方法是“暂停加恢复”，这种方法是在排放量高的时段暂停训练，只要总的训练时间增长不超过一倍即可。这种方法给小模型带来的好处只有几个百分点，但是在半数的地区，它让最大的模型受益达到 10% 到 30%。每千瓦时的排放量随着时间波动，部分是因为由于缺乏足够的能量存储，当风能和太阳能等间歇性清洁能源无法满足需求时，电网必须依赖“脏电”。\t\t\t\t\t                \u003c/div\u003e",
  "Date": "2022-06-28T07:57:41Z",
  "Author": "wanwan"
}