{
  "Source": "solidot",
  "Title": "苹果证实它将扫描手机上的儿童色情照片",
  "Link": "https://www.solidot.org/story?sid=68472",
  "Content": "\u003cdiv class=\"p_mainnew\"\u003e\n\t\t\t\t\t苹果发布\u003ca href=\"https://www.apple.com/child-safety/\"\u003e\u003cu\u003e新闻稿\u003c/u\u003e\u003c/a\u003e，证实它将扫描美国 iPhone 手机上的儿童色情照片。苹果称，它将在三个方面引入儿童安全保护功能：Messages app 将加入新的工具在收到或发送露骨照片时警告儿童及其父母，收到的照片将会被模糊，如果儿童决定浏览照片他们将会被告知父母会收到信息，苹果声称它利用的是设备上的机器学习功能去分析和做出判断，它并不能访问照片；iOS 和 iPadOS 将引入新的技术允许苹果检测储存在  iCloud Photos 中的已知 CSAM（Child Sexual Abuse Material）图像并报告给 National Center for Missing and Exploited Children (NCMEC)，这一哈希匹配是在设备进行的，苹果表示除非发现 CSAM 图像它不会知道用户储存的内容；Siri 和 Search 将在用户尝试搜索 CSAM 相关主题时进行干预。这些功能将包含在今年晚些时候释出的 iOS 15、iPadOS 15、watchOS 8 和 macOS Monterey 中。安全专家对这些功能可能成为政府监视工具或被执法部门滥用\u003ca href=\"https://arstechnica.com/tech-policy/2021/08/apple-explains-how-iphones-will-scan-photos-for-child-sexual-abuse-images/\"\u003e\u003cu\u003e表达了担忧\u003c/u\u003e\u003c/a\u003e。\t\t\t\t\t                \u003c/div\u003e",
  "Date": "2021-08-06T02:05:49Z",
  "Author": "WinterIsComing"
}