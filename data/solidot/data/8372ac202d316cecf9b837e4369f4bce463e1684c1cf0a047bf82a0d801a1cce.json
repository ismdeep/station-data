{
  "Source": "solidot",
  "Title": "为了对抗噪声 AI 研究人员投向生物学",
  "Link": "https://www.solidot.org/story?sid=69923",
  "Content": "\u003cdiv class=\"p_mainnew\"\u003e\n\t\t\t\t\t人工智能可以看到我们看不到的东西——这通常是不利的。虽然机器擅长识别图像，但愚弄它们仍然很容易。只要在输入的图像中添加少量人眼无法察觉的噪声，AI 就会突然将校车、狗或者建筑物识别为完全不同的对象，比如鸵鸟。\n\u003cbr/\u003e\n\u003cbr/\u003e\n6 月发表在预印本服务 arxiv 的一篇\u003ca href=\"https://arxiv.org/abs/2106.09898\"\u003e\u003cu\u003e论文\u003c/u\u003e\u003c/a\u003e中，多伦多大学的 Nicolas Papernot 和同事研究了不同类型的语言处理机器学习模型，找到一种方法，用对人类不可见的方式干预输入文本以进行欺骗。只有在计算机读取文本背后的代码并将字母映射到内存中的字节时，这些隐藏的指令才会被计算机看见。Papernot 的团队表明，即使是少量添加，例如单个空格字符，也会严重破坏模型对文本的理解。混淆也会对人类用户产生影响——在一个例子中，单个字符导致算法输出一个句子，让用户向错误的银行账户汇款。\n\u003cbr/\u003e\n\u003cbr/\u003e\n这些欺骗行为被称为对抗样本攻击，故意改变输入欺骗算法并使其出错。2013 年，研究人员欺骗了一个深度神经网络（一种具有多层人工“神经元”来执行计算的机器学习模型），让此类漏洞在AI研究中变得突出。\n\u003cbr/\u003e\n\u003cbr/\u003e\n目前，我们还没有万无一失的解决方案能够抵挡各种对抗样本媒体——图像、文本或其他形式。但希望已经出现。对于图像识别，研究人员可以故意用对抗图像训练深度神经网络，它就能更轻松地看待它们。不幸的是，这种被称为对抗性训练的方法只能很好地抵抗模型已见过的对抗样本。此外它降低了模型在非对抗性图像上的准确性，并且计算成本高昂。人类很少被这样的攻击欺骗，这个事实让科学家开始\u003ca href=\"https://www.quantamagazine.org/ai-researchers-fight-noise-by-turning-to-biology-20211207/\" target=\"_blank\"\u003e\u003cu\u003e寻找受到生物视觉启发的解决方案\u003c/u\u003e\u003c/a\u003e。\t\t\t\t\t                \u003c/div\u003e",
  "Date": "2021-12-08T10:11:10Z",
  "Author": "wanwan"
}