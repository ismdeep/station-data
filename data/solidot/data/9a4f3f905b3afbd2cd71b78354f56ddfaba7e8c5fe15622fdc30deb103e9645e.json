{
  "Source": "solidot",
  "Title": "在战争模拟游戏中 AI 聊天机器人倾向于选择核攻击",
  "Link": "https://www.solidot.org/story?sid=77297",
  "Content": "\u003cdiv class=\"p_mainnew\"\u003e\n\t\t\t\t\t研究人员在预印本平台 arXiv 上发表论文，测试了 OpenAI 的 GPT-3.5 和 GPT-4、Anthropic 的 Claude 2，以及 Meta 的 Llama 2 等流行大模型在战争游戏中的行为，发现 AI 聊天机器人倾向于选择核攻击。研究人员模拟了三种场景：入侵，网络攻击，以及没有冲突的中立。每一轮 AI 为其下一步行动能采取的行动提供推理，然后从 27 个行动中进行选择，包括开始正式和平谈判，实施贸易限制，升级全面核攻击。在模拟中，AI 倾向于投资军事和升级冲突风险，其中 GPT-4 的基础版本最暴力。研究人员认为，不应该信任 AI 做出的战争与和平的重大决策。\n\u003cp\u003e\u003c/p\u003e\n\u003c!--more--\u003e\n\u003cp\u003e\u003c/p\u003e\n\u003cbr/\u003e\nhttps://www.newscientist.com/article/2415488-ai-chatbots-tend-to-choose-violence-and-nuclear-strikes-in-wargames/\u003cbr/\u003e\nhttps://arxiv.org/abs/2401.03408\t\t\t\t\t                \u003c/div\u003e",
  "Date": "2024-02-04T05:50:42Z",
  "Author": "Wilson"
}