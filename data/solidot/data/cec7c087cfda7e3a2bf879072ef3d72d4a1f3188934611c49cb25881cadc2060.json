{
  "Source": "solidot",
  "Title": "计算机科学家证明为什么更大的神经网络表现更好",
  "Link": "https://www.solidot.org/story?sid=70629",
  "Content": "\u003cdiv class=\"p_mainnew\"\u003e\n\t\t\t\t\t人类多亏有对生拇指。但就算演化给了我们更多的拇指，情况也不会有太大的改善。每只手有一个拇指就足够了。神经网络不是这样，神经网络是执行类人任务的先进人工智能系统。随着它们变得更大，它们就能掌握更多。这会让旁观者大吃一惊。基本的数学结果表明，网络应该只需要这么大，但是现代神经网络的规模通常会远超出预测的需求——这种情况被称为过度参数化。在 12 月会议 NeurIPS 上发布的\u003ca href=\"https://arxiv.org/abs/2105.12806\" target=\"_blank\"\u003e\u003cu\u003e一篇论文\u003c/u\u003e\u003c/a\u003e中，微软研究院的 Sébastien Bubeck 和斯坦福大学的 Mark Sellke 为规模放大成功背后的奥秘\u003ca href=\"https://www.quantamagazine.org/computer-scientists-prove-why-bigger-neural-networks-do-better-20220210/\"\u003e\u003cu\u003e提出了一种新的解释\u003c/u\u003e\u003c/a\u003e。他们表明，神经网络必须比传统预期的大得多，才能避免某些基本问题。这一发现为一个持续了几十年的问题提供了一般性的见解。对神经网络规模的标准预期来自对它们如何记忆数据的分析。但要了解记忆，我们必须首先了解网络的作用。神经网络的一项常见任务是识别图像中的对象。研究人员首先为其提供许多图像和对象标签，训练它学习两者之间的相关性。之后网络将正确识别它看过的图像中的对象。换句话说，训练使网络记住数据。更值得注意的是，一旦网络记住了足够多的训练数据，它就能以不同程度的准确度预测它从未见过的物体的标签。后一个过程被称为泛化。\t\t\t\t\t                \u003c/div\u003e",
  "Date": "2022-02-11T09:35:27Z",
  "Author": "wanwan"
}