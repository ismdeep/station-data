{
  "Source": "solidot",
  "Title": "机器学习模型可植入无法检测到的后门",
  "Link": "https://www.solidot.org/story?sid=71349",
  "Content": "\u003cdiv class=\"p_mainnew\"\u003e\n\t\t\t\t\t今天训练机器学习模型使用的计算资源庞大无比，越来越多的地方将模型训练和开发外包给 Amazon Sagemaker 和 Microsoft Azure 等机器学习即服务（MLaaS）平台。按照 Ken Thompson 在 40 年前演讲的说法，你可以通过投放数据测试新模型是否按照你的预期工作，但怎么知道你可以信任它，知道它不会使用内置的后门作恶？研究人员证明，将无法检测出的后门植入机器学习模型是可能的。根据发表在预印本平台 arXiv 上的\u003ca href=\"https://arxiv.org/abs/2204.06974v1\" target=\"_blank\"\u003e论文\u003c/a\u003e：\n从表面上看，带有后门的分类器行为正常，但实际上该学习器保留了一套可以改变任何输入分类的机制，只要轻微的扰动。重要的是，没有正确的“后门密钥”，该机制就是隐藏的，任何计算受限的观察者都无法检测到它。研究人员展示了多种方法可以植入无法检测的后门，因此如果你拿到的是原始版本且有后门的黑盒的访问权限，那么在计算上甚至找不到他们对哪个输入动了手脚。\t\t\t\t\t                \u003c/div\u003e",
  "Date": "2022-04-25T07:39:34Z",
  "Author": "WinterIsComing"
}