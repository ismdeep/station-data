{
  "Source": "solidot",
  "Title": "Google 演示能完成多项任务的通用 AI 模型 PaLM-E",
  "Link": "https://www.solidot.org/story?sid=74337",
  "Content": "\u003cdiv class=\"p_mainnew\"\u003e\n\t\t\t\t\tGoogle 和柏林科技大学的一组研究人员透露了可用于控制机器人的多模态 Embodied 视觉语言模型（VLM）PaLM-E，有 5620 亿个参数，融合了视觉和语言处理。当用户发出“高阶指令”，如“将抽屉里的米片拿过我”， PaLM-E 能为装备机械臂的移动机器人平台生成一个行动计划，并自行执行。它执行不同任务不需要预先或重复训练。消除数据预处理或注释给予了机器人更强大的自主控制。PaLM-E 是基于 Google 现有的大语言模型 PaLM，通过加入感觉信息和机器人控制使其有具身性(embodied) 。它能与处理语言的相同方式理解感觉信息。\n\u003cp\u003e\u003c/p\u003e\n\u003c!--more--\u003e\n\u003cp\u003e\u003c/p\u003e\n\u003cbr/\u003e\nhttps://palm-e.github.io/#demo\u003cbr/\u003e\nhttps://arstechnica.com/?p=1922315\t\t\t\t\t                \u003c/div\u003e",
  "Date": "2023-03-08T15:43:34Z",
  "Author": "Wilson"
}