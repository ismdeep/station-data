{
  "Source": "tech.meituan.com",
  "Title": "ICRA 2020轨迹预测竞赛冠军的方法总结",
  "Link": "https://tech.meituan.com/2020/06/11/meituan-icra-2020.html",
  "Content": "\u003cdiv class=\"post-content\"\u003e\u003cdiv class=\"content\"\u003e\u003cp\u003e行人轨迹预测问题是无人驾驶技术的重要一环，已成为近年来的一项研究热点。在机器人领域国际顶级会议ICRA 2020上，美团无人配送团队在行人轨迹预测竞赛中夺冠，本文系对该预测方法的一些经验总结，希望能对大家有所帮助或者启发。\u003c/p\u003e\u003ch2 id=\"一-背景\"\u003e一、背景\u003c/h2\u003e\u003cp\u003e6月2日，国际顶级会议ICRA 2020举办了“第二届长时人类运动预测研讨会”。该研讨会由博世有限公司、厄勒布鲁大学、斯图加特大学、瑞士洛桑联邦理工联合组织，同时在该研讨会上，还举办了一项行人轨迹预测竞赛，吸引了来自世界各地的104支队伍参赛。美团无人配送团队通过采用“世界模型”的交互预测方法，夺得了该比赛的第一名。\u003c/p\u003e\u003cp\u003e\u003cimg src=\"https://p0.meituan.net/travelcube/67849d5557cf1a5f7510542ebc2415cd190167.jpg\" alt=\"图1 ICRA 2020 TrajNet++轨迹预测竞赛\"/\u003e\u003c/p\u003e\u003ch2 id=\"二-赛题简介\"\u003e二、赛题简介\u003c/h2\u003e\u003cp\u003e本次竞赛提供了街道、出入口、校园等十个复杂场景下的行人轨迹数据集，要求参赛选手根据这些数据集，利用行人在过去3.6秒的轨迹来预测其在未来4.8秒的运行轨迹。竞赛使用FDE（预测轨迹和真实轨迹的终点距离）来对各种算法进行排名。\u003c/p\u003e\u003cp\u003e本次的赛题数据集，主要来源于各类动态场景下的真实标注数据和模拟合成数据，采集频率为2.5赫兹，即两个时刻之间的时间差为0.4秒。数据集中的行人轨迹都以固定坐标系下的时序坐标序列表示，并且根据行人的周围环境，这些轨迹被分类成不同的类别，例如静态障碍物、线性运动、追随运动、避障行为、团体运动等。在该比赛中，参赛队伍需要根据每个障碍物历史9个时刻的轨迹数据（对应3.6秒的时间）来预测未来12个时刻的轨迹（对应4.8秒的时间）。\u003c/p\u003e\u003cp\u003e该竞赛采用多种评价指标，这些评价指标分别对单模态预测模型和多模态预测模型进行评价。单模态模型是指给定确定的历史轨迹，预测算法只输出一条确定的轨迹；而多模态模型则会输出多条可行的轨迹（或者分布）。本次竞赛的排名以单模态指标中的FDE指标为基准。\u003c/p\u003e\u003ch2 id=\"三-方法介绍\"\u003e三、方法介绍\u003c/h2\u003e\u003cp\u003e其实，美团在很多实际业务中经常要处理行人轨迹预测问题，而行人轨迹预测的难点在于如何在动态复杂环境中，对行人之间的社交行为进行建模。因为在复杂场景中，行人之间的交互非常频繁并且交互的结果将会直接影响他们后续的运动（例如减速让行、绕行避障、加速避障等）。\u003c/p\u003e\u003cp\u003e基于各类带交互数据集，一系列的算法被相继提出，然后对障碍物进行交互预测，这些主流模型的工作重心都是针对复杂场景下行人之间的交互进行建模。常用的方法包括基于LSTM的交互算法（SR LSTM[1]、Social GAN[2]、SoPhie[3]、Peeking into[4]、StarNet[5]等），基于Graph/Attention的交互算法（GRIP[6]、Social STGCNN[7]、STGAT[8]、VectorNet[9]等），以及基于语义地图/原始数据的预测算法等。\u003c/p\u003e\u003cp\u003e我们本次的参赛方法就是由自研算法[10]（如图2所示）改进而来，该方法的设计思路是根据场景中所有障碍物的历史轨迹、跟踪信息以及场景信息，建立并维护一个全局的世界模型来挖掘障碍物之间、障碍物与环境之间的交互特性。然后，再通过查询世界模型来获得每个位置邻域内的交互特征，进而来指导对障碍物的预测。\u003c/p\u003e\u003cp\u003e\u003cimg src=\"https://p0.meituan.net/travelcube/02ad10ca5116e72a4ae4f4f4849f1aaf532118.png\" alt=\"图2 基于世界模型的预测算法\"/\u003e\u003c/p\u003e\u003cp\u003e在实际操作过程中，由于数据集中缺乏场景信息，我们对模型做了适当的调整。在世界模型中（对应上图的Interaction Net），我们仅使用了现有数据集，以及模型能够提供的位置信息和跟踪信息LSTM隐状态信息。最终得到的模型结构设计如下图3所示：\u003c/p\u003e\u003cp\u003e\u003cimg src=\"https://p1.meituan.net/travelcube/b4f1835cd5c92da968a3e6f7d2b8cfdc371690.png\" alt=\"图3 竞赛使用的基于世界模型的预测算法\"/\u003e\u003c/p\u003e\u003cp\u003e整个模型基于Seq2Seq结构，主要包含历史轨迹编码模块（Encoder）、世界模型（Interaction Module）和解码预测模块（Decoder）三个部分。其中，编码器的功能在于对行人历史轨迹进行编码，主要提取行人在动态环境中的运动模式；解码器则是利用编码器得到的行人运动模式特征，来预测他们未来的运动轨迹分布。需要强调一下，在整个编码与解码的过程中，都需要对世界模型进行实时更新（Update）与查询（Query）两种操作。更新操作主要根据时序的推进，将行人的运动信息实时编入世界模型中；查询操作则是根据全局的世界地图以及行人的自身位置，来获取行人当前邻域内的环境特征。\u003c/p\u003e\u003cp\u003e\u003cimg src=\"https://p1.meituan.net/travelcube/62b9aa647efd8d071004a475fece9ab1571212.png\" alt=\"图4 编码阶段\"/\u003e\u003c/p\u003e\u003cp\u003e在图4中，展示了我们模型在历史轨迹编码阶段的计算流程。编码阶段共有9个时刻，对应9个历史观测时间点，每个时刻都执行相同的操作。以t时刻为例。\u003c/p\u003e\u003cp\u003e首先，将t时刻的所有行人坐标数据，包含：\u003c/p\u003e\u003cp\u003e\u003cimg src=\"https://p0.meituan.net/travelcube/2731a632344c68c234876001613175ca16607.png\" alt=\"位置集合\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cimg src=\"https://p0.meituan.net/travelcube/4e5fd0a90192f40270e65c453a60802417594.png\" alt=\"速度集合\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cimg src=\"https://p0.meituan.net/travelcube/2c3695d4abe70af75514e79a55aabb6014352.png\" alt=\"所有行人跟踪信息-上时刻编码得到的LSTM隐状态\"/\u003e\u003c/p\u003e\u003cp\u003e将以上信息输入到世界模型中更新地图信息，即Update操作。整个Update操作经过MLP、MaxPooling以及GRU等模块获得一个全局的时空地图特征R；然后，每个LSTM（对应一个行人），使用其当前观测时刻的坐标信息：\u003c/p\u003e\u003cp\u003e\u003cimg src=\"https://p0.meituan.net/travelcube/880118348a0f89d2ffd22fbb37dd4eba14233.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e解码预测阶段的流程与历史轨迹编码阶段基本一致，但存在两个细微的不同点：\u003c/p\u003e\u003cul\u003e\u003cli\u003e区别1：编码阶段每个行人对应的LSTM隐状态的初始化为0；而解码阶段，LSTM由编码阶段的LSTM隐状态和噪声共同初始化。\u003c/li\u003e\u003cli\u003e区别2：编码阶段行人对应的LSTM和世界模型使用的是行人历史观测坐标；而解码阶段使用的是上时刻预测的行人坐标。\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e\u003cimg src=\"https://p1.meituan.net/travelcube/50c3a9e2448f077372754094fd0f1a87605299.png\" alt=\"图5 解码预测阶段\"/\u003e\u003c/p\u003e\u003ch2 id=\"四-数据预处理与后处理\"\u003e四、数据预处理与后处理\u003c/h2\u003e\u003cp\u003e为了对数据有更好的理解，便于使用更适合的模型，我们对训练数据做了一些预处理操作。首先，数据集给出了各个行人的行为标签，这些标签是根据规则得到的，由于我们采用了交互预测的方法，希望模型能自动学习行人与周围主体之间的位置关系、速度关系等，所以我们就不直接使用标注中的“类型”信息；然后这次比赛的数据采集自马路、校园等不同场景中行人的运动轨迹。场景之间的差异性非常大，训练集和测试集数据分布不太一致。\u003c/p\u003e\u003cp\u003e于是，我们做了数据的可视化工作，将所有轨迹数据的起点放置于坐标轴的原点处，根据历史观测轨迹（前9个时刻）终点的位置朝向，将所有轨迹分为4类：沿左上方运动（top-left moving）、沿右上方运动（top-right moving）、沿左下方运动（bottom-left moving）和沿右下方运动（bottom-right moving）。分布的结果如图6所示，可以发现，训练集和测试集的数据分布存在一定的差距。\u003c/p\u003e\u003cp\u003e\u003cimg src=\"https://p0.meituan.net/travelcube/67b8b7e85829433ee32f24edf3a2bb5981294.png\" alt=\"图6 训练集与测试集历史观测轨迹中行人运动方向分布\"/\u003e\u003c/p\u003e\u003cp\u003e针对上述问题，我们对训练集做了2项预处理来提高训练集与测试集分布的一致性：\u003c/p\u003e\u003cul\u003e\u003cli\u003e平衡性采样；\u003c/li\u003e\u003cli\u003e场景数据正则化（缺失轨迹点插值，轨迹中心化以及随机旋转）。\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e此外，对于预测结果，我们也做了相应的后处理操作进行轨迹修正，主要是轨迹点的裁剪以及基于非极大值抑制的轨迹选择。图7展示了两个场景中行人的运动区域，可以看到有明显的边界，对于超出边界的轨迹，我们做了相应的修正，从而保证轨迹的合理性。\u003c/p\u003e\u003cp\u003e\u003cimg src=\"https://p0.meituan.net/travelcube/7ad869bd9be71e888d619901b1fb7dbe410509.png\" alt=\"图7 训练轨迹的可视化\"/\u003e\u003c/p\u003e\u003cp\u003e最后在训练技巧上，我们也使用K-Fold Cross Validation和Grid Search方法来做自适应的参数调优。最终在测试集上取得FDE 1.24米的性能，而获得比赛第二名的方法的FDE为1.30米。\u003c/p\u003e\u003ch2 id=\"五-总结\"\u003e五、总结\u003c/h2\u003e\u003cp\u003e行人轨迹预测是当前一个非常热门的研究领域，随着越来越多的学者以及研究机构的参与，预测方法也在日益地进步与完善。美团无人配送团队也期待能与业界一起在该领域做出更多、更好的解决方案。比较幸运的是，这次竞赛的场景与我们美团无人配送的场景具备一定的相似性，所以我们相信未来它能够直接为业务赋能。目前，我们已经将该研究工作在竞赛中进行了测试，也验证了算法的性能，同时为该算法在业务中落地提供了一个很好的支撑。\u003c/p\u003e\u003ch2 id=\"六-参考文献\"\u003e六、参考文献\u003c/h2\u003e\u003cul\u003e\u003cli\u003e[1] Zhang P, Ouyang W, Zhang P, et al. Sr-lstm: State refinement for lstm towards pedestrian trajectory prediction[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019: 12085-12094.\u003c/li\u003e\u003cli\u003e[2] Gupta A, Johnson J, Fei-Fei L, et al. Social gan: Socially acceptable trajectories with generative adversarial networks[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 2255-2264.\u003c/li\u003e\u003cli\u003e[3] Sadeghian A, Kosaraju V, Sadeghian A, et al. Sophie: An attentive gan for predicting paths compliant to social and physical constraints[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019: 1349-1358.\u003c/li\u003e\u003cli\u003e[4] Liang J, Jiang L, Niebles J C, et al. Peeking into the future: Predicting future person activities and locations in videos[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019: 5725-5734.\u003c/li\u003e\u003cli\u003e[5] Zhu Y, Qian D, Ren D, et al. StarNet: Pedestrian trajectory prediction using deep neural network in star topology[C]//Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems. 2019: 8075-8080.\u003c/li\u003e\u003cli\u003e[6] Li X, Ying X, Chuah M C. GRIP: Graph-based interaction-aware trajectory prediction[C]//Proceedings of the IEEE Intelligent Transportation Systems Conference. IEEE, 2019: 3960-3966.\u003c/li\u003e\u003cli\u003e[7] Mohamed A, Qian K, Elhoseiny M, et al. Social-STGCNN: A Social spatio-temporal graph convolutional neural network for human trajectory prediction[J]. arXiv preprint arXiv:2002.11927, 2020.\u003c/li\u003e\u003cli\u003e[8] Huang Y, Bi H K, Li Z, et al. STGAT: Modeling spatial-temporal interactions for human trajectory prediction[C]//Proceedings of the IEEE International Conference on Computer Vision. 2019: 6272-6281.\u003c/li\u003e\u003cli\u003e[9] Gao J, Sun C, Zhao H, et al. VectorNet: Encoding HD maps and agent dynamics from vectorized representation[J]. arXiv preprint arXiv:2005.04259, 2020.\u003c/li\u003e\u003cli\u003e[10] Zhu Y, Ren D, Fan M, et al. Robust trajectory forecasting for multiple intelligent agents in dynamic scene[J]. arXiv preprint arXiv:2005.13133, 2020.\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"七-作者简介\"\u003e七、作者简介\u003c/h2\u003e\u003cul\u003e\u003cli\u003e炎亮，美团无人车配送中心算法工程师。\u003c/li\u003e\u003cli\u003e佳禾，浙江大学在读研究生，美团无人车配送中心实习生。\u003c/li\u003e\u003cli\u003e德恒，美团无人车配送中心算法工程师。\u003c/li\u003e\u003cli\u003e冬淳，美团无人车配送中心算法工程师。\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003c/div\u003e",
  "Date": "2020-06-11T00:00:00Z",
  "Author": "soulteary@gmail.com"
}