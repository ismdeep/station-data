{
  "Source": "tech.meituan.com",
  "Title": "美团深度学习系统的工程实践",
  "Link": "https://tech.meituan.com/2018/10/25/dl-system-in-nlu-and-speech.html",
  "Content": "\u003cdiv class=\"post-content\"\u003e\u003cdiv class=\"content\"\u003e\u003ch2 id=\"背景\"\u003e背景\u003c/h2\u003e\u003cp\u003e深度学习作为AI时代的核心技术，已经被应用于多个场景。在系统设计层面，由于其具有计算密集型的特性，所以与传统的机器学习算法在工程实践过程中存在诸多的不同。本文将介绍美团平台在应用深度学习技术的过程中，相关系统设计的一些经验。\u003c/p\u003e\u003cp\u003e本文将首先列举部分深度学习算法所需的计算量，然后再介绍为满足这些计算量，目前业界比较常见的一些解决方案。最后，我们将介绍美团平台在NLU和语音识别两个领域中，设计相关系统的经验。\u003c/p\u003e\u003ch2 id=\"深度学习的计算量\"\u003e深度学习的计算量\u003c/h2\u003e\u003ctable\u003e\u003cthead\u003e\u003ctr\u003e\u003cth align=\"center\"\u003eModel\u003c/th\u003e\u003cth align=\"center\"\u003eInput Size\u003c/th\u003e\u003cth align=\"center\"\u003eParam Size\u003c/th\u003e\u003cth align=\"center\"\u003eFlops\u003c/th\u003e\u003c/tr\u003e\u003c/thead\u003e\u003ctbody\u003e\u003ctr\u003e\u003ctd align=\"center\"\u003eAlexNet\u003c/td\u003e\u003ctd align=\"center\"\u003e227 x 227\u003c/td\u003e\u003ctd align=\"center\"\u003e233 MB\u003c/td\u003e\u003ctd align=\"center\"\u003e727 MFLOPs\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd align=\"center\"\u003eCaffeNet\u003c/td\u003e\u003ctd align=\"center\"\u003e224 x 224\u003c/td\u003e\u003ctd align=\"center\"\u003e233 MB\u003c/td\u003e\u003ctd align=\"center\"\u003e724 MFLOPs\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd align=\"center\"\u003eVGG-VD-16\u003c/td\u003e\u003ctd align=\"center\"\u003e224 x 224\u003c/td\u003e\u003ctd align=\"center\"\u003e528 MB\u003c/td\u003e\u003ctd align=\"center\"\u003e16 GFLOPs\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd align=\"center\"\u003eVGG-VD-19\u003c/td\u003e\u003ctd align=\"center\"\u003e224 x 224\u003c/td\u003e\u003ctd align=\"center\"\u003e548 MB\u003c/td\u003e\u003ctd align=\"center\"\u003e20 GFLOPs\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd align=\"center\"\u003eGoogleNet\u003c/td\u003e\u003ctd align=\"center\"\u003e224 x 224\u003c/td\u003e\u003ctd align=\"center\"\u003e51 MB\u003c/td\u003e\u003ctd align=\"center\"\u003e2 GFLOPs\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd align=\"center\"\u003eResNet-34\u003c/td\u003e\u003ctd align=\"center\"\u003e224 x 224\u003c/td\u003e\u003ctd align=\"center\"\u003e83 MB\u003c/td\u003e\u003ctd align=\"center\"\u003e4 GFLOPs\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd align=\"center\"\u003eResNet-152\u003c/td\u003e\u003ctd align=\"center\"\u003e224 x 224\u003c/td\u003e\u003ctd align=\"center\"\u003e230 MB\u003c/td\u003e\u003ctd align=\"center\"\u003e11 GFLOPs\u003c/td\u003e\u003c/tr\u003e\u003ctr\u003e\u003ctd align=\"center\"\u003eSENet\u003c/td\u003e\u003ctd align=\"center\"\u003e224 x 224\u003c/td\u003e\u003ctd align=\"center\"\u003e440 MB\u003c/td\u003e\u003ctd align=\"center\"\u003e21 GFLOPs\u003c/td\u003e\u003c/tr\u003e\u003c/tbody\u003e\u003c/table\u003e\u003cp\u003e\u003ca href=\"https://github.com/albanie/convnet-burden\"\u003e数据来源\u003c/a\u003e\u003c/p\u003e\u003cp\u003e上表列举了，ImageNet图像识别中常见算法的模型大小以及单张图片一次训练（One Pass）所需要的计算量。\u003c/p\u003e\u003cp\u003e自2012年，Hinton的学生Alex Krizhevsky提出AlexNet，一举摘下ILSVRC 2012的桂冠后，ILSVRC比赛冠军的准确率越来越高。与此同时，其中使用到的深度学习算法也越来越复杂，所需要的计算量也越来越大。SENet与AlexNet相比，计算量多了近30倍。我们知道，ImageNet大概有120万张图片，以SENet为例，如果要完成100个epoch的完整训练，将需要2.52 * 10^18的计算量。如此庞大的计算量，已经远远超出传统的机器学习算法的范畴。更别说，Google在论文\u003ca href=\"https://arxiv.org/abs/1707.02968\"\u003e《Revisiting Unreasonable Effectiveness of Data in Deep Learning Era》\u003c/a\u003e中提及的、比ImageNet大300倍的数据集。\u003c/p\u003e\u003ch3 id=\"物理计算性能\"\u003e物理计算性能\u003c/h3\u003e\u003cp\u003e面对如此庞大的计算量，那么，我们业界当前常用的计算单元的计算力是多少呢？\u003c/p\u003e\u003cul\u003e\u003cli\u003eCPU 物理核：一般浮点运算能力在10^10 FLOPS量级。一台16 Cores的服务器，大致上有200 GFLOPS的运算能力。实际运行，CPU 大概能用到80%的性能，那就160 GFLOPS的运算能力。完成上述SENet运行，需要182天。\u003c/li\u003e\u003cli\u003eNVIDIA GPGPU： 目前的V100，单精度浮点运算的峰值大概为14 TFLOPS， 实际运行中，我们假设能用到50%的峰值性能，那就是7 TFLOPS，需要4天。\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e根据以上数据结果可以看出：在深度学习领域，GPU训练数据集所需要耗费的时间，远远少于CPU，这也是当前深度学习训练都是采用GPU的重要原因。\u003c/p\u003e\u003ch2 id=\"业界的解决方案\"\u003e业界的解决方案\u003c/h2\u003e\u003cp\u003e从前面的计算可知，即使使用GPU来计算，训练一次ImageNet 也需要4天的时间。但对于算法工程师做实验、调参而言，这种耗时数天的等待是难以忍受的。为此，目前业界针对深度学习训练的加速，提出了各种各样的解决方案。\u003c/p\u003e\u003ch3 id=\"异构计算的并行方案\"\u003e异构计算的并行方案\u003c/h3\u003e\u003ch4 id=\"数据并行-data-parallelism\"\u003e数据并行（Data Parallelism）\u003c/h4\u003e\u003cp\u003e\u003cimg src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2018b/13418a57.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e数据并行，即每个计算单元都保留一份完整的模型拷贝，分别训练不同的数据，经过一个Iteration或若干个Iteration后，把各个计算单元的模型做一次同步。这是最常见的深度学习训练方式，好处在于逻辑简单、代码实现方便。\u003c/p\u003e\u003ch4 id=\"模型并行-model-parallelism\"\u003e模型并行（Model Parallelism）\u003c/h4\u003e\u003cp\u003e\u003cimg src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2018b/6bbc5fda.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e模型并行，即各个计算单元存储同一层模型数据的不同部分，训练相同的数据。相对于数据并行，因为各个运算单元每训练完一层神经网络，就必须要同步一次，频繁的同步通信导致系统不能充分地利用硬件的运算能力，所以更为少见。但是在一些业务场景下，Softmax层需要分类的类别可能会有很多，导致Softmax层太大，单个计算单元无法存储，这个时候，需要把模型切割成若干部分，存储在不同的运算单元。模型并行常见于NLU、推荐、金融等领域。\u003c/p\u003e\u003ch4 id=\"流式并行-stream-parallelism\"\u003e流式并行（Stream Parallelism）\u003c/h4\u003e\u003cp\u003e\u003cimg src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2018b/eae8e927.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e流式并行，即每个计算单元都存储不同层的模型数据，训练相同的数据。如上图所示，GPU1只负责第一层神经网络的计算，GPU2只负责2~5层神经网络的计算，GPU3只负责第6层的计算。流式并行的好处在于每个运算单元之间的通信和计算重叠（overlap），如果配置得当，可以非常充分地利用硬件资源。缺点在于，根据不同的模型，需要平衡好各个计算单元的计算量，如果配置不好，很容易形成“堰塞湖”。如上图所示，很有可能出现GPU1 负责的运算量太少，而GPU2 负责的运算量太多，导致GPU1 和GPU2 之间堵塞住大量的Mini-batch，更常见于线上环境。\u003c/p\u003e\u003ch4 id=\"混合并行-hybrid-parallelism\"\u003e混合并行（Hybrid Parallelism）\u003c/h4\u003e\u003cp\u003e\u003cimg src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2018b/7349a1a9.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e混合并行，即上面提到的并行方式的混合。如对于一些图像识别任务来说，可能前几层使用数据并行，最后的Softmax层，使用模型并行。\u003c/p\u003e\u003ch3 id=\"异构计算的硬件解决方案\"\u003e异构计算的硬件解决方案\u003c/h3\u003e\u003cul\u003e\u003cli\u003e单机单卡：一个主机内安装上一块GPU运算卡。常见于个人计算机。\u003c/li\u003e\u003cli\u003e单机多卡：一个主机内安装上多块GPU运算卡。常见的有：1机4卡，1机8卡，甚至有1机10卡。一般公司都采取这种硬件方案。\u003c/li\u003e\u003cli\u003e多机多卡：多台主机内安装多块GPU运算卡。常见于公司内部的计算集群，一般多机之间采取Infiniband 来实现网络的快速通信。\u003c/li\u003e\u003cli\u003e定制化：即类似于Google的TPU解决方案。常见于“巨无霸”公司内部。\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"异构计算的通信解决方案\"\u003e异构计算的通信解决方案\u003c/h3\u003e\u003cp\u003e根据上面的硬件解决方案，我们以ResNet为例：模型的大小为230M，单张图片运算量为11 GFLPOS，Mini-batch假设为128。可以计算出各个硬件模块在深度学习训练中的耗时比较：\u003c/p\u003e\u003cul\u003e\u003cli\u003eGPU：对于V100，假设有6 TFLOPS，一次Mini-batch 理论耗时：0.23s。\u003c/li\u003e\u003cli\u003ePCI-E：常见PCI-E 3.0 * 16，速度为10 GB/s，传输一个模型的理论耗时为：0.023s。\u003c/li\u003e\u003cli\u003e网络：假设为10 GB/s的高速网络，传输一个模型的理论耗时：0.023s。\u003c/li\u003e\u003cli\u003eDisk：普通的磁盘，我们假设200M/s的读取速度，读取一次Mini-batch所需要的图片耗时：0.094s。\u003c/li\u003e\u003c/ul\u003e\u003cp\u003e根据上面的数据结果，我们似乎可以得出一个结论：PCI-E和网络的传输耗时，相对于GPU来说，整整少了一个数量级，所以网络通信同步的时间可以忽略不计。然而问题并没有那么简单，上面例子中的耗时只是单个模型的耗时，但是对于8卡的集群来说，如果使用数据并行，每次同步就需要传输8份模型，这就导致数据传输的时间和GPU的计算时间“旗鼓相当”。这样的话，GPU就得每训练完一个Mini-batch，都得等候很久的一段时间（采取同步更新），这会浪费很多计算资源。因此，网络通信也需要制定对应的解决方案。下面我们以Nvidia NCCL中单机多卡的通信解决方案为例介绍，而多机多卡的通信解决方案其实是类似的。\u003c/p\u003e\u003cp\u003e\u003cimg src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2018b/e9d40284.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e上图是单机4卡机器，在硬件上，两种不同的通信体系。左边为普通的PCI-E通信，即4个GPU之间组成一个环状。右边为NVLink通信，即两两之间相互连接。\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e常见的通信类型如下图所示：\u003c/strong\u003e\u003c/p\u003e\u003cp\u003e\u003cimg src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2018b/b8a5bcdf.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e对于深度学习训练而言，关键的两种通信类型为：Broadcast和Reduce。Broadcast用于Master分发最新的模型给各个GPU。Reduce 用于各个GPU计算完Mini-batch后，把模型更新值汇总到Master上。以Broadcast为例，最简单的通信方式是Master往各个GPU上发送数据，这样的耗时就是4次模型传输的时间，通信时间就会太长，一种简单的优化方法如下图所示：\u003c/p\u003e\u003cp\u003e\u003cimg src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2018b/b1770c43.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e即把所需要传输的数据分成若干块，然后通过接力的方式逐个传递，每个GPU都把自己最新的一块数据发送到下一个GPU卡上。这种传输方式能充分利用硬件层面的通信结构，使得需要的耗时大幅缩减。与此类似的，Reduce的通信优化也可以采取相同的方式进行提速。\u003c/p\u003e\u003ch2 id=\"美团的定制化深度学习系统\"\u003e美团的定制化深度学习系统\u003c/h2\u003e\u003cp\u003e尽管目前在业界已经推出了很多著名的深度学习训练平台，通用的训练平台如TensorFlow、MxNet等等，还有领域专用的训练平台，如语音识别中的Kaldi，但是我们经过调研后，决定内部自主开发一套深度学习系统，理由如下：\u003c/p\u003e\u003cul\u003e\u003cli\u003e通用的训练平台，缺乏了领域特色的功能。如语音识别中的特征提取模块和算法。\u003c/li\u003e\u003cli\u003e通用的训练平台，通常是基于Data-flow Graph，来对计算图中的每个operator进行建模，所以颗粒度很小，需要调度的单元多，导任务调度复杂。\u003c/li\u003e\u003cli\u003e领域特色的训练平台，如Kaldi，在神经网络训练的时候，性能不足。\u003c/li\u003e\u003cli\u003e线上业务存在很多特殊性，如果使用TensorFlow之类作为训练平台，不太适合线上业务的情景。\u003c/li\u003e\u003c/ul\u003e\u003ch3 id=\"nlu线上系统\"\u003eNLU线上系统\u003c/h3\u003e\u003ch4 id=\"线上系统的业务特点\"\u003e线上系统的业务特点\u003c/h4\u003e\u003cp\u003e我们在设计NLU线上系统时，考虑了NLU业务的一些特性。发现其具备如下的一些特点：\n* 随着业务和技术的变化，算法流程也经常发生变化。\n* 算法流程是多个算法串联组成的，不单纯的只有深度学习算法。如分词等算法就不是DL算法。\n* 为了能够快速响应一些紧急问题，需要经常对模型进行热更新。\n* 更重要的是，我们希望构建一个能以“数据驱动”的自动迭代闭环。\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e业务多变\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eNLU任务的算法流程是多层级的，并且业务经常发生变化。如下图所示：\u003c/p\u003e\u003cp\u003e\u003cimg src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2018b/cc4310f7.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e即随着业务要求的变化，NLU系统一开始的算法流程，只需要把一个Query分为两个类，但是到后面，极有可能会变成需要分为三个类别。\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e热更新\u003c/strong\u003e\u003c/p\u003e\u003cp\u003e根据业务需求，或者为了紧急处理一些特殊问题，NLU线上系统经常需要做出快速响应，热更新算法模型。如最近的热点词“skr”，几乎是一夜之间，突然火爆起来。如下图所示的微博，如果不能正确理解“skr”的正确语义，可能就不能准确理解这条微博想要表达的意思。\u003c/p\u003e\u003cp\u003e\u003cimg src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2018b/6b6c334e.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e为了避免影响用户体验，我们可能会对NLU系统，马上进行热更新，把新模型紧急进行上线。\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e数据驱动的自动迭代闭环\u003c/strong\u003e\u003c/p\u003e\u003cp\u003e\u003cimg src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2018b/3c79c7da.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e对于线上系统而言，构建如上图所示的自动迭代闭环，能更好地利用业务数据来提升服务质量。\u003c/p\u003e\u003ch4 id=\"nlu线上系统的核心设计\"\u003eNLU线上系统的核心设计\u003c/h4\u003e\u003cp\u003e\u003cstrong\u003e算法流程的抽象\u003c/strong\u003e\u003c/p\u003e\u003cp\u003e为了适应线上系统串联、多变的算法流程，我们把线上系统的算法进行抽象，如下图所示：\u003c/p\u003e\u003cp\u003e\u003cimg src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2018b/a238d1b9.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e即每一个算法，都依赖于若干个槽位（Slot）和资源（Resource），一旦槽位和资源就位，就会触发对应的算法执行。算法的执行先通过算法适配器，来适配槽位和资源中的数据，转换成算子的输入格式。然后算子执行算法本身，执行完算子后，再经过算法解析器。算法解析器主要用于解析算法执行的结果，触发对应的槽位。如根据算法的结果，触发Top 3的结果。\u003c/p\u003e\u003cp\u003e多个算法串联起来，就构建成如下结果：\u003c/p\u003e\u003cp\u003e\u003cimg src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2018b/a53d86a7.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e热更新流程的设计\u003c/strong\u003e\u003c/p\u003e\u003cp\u003e\u003cimg src=\"https://awps-assets.meituan.net/mit-x/blog-images-bundle-2018b/e9f814f3.png\" alt=\"\"/\u003e\u003c/p\u003e\u003cp\u003e如上图所示，我们把算法的热更新流程设计如上。初试状态为左上角，即多个Query使用同一份模型数据。当遇到模型更新的请求后，系统将会block住新的query（右上角状态）。然后更新模型完后，新的query使用新的模型，旧query依然使用旧模型（右下角状态）。最后，当使用旧模型的query结束后，把旧的模型从内存中删除（左下角），然后系统恢复到初始状态。\u003c/p\u003e\u003ch3 id=\"声学模型训练系统\"\u003e声学模型训练系统\u003c/h3\u003e\u003cp\u003e因为TensorFlow等通用深度学习训练平台，缺乏了特征提取等业务相关的领域功能，而Kaldi的声学模型训练过程又太慢。所以美团开发了一个声学模型训练系统——Mimir，其具备如下特性：\u003c/p\u003e\u003cul\u003e\u003cli\u003e使用比TensorFlow更粗颗粒度的建模单元，使得任务调度、优化更简单方便易行。\u003c/li\u003e\u003cli\u003e使用数据并行的并行方案，单机多卡可达到近线性加速。（采取同步更新策略下，4卡加速比达到3.8）\u003c/li\u003e\u003cli\u003e移植了Kaldi的一些特有的训练算法。\u003c/li\u003e\u003cli\u003e速度上为Kaldi的6~7倍。（800个小时的训练数据，单机单卡的条件下，Kaldi需要6~7天， Mimir只需20个小时）\u003c/li\u003e\u003cli\u003e业务上，移植了Kaldi的特征提取等领域的相关模块。\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"参考资料\"\u003e参考资料\u003c/h2\u003e\u003cul\u003e\u003cli\u003e\u003ca href=\"https://images.nvidia.com/events/sc15/pdfs/NCCL-Woolley.pdf\"\u003eNCCL: ACCELERATED MULTI-GPU COLLECTIVE COMMUNICATIONS\u003c/a\u003e\u003c/li\u003e\u003cli\u003e\u003ca href=\"https://blog.csdn.net/np4rHI455vg29y2/article/details/78958138\"\u003e【深度学习】老师木讲架构：深度学习平台技术演进\u003c/a\u003e\u003c/li\u003e\u003c/ul\u003e\u003ch2 id=\"作者简介\"\u003e作者简介\u003c/h2\u003e\u003cul\u003e\u003cli\u003e剑鹏，美团点评算法专家。2017年加入美团，目前作为语音识别团队的声学模型负责人，负责声学模型相关的算法和系统设计与开发。\u003c/li\u003e\u003c/ul\u003e\u003c/div\u003e\u003c/div\u003e",
  "Date": "2018-10-25T00:00:00Z",
  "Author": "soulteary@gmail.com"
}