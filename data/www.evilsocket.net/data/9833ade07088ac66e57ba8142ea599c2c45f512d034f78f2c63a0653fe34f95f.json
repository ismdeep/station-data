{
  "Source": "www.evilsocket.net",
  "Title": "Presenting Project Ergo: How to Build an Airplane Detector for Satellite Imagery With Deep Learning",
  "Link": "https://www.evilsocket.net/2018/11/22/Presenting-project-Ergo-how-to-build-an-airplane-detector-for-satellite-imagery-with-Deep-Learning/",
  "Content": "\u003cdiv class=\"content\" itemprop=\"articleBody\"\u003e\n\u003cp\u003eIt’s been a while that i’ve been quite intensively playing with \u003ca target=\"_blank\" rel=\"noopener\" href=\"https://blogs.nvidia.com/blog/2016/07/29/whats-difference-artificial-intelligence-machine-learning-deep-learning-ai/\"\u003eDeep Learning\u003c/a\u003e both for work related research and personal projects. More specifically, I’ve been using the \u003ca target=\"_blank\" rel=\"noopener\" href=\"https://keras.io/\"\u003eKeras framework\u003c/a\u003e on top of a \u003ca target=\"_blank\" rel=\"noopener\" href=\"https://www.tensorflow.org/\"\u003eTensorFlow\u003c/a\u003e backend for all sorts of stuff. From big and complex projects for malware detection, to smaller and simpler experiments about ideas i just wanted to quickly implement and test - it didn’t really matter the scope of the project, I always found myself struggling with the same issues: code reuse over tens of crap python and shell scripts, datasets and models that are spread all over my dev and prod servers, no real standard for versioning them, no order, no structure. \u003c/p\u003e\n\u003cp\u003eSo a few days ago I started writing what it was initially meant to be just a simple wrapper for the main commands of my training pipelines but quickly became a full-fledged framework and manager for all my Keras based projects.\u003c/p\u003e\n\u003cp align=\"center\"\u003e\n\u003cimg alt=\"ergo\" src=\"https://i.imgur.com/EO9PdNp.jpg\"/\u003e\n\u003c/p\u003e\n\u003cp\u003eToday I’m pleased to open source and present \u003ca target=\"_blank\" rel=\"noopener\" href=\"https://github.com/evilsocket/ergo\"\u003eproject Ergo\u003c/a\u003e by showcasing an example use-case: we’ll prototype, train and test a \u003ca target=\"_blank\" rel=\"noopener\" href=\"https://medium.freecodecamp.org/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050?gi=bf04ca9f8061\"\u003eConvolutional Neural Network\u003c/a\u003e on top of the \u003ca target=\"_blank\" rel=\"noopener\" href=\"https://www.kaggle.com/rhammell/planesnet\"\u003ePlanesNet\u003c/a\u003e raw dataset in order to build an airplane detector for satellite imagery.\u003c/p\u003e\n\u003ccenter\u003e\n\u003cimg src=\"https://i.imgur.com/sFfGMcS.png\"/\u003e\n\u003c/center\u003e\n\u003cspan id=\"more\"\u003e\u003c/span\u003e\n\u003cp\u003e\u003cem\u003eThis image and the general idea were taken from \u003ca target=\"_blank\" rel=\"noopener\" href=\"https://github.com/rhammell/planesnet-detector\"\u003ethis project\u003c/a\u003e, however the model structure, training algorithm and data preprocessing are different … the point of this post is, as i said, to showcase Ergo with something which is less of a clichè than the handwritten digits recognition problem with the \u003ca target=\"_blank\" rel=\"noopener\" href=\"http://yann.lecun.com/exdb/mnist/\"\u003eMNIST database\u003c/a\u003e.\u003c/em\u003e\u003c/p\u003e\n\u003ch2 id=\"Prerequisites\"\u003e\u003ca href=\"#Prerequisites\" class=\"headerlink\" title=\"Prerequisites\"\u003e\u003c/a\u003ePrerequisites\u003c/h2\u003e\u003cp\u003eFirst thing first, you’ll need \u003ccode\u003epython3\u003c/code\u003e and \u003ccode\u003epip3\u003c/code\u003e, download Ergo’s \u003ca target=\"_blank\" rel=\"noopener\" href=\"https://github.com/evilsocket/ergo/releases\"\u003elatest stable release from GitHub\u003c/a\u003e, extract it somewhere on your disk and:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecd /path/to/ergo\nsudo pip3 install -r requirements.txt\npython3 setup.py build\nsudo python3 setup.py install\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIf you’re interested in visualizing the model and training metrics, you’ll also need to:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esudo apt-get install graphviz python3-tk\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eThis way you’ll have installed all the dependencies, including the default version of TensorFlow which runs on CPU. Since our training dataset will be relatively big and our model moderately complex, we might want to use GPUs instead. In order to do so, make sure you have \u003ca target=\"_blank\" rel=\"noopener\" href=\"https://medium.com/@zhanwenchen/install-cuda-and-cudnn-for-tensorflow-gpu-on-ubuntu-79306e4ac04e\"\u003eCUDA 9.0 and cuDNN 7.0\u003c/a\u003e installed and then:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003esudo pip3 uninstall tensorflow\nsudo pip3 install tensorflow-gpu\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIf everything worked correctly, you’ll be able test your GPU setup, the software versions and what hardware is available with the \u003ccode\u003envidia-smi\u003c/code\u003e and \u003ccode\u003eergo info\u003c/code\u003e commands. For example, on my home training server this is the output:\u003c/p\u003e\n\u003ccenter\u003e\n\u003cimg alt=\"ergo info\" src=\"https://i.imgur.com/blcaser.png\"/\u003e\n\u003c/center\u003e\n\u003ch2 id=\"Airplanes-and-Satellites\"\u003e\u003ca href=\"#Airplanes-and-Satellites\" class=\"headerlink\" title=\"Airplanes and Satellites\"\u003e\u003c/a\u003eAirplanes and Satellites\u003c/h2\u003e\u003cp\u003eNow it’s time to grab our dataset, download the \u003ca target=\"_blank\" rel=\"noopener\" href=\"https://www.kaggle.com/rhammell/planesnet#planesnet.zip\"\u003eplanesnet.zip file from Kaggle\u003c/a\u003e and extract it somewhere on your disk, we will only need the folder filled with PNG files, each one named as \u003ccode\u003e1__20160714_165520_0c59__-118.4316008_33.937964191.png\u003c/code\u003e, where the first \u003ccode\u003e1__\u003c/code\u003e or \u003ccode\u003e0__\u003c/code\u003e tells us the labeling (0=no plane, 1=there’s a plane).\u003c/p\u003e\n\u003cp\u003eWe’ll feed our system with the raw images, preprocess them and train a CNN on top of those labeled vectors next.\u003c/p\u003e\n\u003ch2 id=\"Data-Preprocessing\"\u003e\u003ca href=\"#Data-Preprocessing\" class=\"headerlink\" title=\"Data Preprocessing\"\u003e\u003c/a\u003eData Preprocessing\u003c/h2\u003e\u003cp\u003eNormally we would start a new Ergo project by issuing the \u003ccode\u003eergo create planes-detector\u003c/code\u003e command, this would create a new folder named \u003ccode\u003eplanes-detector\u003c/code\u003e with three files in it:\u003c/p\u003e\n\u003col\u003e\n\u003cli\u003e\u003ccode\u003eprepare.py\u003c/code\u003e, that we will customize to preprocess the dataset \u003c/li\u003e\n\u003cli\u003e\u003ccode\u003emodel.py\u003c/code\u003e, where we will customize the model.\u003c/li\u003e\n\u003cli\u003e\u003ccode\u003etrain.py\u003c/code\u003e, for the training algorithm.\u003c/li\u003e\n\u003c/ol\u003e\n\u003cp\u003eThese files would be filled with some default code and only a minimum amount of changes would be needed in order to implement our experiment, changes that I already made available on \u003ca target=\"_blank\" rel=\"noopener\" href=\"https://github.com/evilsocket/ergo-planes-detector\"\u003ethe planes-detector repo on GitHub\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eThe format that by default Ergo expects the dataset to be is a CSV file, where each row is composed as \u003ccode\u003ey,x0,x1,x2,....\u003c/code\u003e (\u003ccode\u003ey\u003c/code\u003e being the label and \u003ccode\u003exn\u003c/code\u003e the scalars in the input vector), but our inputs are images, which have a width, a height and a RGB depth. In order to transform these 3-dimensional tensors into a flat vector that Ergo understands, we need to customize the \u003ccode\u003eprepare.py\u003c/code\u003e script to do some data preprocessing.\u003c/p\u003e\n\u003cscript data-cfasync=\"false\" src=\"/cdn-cgi/scripts/5c5dd728/cloudflare-static/email-decode.min.js\"\u003e\u003c/script\u003e\u003cscript src=\"https://gist-it.appspot.com/https://github.com/evilsocket/ergo-planes-detector/blob/master/prepare.py\"\u003e\u003c/script\u003e\n\u003cp\u003eThis will loop all the pictures and flatten them to vectors of 1200 elements each (20x20x3), plus the \u003ccode\u003ey\u003c/code\u003e scalar (the label) at the beginning, and eventually return a \u003ccode\u003epanda.DataFrame\u003c/code\u003e that Ergo will now digest.\u003c/p\u003e\n\u003ch2 id=\"The-Model\"\u003e\u003ca href=\"#The-Model\" class=\"headerlink\" title=\"The Model\"\u003e\u003c/a\u003eThe Model\u003c/h2\u003e\u003cp\u003eThis is not a post about how convolutional neural networks (or neural networks at all) work so I won’t go into details about that, chances are that if you have the type of technical problems that Ergo solves, you know already. In short, CNNs can encode visual/spatial patterns from input images and use them as features in order to predict things like \u003ccode\u003ehow much this image looks like a cat\u003c/code\u003e … or a plane :) TLDR: CNNs are great for images.\u003c/p\u003e\n\u003cp\u003eThis is how our \u003ccode\u003emodel.py\u003c/code\u003e looks like:\u003c/p\u003e\n\u003cscript src=\"https://gist-it.appspot.com/https://github.com/evilsocket/ergo-planes-detector/blob/master/model.py\"\u003e\u003c/script\u003e\n\u003cp\u003eOther than \u003ccode\u003ereshaping\u003c/code\u003e the flat input back to the 3-dimensional shape that our convolutional layers understand, two \u003ca target=\"_blank\" rel=\"noopener\" href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D\"\u003econvolutional layers\u003c/a\u003e with respectively 32 and 64 filters with a 3x3 kernel are present, plus the usual suspects that help us getting more accurate results after training (\u003ccode\u003eMaxPooling2D\u003c/code\u003e to pick the best visual features and a couple of \u003ccode\u003eDropout\u003c/code\u003e filter layers to avoid \u003ca target=\"_blank\" rel=\"noopener\" href=\"https://en.wikipedia.org/wiki/Overfitting\"\u003eoverfitting\u003c/a\u003e) and the \u003ccode\u003eDense\u003c/code\u003e hidden and output layers. Pretty standard model for simple image recognition problems.\u003c/p\u003e\n\u003ch2 id=\"The-Training\"\u003e\u003ca href=\"#The-Training\" class=\"headerlink\" title=\"The Training\"\u003e\u003c/a\u003eThe Training\u003c/h2\u003e\u003cp\u003eWe can finally start talking about training. The \u003ccode\u003etrain.py\u003c/code\u003e file was almost left unchanged and I only added a few lines to integrate it with \u003ca target=\"_blank\" rel=\"noopener\" href=\"https://www.tensorflow.org/guide/summaries_and_tensorboard\"\u003eTensorBoard\u003c/a\u003e.\u003c/p\u003e\n\u003cscript src=\"https://gist-it.appspot.com/https://github.com/evilsocket/ergo-planes-detector/blob/master/train.py\"\u003e\u003c/script\u003e\n\u003cp\u003eThe data preprocessing, import and training process can now be started with:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eergo train /path/to/planes-detector-project --dataset /path/to/planesnet-pictures\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eIf running on multiple GPUs, you can use the \u003ccode\u003e--gpus N\u003c/code\u003e optional argument to detemine how many to use, while the \u003ccode\u003e--test N\u003c/code\u003e and \u003ccode\u003e--validation N\u003c/code\u003e arguments can be used to partition the dataset (by default both test and validation sets will be 15% of the global one, while the rest will be used for training).\u003c/p\u003e\n\u003cp\u003eDepending on your hardware configuration this process can take from a few minutes, up to even hours (remember you can monitor it with \u003ccode\u003etensorboard --log_dir=/path/to/planes-detector-project/logs\u003c/code\u003e), but eventually you will see something like:\u003c/p\u003e\n\u003ccenter\u003e\n\u003cimg alt=\"training\" src=\"https://i.imgur.com/foFQrba.png\"/\u003e\n\u003c/center\u003e\n\u003cp\u003eOther than manually inspecting the model yaml file, and some \u003ccode\u003emodel.stats\u003c/code\u003e, you can now:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eergo view /path/to/planes-detector-project\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eto see the model structure, the \u003ccode\u003eaccuracy\u003c/code\u003e and \u003ccode\u003eloss\u003c/code\u003e metrics during training and validation:\u003c/p\u003e\n\u003ccenter\u003e\n\u003cimg alt=\"ergo view\" src=\"https://i.imgur.com/rUQ1Het.png\"/\u003e\n\u003c/center\u003e\n\u003cp\u003e\u003cstrong\u003eNot bad!\u003c/strong\u003e Over 98% accuracy on a dataset of thousands of images! \u003c/p\u003e\n\u003cp\u003eWe can now clean the project from the temporary train, validation and test datasets:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eergo clean /path/to/planes-detector-project\u003c/code\u003e\u003c/pre\u003e\n\u003ch2 id=\"Using-the-Model\"\u003e\u003ca href=\"#Using-the-Model\" class=\"headerlink\" title=\"Using the Model\"\u003e\u003c/a\u003eUsing the Model\u003c/h2\u003e\u003cp\u003eIt is possible now to load the trained weights \u003ccode\u003emodel.h5\u003c/code\u003e file in your own project and use it as you like, for instance you might use a sliding window of 20x20 pixels on a bigger image and mark the areas that this NN detected as planes. Another option is to use Ergo itself and expose the model as a REST API:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003eergo serve /path/to/planes-detector-project\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003eYou’ll be able to access and test the model predictions via a simple:\u003c/p\u003e\n\u003cpre\u003e\u003ccode\u003ecurl http://127.0.0.1:8080/?x=0.345,1.0,0.9,....\u003c/code\u003e\u003c/pre\u003e\n\u003cp\u003e__\u003cbr/\u003eAs usual, \u003cstrong\u003eenjoy\u003c/strong\u003e \u0026lt;3\u003c/p\u003e\n\u003c/div\u003e",
  "Date": "2018-11-22T17:15:50Z",
  "Author": "Simone Margaritelli"
}